{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=en_US.UTF-8\n",
    "!CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "from embedding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    !pip install nltk\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('../train-v1.1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ra2630/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ra2630/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltkStopWords = stopwords.words('english')\n",
    "punctuations = [',', '?', '.', '-',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(data):\n",
    "    contexts = []\n",
    "    qas = []\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        for j in range(len(data[\"data\"][i][\"paragraphs\"])):\n",
    "            contexts.append(data[\"data\"][i][\"paragraphs\"][j][\"context\"])\n",
    "            qas.append(data[\"data\"][i][\"paragraphs\"][j][\"qas\"])\n",
    "    return (contexts,qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CapPassage = False\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "contexts,qas = extractor(data)\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind,ind+sll\n",
    "    return (-1,-1)\n",
    "\n",
    "def capPassage(passage,answer,cap_length = 30):\n",
    "    y = np.zeros(cap_length)\n",
    "    left,right = find_sub_list(answer,passage)\n",
    "    if(left==-1):\n",
    "        return passage[0:cap_length]\n",
    "    left = left - int((cap_length - len(answer))/2)\n",
    "    right = right + int((cap_length + len(answer))/2)\n",
    "    if(left < 0):\n",
    "        left = 0\n",
    "    if(right > len(passage)):\n",
    "        right = len(passage)\n",
    "    return passage[left:right]\n",
    "    \n",
    "def findAnsVec(answer,passage):\n",
    "    ans = np.zeros((len(passage)))\n",
    "    start,end = find_sub_list(answer,passage)\n",
    "    if(start==-1):\n",
    "        start = passage.index(answer[0])\n",
    "        end = start + len(answer)\n",
    "    ans[start:end] = 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_comp_all = []\n",
    "X_train_comp_ans_all = []\n",
    "X_train_ans_all = []\n",
    "Y_train_ques_all = []\n",
    "invalid = 0\n",
    "X_train_ans_label_all = []\n",
    "for i,context in enumerate(contexts):\n",
    "    passage = word_tokenize(context.lower())\n",
    "    \n",
    "    a_lab = np.zeros(len(passage))\n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "        start,end = find_sub_list(answer,passage)\n",
    "        if start == -1:\n",
    "            continue\n",
    "        a_lab[start:end+1] = 1\n",
    "            \n",
    "            \n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        try:\n",
    "            question = word_tokenize(qas[i][j]['question'].lower())\n",
    "            answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "            \n",
    "            if CapPassage:\n",
    "                cappedPassage = capPassage(passage,answer)\n",
    "            else:\n",
    "                cappedPassage = passage\n",
    "            \n",
    "            X_train_comp_ans_all.append(findAnsVec(answer,passage))\n",
    "            X_train_ans_label_all.append(a_lab)\n",
    "            X_train_comp_all.append(cappedPassage)\n",
    "            X_train_ans_all.append(answer)\n",
    "            Y_train_ques_all.append(question)\n",
    "        except Exception as e:\n",
    "            invalid = invalid+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "def findKMostFrequentWords(k):\n",
    "    ctr = Counter([item for sublist in X_train_comp_all for item in sublist] + [item for sublist in Y_train_ques_all for item in sublist])\n",
    "    sorted_ctr = sorted(ctr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return [item[0] for item in sorted_ctr[0:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordToTake = 2000\n",
    "words = findKMostFrequentWords(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_word_to_idx_reduced = {}\n",
    "_idx_to_word_reduced = []\n",
    "\n",
    "\n",
    "def _add_word_reduced(word):\n",
    "    idx = len(_idx_to_word_reduced)\n",
    "    _word_to_idx_reduced[word] = idx\n",
    "    _idx_to_word_reduced.append(word)\n",
    "    return idx\n",
    "\n",
    "\n",
    "UNKNOWN_TOKEN = _add_word_reduced(UNKNOWN_WORD)\n",
    "START_TOKEN = _add_word_reduced(START_WORD)\n",
    "END_TOKEN = _add_word_reduced(END_WORD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dimensions = glove.shape[1]\n",
    "reduced_glove = []\n",
    "reduced_glove.append(np.zeros(dimensions))\n",
    "reduced_glove.append(-np.ones(dimensions))\n",
    "reduced_glove.append(np.ones(dimensions))\n",
    "\n",
    "for word in words:\n",
    "    l = look_up_word(word)\n",
    "    if(l != UNKNOWN_TOKEN):\n",
    "        idx = _add_word_reduced(word)\n",
    "        reduced_glove.append(glove[l])\n",
    "        if(len(reduced_glove) == wordToTake):\n",
    "            break\n",
    "        \n",
    "def look_up_word_reduced(word):\n",
    "    return _word_to_idx_reduced.get(word, UNKNOWN_TOKEN)\n",
    "\n",
    "\n",
    "def look_up_token_reduced(token):\n",
    "    return _idx_to_word_reduced[token]\n",
    "\n",
    "reduced_glove = np.array(reduced_glove)\n",
    "reduced_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda2.cims.nyu.edu\n",
      "1493\n",
      "rev\n",
      ".\n",
      "john\n",
      "j.\n",
      "cavanaugh\n",
      ",\n",
      "c.s.c\n",
      ".\n",
      "served\n",
      "more\n",
      "than\n",
      "half\n",
      ",\n",
      "lobund\n",
      "institute\n",
      "for\n",
      "animal\n",
      "studies\n",
      "and\n",
      "medieval\n",
      "institute\n",
      ".\n",
      "hall\n",
      "of\n",
      "liberal\n",
      "arts\n",
      "(\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "print(invalid)\n",
    "for i in np.where(X_train_ans_label_all[110] == 1)[0]:\n",
    "    print(X_train_comp_all[110][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['architecturally', ',', 'the', 'school', 'has', 'a', 'catholic', 'character', '.', 'atop', 'the', 'main', 'building', \"'s\", 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'virgin', 'mary', '.', 'immediately', 'in', 'front', 'of', 'the', 'main', 'building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'christ', 'with', 'arms', 'upraised', 'with', 'the', 'legend', '``', 'ad', 'me', 'omnes', \"''\", '.', 'next', 'to', 'the', 'main', 'building', 'is', 'the', 'basilica', 'of', 'the', 'sacred', 'heart', '.', 'immediately', 'behind', 'the', 'basilica', 'is', 'the', 'grotto', ',', 'a', 'marian', 'place', 'of', 'prayer', 'and', 'reflection', '.', 'it', 'is', 'a', 'replica', 'of', 'the', 'grotto', 'at', 'lourdes', ',', 'france', 'where', 'the', 'virgin', 'mary', 'reputedly', 'appeared', 'to', 'saint', 'bernadette', 'soubirous', 'in', '1858.', 'at', 'the', 'end', 'of', 'the', 'main', 'drive', '(', 'and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', '3', 'statues', 'and', 'the', 'gold', 'dome', ')', ',', 'is', 'a', 'simple', ',', 'modern', 'stone', 'statue', 'of', 'mary', '.']\n",
      "['saint', 'bernadette', 'soubirous']\n"
     ]
    }
   ],
   "source": [
    "print(X_train_comp_all[0])\n",
    "print(X_train_ans_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 104)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sub_list(X_train_ans_all[0] , X_train_comp_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1493\n",
      "['the', 'success', 'of', 'its', 'football', 'team', 'made', 'notre', 'dame', 'a', 'household', 'name', '.', 'the', 'success', 'of', 'note', 'dame', 'reflected', 'rising', 'status', 'of', 'irish', 'americans', 'and', 'catholics', 'in', 'the', '1920s', '.', 'catholics', 'rallied', 'up', 'around', 'the', 'team', 'and', 'listen', 'to', 'the', 'games', 'on', 'the', 'radio', ',', 'especially', 'when', 'it', 'knocked', 'off', 'the', 'schools', 'that', 'symbolized', 'the', 'protestant', 'establishment', 'in', 'america', 'â€”', 'harvard', ',', 'yale', ',', 'princeton', ',', 'and', 'army', '.', 'yet', 'this', 'role', 'as', 'high-profile', 'flagship', 'institution', 'of', 'catholicism', 'made', 'it', 'an', 'easy', 'target', 'of', 'anti-catholicism', '.', 'the', 'most', 'remarkable', 'episode', 'of', 'violence', 'was', 'the', 'clash', 'between', 'notre', 'dame', 'students', 'and', 'the', 'ku', 'klux', 'klan', 'in', '1924.', 'nativism', 'and', 'anti-catholicism', ',', 'especially', 'when', 'directed', 'towards', 'immigrants', ',', 'were', 'cornerstones', 'of', 'the', 'kkk', \"'s\", 'rhetoric', ',', 'and', 'notre', 'dame', 'was', 'seen', 'as', 'a', 'symbol', 'of', 'the', 'threat', 'posed', 'by', 'the', 'catholic', 'church', '.', 'the', 'klan', 'decided', 'to', 'have', 'a', 'week-long', 'klavern', 'in', 'south', 'bend', '.', 'clashes', 'with', 'the', 'student', 'body', 'started', 'on', 'march', '17', ',', 'when', 'students', ',', 'aware', 'of', 'the', 'anti-catholic', 'animosity', ',', 'blocked', 'the', 'klansmen', 'from', 'descending', 'from', 'their', 'trains', 'in', 'the', 'south', 'bend', 'station', 'and', 'ripped', 'the', 'kkk', 'clothes', 'and', 'regalia', '.', 'on', 'may', '19', 'thousands', 'of', 'students', 'massed', 'downtown', 'protesting', 'the', 'klavern', ',', 'and', 'only', 'the', 'arrival', 'of', 'college', 'president', 'fr', '.', 'matthew', 'walsh', 'prevented', 'any', 'further', 'clashes', '.', 'the', 'next', 'day', ',', 'football', 'coach', 'knute', 'rockne', 'spoke', 'at', 'a', 'campus', 'rally', 'and', 'implored', 'the', 'students', 'to', 'obey', 'the', 'college', 'president', 'and', 'refrain', 'from', 'further', 'violence', '.', 'a', 'few', 'days', 'later', 'the', 'klavern', 'broke', 'up', ',', 'but', 'the', 'hostility', 'shown', 'by', 'the', 'students', 'was', 'an', 'omen', 'and', 'a', 'contribution', 'to', 'the', 'downfall', 'of', 'the', 'kkk', 'in', 'indiana', '.']\n",
      "['the', 'ku', 'klux', 'klan']\n",
      "['notre', 'dame', 'students', 'had', 'a', 'showdown', 'in', '1924', 'with', 'which', 'anti-catholic', 'group', '?']\n",
      "['in', '1995', ',', 'oklahoma', 'city', 'was', 'the', 'site', 'of', 'one', 'of', 'the', 'most', 'destructive', 'acts', 'of', 'domestic', 'terrorism', 'in', 'american', 'history', '.', 'the', 'oklahoma', 'city', 'bombing', 'of', 'april', '19', ',', '1995', ',', 'in', 'which', 'timothy', 'mcveigh', 'and', 'terry', 'nichols', 'detonated', 'an', 'explosive', 'outside', 'of', 'the', 'alfred', 'p.', 'murrah', 'federal', 'building', ',', 'killed', '168', 'people', ',', 'including', '19', 'children', '.', 'the', 'two', 'men', 'were', 'convicted', 'of', 'the', 'bombing', ':', 'mcveigh', 'was', 'sentenced', 'to', 'death', 'and', 'executed', 'by', 'the', 'federal', 'government', 'on', 'june', '11', ',', '2001', ';', 'his', 'partner', 'nichols', 'is', 'serving', 'a', 'sentence', 'of', 'life', 'in', 'prison', 'without', 'the', 'possibility', 'of', 'parole', '.', 'mcveigh', \"'s\", 'army', 'buddy', ',', 'michael', 'fortier', ',', 'was', 'sentenced', 'to', '12', 'years', 'in', 'federal', 'prison', 'and', 'ordered', 'to', 'pay', 'a', '$', '75,000', 'fine', 'for', 'his', 'role', 'in', 'the', 'bombing', 'plot', '(', 'i.e', '.', 'assisting', 'in', 'the', 'sale', 'of', 'guns', 'to', 'raise', 'funds', 'for', 'the', 'bombing', ',', 'and', 'examining', 'the', 'murrah', 'federal', 'building', 'as', 'a', 'possible', 'target', 'before', 'the', 'terrorist', 'attack', ')', '.', 'his', 'wife', ',', 'lori', 'fortier', ',', 'who', 'has', 'since', 'died', ',', 'was', 'granted', 'immunity', 'from', 'prosecution', 'in', 'return', 'for', 'her', 'testimony', 'in', 'the', 'case', '.']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "['19']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "['how', 'many', 'children', 'died', 'in', 'the', 'oklahoma', 'city', 'bombing', '?']\n"
     ]
    }
   ],
   "source": [
    "print(invalid)\n",
    "print(X_train_comp_all[101])\n",
    "print(X_train_ans_all[101])\n",
    "print(Y_train_ques_all[101])\n",
    "\n",
    "c = list(zip(X_train_comp_all,X_train_comp_ans_all, X_train_ans_all, X_train_ans_label_all,Y_train_ques_all))\n",
    "np.random.shuffle(c)\n",
    "X_train_comp_all_shuffled,X_train_comp_ans_all_shuffled, X_train_ans_shuffled, X_train_ans_label_shuffled,Y_train_ques_all_shuffled = zip(*c)\n",
    "\n",
    "print(X_train_comp_all_shuffled[101])\n",
    "print(X_train_comp_ans_all_shuffled[101])\n",
    "print(X_train_ans_shuffled[101])\n",
    "print(X_train_ans_label_shuffled[101])\n",
    "print(Y_train_ques_all_shuffled[101])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_to_take_train = 1000\n",
    "\n",
    "X_train_comp = X_train_comp_all_shuffled[0:examples_to_take_train]\n",
    "X_train_comp_ans = X_train_comp_ans_all_shuffled[0:examples_to_take_train]\n",
    "X_train_ans = X_train_ans_shuffled[0:examples_to_take_train]\n",
    "X_train_ans_label = X_train_ans_label_shuffled[0:examples_to_take_train]\n",
    "Y_train_ques = Y_train_ques_all_shuffled[0:examples_to_take_train]\n",
    "answer_indices = [np.where(x==1)[0].tolist() for x in X_train_comp_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_document_len = len(max(X_train_comp,key=len))\n",
    "max_answer_len = len(max(X_train_ans,key=len))\n",
    "max_question_len = len(max(Y_train_ques,key=len)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "answer_masks = np.zeros((examples_to_take_train, max_answer_len, max_document_len), dtype=np.int32)\n",
    "answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "question_input_tokens = np.full((examples_to_take_train, max_question_len), END_TOKEN, dtype=np.int32)\n",
    "question_output_tokens = np.full((examples_to_take_train, max_question_len), END_TOKEN, dtype=np.int32)\n",
    "question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "suppression_answer = np.zeros((examples_to_take_train, reduced_glove.shape[0], 1),dtype=np.int32)\n",
    "expression_contexts = np.zeros((examples_to_take_train, max_question_len,reduced_glove.shape[0]),dtype=np.int32)\n",
    "expression_probabilities = np.zeros((examples_to_take_train, max_question_len,reduced_glove.shape[0]),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 638)\n"
     ]
    }
   ],
   "source": [
    "print(answer_labels.shape)\n",
    "for i in range(examples_to_take_train):\n",
    "    answer_labels[i,0:len(X_train_ans_label[i])] = X_train_ans_label[i]\n",
    "    for j, word in enumerate(X_train_comp[i]):\n",
    "        document_tokens[i, j] = look_up_word_reduced(word)\n",
    "    document_lengths[i] = len(X_train_comp[i])\n",
    "\n",
    "    for j, index in enumerate(answer_indices[i]):\n",
    "        answer_masks[i, j, index] = 1\n",
    "    answer_lengths[i] = len(answer_indices[i])\n",
    "    \n",
    "    #print(Y_train_ques[i])\n",
    "    question_input_words = ([START_WORD] + Y_train_ques[i])\n",
    "    question_output_words = (Y_train_ques[i] + [END_WORD])\n",
    "\n",
    "    for j, word in enumerate(question_input_words):\n",
    "            question_input_tokens[i, j] = look_up_word_reduced(word)\n",
    "    for j, word in enumerate(question_output_words):\n",
    "        question_output_tokens[i, j] = look_up_word_reduced(word)\n",
    "    question_lengths[i] = len(question_input_words)\n",
    "    \n",
    "    for j, word in enumerate(X_train_ans[i]):\n",
    "        if(word not in Y_train_ques[i]):\n",
    "            suppression_answer[i, look_up_word_reduced(word),:] = 1\n",
    "            \n",
    "    words_to_consider_expression = set(X_train_comp[i] + nltkStopWords + punctuations)\n",
    "\n",
    "    for j,word in enumerate(words_to_consider_expression):\n",
    "        expression_contexts[i,:,look_up_word_reduced(word)] = 1\n",
    "        \n",
    "    for j,word in enumerate(words_to_consider_expression):\n",
    "        expression_probabilities[i,:,look_up_word_reduced(word)] = len(np.where(expression_contexts[i][0] == 1)[0]) / float(wordToTake)\n",
    "    expression_probabilities[i,:,np.where(expression_probabilities[i][0] == 0)[0]] = len(np.where(expression_contexts[i][0] == 0)[0]) / float(wordToTake)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"''\",\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '13',\n",
       " '2011',\n",
       " '2015',\n",
       " '70',\n",
       " '75',\n",
       " '80',\n",
       " '?',\n",
       " '[',\n",
       " ']',\n",
       " '``',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'american',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'bike',\n",
       " 'boston',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'cities',\n",
       " 'city',\n",
       " 'commuters',\n",
       " 'commutes',\n",
       " 'compactness',\n",
       " 'comparably',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'country',\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'factors',\n",
       " 'few',\n",
       " 'foot',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'highest',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'hosts',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'large',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'major',\n",
       " 'making',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'nicknamed',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'owing',\n",
       " 'own',\n",
       " 'pedestrian',\n",
       " 'percent',\n",
       " 'percentage',\n",
       " 'populated',\n",
       " 'population',\n",
       " 'ranked',\n",
       " 'ranks',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'score',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'states',\n",
       " 'still',\n",
       " 'student',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'transit',\n",
       " 'under',\n",
       " 'united',\n",
       " 'until',\n",
       " 'up',\n",
       " 'update',\n",
       " 'us',\n",
       " 've',\n",
       " 'very',\n",
       " 'walk',\n",
       " 'walkable',\n",
       " 'walking',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_consider_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1826\n",
      "174\n",
      "1826\n",
      "174\n"
     ]
    }
   ],
   "source": [
    "print(len(np.where(expression_contexts[10][0] == 0)[0]))\n",
    "print(len(np.where(expression_contexts[10][0] == 1)[0]))\n",
    "\n",
    "print(len(np.where(expression_probabilities[10][0] > 0.5)[0]))\n",
    "print(len(np.where(expression_probabilities[10][0] < 0.5)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(data):\n",
    "    flat_list = [item for sublist in data for item in sublist]\n",
    "    vocabulary = sorted(set(flat_list))\n",
    "    vocabulary.append(\"<UNK>\")\n",
    "    vocabulary.append(\"unk\")\n",
    "    vocabulary.append(\"eos\")\n",
    "    vocabulary = [\"<EOS>\"] + vocabulary\n",
    "    word_to_index = { word:i for i,word in enumerate(vocabulary) }\n",
    "    index_to_word = { i:word for i,word in enumerate(vocabulary) }\n",
    "    return (vocabulary,word_to_index,index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 300)\n",
      "17719\n",
      "1299\n",
      "17273\n"
     ]
    }
   ],
   "source": [
    "print(reduced_glove.shape)\n",
    "vocabulary_comp,word_to_index_comp,index_to_word_comp = create_vocabulary(X_train_comp + Y_train_ques)\n",
    "print(len(vocabulary_comp))\n",
    "print(word_to_index_comp[\"?\"])\n",
    "print(word_to_index_comp[\"what\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_vector(data,vocabulary,word_to_index,index_to_word, maxLen):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,word in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        if(word not in word_to_index):\n",
    "            word = \"<UNK>\"\n",
    "        one_hot[i][word_to_index[word]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def create_one_hot_vector_from_indices(data,maxLen,vocabulary):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,indice in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        one_hot[i][int(indice)] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def create_one_hot_training_Set(data,maxLen,vocabulary):\n",
    "    one_hot_data = np.zeros([data.shape[0],maxLen,len(vocabulary)])\n",
    "    for i in range(data.shape[0]):\n",
    "        one_hot_data[i] = create_one_hot_vector_from_indices(data[i],maxLen,vocabulary)\n",
    "    return one_hot_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_to_indices_glove(X,max_len):\n",
    "    \n",
    "    m = len(X)                                 \n",
    "    \n",
    "    X_indices = np.full([m,max_len],look_up_word_reduced(END_WORD))\n",
    "    \n",
    "    for i in range(m):\n",
    "        j = 0\n",
    "        for w in X[i]:\n",
    "            if(j>=max_len):\n",
    "                break;\n",
    "            \n",
    "            X_indices[i, j] = look_up_word_reduced(w)\n",
    "            j = j+1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = context_to_indices_glove(X_train_comp, max_document_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3,  386,  544, 1043,   43,   42,    0,    8,    3,    0,  226,\n",
       "          0,    4,   32,    0, 1043,   45,    3,   47,    4,    3,    0,\n",
       "          0,    4,   12,  333,    8,    0,   57,    0,    8,    3,  105,\n",
       "        472,   83,    4,   20,  849,    0,    0,   15,    0,   14,    4,\n",
       "          0,    0,    0,   15,    0,   14,    4,    7,    0,    0, 1042,\n",
       "       1152,   15,    0,   14,    6,    0,  576,   39,    0,    8,   35,\n",
       "       1898,    0,    0,    6,    8,    3,    0,    4,   20,    3,    0,\n",
       "          5,    3,    0,    8,  386,    7,    3,  436,  115,    5,    0,\n",
       "         57,  532,    4,   57, 1155,    8,  832,    4,    7,  160,    9,\n",
       "         37,  145,    0, 1014,    0,  718,    4,    0,    0,    7, 1732,\n",
       "          0,    4,    7,    0,    8,    0,    6,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 638)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def create_batch(inputs,batch_size,shuffle=False):\n",
    "    num_batches = len(inputs[0]) // batch_size + 1\n",
    "    outputs = []\n",
    "    for index,inp in enumerate(inputs):\n",
    "    \n",
    "        output = {'document_tokens':[],\n",
    "                    'document_lengths':[],\n",
    "                    'answer_labels':[],\n",
    "                    'answer_masks': [],\n",
    "                    'answer_lengths': [],\n",
    "                    'question_input_tokens':[],\n",
    "                    'question_output_tokens':[],\n",
    "                    'question_lengths':[],\n",
    "                    'suppression_answer':[],\n",
    "                    'expression_contexts': [],\n",
    "                    'expression_probabilities':[]}\n",
    "    \n",
    "        start = 0\n",
    "        for i in range(num_batches):\n",
    "            if i == num_batches - 1:\n",
    "                end = None\n",
    "            else:\n",
    "                end = start+batch_size\n",
    "            maxD = max(inputs[1][start:end])\n",
    "            maxA = max(inputs[4][start:end])\n",
    "            maxQ = max(inputs[7][start:end])\n",
    "            if index == 0:\n",
    "                outputs['document_tokens'] = inp[start:end,:maxD]\n",
    "            elif index==1:\n",
    "                outputs['document_lengths'] = inp[start:end]\n",
    "            elif index == 2:\n",
    "                outputs['answer_labels']=inp[start:end,:maxD]\n",
    "            elif index==3:\n",
    "                outputs['answer_masks']=inp[start:end,:maxA,:maxD]\n",
    "            elif index==4:\n",
    "                outputs['answer_lengths']=inp[start:end]\n",
    "            elif index==5:\n",
    "                output['question_input_tokens']=inp[start:end,:maxQ]\n",
    "            elif index==6:\n",
    "                output['question_output_tokens']=inp[start:end,:maxQ]\n",
    "            elif index==7:\n",
    "                output['question_lengths'] = inp[start:end]\n",
    "            elif index==8:\n",
    "                output['suppression_answer'] = inp[start:end]\n",
    "            elif index==9:\n",
    "                output['expression_contexts'] = inp[start:end,0:maxQ,:]\n",
    "            elif index==10: \n",
    "                output['expression_probabilities'] = inp[start:end,0:maxQ,:]\n",
    "            start = start + batch_size\n",
    "        outputs.append(output)\n",
    "    \n",
    "    return outputs\n",
    "\n",
    "def createBatch(inputs,batch_size,shuffle=False):\n",
    "    outputs = []\n",
    "    num_batches = math.ceil(len(inputs[0])/batch_size)\n",
    "    \n",
    "    for index,inp in enumerate(inputs):\n",
    "        start = 0\n",
    "        output = []\n",
    "        for i in range(num_batches-1):\n",
    "            maxD = max(inputs[1][start:start+batch_size])\n",
    "            maxA = max(inputs[4][start:start+batch_size])\n",
    "            maxQ = max(inputs[7][start:start+batch_size])\n",
    "            if index == 0 or index == 2:\n",
    "                output.append(inp[start:start+batch_size,0:maxD]) \n",
    "            elif index==3:\n",
    "                output.append(inp[start:start+batch_size,0:maxA,0:maxD]) \n",
    "            elif index==5 or index==6:\n",
    "                output.append(inp[start:start+batch_size,0:maxQ])\n",
    "            elif index==9 or index==10:\n",
    "                output.append(inp[start:start+batch_size,0:maxQ,:])\n",
    "            else: \n",
    "                output.append(inp[start:start+batch_size])\n",
    "            start = start + batch_size\n",
    "        \n",
    "        # Remaining training sample i.e. training mod batch_size\n",
    "        maxD = max(inputs[1][start:])\n",
    "        maxA = max(inputs[4][start:])\n",
    "        maxQ = max(inputs[7][start:])\n",
    "        if index == 0 or index == 2:\n",
    "            output.append(inp[start:,0:maxD]) \n",
    "        elif index==3:\n",
    "            output.append(inp[start:,0:maxA,0:maxD]) \n",
    "        elif index==5 or index==6:\n",
    "            output.append(inp[start:,0:maxQ])\n",
    "        elif index==9 or index==10:\n",
    "            output.append(inp[start:,0:maxQ,:]) \n",
    "        else: \n",
    "            output.append(inp[start:])\n",
    "        outputs.append(output)\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches =  32\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "batch_input = createBatch([document_tokens,document_lengths,answer_labels,answer_masks,answer_lengths,question_input_tokens,question_output_tokens,question_lengths,suppression_answer,expression_contexts,expression_probabilities]\n",
    "                    ,batch_size)\n",
    "number_of_batches = len(batch_input[0])\n",
    "print(\"Number of batches = \", number_of_batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, output_size)\n",
    "        \n",
    "        # TODO: Verify\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(reduced_glove).float())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerEncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(AnswerEncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True, bidirectional=True) #Input_size = Hidden_Size\n",
    "        if use_cuda:\n",
    "            self.gru = self.gru.cuda()\n",
    "        self.fc = nn.Linear(hidden_size*2, 1)\n",
    "        if use_cuda:\n",
    "            self.fc = self.fc.cuda()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        final_output = self.fc(output)\n",
    "        final_output = F.sigmoid(final_output)\n",
    "        self.hiddenState = hidden\n",
    "        return final_output, output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(2, batch_size, self.hidden_size)) #2 for BiDirectional\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionEncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super(QuestionEncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True)\n",
    "        if use_cuda:\n",
    "            self.gru = self.gru.cuda()\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        self.hiddenState = hidden\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        #result = Variable(torch.zeros(1, batch_size, self.hidden_size))\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super(QuestionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True)\n",
    "        if use_cuda:\n",
    "            self.gru = self.gru.cuda()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        self.hiddenState = hidden\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        #result = Variable(torch(1, batch_size, self.hidden_size))\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result\n",
    "    \n",
    "'''\n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self,input_size, output_size):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "'''\n",
    "\n",
    "class QuestionGenerationFC(nn.Module):\n",
    "    def __init__(self,input_size, output_size):\n",
    "        super(QuestionGenerationFC, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        if use_cuda:\n",
    "            self.fc = self.fc.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "        return output\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters =  20\n"
     ]
    }
   ],
   "source": [
    "hidden_size = reduced_glove.shape[1]\n",
    "\n",
    "embedder = Embedder(input_size = reduced_glove.shape[0], output_size = reduced_glove.shape[1])\n",
    "#fcLayer = FCLayer(hidden_size, hidden_size)\n",
    "answerEncoder = AnswerEncoderRNN(input_size = hidden_size, hidden_size=int(hidden_size/2))\n",
    "questionEncoder = QuestionEncoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionDecoder = QuestionDecoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionGenerator = QuestionGenerationFC(input_size = hidden_size, output_size=reduced_glove.shape[0])\n",
    "\n",
    "answerEncoder.train()\n",
    "questionEncoder.train()\n",
    "questionDecoder.train()\n",
    "questionGenerator.train()\n",
    "\n",
    "train_param = []\n",
    "\n",
    "for model in [embedder, answerEncoder, questionEncoder, questionDecoder, questionGenerator]:\n",
    "    train_param += [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "print(\"Number of trainable parameters = \", len(train_param))\n",
    "\n",
    "optimizer = torch.optim.Adam(train_param, 0.0001)\n",
    "criterion1 = nn.BCELoss()\n",
    "#criterion2 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.NLLLoss()\n",
    "#criterion2 = nn.MultiLabelSoftMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 1\tNet Loss: 1285.2201 \tAnswer Loss: 0.0935 \tQuestion Loss: 1285.1266\n",
      "Batch: 1 \t Epoch : 1\tNet Loss: 1225.1450 \tAnswer Loss: 0.3488 \tQuestion Loss: 1224.7961\n",
      "Batch: 2 \t Epoch : 1\tNet Loss: 1347.7788 \tAnswer Loss: 0.4700 \tQuestion Loss: 1347.3088\n",
      "Batch: 3 \t Epoch : 1\tNet Loss: 1136.5720 \tAnswer Loss: 0.6469 \tQuestion Loss: 1135.9250\n",
      "Batch: 4 \t Epoch : 1\tNet Loss: 1204.6637 \tAnswer Loss: 0.2880 \tQuestion Loss: 1204.3757\n",
      "Batch: 5 \t Epoch : 1\tNet Loss: 1362.7119 \tAnswer Loss: 0.4569 \tQuestion Loss: 1362.2550\n",
      "Batch: 6 \t Epoch : 1\tNet Loss: 1293.2693 \tAnswer Loss: 0.4847 \tQuestion Loss: 1292.7845\n",
      "Batch: 7 \t Epoch : 1\tNet Loss: 1374.1400 \tAnswer Loss: 0.3017 \tQuestion Loss: 1373.8383\n",
      "Batch: 8 \t Epoch : 1\tNet Loss: 1278.2900 \tAnswer Loss: 0.2638 \tQuestion Loss: 1278.0262\n",
      "Batch: 9 \t Epoch : 1\tNet Loss: 1202.0615 \tAnswer Loss: 0.3475 \tQuestion Loss: 1201.7140\n",
      "Batch: 10 \t Epoch : 1\tNet Loss: 1191.8167 \tAnswer Loss: 0.4037 \tQuestion Loss: 1191.4130\n",
      "Batch: 11 \t Epoch : 1\tNet Loss: 1378.3131 \tAnswer Loss: 0.3095 \tQuestion Loss: 1378.0037\n",
      "Batch: 12 \t Epoch : 1\tNet Loss: 1223.5883 \tAnswer Loss: 0.4332 \tQuestion Loss: 1223.1550\n",
      "Batch: 13 \t Epoch : 1\tNet Loss: 1179.4270 \tAnswer Loss: 0.4595 \tQuestion Loss: 1178.9675\n",
      "Batch: 14 \t Epoch : 1\tNet Loss: 1289.7947 \tAnswer Loss: 0.3817 \tQuestion Loss: 1289.4130\n",
      "Batch: 15 \t Epoch : 1\tNet Loss: 1049.0096 \tAnswer Loss: 0.3822 \tQuestion Loss: 1048.6274\n",
      "Batch: 16 \t Epoch : 1\tNet Loss: 1234.0125 \tAnswer Loss: 0.3816 \tQuestion Loss: 1233.6309\n",
      "Batch: 17 \t Epoch : 1\tNet Loss: 1215.9054 \tAnswer Loss: 0.3161 \tQuestion Loss: 1215.5894\n",
      "Batch: 18 \t Epoch : 1\tNet Loss: 1131.9448 \tAnswer Loss: 0.5105 \tQuestion Loss: 1131.4343\n",
      "Batch: 19 \t Epoch : 1\tNet Loss: 1106.4465 \tAnswer Loss: 0.3818 \tQuestion Loss: 1106.0647\n",
      "Batch: 20 \t Epoch : 1\tNet Loss: 1115.3693 \tAnswer Loss: 0.4106 \tQuestion Loss: 1114.9587\n",
      "Batch: 21 \t Epoch : 1\tNet Loss: 1308.2643 \tAnswer Loss: 0.3188 \tQuestion Loss: 1307.9454\n",
      "Batch: 22 \t Epoch : 1\tNet Loss: 1130.1050 \tAnswer Loss: 0.5318 \tQuestion Loss: 1129.5732\n",
      "Batch: 23 \t Epoch : 1\tNet Loss: 1332.7441 \tAnswer Loss: 0.3902 \tQuestion Loss: 1332.3540\n",
      "Batch: 24 \t Epoch : 1\tNet Loss: 1320.5145 \tAnswer Loss: 0.4863 \tQuestion Loss: 1320.0282\n",
      "Batch: 25 \t Epoch : 1\tNet Loss: 1306.2328 \tAnswer Loss: 0.3610 \tQuestion Loss: 1305.8718\n",
      "Batch: 26 \t Epoch : 1\tNet Loss: 1395.6656 \tAnswer Loss: 0.3722 \tQuestion Loss: 1395.2935\n",
      "Batch: 27 \t Epoch : 1\tNet Loss: 1362.1813 \tAnswer Loss: 0.4254 \tQuestion Loss: 1361.7559\n",
      "Batch: 28 \t Epoch : 1\tNet Loss: 1354.4889 \tAnswer Loss: 0.3878 \tQuestion Loss: 1354.1011\n",
      "Batch: 29 \t Epoch : 1\tNet Loss: 1229.6029 \tAnswer Loss: 0.2976 \tQuestion Loss: 1229.3053\n",
      "Batch: 30 \t Epoch : 1\tNet Loss: 1151.8621 \tAnswer Loss: 0.3262 \tQuestion Loss: 1151.5358\n",
      "Average Loss after Epoch 1 : 1209.9107\n",
      "Batch: 0 \t Epoch : 2\tNet Loss: 1232.2466 \tAnswer Loss: 0.0934 \tQuestion Loss: 1232.1532\n",
      "Batch: 1 \t Epoch : 2\tNet Loss: 1171.0333 \tAnswer Loss: 0.3485 \tQuestion Loss: 1170.6848\n",
      "Batch: 2 \t Epoch : 2\tNet Loss: 1289.1792 \tAnswer Loss: 0.4697 \tQuestion Loss: 1288.7095\n",
      "Batch: 3 \t Epoch : 2\tNet Loss: 1086.7366 \tAnswer Loss: 0.6467 \tQuestion Loss: 1086.0898\n",
      "Batch: 4 \t Epoch : 2\tNet Loss: 1156.3114 \tAnswer Loss: 0.2877 \tQuestion Loss: 1156.0237\n",
      "Batch: 5 \t Epoch : 2\tNet Loss: 1310.6709 \tAnswer Loss: 0.4565 \tQuestion Loss: 1310.2144\n",
      "Batch: 6 \t Epoch : 2\tNet Loss: 1245.6442 \tAnswer Loss: 0.4844 \tQuestion Loss: 1245.1597\n",
      "Batch: 7 \t Epoch : 2\tNet Loss: 1319.9698 \tAnswer Loss: 0.3015 \tQuestion Loss: 1319.6683\n",
      "Batch: 8 \t Epoch : 2\tNet Loss: 1228.6152 \tAnswer Loss: 0.2636 \tQuestion Loss: 1228.3516\n",
      "Batch: 9 \t Epoch : 2\tNet Loss: 1150.3060 \tAnswer Loss: 0.3472 \tQuestion Loss: 1149.9589\n",
      "Batch: 10 \t Epoch : 2\tNet Loss: 1145.0410 \tAnswer Loss: 0.4035 \tQuestion Loss: 1144.6376\n",
      "Batch: 11 \t Epoch : 2\tNet Loss: 1324.7919 \tAnswer Loss: 0.3092 \tQuestion Loss: 1324.4827\n",
      "Batch: 12 \t Epoch : 2\tNet Loss: 1174.7622 \tAnswer Loss: 0.4330 \tQuestion Loss: 1174.3292\n",
      "Batch: 13 \t Epoch : 2\tNet Loss: 1129.0309 \tAnswer Loss: 0.4593 \tQuestion Loss: 1128.5717\n",
      "Batch: 14 \t Epoch : 2\tNet Loss: 1234.2729 \tAnswer Loss: 0.3816 \tQuestion Loss: 1233.8914\n",
      "Batch: 15 \t Epoch : 2\tNet Loss: 999.3309 \tAnswer Loss: 0.3819 \tQuestion Loss: 998.9490\n",
      "Batch: 16 \t Epoch : 2\tNet Loss: 1181.8546 \tAnswer Loss: 0.3814 \tQuestion Loss: 1181.4731\n",
      "Batch: 17 \t Epoch : 2\tNet Loss: 1165.1414 \tAnswer Loss: 0.3158 \tQuestion Loss: 1164.8256\n",
      "Batch: 18 \t Epoch : 2\tNet Loss: 1088.1537 \tAnswer Loss: 0.5102 \tQuestion Loss: 1087.6434\n",
      "Batch: 19 \t Epoch : 2\tNet Loss: 1060.7748 \tAnswer Loss: 0.3815 \tQuestion Loss: 1060.3932\n",
      "Batch: 20 \t Epoch : 2\tNet Loss: 1063.2987 \tAnswer Loss: 0.4104 \tQuestion Loss: 1062.8883\n",
      "Batch: 21 \t Epoch : 2\tNet Loss: 1250.2394 \tAnswer Loss: 0.3186 \tQuestion Loss: 1249.9208\n",
      "Batch: 22 \t Epoch : 2\tNet Loss: 1086.0442 \tAnswer Loss: 0.5315 \tQuestion Loss: 1085.5127\n",
      "Batch: 23 \t Epoch : 2\tNet Loss: 1280.3309 \tAnswer Loss: 0.3899 \tQuestion Loss: 1279.9410\n",
      "Batch: 24 \t Epoch : 2\tNet Loss: 1265.5293 \tAnswer Loss: 0.4860 \tQuestion Loss: 1265.0433\n",
      "Batch: 25 \t Epoch : 2\tNet Loss: 1254.3708 \tAnswer Loss: 0.3608 \tQuestion Loss: 1254.0100\n",
      "Batch: 26 \t Epoch : 2\tNet Loss: 1342.3606 \tAnswer Loss: 0.3719 \tQuestion Loss: 1341.9886\n",
      "Batch: 27 \t Epoch : 2\tNet Loss: 1308.3201 \tAnswer Loss: 0.4253 \tQuestion Loss: 1307.8948\n",
      "Batch: 28 \t Epoch : 2\tNet Loss: 1301.7764 \tAnswer Loss: 0.3876 \tQuestion Loss: 1301.3888\n",
      "Batch: 29 \t Epoch : 2\tNet Loss: 1179.2428 \tAnswer Loss: 0.2974 \tQuestion Loss: 1178.9454\n",
      "Batch: 30 \t Epoch : 2\tNet Loss: 1107.6731 \tAnswer Loss: 0.3261 \tQuestion Loss: 1107.3469\n",
      "Average Loss after Epoch 2 : 1160.4079\n",
      "Batch: 0 \t Epoch : 3\tNet Loss: 1182.6046 \tAnswer Loss: 0.0934 \tQuestion Loss: 1182.5112\n",
      "Batch: 1 \t Epoch : 3\tNet Loss: 1120.3420 \tAnswer Loss: 0.3482 \tQuestion Loss: 1119.9939\n",
      "Batch: 2 \t Epoch : 3\tNet Loss: 1234.1746 \tAnswer Loss: 0.4694 \tQuestion Loss: 1233.7051\n",
      "Batch: 3 \t Epoch : 3\tNet Loss: 1040.7963 \tAnswer Loss: 0.6465 \tQuestion Loss: 1040.1498\n",
      "Batch: 4 \t Epoch : 3\tNet Loss: 1110.9672 \tAnswer Loss: 0.2875 \tQuestion Loss: 1110.6797\n",
      "Batch: 5 \t Epoch : 3\tNet Loss: 1261.2322 \tAnswer Loss: 0.4562 \tQuestion Loss: 1260.7759\n",
      "Batch: 6 \t Epoch : 3\tNet Loss: 1201.3160 \tAnswer Loss: 0.4842 \tQuestion Loss: 1200.8318\n",
      "Batch: 7 \t Epoch : 3\tNet Loss: 1268.8590 \tAnswer Loss: 0.3013 \tQuestion Loss: 1268.5577\n",
      "Batch: 8 \t Epoch : 3\tNet Loss: 1181.5828 \tAnswer Loss: 0.2635 \tQuestion Loss: 1181.3192\n",
      "Batch: 9 \t Epoch : 3\tNet Loss: 1101.8307 \tAnswer Loss: 0.3469 \tQuestion Loss: 1101.4838\n",
      "Batch: 10 \t Epoch : 3\tNet Loss: 1100.9562 \tAnswer Loss: 0.4033 \tQuestion Loss: 1100.5529\n",
      "Batch: 11 \t Epoch : 3\tNet Loss: 1274.0017 \tAnswer Loss: 0.3089 \tQuestion Loss: 1273.6927\n",
      "Batch: 12 \t Epoch : 3\tNet Loss: 1128.5203 \tAnswer Loss: 0.4328 \tQuestion Loss: 1128.0875\n",
      "Batch: 13 \t Epoch : 3\tNet Loss: 1082.5438 \tAnswer Loss: 0.4591 \tQuestion Loss: 1082.0847\n",
      "Batch: 14 \t Epoch : 3\tNet Loss: 1183.2273 \tAnswer Loss: 0.3815 \tQuestion Loss: 1182.8458\n",
      "Batch: 15 \t Epoch : 3\tNet Loss: 953.8769 \tAnswer Loss: 0.3816 \tQuestion Loss: 953.4953\n",
      "Batch: 16 \t Epoch : 3\tNet Loss: 1133.2808 \tAnswer Loss: 0.3812 \tQuestion Loss: 1132.8995\n",
      "Batch: 17 \t Epoch : 3\tNet Loss: 1118.2502 \tAnswer Loss: 0.3156 \tQuestion Loss: 1117.9346\n",
      "Batch: 18 \t Epoch : 3\tNet Loss: 1047.2321 \tAnswer Loss: 0.5099 \tQuestion Loss: 1046.7222\n",
      "Batch: 19 \t Epoch : 3\tNet Loss: 1017.8857 \tAnswer Loss: 0.3813 \tQuestion Loss: 1017.5045\n",
      "Batch: 20 \t Epoch : 3\tNet Loss: 1015.7767 \tAnswer Loss: 0.4102 \tQuestion Loss: 1015.3665\n",
      "Batch: 21 \t Epoch : 3\tNet Loss: 1196.1379 \tAnswer Loss: 0.3185 \tQuestion Loss: 1195.8195\n",
      "Batch: 22 \t Epoch : 3\tNet Loss: 1044.7892 \tAnswer Loss: 0.5313 \tQuestion Loss: 1044.2579\n",
      "Batch: 23 \t Epoch : 3\tNet Loss: 1231.2812 \tAnswer Loss: 0.3897 \tQuestion Loss: 1230.8916\n",
      "Batch: 24 \t Epoch : 3\tNet Loss: 1214.3895 \tAnswer Loss: 0.4857 \tQuestion Loss: 1213.9038\n",
      "Batch: 25 \t Epoch : 3\tNet Loss: 1205.9728 \tAnswer Loss: 0.3606 \tQuestion Loss: 1205.6122\n",
      "Batch: 26 \t Epoch : 3\tNet Loss: 1292.3928 \tAnswer Loss: 0.3717 \tQuestion Loss: 1292.0211\n",
      "Batch: 27 \t Epoch : 3\tNet Loss: 1257.3379 \tAnswer Loss: 0.4252 \tQuestion Loss: 1256.9127\n",
      "Batch: 28 \t Epoch : 3\tNet Loss: 1252.0400 \tAnswer Loss: 0.3875 \tQuestion Loss: 1251.6526\n",
      "Batch: 29 \t Epoch : 3\tNet Loss: 1132.4288 \tAnswer Loss: 0.2972 \tQuestion Loss: 1132.1316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 30 \t Epoch : 3\tNet Loss: 1065.8584 \tAnswer Loss: 0.3260 \tQuestion Loss: 1065.5323\n",
      "Average Loss after Epoch 3 : 1114.1214\n",
      "Batch: 0 \t Epoch : 4\tNet Loss: 1136.0283 \tAnswer Loss: 0.0934 \tQuestion Loss: 1135.9348\n",
      "Batch: 1 \t Epoch : 4\tNet Loss: 1073.0780 \tAnswer Loss: 0.3479 \tQuestion Loss: 1072.7301\n",
      "Batch: 2 \t Epoch : 4\tNet Loss: 1183.1899 \tAnswer Loss: 0.4692 \tQuestion Loss: 1182.7207\n",
      "Batch: 3 \t Epoch : 4\tNet Loss: 998.6674 \tAnswer Loss: 0.6463 \tQuestion Loss: 998.0211\n",
      "Batch: 4 \t Epoch : 4\tNet Loss: 1068.6509 \tAnswer Loss: 0.2873 \tQuestion Loss: 1068.3635\n",
      "Batch: 5 \t Epoch : 4\tNet Loss: 1214.4072 \tAnswer Loss: 0.4560 \tQuestion Loss: 1213.9512\n",
      "Batch: 6 \t Epoch : 4\tNet Loss: 1159.8948 \tAnswer Loss: 0.4840 \tQuestion Loss: 1159.4108\n",
      "Batch: 7 \t Epoch : 4\tNet Loss: 1220.9806 \tAnswer Loss: 0.3011 \tQuestion Loss: 1220.6794\n",
      "Batch: 8 \t Epoch : 4\tNet Loss: 1137.1965 \tAnswer Loss: 0.2634 \tQuestion Loss: 1136.9331\n",
      "Batch: 9 \t Epoch : 4\tNet Loss: 1056.8293 \tAnswer Loss: 0.3467 \tQuestion Loss: 1056.4827\n",
      "Batch: 10 \t Epoch : 4\tNet Loss: 1059.4822 \tAnswer Loss: 0.4031 \tQuestion Loss: 1059.0791\n",
      "Batch: 11 \t Epoch : 4\tNet Loss: 1226.3168 \tAnswer Loss: 0.3087 \tQuestion Loss: 1226.0081\n",
      "Batch: 12 \t Epoch : 4\tNet Loss: 1084.8568 \tAnswer Loss: 0.4326 \tQuestion Loss: 1084.4242\n",
      "Batch: 13 \t Epoch : 4\tNet Loss: 1039.8883 \tAnswer Loss: 0.4589 \tQuestion Loss: 1039.4294\n",
      "Batch: 14 \t Epoch : 4\tNet Loss: 1136.4846 \tAnswer Loss: 0.3814 \tQuestion Loss: 1136.1031\n",
      "Batch: 15 \t Epoch : 4\tNet Loss: 912.5728 \tAnswer Loss: 0.3813 \tQuestion Loss: 912.1915\n",
      "Batch: 16 \t Epoch : 4\tNet Loss: 1088.4934 \tAnswer Loss: 0.3810 \tQuestion Loss: 1088.1124\n",
      "Batch: 17 \t Epoch : 4\tNet Loss: 1075.2661 \tAnswer Loss: 0.3154 \tQuestion Loss: 1074.9507\n",
      "Batch: 18 \t Epoch : 4\tNet Loss: 1009.0736 \tAnswer Loss: 0.5097 \tQuestion Loss: 1008.5640\n",
      "Batch: 19 \t Epoch : 4\tNet Loss: 977.7953 \tAnswer Loss: 0.3811 \tQuestion Loss: 977.4142\n",
      "Batch: 20 \t Epoch : 4\tNet Loss: 972.6083 \tAnswer Loss: 0.4100 \tQuestion Loss: 972.1982\n",
      "Batch: 21 \t Epoch : 4\tNet Loss: 1145.9669 \tAnswer Loss: 0.3183 \tQuestion Loss: 1145.6486\n",
      "Batch: 22 \t Epoch : 4\tNet Loss: 1006.4324 \tAnswer Loss: 0.5311 \tQuestion Loss: 1005.9012\n",
      "Batch: 23 \t Epoch : 4\tNet Loss: 1185.6989 \tAnswer Loss: 0.3895 \tQuestion Loss: 1185.3093\n",
      "Batch: 24 \t Epoch : 4\tNet Loss: 1167.2906 \tAnswer Loss: 0.4854 \tQuestion Loss: 1166.8052\n",
      "Batch: 25 \t Epoch : 4\tNet Loss: 1160.8232 \tAnswer Loss: 0.3604 \tQuestion Loss: 1160.4628\n",
      "Batch: 26 \t Epoch : 4\tNet Loss: 1245.7573 \tAnswer Loss: 0.3715 \tQuestion Loss: 1245.3857\n",
      "Batch: 27 \t Epoch : 4\tNet Loss: 1209.6880 \tAnswer Loss: 0.4250 \tQuestion Loss: 1209.2629\n",
      "Batch: 28 \t Epoch : 4\tNet Loss: 1205.6656 \tAnswer Loss: 0.3873 \tQuestion Loss: 1205.2783\n",
      "Batch: 29 \t Epoch : 4\tNet Loss: 1088.9423 \tAnswer Loss: 0.2970 \tQuestion Loss: 1088.6453\n",
      "Batch: 30 \t Epoch : 4\tNet Loss: 1026.3641 \tAnswer Loss: 0.3259 \tQuestion Loss: 1026.0382\n",
      "Average Loss after Epoch 4 : 1071.0747\n",
      "Batch: 0 \t Epoch : 5\tNet Loss: 1092.7148 \tAnswer Loss: 0.0935 \tQuestion Loss: 1092.6213\n",
      "Batch: 1 \t Epoch : 5\tNet Loss: 1029.1918 \tAnswer Loss: 0.3477 \tQuestion Loss: 1028.8441\n",
      "Batch: 2 \t Epoch : 5\tNet Loss: 1135.9258 \tAnswer Loss: 0.4690 \tQuestion Loss: 1135.4568\n",
      "Batch: 3 \t Epoch : 5\tNet Loss: 959.9206 \tAnswer Loss: 0.6462 \tQuestion Loss: 959.2744\n",
      "Batch: 4 \t Epoch : 5\tNet Loss: 1029.3319 \tAnswer Loss: 0.2871 \tQuestion Loss: 1029.0448\n",
      "Batch: 5 \t Epoch : 5\tNet Loss: 1170.2290 \tAnswer Loss: 0.4558 \tQuestion Loss: 1169.7732\n",
      "Batch: 6 \t Epoch : 5\tNet Loss: 1121.1682 \tAnswer Loss: 0.4838 \tQuestion Loss: 1120.6843\n",
      "Batch: 7 \t Epoch : 5\tNet Loss: 1176.3795 \tAnswer Loss: 0.3010 \tQuestion Loss: 1176.0785\n",
      "Batch: 8 \t Epoch : 5\tNet Loss: 1095.4434 \tAnswer Loss: 0.2633 \tQuestion Loss: 1095.1801\n",
      "Batch: 9 \t Epoch : 5\tNet Loss: 1015.3550 \tAnswer Loss: 0.3465 \tQuestion Loss: 1015.0085\n",
      "Batch: 10 \t Epoch : 5\tNet Loss: 1020.5051 \tAnswer Loss: 0.4029 \tQuestion Loss: 1020.1021\n",
      "Batch: 11 \t Epoch : 5\tNet Loss: 1181.7448 \tAnswer Loss: 0.3085 \tQuestion Loss: 1181.4363\n",
      "Batch: 12 \t Epoch : 5\tNet Loss: 1043.7825 \tAnswer Loss: 0.4324 \tQuestion Loss: 1043.3501\n",
      "Batch: 13 \t Epoch : 5\tNet Loss: 1000.8036 \tAnswer Loss: 0.4587 \tQuestion Loss: 1000.3449\n",
      "Batch: 14 \t Epoch : 5\tNet Loss: 1093.7611 \tAnswer Loss: 0.3813 \tQuestion Loss: 1093.3798\n",
      "Batch: 15 \t Epoch : 5\tNet Loss: 875.1710 \tAnswer Loss: 0.3811 \tQuestion Loss: 874.7899\n",
      "Batch: 16 \t Epoch : 5\tNet Loss: 1047.4431 \tAnswer Loss: 0.3809 \tQuestion Loss: 1047.0623\n",
      "Batch: 17 \t Epoch : 5\tNet Loss: 1036.0269 \tAnswer Loss: 0.3153 \tQuestion Loss: 1035.7115\n",
      "Batch: 18 \t Epoch : 5\tNet Loss: 973.3446 \tAnswer Loss: 0.5094 \tQuestion Loss: 972.8351\n",
      "Batch: 19 \t Epoch : 5\tNet Loss: 940.4890 \tAnswer Loss: 0.3809 \tQuestion Loss: 940.1081\n",
      "Batch: 20 \t Epoch : 5\tNet Loss: 933.4581 \tAnswer Loss: 0.4099 \tQuestion Loss: 933.0482\n",
      "Batch: 21 \t Epoch : 5\tNet Loss: 1099.5646 \tAnswer Loss: 0.3182 \tQuestion Loss: 1099.2463\n",
      "Batch: 22 \t Epoch : 5\tNet Loss: 970.8754 \tAnswer Loss: 0.5309 \tQuestion Loss: 970.3444\n",
      "Batch: 23 \t Epoch : 5\tNet Loss: 1143.5482 \tAnswer Loss: 0.3893 \tQuestion Loss: 1143.1589\n",
      "Batch: 24 \t Epoch : 5\tNet Loss: 1123.8357 \tAnswer Loss: 0.4852 \tQuestion Loss: 1123.3505\n",
      "Batch: 25 \t Epoch : 5\tNet Loss: 1118.7034 \tAnswer Loss: 0.3603 \tQuestion Loss: 1118.3431\n",
      "Batch: 26 \t Epoch : 5\tNet Loss: 1202.2600 \tAnswer Loss: 0.3714 \tQuestion Loss: 1201.8887\n",
      "Batch: 27 \t Epoch : 5\tNet Loss: 1165.3206 \tAnswer Loss: 0.4249 \tQuestion Loss: 1164.8956\n",
      "Batch: 28 \t Epoch : 5\tNet Loss: 1162.4072 \tAnswer Loss: 0.3872 \tQuestion Loss: 1162.0200\n",
      "Batch: 29 \t Epoch : 5\tNet Loss: 1048.5696 \tAnswer Loss: 0.2969 \tQuestion Loss: 1048.2727\n",
      "Batch: 30 \t Epoch : 5\tNet Loss: 989.1943 \tAnswer Loss: 0.3258 \tQuestion Loss: 988.8685\n",
      "Average Loss after Epoch 5 : 1031.1396\n",
      "Batch: 0 \t Epoch : 6\tNet Loss: 1052.5182 \tAnswer Loss: 0.0935 \tQuestion Loss: 1052.4247\n",
      "Batch: 1 \t Epoch : 6\tNet Loss: 988.4028 \tAnswer Loss: 0.3475 \tQuestion Loss: 988.0553\n",
      "Batch: 2 \t Epoch : 6\tNet Loss: 1091.9445 \tAnswer Loss: 0.4688 \tQuestion Loss: 1091.4756\n",
      "Batch: 3 \t Epoch : 6\tNet Loss: 924.0699 \tAnswer Loss: 0.6460 \tQuestion Loss: 923.4239\n",
      "Batch: 4 \t Epoch : 6\tNet Loss: 992.6162 \tAnswer Loss: 0.2870 \tQuestion Loss: 992.3292\n",
      "Batch: 5 \t Epoch : 6\tNet Loss: 1128.6930 \tAnswer Loss: 0.4556 \tQuestion Loss: 1128.2374\n",
      "Batch: 6 \t Epoch : 6\tNet Loss: 1084.7183 \tAnswer Loss: 0.4837 \tQuestion Loss: 1084.2346\n",
      "Batch: 7 \t Epoch : 6\tNet Loss: 1134.7089 \tAnswer Loss: 0.3008 \tQuestion Loss: 1134.4080\n",
      "Batch: 8 \t Epoch : 6\tNet Loss: 1056.1801 \tAnswer Loss: 0.2632 \tQuestion Loss: 1055.9169\n",
      "Batch: 9 \t Epoch : 6\tNet Loss: 976.9957 \tAnswer Loss: 0.3463 \tQuestion Loss: 976.6495\n",
      "Batch: 10 \t Epoch : 6\tNet Loss: 983.7175 \tAnswer Loss: 0.4028 \tQuestion Loss: 983.3146\n",
      "Batch: 11 \t Epoch : 6\tNet Loss: 1139.8850 \tAnswer Loss: 0.3083 \tQuestion Loss: 1139.5767\n",
      "Batch: 12 \t Epoch : 6\tNet Loss: 1005.0363 \tAnswer Loss: 0.4323 \tQuestion Loss: 1004.6040\n",
      "Batch: 13 \t Epoch : 6\tNet Loss: 964.8453 \tAnswer Loss: 0.4586 \tQuestion Loss: 964.3867\n",
      "Batch: 14 \t Epoch : 6\tNet Loss: 1054.5144 \tAnswer Loss: 0.3813 \tQuestion Loss: 1054.1332\n",
      "Batch: 15 \t Epoch : 6\tNet Loss: 841.2360 \tAnswer Loss: 0.3809 \tQuestion Loss: 840.8551\n",
      "Batch: 16 \t Epoch : 6\tNet Loss: 1009.6253 \tAnswer Loss: 0.3807 \tQuestion Loss: 1009.2446\n",
      "Batch: 17 \t Epoch : 6\tNet Loss: 999.9999 \tAnswer Loss: 0.3151 \tQuestion Loss: 999.6848\n",
      "Batch: 18 \t Epoch : 6\tNet Loss: 939.7680 \tAnswer Loss: 0.5092 \tQuestion Loss: 939.2588\n",
      "Batch: 19 \t Epoch : 6\tNet Loss: 905.8624 \tAnswer Loss: 0.3808 \tQuestion Loss: 905.4816\n",
      "Batch: 20 \t Epoch : 6\tNet Loss: 897.7499 \tAnswer Loss: 0.4097 \tQuestion Loss: 897.3401\n",
      "Batch: 21 \t Epoch : 6\tNet Loss: 1056.4689 \tAnswer Loss: 0.3181 \tQuestion Loss: 1056.1508\n",
      "Batch: 22 \t Epoch : 6\tNet Loss: 937.8469 \tAnswer Loss: 0.5308 \tQuestion Loss: 937.3162\n",
      "Batch: 23 \t Epoch : 6\tNet Loss: 1104.5216 \tAnswer Loss: 0.3892 \tQuestion Loss: 1104.1324\n",
      "Batch: 24 \t Epoch : 6\tNet Loss: 1083.5300 \tAnswer Loss: 0.4850 \tQuestion Loss: 1083.0450\n",
      "Batch: 25 \t Epoch : 6\tNet Loss: 1079.4932 \tAnswer Loss: 0.3602 \tQuestion Loss: 1079.1331\n",
      "Batch: 26 \t Epoch : 6\tNet Loss: 1161.6438 \tAnswer Loss: 0.3712 \tQuestion Loss: 1161.2726\n",
      "Batch: 27 \t Epoch : 6\tNet Loss: 1123.9109 \tAnswer Loss: 0.4248 \tQuestion Loss: 1123.4861\n",
      "Batch: 28 \t Epoch : 6\tNet Loss: 1121.7981 \tAnswer Loss: 0.3871 \tQuestion Loss: 1121.4110\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 29 \t Epoch : 6\tNet Loss: 1011.0238 \tAnswer Loss: 0.2967 \tQuestion Loss: 1010.7271\n",
      "Batch: 30 \t Epoch : 6\tNet Loss: 954.2358 \tAnswer Loss: 0.3257 \tQuestion Loss: 953.9101\n",
      "Average Loss after Epoch 6 : 993.9863\n",
      "Batch: 0 \t Epoch : 7\tNet Loss: 1015.0531 \tAnswer Loss: 0.0935 \tQuestion Loss: 1014.9596\n",
      "Batch: 1 \t Epoch : 7\tNet Loss: 950.4347 \tAnswer Loss: 0.3473 \tQuestion Loss: 950.0873\n",
      "Batch: 2 \t Epoch : 7\tNet Loss: 1050.9348 \tAnswer Loss: 0.4687 \tQuestion Loss: 1050.4662\n",
      "Batch: 3 \t Epoch : 7\tNet Loss: 890.7523 \tAnswer Loss: 0.6459 \tQuestion Loss: 890.1064\n",
      "Batch: 4 \t Epoch : 7\tNet Loss: 958.2288 \tAnswer Loss: 0.2868 \tQuestion Loss: 957.9420\n",
      "Batch: 5 \t Epoch : 7\tNet Loss: 1089.7615 \tAnswer Loss: 0.4554 \tQuestion Loss: 1089.3060\n",
      "Batch: 6 \t Epoch : 7\tNet Loss: 1050.3077 \tAnswer Loss: 0.4836 \tQuestion Loss: 1049.8242\n",
      "Batch: 7 \t Epoch : 7\tNet Loss: 1095.6746 \tAnswer Loss: 0.3007 \tQuestion Loss: 1095.3738\n",
      "Batch: 8 \t Epoch : 7\tNet Loss: 1019.3719 \tAnswer Loss: 0.2631 \tQuestion Loss: 1019.1088\n",
      "Batch: 9 \t Epoch : 7\tNet Loss: 941.4436 \tAnswer Loss: 0.3461 \tQuestion Loss: 941.0975\n",
      "Batch: 10 \t Epoch : 7\tNet Loss: 949.1164 \tAnswer Loss: 0.4027 \tQuestion Loss: 948.7137\n",
      "Batch: 11 \t Epoch : 7\tNet Loss: 1100.5464 \tAnswer Loss: 0.3082 \tQuestion Loss: 1100.2383\n",
      "Batch: 12 \t Epoch : 7\tNet Loss: 968.6526 \tAnswer Loss: 0.4321 \tQuestion Loss: 968.2205\n",
      "Batch: 13 \t Epoch : 7\tNet Loss: 931.5629 \tAnswer Loss: 0.4585 \tQuestion Loss: 931.1044\n",
      "Batch: 14 \t Epoch : 7\tNet Loss: 1018.5384 \tAnswer Loss: 0.3812 \tQuestion Loss: 1018.1572\n",
      "Batch: 15 \t Epoch : 7\tNet Loss: 810.2272 \tAnswer Loss: 0.3807 \tQuestion Loss: 809.8465\n",
      "Batch: 16 \t Epoch : 7\tNet Loss: 974.6113 \tAnswer Loss: 0.3806 \tQuestion Loss: 974.2307\n",
      "Batch: 17 \t Epoch : 7\tNet Loss: 966.8018 \tAnswer Loss: 0.3150 \tQuestion Loss: 966.4868\n",
      "Batch: 18 \t Epoch : 7\tNet Loss: 908.3102 \tAnswer Loss: 0.5090 \tQuestion Loss: 907.8011\n",
      "Batch: 19 \t Epoch : 7\tNet Loss: 873.8207 \tAnswer Loss: 0.3806 \tQuestion Loss: 873.4401\n",
      "Batch: 20 \t Epoch : 7\tNet Loss: 864.9892 \tAnswer Loss: 0.4096 \tQuestion Loss: 864.5796\n",
      "Batch: 21 \t Epoch : 7\tNet Loss: 1016.2739 \tAnswer Loss: 0.3180 \tQuestion Loss: 1015.9559\n",
      "Batch: 22 \t Epoch : 7\tNet Loss: 907.0624 \tAnswer Loss: 0.5306 \tQuestion Loss: 906.5318\n",
      "Batch: 23 \t Epoch : 7\tNet Loss: 1068.1508 \tAnswer Loss: 0.3890 \tQuestion Loss: 1067.7617\n",
      "Batch: 24 \t Epoch : 7\tNet Loss: 1046.0206 \tAnswer Loss: 0.4848 \tQuestion Loss: 1045.5358\n",
      "Batch: 25 \t Epoch : 7\tNet Loss: 1043.0482 \tAnswer Loss: 0.3600 \tQuestion Loss: 1042.6882\n",
      "Batch: 26 \t Epoch : 7\tNet Loss: 1123.7052 \tAnswer Loss: 0.3711 \tQuestion Loss: 1123.3341\n",
      "Batch: 27 \t Epoch : 7\tNet Loss: 1085.0946 \tAnswer Loss: 0.4247 \tQuestion Loss: 1084.6699\n",
      "Batch: 28 \t Epoch : 7\tNet Loss: 1083.4503 \tAnswer Loss: 0.3870 \tQuestion Loss: 1083.0634\n",
      "Batch: 29 \t Epoch : 7\tNet Loss: 975.9509 \tAnswer Loss: 0.2966 \tQuestion Loss: 975.6543\n",
      "Batch: 30 \t Epoch : 7\tNet Loss: 921.2847 \tAnswer Loss: 0.3256 \tQuestion Loss: 920.9590\n",
      "Average Loss after Epoch 7 : 959.3494\n",
      "Batch: 0 \t Epoch : 8\tNet Loss: 979.9617 \tAnswer Loss: 0.0935 \tQuestion Loss: 979.8682\n",
      "Batch: 1 \t Epoch : 8\tNet Loss: 915.0462 \tAnswer Loss: 0.3472 \tQuestion Loss: 914.6990\n",
      "Batch: 2 \t Epoch : 8\tNet Loss: 1012.5784 \tAnswer Loss: 0.4685 \tQuestion Loss: 1012.1099\n",
      "Batch: 3 \t Epoch : 8\tNet Loss: 859.6017 \tAnswer Loss: 0.6458 \tQuestion Loss: 858.9560\n",
      "Batch: 4 \t Epoch : 8\tNet Loss: 925.8889 \tAnswer Loss: 0.2867 \tQuestion Loss: 925.6022\n",
      "Batch: 5 \t Epoch : 8\tNet Loss: 1053.2343 \tAnswer Loss: 0.4553 \tQuestion Loss: 1052.7789\n",
      "Batch: 6 \t Epoch : 8\tNet Loss: 1017.6670 \tAnswer Loss: 0.4834 \tQuestion Loss: 1017.1835\n",
      "Batch: 7 \t Epoch : 8\tNet Loss: 1058.9872 \tAnswer Loss: 0.3006 \tQuestion Loss: 1058.6865\n",
      "Batch: 8 \t Epoch : 8\tNet Loss: 984.8849 \tAnswer Loss: 0.2630 \tQuestion Loss: 984.6218\n",
      "Batch: 9 \t Epoch : 8\tNet Loss: 908.3551 \tAnswer Loss: 0.3460 \tQuestion Loss: 908.0091\n",
      "Batch: 10 \t Epoch : 8\tNet Loss: 916.5834 \tAnswer Loss: 0.4026 \tQuestion Loss: 916.1808\n",
      "Batch: 11 \t Epoch : 8\tNet Loss: 1063.5028 \tAnswer Loss: 0.3080 \tQuestion Loss: 1063.1948\n",
      "Batch: 12 \t Epoch : 8\tNet Loss: 934.4124 \tAnswer Loss: 0.4320 \tQuestion Loss: 933.9803\n",
      "Batch: 13 \t Epoch : 8\tNet Loss: 900.5338 \tAnswer Loss: 0.4584 \tQuestion Loss: 900.0754\n",
      "Batch: 14 \t Epoch : 8\tNet Loss: 985.3784 \tAnswer Loss: 0.3811 \tQuestion Loss: 984.9973\n",
      "Batch: 15 \t Epoch : 8\tNet Loss: 781.6600 \tAnswer Loss: 0.3806 \tQuestion Loss: 781.2794\n",
      "Batch: 16 \t Epoch : 8\tNet Loss: 942.0195 \tAnswer Loss: 0.3805 \tQuestion Loss: 941.6391\n",
      "Batch: 17 \t Epoch : 8\tNet Loss: 935.9867 \tAnswer Loss: 0.3149 \tQuestion Loss: 935.6718\n",
      "Batch: 18 \t Epoch : 8\tNet Loss: 878.6595 \tAnswer Loss: 0.5089 \tQuestion Loss: 878.1507\n",
      "Batch: 19 \t Epoch : 8\tNet Loss: 843.9890 \tAnswer Loss: 0.3805 \tQuestion Loss: 843.6085\n",
      "Batch: 20 \t Epoch : 8\tNet Loss: 834.6441 \tAnswer Loss: 0.4095 \tQuestion Loss: 834.2346\n",
      "Batch: 21 \t Epoch : 8\tNet Loss: 978.5612 \tAnswer Loss: 0.3179 \tQuestion Loss: 978.2433\n",
      "Batch: 22 \t Epoch : 8\tNet Loss: 878.1409 \tAnswer Loss: 0.5305 \tQuestion Loss: 877.6104\n",
      "Batch: 23 \t Epoch : 8\tNet Loss: 1034.0165 \tAnswer Loss: 0.3889 \tQuestion Loss: 1033.6276\n",
      "Batch: 24 \t Epoch : 8\tNet Loss: 1010.8717 \tAnswer Loss: 0.4847 \tQuestion Loss: 1010.3870\n",
      "Batch: 25 \t Epoch : 8\tNet Loss: 1008.9581 \tAnswer Loss: 0.3599 \tQuestion Loss: 1008.5982\n",
      "Batch: 26 \t Epoch : 8\tNet Loss: 1088.0453 \tAnswer Loss: 0.3710 \tQuestion Loss: 1087.6743\n",
      "Batch: 27 \t Epoch : 8\tNet Loss: 1048.4110 \tAnswer Loss: 0.4247 \tQuestion Loss: 1047.9863\n",
      "Batch: 28 \t Epoch : 8\tNet Loss: 1047.0154 \tAnswer Loss: 0.3869 \tQuestion Loss: 1046.6284\n",
      "Batch: 29 \t Epoch : 8\tNet Loss: 942.9723 \tAnswer Loss: 0.2965 \tQuestion Loss: 942.6758\n",
      "Batch: 30 \t Epoch : 8\tNet Loss: 890.0211 \tAnswer Loss: 0.3255 \tQuestion Loss: 889.6956\n",
      "Average Loss after Epoch 8 : 926.8934\n",
      "Batch: 0 \t Epoch : 9\tNet Loss: 946.8271 \tAnswer Loss: 0.0935 \tQuestion Loss: 946.7335\n",
      "Batch: 1 \t Epoch : 9\tNet Loss: 881.8743 \tAnswer Loss: 0.3470 \tQuestion Loss: 881.5273\n",
      "Batch: 2 \t Epoch : 9\tNet Loss: 976.4455 \tAnswer Loss: 0.4684 \tQuestion Loss: 975.9771\n",
      "Batch: 3 \t Epoch : 9\tNet Loss: 830.2454 \tAnswer Loss: 0.6457 \tQuestion Loss: 829.5997\n",
      "Batch: 4 \t Epoch : 9\tNet Loss: 895.3012 \tAnswer Loss: 0.2866 \tQuestion Loss: 895.0146\n",
      "Batch: 5 \t Epoch : 9\tNet Loss: 1018.7050 \tAnswer Loss: 0.4552 \tQuestion Loss: 1018.2499\n",
      "Batch: 6 \t Epoch : 9\tNet Loss: 986.5501 \tAnswer Loss: 0.4833 \tQuestion Loss: 986.0668\n",
      "Batch: 7 \t Epoch : 9\tNet Loss: 1024.2686 \tAnswer Loss: 0.3005 \tQuestion Loss: 1023.9680\n",
      "Batch: 8 \t Epoch : 9\tNet Loss: 952.4293 \tAnswer Loss: 0.2630 \tQuestion Loss: 952.1663\n",
      "Batch: 9 \t Epoch : 9\tNet Loss: 877.3111 \tAnswer Loss: 0.3459 \tQuestion Loss: 876.9653\n",
      "Batch: 10 \t Epoch : 9\tNet Loss: 885.8452 \tAnswer Loss: 0.4025 \tQuestion Loss: 885.4427\n",
      "Batch: 11 \t Epoch : 9\tNet Loss: 1028.4797 \tAnswer Loss: 0.3079 \tQuestion Loss: 1028.1719\n",
      "Batch: 12 \t Epoch : 9\tNet Loss: 901.9392 \tAnswer Loss: 0.4319 \tQuestion Loss: 901.5073\n",
      "Batch: 13 \t Epoch : 9\tNet Loss: 871.4252 \tAnswer Loss: 0.4583 \tQuestion Loss: 870.9669\n",
      "Batch: 14 \t Epoch : 9\tNet Loss: 954.4059 \tAnswer Loss: 0.3811 \tQuestion Loss: 954.0248\n",
      "Batch: 15 \t Epoch : 9\tNet Loss: 755.1094 \tAnswer Loss: 0.3804 \tQuestion Loss: 754.7289\n",
      "Batch: 16 \t Epoch : 9\tNet Loss: 911.4695 \tAnswer Loss: 0.3803 \tQuestion Loss: 911.0891\n",
      "Batch: 17 \t Epoch : 9\tNet Loss: 907.1155 \tAnswer Loss: 0.3148 \tQuestion Loss: 906.8008\n",
      "Batch: 18 \t Epoch : 9\tNet Loss: 850.4623 \tAnswer Loss: 0.5087 \tQuestion Loss: 849.9536\n",
      "Batch: 19 \t Epoch : 9\tNet Loss: 816.0046 \tAnswer Loss: 0.3804 \tQuestion Loss: 815.6242\n",
      "Batch: 20 \t Epoch : 9\tNet Loss: 806.2629 \tAnswer Loss: 0.4094 \tQuestion Loss: 805.8535\n",
      "Batch: 21 \t Epoch : 9\tNet Loss: 942.9841 \tAnswer Loss: 0.3178 \tQuestion Loss: 942.6664\n",
      "Batch: 22 \t Epoch : 9\tNet Loss: 850.7385 \tAnswer Loss: 0.5304 \tQuestion Loss: 850.2081\n",
      "Batch: 23 \t Epoch : 9\tNet Loss: 1001.7082 \tAnswer Loss: 0.3888 \tQuestion Loss: 1001.3195\n",
      "Batch: 24 \t Epoch : 9\tNet Loss: 977.6960 \tAnswer Loss: 0.4845 \tQuestion Loss: 977.2115\n",
      "Batch: 25 \t Epoch : 9\tNet Loss: 976.8016 \tAnswer Loss: 0.3598 \tQuestion Loss: 976.4418\n",
      "Batch: 26 \t Epoch : 9\tNet Loss: 1054.2434 \tAnswer Loss: 0.3709 \tQuestion Loss: 1053.8726\n",
      "Batch: 27 \t Epoch : 9\tNet Loss: 1013.4534 \tAnswer Loss: 0.4246 \tQuestion Loss: 1013.0288\n",
      "Batch: 28 \t Epoch : 9\tNet Loss: 1012.2628 \tAnswer Loss: 0.3868 \tQuestion Loss: 1011.8760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 29 \t Epoch : 9\tNet Loss: 911.7618 \tAnswer Loss: 0.2964 \tQuestion Loss: 911.4654\n",
      "Batch: 30 \t Epoch : 9\tNet Loss: 860.2070 \tAnswer Loss: 0.3254 \tQuestion Loss: 859.8816\n",
      "Average Loss after Epoch 9 : 896.2604\n",
      "Batch: 0 \t Epoch : 10\tNet Loss: 915.3244 \tAnswer Loss: 0.0935 \tQuestion Loss: 915.2308\n",
      "Batch: 1 \t Epoch : 10\tNet Loss: 850.5538 \tAnswer Loss: 0.3469 \tQuestion Loss: 850.2069\n",
      "Batch: 2 \t Epoch : 10\tNet Loss: 942.1748 \tAnswer Loss: 0.4683 \tQuestion Loss: 941.7065\n",
      "Batch: 3 \t Epoch : 10\tNet Loss: 802.3641 \tAnswer Loss: 0.6456 \tQuestion Loss: 801.7186\n",
      "Batch: 4 \t Epoch : 10\tNet Loss: 866.1948 \tAnswer Loss: 0.2865 \tQuestion Loss: 865.9083\n",
      "Batch: 5 \t Epoch : 10\tNet Loss: 985.8471 \tAnswer Loss: 0.4550 \tQuestion Loss: 985.3921\n",
      "Batch: 6 \t Epoch : 10\tNet Loss: 956.7692 \tAnswer Loss: 0.4832 \tQuestion Loss: 956.2860\n",
      "Batch: 7 \t Epoch : 10\tNet Loss: 991.1663 \tAnswer Loss: 0.3004 \tQuestion Loss: 990.8658\n",
      "Batch: 8 \t Epoch : 10\tNet Loss: 921.7026 \tAnswer Loss: 0.2629 \tQuestion Loss: 921.4398\n",
      "Batch: 9 \t Epoch : 10\tNet Loss: 847.9363 \tAnswer Loss: 0.3457 \tQuestion Loss: 847.5906\n",
      "Batch: 10 \t Epoch : 10\tNet Loss: 856.6815 \tAnswer Loss: 0.4024 \tQuestion Loss: 856.2791\n",
      "Batch: 11 \t Epoch : 10\tNet Loss: 995.2071 \tAnswer Loss: 0.3078 \tQuestion Loss: 994.8993\n",
      "Batch: 12 \t Epoch : 10\tNet Loss: 870.9775 \tAnswer Loss: 0.4318 \tQuestion Loss: 870.5457\n",
      "Batch: 13 \t Epoch : 10\tNet Loss: 843.9019 \tAnswer Loss: 0.4582 \tQuestion Loss: 843.4437\n",
      "Batch: 14 \t Epoch : 10\tNet Loss: 925.1875 \tAnswer Loss: 0.3810 \tQuestion Loss: 924.8065\n",
      "Batch: 15 \t Epoch : 10\tNet Loss: 730.2239 \tAnswer Loss: 0.3803 \tQuestion Loss: 729.8436\n",
      "Batch: 16 \t Epoch : 10\tNet Loss: 882.6064 \tAnswer Loss: 0.3802 \tQuestion Loss: 882.2262\n",
      "Batch: 17 \t Epoch : 10\tNet Loss: 879.7900 \tAnswer Loss: 0.3147 \tQuestion Loss: 879.4753\n",
      "Batch: 18 \t Epoch : 10\tNet Loss: 823.4654 \tAnswer Loss: 0.5086 \tQuestion Loss: 822.9568\n",
      "Batch: 19 \t Epoch : 10\tNet Loss: 789.5839 \tAnswer Loss: 0.3803 \tQuestion Loss: 789.2037\n",
      "Batch: 20 \t Epoch : 10\tNet Loss: 779.5262 \tAnswer Loss: 0.4093 \tQuestion Loss: 779.1169\n",
      "Batch: 21 \t Epoch : 10\tNet Loss: 909.2692 \tAnswer Loss: 0.3177 \tQuestion Loss: 908.9515\n",
      "Batch: 22 \t Epoch : 10\tNet Loss: 824.5673 \tAnswer Loss: 0.5303 \tQuestion Loss: 824.0370\n",
      "Batch: 23 \t Epoch : 10\tNet Loss: 970.8483 \tAnswer Loss: 0.3886 \tQuestion Loss: 970.4596\n",
      "Batch: 24 \t Epoch : 10\tNet Loss: 946.1981 \tAnswer Loss: 0.4844 \tQuestion Loss: 945.7137\n",
      "Batch: 25 \t Epoch : 10\tNet Loss: 946.2321 \tAnswer Loss: 0.3597 \tQuestion Loss: 945.8723\n",
      "Batch: 26 \t Epoch : 10\tNet Loss: 1022.0024 \tAnswer Loss: 0.3708 \tQuestion Loss: 1021.6316\n",
      "Batch: 27 \t Epoch : 10\tNet Loss: 979.9311 \tAnswer Loss: 0.4245 \tQuestion Loss: 979.5066\n",
      "Batch: 28 \t Epoch : 10\tNet Loss: 979.0076 \tAnswer Loss: 0.3867 \tQuestion Loss: 978.6209\n",
      "Batch: 29 \t Epoch : 10\tNet Loss: 882.0840 \tAnswer Loss: 0.2963 \tQuestion Loss: 881.7878\n",
      "Batch: 30 \t Epoch : 10\tNet Loss: 831.6718 \tAnswer Loss: 0.3253 \tQuestion Loss: 831.3464\n",
      "Average Loss after Epoch 10 : 867.1561\n",
      "Batch: 0 \t Epoch : 11\tNet Loss: 885.2227 \tAnswer Loss: 0.0935 \tQuestion Loss: 885.1291\n",
      "Batch: 1 \t Epoch : 11\tNet Loss: 820.7898 \tAnswer Loss: 0.3468 \tQuestion Loss: 820.4430\n",
      "Batch: 2 \t Epoch : 11\tNet Loss: 909.5455 \tAnswer Loss: 0.4682 \tQuestion Loss: 909.0773\n",
      "Batch: 3 \t Epoch : 11\tNet Loss: 775.7001 \tAnswer Loss: 0.6455 \tQuestion Loss: 775.0546\n",
      "Batch: 4 \t Epoch : 11\tNet Loss: 838.3625 \tAnswer Loss: 0.2864 \tQuestion Loss: 838.0762\n",
      "Batch: 5 \t Epoch : 11\tNet Loss: 954.4656 \tAnswer Loss: 0.4549 \tQuestion Loss: 954.0107\n",
      "Batch: 6 \t Epoch : 11\tNet Loss: 928.1669 \tAnswer Loss: 0.4831 \tQuestion Loss: 927.6838\n",
      "Batch: 7 \t Epoch : 11\tNet Loss: 959.4421 \tAnswer Loss: 0.3003 \tQuestion Loss: 959.1417\n",
      "Batch: 8 \t Epoch : 11\tNet Loss: 892.4798 \tAnswer Loss: 0.2628 \tQuestion Loss: 892.2170\n",
      "Batch: 9 \t Epoch : 11\tNet Loss: 819.9626 \tAnswer Loss: 0.3456 \tQuestion Loss: 819.6170\n",
      "Batch: 10 \t Epoch : 11\tNet Loss: 828.9326 \tAnswer Loss: 0.4023 \tQuestion Loss: 828.5303\n",
      "Batch: 11 \t Epoch : 11\tNet Loss: 963.4756 \tAnswer Loss: 0.3077 \tQuestion Loss: 963.1679\n",
      "Batch: 12 \t Epoch : 11\tNet Loss: 841.3668 \tAnswer Loss: 0.4317 \tQuestion Loss: 840.9351\n",
      "Batch: 13 \t Epoch : 11\tNet Loss: 817.6712 \tAnswer Loss: 0.4581 \tQuestion Loss: 817.2131\n",
      "Batch: 14 \t Epoch : 11\tNet Loss: 897.4377 \tAnswer Loss: 0.3810 \tQuestion Loss: 897.0568\n",
      "Batch: 15 \t Epoch : 11\tNet Loss: 706.7460 \tAnswer Loss: 0.3802 \tQuestion Loss: 706.3658\n",
      "Batch: 16 \t Epoch : 11\tNet Loss: 855.1691 \tAnswer Loss: 0.3801 \tQuestion Loss: 854.7889\n",
      "Batch: 17 \t Epoch : 11\tNet Loss: 853.7013 \tAnswer Loss: 0.3146 \tQuestion Loss: 853.3867\n",
      "Batch: 18 \t Epoch : 11\tNet Loss: 797.5157 \tAnswer Loss: 0.5084 \tQuestion Loss: 797.0073\n",
      "Batch: 19 \t Epoch : 11\tNet Loss: 764.5047 \tAnswer Loss: 0.3802 \tQuestion Loss: 764.1245\n",
      "Batch: 20 \t Epoch : 11\tNet Loss: 754.2105 \tAnswer Loss: 0.4093 \tQuestion Loss: 753.8013\n",
      "Batch: 21 \t Epoch : 11\tNet Loss: 877.2252 \tAnswer Loss: 0.3176 \tQuestion Loss: 876.9076\n",
      "Batch: 22 \t Epoch : 11\tNet Loss: 799.4220 \tAnswer Loss: 0.5302 \tQuestion Loss: 798.8918\n",
      "Batch: 23 \t Epoch : 11\tNet Loss: 941.1893 \tAnswer Loss: 0.3885 \tQuestion Loss: 940.8007\n",
      "Batch: 24 \t Epoch : 11\tNet Loss: 916.1704 \tAnswer Loss: 0.4842 \tQuestion Loss: 915.6862\n",
      "Batch: 25 \t Epoch : 11\tNet Loss: 917.0051 \tAnswer Loss: 0.3596 \tQuestion Loss: 916.6454\n",
      "Batch: 26 \t Epoch : 11\tNet Loss: 991.1661 \tAnswer Loss: 0.3707 \tQuestion Loss: 990.7955\n",
      "Batch: 27 \t Epoch : 11\tNet Loss: 947.6737 \tAnswer Loss: 0.4244 \tQuestion Loss: 947.2493\n",
      "Batch: 28 \t Epoch : 11\tNet Loss: 947.1093 \tAnswer Loss: 0.3867 \tQuestion Loss: 946.7227\n",
      "Batch: 29 \t Epoch : 11\tNet Loss: 853.7928 \tAnswer Loss: 0.2962 \tQuestion Loss: 853.4966\n",
      "Batch: 30 \t Epoch : 11\tNet Loss: 804.2999 \tAnswer Loss: 0.3253 \tQuestion Loss: 803.9747\n",
      "Average Loss after Epoch 11 : 839.3726\n",
      "Batch: 0 \t Epoch : 12\tNet Loss: 856.3851 \tAnswer Loss: 0.0935 \tQuestion Loss: 856.2915\n",
      "Batch: 1 \t Epoch : 12\tNet Loss: 792.3765 \tAnswer Loss: 0.3467 \tQuestion Loss: 792.0298\n",
      "Batch: 2 \t Epoch : 12\tNet Loss: 878.4622 \tAnswer Loss: 0.4681 \tQuestion Loss: 877.9940\n",
      "Batch: 3 \t Epoch : 12\tNet Loss: 750.0541 \tAnswer Loss: 0.6454 \tQuestion Loss: 749.4088\n",
      "Batch: 4 \t Epoch : 12\tNet Loss: 811.6903 \tAnswer Loss: 0.2863 \tQuestion Loss: 811.4040\n",
      "Batch: 5 \t Epoch : 12\tNet Loss: 924.4518 \tAnswer Loss: 0.4548 \tQuestion Loss: 923.9969\n",
      "Batch: 6 \t Epoch : 12\tNet Loss: 900.6373 \tAnswer Loss: 0.4830 \tQuestion Loss: 900.1542\n",
      "Batch: 7 \t Epoch : 12\tNet Loss: 928.9511 \tAnswer Loss: 0.3002 \tQuestion Loss: 928.6509\n",
      "Batch: 8 \t Epoch : 12\tNet Loss: 864.6218 \tAnswer Loss: 0.2628 \tQuestion Loss: 864.3590\n",
      "Batch: 9 \t Epoch : 12\tNet Loss: 793.2220 \tAnswer Loss: 0.3455 \tQuestion Loss: 792.8765\n",
      "Batch: 10 \t Epoch : 12\tNet Loss: 802.4901 \tAnswer Loss: 0.4022 \tQuestion Loss: 802.0878\n",
      "Batch: 11 \t Epoch : 12\tNet Loss: 933.1407 \tAnswer Loss: 0.3076 \tQuestion Loss: 932.8331\n",
      "Batch: 12 \t Epoch : 12\tNet Loss: 812.9998 \tAnswer Loss: 0.4316 \tQuestion Loss: 812.5682\n",
      "Batch: 13 \t Epoch : 12\tNet Loss: 792.5141 \tAnswer Loss: 0.4580 \tQuestion Loss: 792.0562\n",
      "Batch: 14 \t Epoch : 12\tNet Loss: 870.9484 \tAnswer Loss: 0.3809 \tQuestion Loss: 870.5674\n",
      "Batch: 15 \t Epoch : 12\tNet Loss: 684.4868 \tAnswer Loss: 0.3801 \tQuestion Loss: 684.1067\n",
      "Batch: 16 \t Epoch : 12\tNet Loss: 828.9730 \tAnswer Loss: 0.3800 \tQuestion Loss: 828.5929\n",
      "Batch: 17 \t Epoch : 12\tNet Loss: 828.6422 \tAnswer Loss: 0.3145 \tQuestion Loss: 828.3277\n",
      "Batch: 18 \t Epoch : 12\tNet Loss: 772.5253 \tAnswer Loss: 0.5083 \tQuestion Loss: 772.0170\n",
      "Batch: 19 \t Epoch : 12\tNet Loss: 740.6008 \tAnswer Loss: 0.3801 \tQuestion Loss: 740.2208\n",
      "Batch: 20 \t Epoch : 12\tNet Loss: 730.1290 \tAnswer Loss: 0.4092 \tQuestion Loss: 729.7198\n",
      "Batch: 21 \t Epoch : 12\tNet Loss: 846.7162 \tAnswer Loss: 0.3175 \tQuestion Loss: 846.3987\n",
      "Batch: 22 \t Epoch : 12\tNet Loss: 775.1615 \tAnswer Loss: 0.5301 \tQuestion Loss: 774.6314\n",
      "Batch: 23 \t Epoch : 12\tNet Loss: 912.5826 \tAnswer Loss: 0.3884 \tQuestion Loss: 912.1942\n",
      "Batch: 24 \t Epoch : 12\tNet Loss: 887.4662 \tAnswer Loss: 0.4841 \tQuestion Loss: 886.9821\n",
      "Batch: 25 \t Epoch : 12\tNet Loss: 888.9434 \tAnswer Loss: 0.3596 \tQuestion Loss: 888.5838\n",
      "Batch: 26 \t Epoch : 12\tNet Loss: 961.6420 \tAnswer Loss: 0.3706 \tQuestion Loss: 961.2714\n",
      "Batch: 27 \t Epoch : 12\tNet Loss: 916.5703 \tAnswer Loss: 0.4244 \tQuestion Loss: 916.1459\n",
      "Batch: 28 \t Epoch : 12\tNet Loss: 916.4515 \tAnswer Loss: 0.3866 \tQuestion Loss: 916.0649\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 29 \t Epoch : 12\tNet Loss: 826.7694 \tAnswer Loss: 0.2961 \tQuestion Loss: 826.4733\n",
      "Batch: 30 \t Epoch : 12\tNet Loss: 778.0050 \tAnswer Loss: 0.3252 \tQuestion Loss: 777.6798\n",
      "Average Loss after Epoch 12 : 812.7691\n",
      "Batch: 0 \t Epoch : 13\tNet Loss: 828.7115 \tAnswer Loss: 0.0935 \tQuestion Loss: 828.6180\n",
      "Batch: 1 \t Epoch : 13\tNet Loss: 765.1636 \tAnswer Loss: 0.3466 \tQuestion Loss: 764.8170\n",
      "Batch: 2 \t Epoch : 13\tNet Loss: 848.8610 \tAnswer Loss: 0.4680 \tQuestion Loss: 848.3929\n",
      "Batch: 3 \t Epoch : 13\tNet Loss: 725.2749 \tAnswer Loss: 0.6453 \tQuestion Loss: 724.6296\n",
      "Batch: 4 \t Epoch : 13\tNet Loss: 786.0872 \tAnswer Loss: 0.2862 \tQuestion Loss: 785.8010\n",
      "Batch: 5 \t Epoch : 13\tNet Loss: 895.7130 \tAnswer Loss: 0.4547 \tQuestion Loss: 895.2583\n",
      "Batch: 6 \t Epoch : 13\tNet Loss: 874.0981 \tAnswer Loss: 0.4830 \tQuestion Loss: 873.6151\n",
      "Batch: 7 \t Epoch : 13\tNet Loss: 899.5807 \tAnswer Loss: 0.3002 \tQuestion Loss: 899.2806\n",
      "Batch: 8 \t Epoch : 13\tNet Loss: 838.0161 \tAnswer Loss: 0.2627 \tQuestion Loss: 837.7533\n",
      "Batch: 9 \t Epoch : 13\tNet Loss: 767.5723 \tAnswer Loss: 0.3454 \tQuestion Loss: 767.2269\n",
      "Batch: 10 \t Epoch : 13\tNet Loss: 777.2557 \tAnswer Loss: 0.4022 \tQuestion Loss: 776.8536\n",
      "Batch: 11 \t Epoch : 13\tNet Loss: 904.0869 \tAnswer Loss: 0.3075 \tQuestion Loss: 903.7794\n",
      "Batch: 12 \t Epoch : 13\tNet Loss: 785.7912 \tAnswer Loss: 0.4315 \tQuestion Loss: 785.3597\n",
      "Batch: 13 \t Epoch : 13\tNet Loss: 768.2650 \tAnswer Loss: 0.4579 \tQuestion Loss: 767.8071\n",
      "Batch: 14 \t Epoch : 13\tNet Loss: 845.5449 \tAnswer Loss: 0.3809 \tQuestion Loss: 845.1641\n",
      "Batch: 15 \t Epoch : 13\tNet Loss: 663.3015 \tAnswer Loss: 0.3799 \tQuestion Loss: 662.9216\n",
      "Batch: 16 \t Epoch : 13\tNet Loss: 803.8632 \tAnswer Loss: 0.3799 \tQuestion Loss: 803.4832\n",
      "Batch: 17 \t Epoch : 13\tNet Loss: 804.4719 \tAnswer Loss: 0.3144 \tQuestion Loss: 804.1575\n",
      "Batch: 18 \t Epoch : 13\tNet Loss: 748.4202 \tAnswer Loss: 0.5082 \tQuestion Loss: 747.9120\n",
      "Batch: 19 \t Epoch : 13\tNet Loss: 717.7300 \tAnswer Loss: 0.3800 \tQuestion Loss: 717.3501\n",
      "Batch: 20 \t Epoch : 13\tNet Loss: 707.1169 \tAnswer Loss: 0.4091 \tQuestion Loss: 706.7078\n",
      "Batch: 21 \t Epoch : 13\tNet Loss: 817.5843 \tAnswer Loss: 0.3174 \tQuestion Loss: 817.2669\n",
      "Batch: 22 \t Epoch : 13\tNet Loss: 751.6740 \tAnswer Loss: 0.5300 \tQuestion Loss: 751.1440\n",
      "Batch: 23 \t Epoch : 13\tNet Loss: 884.9093 \tAnswer Loss: 0.3883 \tQuestion Loss: 884.5209\n",
      "Batch: 24 \t Epoch : 13\tNet Loss: 859.9586 \tAnswer Loss: 0.4840 \tQuestion Loss: 859.4746\n",
      "Batch: 25 \t Epoch : 13\tNet Loss: 861.8960 \tAnswer Loss: 0.3595 \tQuestion Loss: 861.5365\n",
      "Batch: 26 \t Epoch : 13\tNet Loss: 933.3270 \tAnswer Loss: 0.3705 \tQuestion Loss: 932.9565\n",
      "Batch: 27 \t Epoch : 13\tNet Loss: 886.5183 \tAnswer Loss: 0.4243 \tQuestion Loss: 886.0940\n",
      "Batch: 28 \t Epoch : 13\tNet Loss: 886.9175 \tAnswer Loss: 0.3865 \tQuestion Loss: 886.5310\n",
      "Batch: 29 \t Epoch : 13\tNet Loss: 800.8914 \tAnswer Loss: 0.2960 \tQuestion Loss: 800.5954\n",
      "Batch: 30 \t Epoch : 13\tNet Loss: 752.7089 \tAnswer Loss: 0.3251 \tQuestion Loss: 752.3839\n",
      "Average Loss after Epoch 13 : 787.2285\n",
      "Batch: 0 \t Epoch : 14\tNet Loss: 802.0938 \tAnswer Loss: 0.0935 \tQuestion Loss: 802.0002\n",
      "Batch: 1 \t Epoch : 14\tNet Loss: 739.0264 \tAnswer Loss: 0.3465 \tQuestion Loss: 738.6799\n",
      "Batch: 2 \t Epoch : 14\tNet Loss: 820.6554 \tAnswer Loss: 0.4680 \tQuestion Loss: 820.1874\n",
      "Batch: 3 \t Epoch : 14\tNet Loss: 701.2595 \tAnswer Loss: 0.6452 \tQuestion Loss: 700.6143\n",
      "Batch: 4 \t Epoch : 14\tNet Loss: 761.4478 \tAnswer Loss: 0.2861 \tQuestion Loss: 761.1617\n",
      "Batch: 5 \t Epoch : 14\tNet Loss: 868.1377 \tAnswer Loss: 0.4546 \tQuestion Loss: 867.6830\n",
      "Batch: 6 \t Epoch : 14\tNet Loss: 848.4667 \tAnswer Loss: 0.4829 \tQuestion Loss: 847.9838\n",
      "Batch: 7 \t Epoch : 14\tNet Loss: 871.2250 \tAnswer Loss: 0.3001 \tQuestion Loss: 870.9249\n",
      "Batch: 8 \t Epoch : 14\tNet Loss: 812.5486 \tAnswer Loss: 0.2627 \tQuestion Loss: 812.2859\n",
      "Batch: 9 \t Epoch : 14\tNet Loss: 742.8770 \tAnswer Loss: 0.3453 \tQuestion Loss: 742.5317\n",
      "Batch: 10 \t Epoch : 14\tNet Loss: 753.1270 \tAnswer Loss: 0.4021 \tQuestion Loss: 752.7249\n",
      "Batch: 11 \t Epoch : 14\tNet Loss: 876.2044 \tAnswer Loss: 0.3074 \tQuestion Loss: 875.8970\n",
      "Batch: 12 \t Epoch : 14\tNet Loss: 759.6664 \tAnswer Loss: 0.4314 \tQuestion Loss: 759.2350\n",
      "Batch: 13 \t Epoch : 14\tNet Loss: 744.7938 \tAnswer Loss: 0.4578 \tQuestion Loss: 744.3360\n",
      "Batch: 14 \t Epoch : 14\tNet Loss: 821.0887 \tAnswer Loss: 0.3808 \tQuestion Loss: 820.7079\n",
      "Batch: 15 \t Epoch : 14\tNet Loss: 643.0709 \tAnswer Loss: 0.3798 \tQuestion Loss: 642.6910\n",
      "Batch: 16 \t Epoch : 14\tNet Loss: 779.7068 \tAnswer Loss: 0.3799 \tQuestion Loss: 779.3269\n",
      "Batch: 17 \t Epoch : 14\tNet Loss: 781.0869 \tAnswer Loss: 0.3143 \tQuestion Loss: 780.7726\n",
      "Batch: 18 \t Epoch : 14\tNet Loss: 725.1354 \tAnswer Loss: 0.5081 \tQuestion Loss: 724.6273\n",
      "Batch: 19 \t Epoch : 14\tNet Loss: 695.7632 \tAnswer Loss: 0.3799 \tQuestion Loss: 695.3833\n",
      "Batch: 20 \t Epoch : 14\tNet Loss: 685.0371 \tAnswer Loss: 0.4090 \tQuestion Loss: 684.6281\n",
      "Batch: 21 \t Epoch : 14\tNet Loss: 789.6562 \tAnswer Loss: 0.3173 \tQuestion Loss: 789.3389\n",
      "Batch: 22 \t Epoch : 14\tNet Loss: 728.8645 \tAnswer Loss: 0.5299 \tQuestion Loss: 728.3347\n",
      "Batch: 23 \t Epoch : 14\tNet Loss: 858.0671 \tAnswer Loss: 0.3883 \tQuestion Loss: 857.6789\n",
      "Batch: 24 \t Epoch : 14\tNet Loss: 833.5284 \tAnswer Loss: 0.4839 \tQuestion Loss: 833.0446\n",
      "Batch: 25 \t Epoch : 14\tNet Loss: 835.7394 \tAnswer Loss: 0.3594 \tQuestion Loss: 835.3799\n",
      "Batch: 26 \t Epoch : 14\tNet Loss: 906.1131 \tAnswer Loss: 0.3704 \tQuestion Loss: 905.7427\n",
      "Batch: 27 \t Epoch : 14\tNet Loss: 857.4197 \tAnswer Loss: 0.4242 \tQuestion Loss: 856.9955\n",
      "Batch: 28 \t Epoch : 14\tNet Loss: 858.4026 \tAnswer Loss: 0.3864 \tQuestion Loss: 858.0162\n",
      "Batch: 29 \t Epoch : 14\tNet Loss: 776.0359 \tAnswer Loss: 0.2959 \tQuestion Loss: 775.7400\n",
      "Batch: 30 \t Epoch : 14\tNet Loss: 728.3397 \tAnswer Loss: 0.3250 \tQuestion Loss: 728.0147\n",
      "Average Loss after Epoch 14 : 762.6433\n",
      "Batch: 0 \t Epoch : 15\tNet Loss: 776.4379 \tAnswer Loss: 0.0935 \tQuestion Loss: 776.3444\n",
      "Batch: 1 \t Epoch : 15\tNet Loss: 713.8519 \tAnswer Loss: 0.3464 \tQuestion Loss: 713.5055\n",
      "Batch: 2 \t Epoch : 15\tNet Loss: 793.7407 \tAnswer Loss: 0.4679 \tQuestion Loss: 793.2728\n",
      "Batch: 3 \t Epoch : 15\tNet Loss: 677.9326 \tAnswer Loss: 0.6451 \tQuestion Loss: 677.2875\n",
      "Batch: 4 \t Epoch : 15\tNet Loss: 737.6653 \tAnswer Loss: 0.2861 \tQuestion Loss: 737.3792\n",
      "Batch: 5 \t Epoch : 15\tNet Loss: 841.6123 \tAnswer Loss: 0.4546 \tQuestion Loss: 841.1578\n",
      "Batch: 6 \t Epoch : 15\tNet Loss: 823.6620 \tAnswer Loss: 0.4828 \tQuestion Loss: 823.1792\n",
      "Batch: 7 \t Epoch : 15\tNet Loss: 843.7939 \tAnswer Loss: 0.3000 \tQuestion Loss: 843.4940\n",
      "Batch: 8 \t Epoch : 15\tNet Loss: 788.1092 \tAnswer Loss: 0.2626 \tQuestion Loss: 787.8466\n",
      "Batch: 9 \t Epoch : 15\tNet Loss: 719.0317 \tAnswer Loss: 0.3452 \tQuestion Loss: 718.6865\n",
      "Batch: 10 \t Epoch : 15\tNet Loss: 730.0001 \tAnswer Loss: 0.4020 \tQuestion Loss: 729.5981\n",
      "Batch: 11 \t Epoch : 15\tNet Loss: 849.3881 \tAnswer Loss: 0.3073 \tQuestion Loss: 849.0807\n",
      "Batch: 12 \t Epoch : 15\tNet Loss: 734.5571 \tAnswer Loss: 0.4313 \tQuestion Loss: 734.1258\n",
      "Batch: 13 \t Epoch : 15\tNet Loss: 721.9979 \tAnswer Loss: 0.4577 \tQuestion Loss: 721.5401\n",
      "Batch: 14 \t Epoch : 15\tNet Loss: 797.4739 \tAnswer Loss: 0.3808 \tQuestion Loss: 797.0931\n",
      "Batch: 15 \t Epoch : 15\tNet Loss: 623.6890 \tAnswer Loss: 0.3797 \tQuestion Loss: 623.3093\n",
      "Batch: 16 \t Epoch : 15\tNet Loss: 756.3957 \tAnswer Loss: 0.3798 \tQuestion Loss: 756.0159\n",
      "Batch: 17 \t Epoch : 15\tNet Loss: 758.4128 \tAnswer Loss: 0.3142 \tQuestion Loss: 758.0986\n",
      "Batch: 18 \t Epoch : 15\tNet Loss: 702.6171 \tAnswer Loss: 0.5080 \tQuestion Loss: 702.1091\n",
      "Batch: 19 \t Epoch : 15\tNet Loss: 674.5913 \tAnswer Loss: 0.3798 \tQuestion Loss: 674.2115\n",
      "Batch: 20 \t Epoch : 15\tNet Loss: 663.7766 \tAnswer Loss: 0.4090 \tQuestion Loss: 663.3676\n",
      "Batch: 21 \t Epoch : 15\tNet Loss: 762.7758 \tAnswer Loss: 0.3172 \tQuestion Loss: 762.4586\n",
      "Batch: 22 \t Epoch : 15\tNet Loss: 706.6509 \tAnswer Loss: 0.5298 \tQuestion Loss: 706.1211\n",
      "Batch: 23 \t Epoch : 15\tNet Loss: 831.9767 \tAnswer Loss: 0.3882 \tQuestion Loss: 831.5885\n",
      "Batch: 24 \t Epoch : 15\tNet Loss: 808.0651 \tAnswer Loss: 0.4838 \tQuestion Loss: 807.5814\n",
      "Batch: 25 \t Epoch : 15\tNet Loss: 810.3768 \tAnswer Loss: 0.3593 \tQuestion Loss: 810.0175\n",
      "Batch: 26 \t Epoch : 15\tNet Loss: 879.8983 \tAnswer Loss: 0.3703 \tQuestion Loss: 879.5280\n",
      "Batch: 27 \t Epoch : 15\tNet Loss: 829.1924 \tAnswer Loss: 0.4242 \tQuestion Loss: 828.7682\n",
      "Batch: 28 \t Epoch : 15\tNet Loss: 830.8205 \tAnswer Loss: 0.3863 \tQuestion Loss: 830.4341\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 29 \t Epoch : 15\tNet Loss: 752.0961 \tAnswer Loss: 0.2958 \tQuestion Loss: 751.8003\n",
      "Batch: 30 \t Epoch : 15\tNet Loss: 704.8323 \tAnswer Loss: 0.3249 \tQuestion Loss: 704.5074\n",
      "Average Loss after Epoch 15 : 738.9194\n",
      "Batch: 0 \t Epoch : 16\tNet Loss: 751.6650 \tAnswer Loss: 0.0935 \tQuestion Loss: 751.5715\n",
      "Batch: 1 \t Epoch : 16\tNet Loss: 689.5443 \tAnswer Loss: 0.3463 \tQuestion Loss: 689.1980\n",
      "Batch: 2 \t Epoch : 16\tNet Loss: 768.0086 \tAnswer Loss: 0.4678 \tQuestion Loss: 767.5408\n",
      "Batch: 3 \t Epoch : 16\tNet Loss: 655.2354 \tAnswer Loss: 0.6450 \tQuestion Loss: 654.5903\n",
      "Batch: 4 \t Epoch : 16\tNet Loss: 714.6505 \tAnswer Loss: 0.2860 \tQuestion Loss: 714.3644\n",
      "Batch: 5 \t Epoch : 16\tNet Loss: 816.0278 \tAnswer Loss: 0.4545 \tQuestion Loss: 815.5733\n",
      "Batch: 6 \t Epoch : 16\tNet Loss: 799.6123 \tAnswer Loss: 0.4827 \tQuestion Loss: 799.1296\n",
      "Batch: 7 \t Epoch : 16\tNet Loss: 817.2155 \tAnswer Loss: 0.2999 \tQuestion Loss: 816.9156\n",
      "Batch: 8 \t Epoch : 16\tNet Loss: 764.5921 \tAnswer Loss: 0.2626 \tQuestion Loss: 764.3295\n",
      "Batch: 9 \t Epoch : 16\tNet Loss: 695.9572 \tAnswer Loss: 0.3451 \tQuestion Loss: 695.6121\n",
      "Batch: 10 \t Epoch : 16\tNet Loss: 707.7769 \tAnswer Loss: 0.4020 \tQuestion Loss: 707.3749\n",
      "Batch: 11 \t Epoch : 16\tNet Loss: 823.5367 \tAnswer Loss: 0.3072 \tQuestion Loss: 823.2295\n",
      "Batch: 12 \t Epoch : 16\tNet Loss: 710.4008 \tAnswer Loss: 0.4312 \tQuestion Loss: 709.9695\n",
      "Batch: 13 \t Epoch : 16\tNet Loss: 699.8004 \tAnswer Loss: 0.4577 \tQuestion Loss: 699.3428\n",
      "Batch: 14 \t Epoch : 16\tNet Loss: 774.6216 \tAnswer Loss: 0.3807 \tQuestion Loss: 774.2409\n",
      "Batch: 15 \t Epoch : 16\tNet Loss: 605.0599 \tAnswer Loss: 0.3796 \tQuestion Loss: 604.6803\n",
      "Batch: 16 \t Epoch : 16\tNet Loss: 733.8437 \tAnswer Loss: 0.3797 \tQuestion Loss: 733.4640\n",
      "Batch: 17 \t Epoch : 16\tNet Loss: 736.3860 \tAnswer Loss: 0.3142 \tQuestion Loss: 736.0719\n",
      "Batch: 18 \t Epoch : 16\tNet Loss: 680.8276 \tAnswer Loss: 0.5079 \tQuestion Loss: 680.3198\n",
      "Batch: 19 \t Epoch : 16\tNet Loss: 654.1223 \tAnswer Loss: 0.3797 \tQuestion Loss: 653.7426\n",
      "Batch: 20 \t Epoch : 16\tNet Loss: 643.2393 \tAnswer Loss: 0.4089 \tQuestion Loss: 642.8304\n",
      "Batch: 21 \t Epoch : 16\tNet Loss: 736.8231 \tAnswer Loss: 0.3172 \tQuestion Loss: 736.5060\n",
      "Batch: 22 \t Epoch : 16\tNet Loss: 684.9655 \tAnswer Loss: 0.5297 \tQuestion Loss: 684.4358\n",
      "Batch: 23 \t Epoch : 16\tNet Loss: 806.5839 \tAnswer Loss: 0.3881 \tQuestion Loss: 806.1958\n",
      "Batch: 24 \t Epoch : 16\tNet Loss: 783.4648 \tAnswer Loss: 0.4837 \tQuestion Loss: 782.9811\n",
      "Batch: 25 \t Epoch : 16\tNet Loss: 785.7407 \tAnswer Loss: 0.3593 \tQuestion Loss: 785.3815\n",
      "Batch: 26 \t Epoch : 16\tNet Loss: 854.5811 \tAnswer Loss: 0.3703 \tQuestion Loss: 854.2108\n",
      "Batch: 27 \t Epoch : 16\tNet Loss: 801.7722 \tAnswer Loss: 0.4241 \tQuestion Loss: 801.3481\n",
      "Batch: 28 \t Epoch : 16\tNet Loss: 804.1043 \tAnswer Loss: 0.3863 \tQuestion Loss: 803.7180\n",
      "Batch: 29 \t Epoch : 16\tNet Loss: 728.9821 \tAnswer Loss: 0.2958 \tQuestion Loss: 728.6863\n",
      "Batch: 30 \t Epoch : 16\tNet Loss: 682.1231 \tAnswer Loss: 0.3249 \tQuestion Loss: 681.7983\n",
      "Average Loss after Epoch 16 : 715.9770\n",
      "Batch: 0 \t Epoch : 17\tNet Loss: 727.7033 \tAnswer Loss: 0.0935 \tQuestion Loss: 727.6098\n",
      "Batch: 1 \t Epoch : 17\tNet Loss: 666.0303 \tAnswer Loss: 0.3462 \tQuestion Loss: 665.6841\n",
      "Batch: 2 \t Epoch : 17\tNet Loss: 743.3492 \tAnswer Loss: 0.4677 \tQuestion Loss: 742.8815\n",
      "Batch: 3 \t Epoch : 17\tNet Loss: 633.1200 \tAnswer Loss: 0.6450 \tQuestion Loss: 632.4750\n",
      "Batch: 4 \t Epoch : 17\tNet Loss: 692.3301 \tAnswer Loss: 0.2859 \tQuestion Loss: 692.0441\n",
      "Batch: 5 \t Epoch : 17\tNet Loss: 791.2866 \tAnswer Loss: 0.4544 \tQuestion Loss: 790.8322\n",
      "Batch: 6 \t Epoch : 17\tNet Loss: 776.2536 \tAnswer Loss: 0.4827 \tQuestion Loss: 775.7709\n",
      "Batch: 7 \t Epoch : 17\tNet Loss: 791.4292 \tAnswer Loss: 0.2999 \tQuestion Loss: 791.1293\n",
      "Batch: 8 \t Epoch : 17\tNet Loss: 741.8947 \tAnswer Loss: 0.2625 \tQuestion Loss: 741.6321\n",
      "Batch: 9 \t Epoch : 17\tNet Loss: 673.5955 \tAnswer Loss: 0.3451 \tQuestion Loss: 673.2505\n",
      "Batch: 10 \t Epoch : 17\tNet Loss: 686.3665 \tAnswer Loss: 0.4019 \tQuestion Loss: 685.9646\n",
      "Batch: 11 \t Epoch : 17\tNet Loss: 798.5541 \tAnswer Loss: 0.3071 \tQuestion Loss: 798.2469\n",
      "Batch: 12 \t Epoch : 17\tNet Loss: 687.1364 \tAnswer Loss: 0.4312 \tQuestion Loss: 686.7052\n",
      "Batch: 13 \t Epoch : 17\tNet Loss: 678.1407 \tAnswer Loss: 0.4576 \tQuestion Loss: 677.6832\n",
      "Batch: 14 \t Epoch : 17\tNet Loss: 752.4697 \tAnswer Loss: 0.3807 \tQuestion Loss: 752.0890\n",
      "Batch: 15 \t Epoch : 17\tNet Loss: 587.0865 \tAnswer Loss: 0.3796 \tQuestion Loss: 586.7069\n",
      "Batch: 16 \t Epoch : 17\tNet Loss: 711.9805 \tAnswer Loss: 0.3796 \tQuestion Loss: 711.6010\n",
      "Batch: 17 \t Epoch : 17\tNet Loss: 714.9472 \tAnswer Loss: 0.3141 \tQuestion Loss: 714.6331\n",
      "Batch: 18 \t Epoch : 17\tNet Loss: 659.7309 \tAnswer Loss: 0.5078 \tQuestion Loss: 659.2231\n",
      "Batch: 19 \t Epoch : 17\tNet Loss: 634.2778 \tAnswer Loss: 0.3796 \tQuestion Loss: 633.8981\n",
      "Batch: 20 \t Epoch : 17\tNet Loss: 623.3389 \tAnswer Loss: 0.4088 \tQuestion Loss: 622.9301\n",
      "Batch: 21 \t Epoch : 17\tNet Loss: 711.7109 \tAnswer Loss: 0.3171 \tQuestion Loss: 711.3939\n",
      "Batch: 22 \t Epoch : 17\tNet Loss: 663.7581 \tAnswer Loss: 0.5296 \tQuestion Loss: 663.2285\n",
      "Batch: 23 \t Epoch : 17\tNet Loss: 781.8407 \tAnswer Loss: 0.3880 \tQuestion Loss: 781.4527\n",
      "Batch: 24 \t Epoch : 17\tNet Loss: 759.6321 \tAnswer Loss: 0.4836 \tQuestion Loss: 759.1486\n",
      "Batch: 25 \t Epoch : 17\tNet Loss: 761.7804 \tAnswer Loss: 0.3592 \tQuestion Loss: 761.4212\n",
      "Batch: 26 \t Epoch : 17\tNet Loss: 830.0605 \tAnswer Loss: 0.3702 \tQuestion Loss: 829.6903\n",
      "Batch: 27 \t Epoch : 17\tNet Loss: 775.1051 \tAnswer Loss: 0.4241 \tQuestion Loss: 774.6810\n",
      "Batch: 28 \t Epoch : 17\tNet Loss: 778.1940 \tAnswer Loss: 0.3862 \tQuestion Loss: 777.8077\n",
      "Batch: 29 \t Epoch : 17\tNet Loss: 706.6148 \tAnswer Loss: 0.2957 \tQuestion Loss: 706.3191\n",
      "Batch: 30 \t Epoch : 17\tNet Loss: 660.1508 \tAnswer Loss: 0.3248 \tQuestion Loss: 659.8260\n",
      "Average Loss after Epoch 17 : 693.7459\n",
      "Batch: 0 \t Epoch : 18\tNet Loss: 704.4823 \tAnswer Loss: 0.0935 \tQuestion Loss: 704.3888\n",
      "Batch: 1 \t Epoch : 18\tNet Loss: 643.2540 \tAnswer Loss: 0.3462 \tQuestion Loss: 642.9078\n",
      "Batch: 2 \t Epoch : 18\tNet Loss: 719.6529 \tAnswer Loss: 0.4677 \tQuestion Loss: 719.1852\n",
      "Batch: 3 \t Epoch : 18\tNet Loss: 611.5496 \tAnswer Loss: 0.6449 \tQuestion Loss: 610.9047\n",
      "Batch: 4 \t Epoch : 18\tNet Loss: 670.6426 \tAnswer Loss: 0.2859 \tQuestion Loss: 670.3567\n",
      "Batch: 5 \t Epoch : 18\tNet Loss: 767.3008 \tAnswer Loss: 0.4543 \tQuestion Loss: 766.8465\n",
      "Batch: 6 \t Epoch : 18\tNet Loss: 753.5278 \tAnswer Loss: 0.4826 \tQuestion Loss: 753.0452\n",
      "Batch: 7 \t Epoch : 18\tNet Loss: 766.3762 \tAnswer Loss: 0.2998 \tQuestion Loss: 766.0764\n",
      "Batch: 8 \t Epoch : 18\tNet Loss: 719.9209 \tAnswer Loss: 0.2625 \tQuestion Loss: 719.6584\n",
      "Batch: 9 \t Epoch : 18\tNet Loss: 651.8975 \tAnswer Loss: 0.3450 \tQuestion Loss: 651.5525\n",
      "Batch: 10 \t Epoch : 18\tNet Loss: 665.6893 \tAnswer Loss: 0.4018 \tQuestion Loss: 665.2875\n",
      "Batch: 11 \t Epoch : 18\tNet Loss: 774.3500 \tAnswer Loss: 0.3071 \tQuestion Loss: 774.0429\n",
      "Batch: 12 \t Epoch : 18\tNet Loss: 664.7036 \tAnswer Loss: 0.4311 \tQuestion Loss: 664.2725\n",
      "Batch: 13 \t Epoch : 18\tNet Loss: 656.9690 \tAnswer Loss: 0.4575 \tQuestion Loss: 656.5115\n",
      "Batch: 14 \t Epoch : 18\tNet Loss: 730.9625 \tAnswer Loss: 0.3806 \tQuestion Loss: 730.5818\n",
      "Batch: 15 \t Epoch : 18\tNet Loss: 569.6780 \tAnswer Loss: 0.3795 \tQuestion Loss: 569.2985\n",
      "Batch: 16 \t Epoch : 18\tNet Loss: 690.7480 \tAnswer Loss: 0.3795 \tQuestion Loss: 690.3685\n",
      "Batch: 17 \t Epoch : 18\tNet Loss: 694.0466 \tAnswer Loss: 0.3140 \tQuestion Loss: 693.7325\n",
      "Batch: 18 \t Epoch : 18\tNet Loss: 639.2940 \tAnswer Loss: 0.5076 \tQuestion Loss: 638.7864\n",
      "Batch: 19 \t Epoch : 18\tNet Loss: 614.9941 \tAnswer Loss: 0.3796 \tQuestion Loss: 614.6146\n",
      "Batch: 20 \t Epoch : 18\tNet Loss: 603.9988 \tAnswer Loss: 0.4088 \tQuestion Loss: 603.5901\n",
      "Batch: 21 \t Epoch : 18\tNet Loss: 687.3803 \tAnswer Loss: 0.3170 \tQuestion Loss: 687.0633\n",
      "Batch: 22 \t Epoch : 18\tNet Loss: 642.9899 \tAnswer Loss: 0.5295 \tQuestion Loss: 642.4603\n",
      "Batch: 23 \t Epoch : 18\tNet Loss: 757.7020 \tAnswer Loss: 0.3879 \tQuestion Loss: 757.3141\n",
      "Batch: 24 \t Epoch : 18\tNet Loss: 736.4768 \tAnswer Loss: 0.4835 \tQuestion Loss: 735.9933\n",
      "Batch: 25 \t Epoch : 18\tNet Loss: 738.4571 \tAnswer Loss: 0.3592 \tQuestion Loss: 738.0980\n",
      "Batch: 26 \t Epoch : 18\tNet Loss: 806.2486 \tAnswer Loss: 0.3701 \tQuestion Loss: 805.8785\n",
      "Batch: 27 \t Epoch : 18\tNet Loss: 749.1461 \tAnswer Loss: 0.4240 \tQuestion Loss: 748.7221\n",
      "Batch: 28 \t Epoch : 18\tNet Loss: 753.0357 \tAnswer Loss: 0.3861 \tQuestion Loss: 752.6496\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 29 \t Epoch : 18\tNet Loss: 684.9265 \tAnswer Loss: 0.2956 \tQuestion Loss: 684.6309\n",
      "Batch: 30 \t Epoch : 18\tNet Loss: 638.8588 \tAnswer Loss: 0.3247 \tQuestion Loss: 638.5341\n",
      "Average Loss after Epoch 18 : 672.1644\n",
      "Batch: 0 \t Epoch : 19\tNet Loss: 681.9343 \tAnswer Loss: 0.0935 \tQuestion Loss: 681.8408\n",
      "Batch: 1 \t Epoch : 19\tNet Loss: 621.1736 \tAnswer Loss: 0.3461 \tQuestion Loss: 620.8275\n",
      "Batch: 2 \t Epoch : 19\tNet Loss: 696.8262 \tAnswer Loss: 0.4676 \tQuestion Loss: 696.3586\n",
      "Batch: 3 \t Epoch : 19\tNet Loss: 590.4977 \tAnswer Loss: 0.6448 \tQuestion Loss: 589.8530\n",
      "Batch: 4 \t Epoch : 19\tNet Loss: 649.5316 \tAnswer Loss: 0.2858 \tQuestion Loss: 649.2457\n",
      "Batch: 5 \t Epoch : 19\tNet Loss: 744.0009 \tAnswer Loss: 0.4542 \tQuestion Loss: 743.5467\n",
      "Batch: 6 \t Epoch : 19\tNet Loss: 731.3901 \tAnswer Loss: 0.4825 \tQuestion Loss: 730.9076\n",
      "Batch: 7 \t Epoch : 19\tNet Loss: 742.0057 \tAnswer Loss: 0.2997 \tQuestion Loss: 741.7059\n",
      "Batch: 8 \t Epoch : 19\tNet Loss: 698.5828 \tAnswer Loss: 0.2624 \tQuestion Loss: 698.3204\n",
      "Batch: 9 \t Epoch : 19\tNet Loss: 630.8251 \tAnswer Loss: 0.3449 \tQuestion Loss: 630.4802\n",
      "Batch: 10 \t Epoch : 19\tNet Loss: 645.6771 \tAnswer Loss: 0.4018 \tQuestion Loss: 645.2753\n",
      "Batch: 11 \t Epoch : 19\tNet Loss: 750.8417 \tAnswer Loss: 0.3070 \tQuestion Loss: 750.5347\n",
      "Batch: 12 \t Epoch : 19\tNet Loss: 643.0495 \tAnswer Loss: 0.4310 \tQuestion Loss: 642.6185\n",
      "Batch: 13 \t Epoch : 19\tNet Loss: 636.2478 \tAnswer Loss: 0.4574 \tQuestion Loss: 635.7903\n",
      "Batch: 14 \t Epoch : 19\tNet Loss: 710.0506 \tAnswer Loss: 0.3806 \tQuestion Loss: 709.6700\n",
      "Batch: 15 \t Epoch : 19\tNet Loss: 552.7559 \tAnswer Loss: 0.3794 \tQuestion Loss: 552.3765\n",
      "Batch: 16 \t Epoch : 19\tNet Loss: 670.0983 \tAnswer Loss: 0.3794 \tQuestion Loss: 669.7189\n",
      "Batch: 17 \t Epoch : 19\tNet Loss: 673.6429 \tAnswer Loss: 0.3139 \tQuestion Loss: 673.3290\n",
      "Batch: 18 \t Epoch : 19\tNet Loss: 619.4904 \tAnswer Loss: 0.5075 \tQuestion Loss: 618.9828\n",
      "Batch: 19 \t Epoch : 19\tNet Loss: 596.2231 \tAnswer Loss: 0.3795 \tQuestion Loss: 595.8436\n",
      "Batch: 20 \t Epoch : 19\tNet Loss: 585.1561 \tAnswer Loss: 0.4087 \tQuestion Loss: 584.7474\n",
      "Batch: 21 \t Epoch : 19\tNet Loss: 663.7983 \tAnswer Loss: 0.3169 \tQuestion Loss: 663.4813\n",
      "Batch: 22 \t Epoch : 19\tNet Loss: 622.6366 \tAnswer Loss: 0.5294 \tQuestion Loss: 622.1072\n",
      "Batch: 23 \t Epoch : 19\tNet Loss: 734.1292 \tAnswer Loss: 0.3878 \tQuestion Loss: 733.7413\n",
      "Batch: 24 \t Epoch : 19\tNet Loss: 713.9255 \tAnswer Loss: 0.4834 \tQuestion Loss: 713.4421\n",
      "Batch: 25 \t Epoch : 19\tNet Loss: 715.7447 \tAnswer Loss: 0.3591 \tQuestion Loss: 715.3856\n",
      "Batch: 26 \t Epoch : 19\tNet Loss: 783.0746 \tAnswer Loss: 0.3700 \tQuestion Loss: 782.7046\n",
      "Batch: 27 \t Epoch : 19\tNet Loss: 723.8633 \tAnswer Loss: 0.4239 \tQuestion Loss: 723.4393\n",
      "Batch: 28 \t Epoch : 19\tNet Loss: 728.5862 \tAnswer Loss: 0.3861 \tQuestion Loss: 728.2001\n",
      "Batch: 29 \t Epoch : 19\tNet Loss: 663.8594 \tAnswer Loss: 0.2956 \tQuestion Loss: 663.5639\n",
      "Batch: 30 \t Epoch : 19\tNet Loss: 618.1987 \tAnswer Loss: 0.3246 \tQuestion Loss: 617.8741\n",
      "Average Loss after Epoch 19 : 651.1818\n",
      "Batch: 0 \t Epoch : 20\tNet Loss: 660.0025 \tAnswer Loss: 0.0935 \tQuestion Loss: 659.9090\n",
      "Batch: 1 \t Epoch : 20\tNet Loss: 599.7601 \tAnswer Loss: 0.3460 \tQuestion Loss: 599.4141\n",
      "Batch: 2 \t Epoch : 20\tNet Loss: 674.7921 \tAnswer Loss: 0.4676 \tQuestion Loss: 674.3246\n",
      "Batch: 3 \t Epoch : 20\tNet Loss: 569.9434 \tAnswer Loss: 0.6447 \tQuestion Loss: 569.2986\n",
      "Batch: 4 \t Epoch : 20\tNet Loss: 628.9526 \tAnswer Loss: 0.2857 \tQuestion Loss: 628.6668\n",
      "Batch: 5 \t Epoch : 20\tNet Loss: 721.3332 \tAnswer Loss: 0.4542 \tQuestion Loss: 720.8790\n",
      "Batch: 6 \t Epoch : 20\tNet Loss: 709.8088 \tAnswer Loss: 0.4825 \tQuestion Loss: 709.3264\n",
      "Batch: 7 \t Epoch : 20\tNet Loss: 718.2787 \tAnswer Loss: 0.2997 \tQuestion Loss: 717.9791\n",
      "Batch: 8 \t Epoch : 20\tNet Loss: 677.8105 \tAnswer Loss: 0.2624 \tQuestion Loss: 677.5481\n",
      "Batch: 9 \t Epoch : 20\tNet Loss: 610.3485 \tAnswer Loss: 0.3448 \tQuestion Loss: 610.0037\n",
      "Batch: 10 \t Epoch : 20\tNet Loss: 626.2773 \tAnswer Loss: 0.4017 \tQuestion Loss: 625.8756\n",
      "Batch: 11 \t Epoch : 20\tNet Loss: 727.9600 \tAnswer Loss: 0.3069 \tQuestion Loss: 727.6530\n",
      "Batch: 12 \t Epoch : 20\tNet Loss: 622.1309 \tAnswer Loss: 0.4309 \tQuestion Loss: 621.7000\n",
      "Batch: 13 \t Epoch : 20\tNet Loss: 615.9503 \tAnswer Loss: 0.4574 \tQuestion Loss: 615.4929\n",
      "Batch: 14 \t Epoch : 20\tNet Loss: 689.6837 \tAnswer Loss: 0.3806 \tQuestion Loss: 689.3032\n",
      "Batch: 15 \t Epoch : 20\tNet Loss: 536.2535 \tAnswer Loss: 0.3793 \tQuestion Loss: 535.8742\n",
      "Batch: 16 \t Epoch : 20\tNet Loss: 649.9999 \tAnswer Loss: 0.3794 \tQuestion Loss: 649.6205\n",
      "Batch: 17 \t Epoch : 20\tNet Loss: 653.7085 \tAnswer Loss: 0.3139 \tQuestion Loss: 653.3947\n",
      "Batch: 18 \t Epoch : 20\tNet Loss: 600.2936 \tAnswer Loss: 0.5074 \tQuestion Loss: 599.7861\n",
      "Batch: 19 \t Epoch : 20\tNet Loss: 577.9282 \tAnswer Loss: 0.3794 \tQuestion Loss: 577.5488\n",
      "Batch: 20 \t Epoch : 20\tNet Loss: 566.7622 \tAnswer Loss: 0.4087 \tQuestion Loss: 566.3536\n",
      "Batch: 21 \t Epoch : 20\tNet Loss: 640.9479 \tAnswer Loss: 0.3169 \tQuestion Loss: 640.6310\n",
      "Batch: 22 \t Epoch : 20\tNet Loss: 602.6859 \tAnswer Loss: 0.5293 \tQuestion Loss: 602.1566\n",
      "Batch: 23 \t Epoch : 20\tNet Loss: 711.0984 \tAnswer Loss: 0.3877 \tQuestion Loss: 710.7107\n",
      "Batch: 24 \t Epoch : 20\tNet Loss: 691.9193 \tAnswer Loss: 0.4833 \tQuestion Loss: 691.4360\n",
      "Batch: 25 \t Epoch : 20\tNet Loss: 693.6255 \tAnswer Loss: 0.3590 \tQuestion Loss: 693.2665\n",
      "Batch: 26 \t Epoch : 20\tNet Loss: 760.4843 \tAnswer Loss: 0.3700 \tQuestion Loss: 760.1143\n",
      "Batch: 27 \t Epoch : 20\tNet Loss: 699.2361 \tAnswer Loss: 0.4239 \tQuestion Loss: 698.8123\n",
      "Batch: 28 \t Epoch : 20\tNet Loss: 704.8099 \tAnswer Loss: 0.3860 \tQuestion Loss: 704.4240\n",
      "Batch: 29 \t Epoch : 20\tNet Loss: 643.3695 \tAnswer Loss: 0.2955 \tQuestion Loss: 643.0740\n",
      "Batch: 30 \t Epoch : 20\tNet Loss: 598.1340 \tAnswer Loss: 0.3245 \tQuestion Loss: 597.8094\n",
      "Average Loss after Epoch 20 : 630.7590\n",
      "Batch: 0 \t Epoch : 21\tNet Loss: 638.6469 \tAnswer Loss: 0.0935 \tQuestion Loss: 638.5533\n",
      "Batch: 1 \t Epoch : 21\tNet Loss: 578.9909 \tAnswer Loss: 0.3460 \tQuestion Loss: 578.6450\n",
      "Batch: 2 \t Epoch : 21\tNet Loss: 653.4922 \tAnswer Loss: 0.4675 \tQuestion Loss: 653.0247\n",
      "Batch: 3 \t Epoch : 21\tNet Loss: 549.8752 \tAnswer Loss: 0.6446 \tQuestion Loss: 549.2305\n",
      "Batch: 4 \t Epoch : 21\tNet Loss: 608.8736 \tAnswer Loss: 0.2857 \tQuestion Loss: 608.5879\n",
      "Batch: 5 \t Epoch : 21\tNet Loss: 699.2625 \tAnswer Loss: 0.4541 \tQuestion Loss: 698.8084\n",
      "Batch: 6 \t Epoch : 21\tNet Loss: 688.7581 \tAnswer Loss: 0.4824 \tQuestion Loss: 688.2757\n",
      "Batch: 7 \t Epoch : 21\tNet Loss: 695.1678 \tAnswer Loss: 0.2996 \tQuestion Loss: 694.8682\n",
      "Batch: 8 \t Epoch : 21\tNet Loss: 657.5498 \tAnswer Loss: 0.2623 \tQuestion Loss: 657.2875\n",
      "Batch: 9 \t Epoch : 21\tNet Loss: 590.4457 \tAnswer Loss: 0.3447 \tQuestion Loss: 590.1010\n",
      "Batch: 10 \t Epoch : 21\tNet Loss: 607.4443 \tAnswer Loss: 0.4017 \tQuestion Loss: 607.0426\n",
      "Batch: 11 \t Epoch : 21\tNet Loss: 705.6560 \tAnswer Loss: 0.3069 \tQuestion Loss: 705.3491\n",
      "Batch: 12 \t Epoch : 21\tNet Loss: 601.9096 \tAnswer Loss: 0.4308 \tQuestion Loss: 601.4788\n",
      "Batch: 13 \t Epoch : 21\tNet Loss: 596.0621 \tAnswer Loss: 0.4573 \tQuestion Loss: 595.6049\n",
      "Batch: 14 \t Epoch : 21\tNet Loss: 669.8130 \tAnswer Loss: 0.3805 \tQuestion Loss: 669.4325\n",
      "Batch: 15 \t Epoch : 21\tNet Loss: 520.1194 \tAnswer Loss: 0.3792 \tQuestion Loss: 519.7402\n",
      "Batch: 16 \t Epoch : 21\tNet Loss: 630.4319 \tAnswer Loss: 0.3793 \tQuestion Loss: 630.0527\n",
      "Batch: 17 \t Epoch : 21\tNet Loss: 634.2248 \tAnswer Loss: 0.3138 \tQuestion Loss: 633.9110\n",
      "Batch: 18 \t Epoch : 21\tNet Loss: 581.6774 \tAnswer Loss: 0.5073 \tQuestion Loss: 581.1700\n",
      "Batch: 19 \t Epoch : 21\tNet Loss: 560.0780 \tAnswer Loss: 0.3793 \tQuestion Loss: 559.6987\n",
      "Batch: 20 \t Epoch : 21\tNet Loss: 548.7839 \tAnswer Loss: 0.4086 \tQuestion Loss: 548.3754\n",
      "Batch: 21 \t Epoch : 21\tNet Loss: 618.8214 \tAnswer Loss: 0.3168 \tQuestion Loss: 618.5046\n",
      "Batch: 22 \t Epoch : 21\tNet Loss: 583.1346 \tAnswer Loss: 0.5293 \tQuestion Loss: 582.6054\n",
      "Batch: 23 \t Epoch : 21\tNet Loss: 688.5959 \tAnswer Loss: 0.3877 \tQuestion Loss: 688.2083\n",
      "Batch: 24 \t Epoch : 21\tNet Loss: 670.4115 \tAnswer Loss: 0.4832 \tQuestion Loss: 669.9283\n",
      "Batch: 25 \t Epoch : 21\tNet Loss: 672.0847 \tAnswer Loss: 0.3590 \tQuestion Loss: 671.7256\n",
      "Batch: 26 \t Epoch : 21\tNet Loss: 738.4337 \tAnswer Loss: 0.3699 \tQuestion Loss: 738.0638\n",
      "Batch: 27 \t Epoch : 21\tNet Loss: 675.2502 \tAnswer Loss: 0.4238 \tQuestion Loss: 674.8264\n",
      "Batch: 28 \t Epoch : 21\tNet Loss: 681.6783 \tAnswer Loss: 0.3859 \tQuestion Loss: 681.2924\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 29 \t Epoch : 21\tNet Loss: 623.4197 \tAnswer Loss: 0.2954 \tQuestion Loss: 623.1243\n",
      "Batch: 30 \t Epoch : 21\tNet Loss: 578.6365 \tAnswer Loss: 0.3244 \tQuestion Loss: 578.3121\n",
      "Average Loss after Epoch 21 : 610.8666\n",
      "Batch: 0 \t Epoch : 22\tNet Loss: 617.8361 \tAnswer Loss: 0.0935 \tQuestion Loss: 617.7426\n",
      "Batch: 1 \t Epoch : 22\tNet Loss: 558.8492 \tAnswer Loss: 0.3459 \tQuestion Loss: 558.5034\n",
      "Batch: 2 \t Epoch : 22\tNet Loss: 632.8842 \tAnswer Loss: 0.4674 \tQuestion Loss: 632.4167\n",
      "Batch: 3 \t Epoch : 22\tNet Loss: 530.2896 \tAnswer Loss: 0.6446 \tQuestion Loss: 529.6450\n",
      "Batch: 4 \t Epoch : 22\tNet Loss: 589.2732 \tAnswer Loss: 0.2856 \tQuestion Loss: 588.9875\n",
      "Batch: 5 \t Epoch : 22\tNet Loss: 677.7640 \tAnswer Loss: 0.4540 \tQuestion Loss: 677.3101\n",
      "Batch: 6 \t Epoch : 22\tNet Loss: 668.2127 \tAnswer Loss: 0.4823 \tQuestion Loss: 667.7303\n",
      "Batch: 7 \t Epoch : 22\tNet Loss: 672.6478 \tAnswer Loss: 0.2995 \tQuestion Loss: 672.3482\n",
      "Batch: 8 \t Epoch : 22\tNet Loss: 637.7611 \tAnswer Loss: 0.2623 \tQuestion Loss: 637.4988\n",
      "Batch: 9 \t Epoch : 22\tNet Loss: 571.0947 \tAnswer Loss: 0.3447 \tQuestion Loss: 570.7501\n",
      "Batch: 10 \t Epoch : 22\tNet Loss: 589.1339 \tAnswer Loss: 0.4016 \tQuestion Loss: 588.7323\n",
      "Batch: 11 \t Epoch : 22\tNet Loss: 683.8930 \tAnswer Loss: 0.3068 \tQuestion Loss: 683.5862\n",
      "Batch: 12 \t Epoch : 22\tNet Loss: 582.3516 \tAnswer Loss: 0.4308 \tQuestion Loss: 581.9208\n",
      "Batch: 13 \t Epoch : 22\tNet Loss: 576.5795 \tAnswer Loss: 0.4572 \tQuestion Loss: 576.1223\n",
      "Batch: 14 \t Epoch : 22\tNet Loss: 650.3892 \tAnswer Loss: 0.3805 \tQuestion Loss: 650.0087\n",
      "Batch: 15 \t Epoch : 22\tNet Loss: 504.3130 \tAnswer Loss: 0.3791 \tQuestion Loss: 503.9338\n",
      "Batch: 16 \t Epoch : 22\tNet Loss: 611.3798 \tAnswer Loss: 0.3792 \tQuestion Loss: 611.0006\n",
      "Batch: 17 \t Epoch : 22\tNet Loss: 615.1806 \tAnswer Loss: 0.3137 \tQuestion Loss: 614.8669\n",
      "Batch: 18 \t Epoch : 22\tNet Loss: 563.6125 \tAnswer Loss: 0.5072 \tQuestion Loss: 563.1052\n",
      "Batch: 19 \t Epoch : 22\tNet Loss: 542.6467 \tAnswer Loss: 0.3793 \tQuestion Loss: 542.2674\n",
      "Batch: 20 \t Epoch : 22\tNet Loss: 531.1993 \tAnswer Loss: 0.4085 \tQuestion Loss: 530.7908\n",
      "Batch: 21 \t Epoch : 22\tNet Loss: 597.4100 \tAnswer Loss: 0.3167 \tQuestion Loss: 597.0933\n",
      "Batch: 22 \t Epoch : 22\tNet Loss: 563.9835 \tAnswer Loss: 0.5292 \tQuestion Loss: 563.4543\n",
      "Batch: 23 \t Epoch : 22\tNet Loss: 666.6139 \tAnswer Loss: 0.3876 \tQuestion Loss: 666.2263\n",
      "Batch: 24 \t Epoch : 22\tNet Loss: 649.3679 \tAnswer Loss: 0.4831 \tQuestion Loss: 648.8848\n",
      "Batch: 25 \t Epoch : 22\tNet Loss: 651.1010 \tAnswer Loss: 0.3589 \tQuestion Loss: 650.7421\n",
      "Batch: 26 \t Epoch : 22\tNet Loss: 716.8878 \tAnswer Loss: 0.3698 \tQuestion Loss: 716.5180\n",
      "Batch: 27 \t Epoch : 22\tNet Loss: 651.8936 \tAnswer Loss: 0.4238 \tQuestion Loss: 651.4698\n",
      "Batch: 28 \t Epoch : 22\tNet Loss: 659.1712 \tAnswer Loss: 0.3858 \tQuestion Loss: 658.7854\n",
      "Batch: 29 \t Epoch : 22\tNet Loss: 603.9777 \tAnswer Loss: 0.2953 \tQuestion Loss: 603.6823\n",
      "Batch: 30 \t Epoch : 22\tNet Loss: 559.6849 \tAnswer Loss: 0.3244 \tQuestion Loss: 559.3606\n",
      "Average Loss after Epoch 22 : 591.4807\n",
      "Batch: 0 \t Epoch : 23\tNet Loss: 597.5508 \tAnswer Loss: 0.0935 \tQuestion Loss: 597.4573\n",
      "Batch: 1 \t Epoch : 23\tNet Loss: 539.3196 \tAnswer Loss: 0.3458 \tQuestion Loss: 538.9738\n",
      "Batch: 2 \t Epoch : 23\tNet Loss: 612.9298 \tAnswer Loss: 0.4674 \tQuestion Loss: 612.4624\n",
      "Batch: 3 \t Epoch : 23\tNet Loss: 511.1857 \tAnswer Loss: 0.6445 \tQuestion Loss: 510.5412\n",
      "Batch: 4 \t Epoch : 23\tNet Loss: 570.1360 \tAnswer Loss: 0.2856 \tQuestion Loss: 569.8504\n",
      "Batch: 5 \t Epoch : 23\tNet Loss: 656.8226 \tAnswer Loss: 0.4539 \tQuestion Loss: 656.3687\n",
      "Batch: 6 \t Epoch : 23\tNet Loss: 648.1447 \tAnswer Loss: 0.4823 \tQuestion Loss: 647.6624\n",
      "Batch: 7 \t Epoch : 23\tNet Loss: 650.7008 \tAnswer Loss: 0.2995 \tQuestion Loss: 650.4013\n",
      "Batch: 8 \t Epoch : 23\tNet Loss: 618.4172 \tAnswer Loss: 0.2623 \tQuestion Loss: 618.1550\n",
      "Batch: 9 \t Epoch : 23\tNet Loss: 552.2750 \tAnswer Loss: 0.3446 \tQuestion Loss: 551.9304\n",
      "Batch: 10 \t Epoch : 23\tNet Loss: 571.3075 \tAnswer Loss: 0.4016 \tQuestion Loss: 570.9059\n",
      "Batch: 11 \t Epoch : 23\tNet Loss: 662.6446 \tAnswer Loss: 0.3067 \tQuestion Loss: 662.3379\n",
      "Batch: 12 \t Epoch : 23\tNet Loss: 563.4227 \tAnswer Loss: 0.4307 \tQuestion Loss: 562.9921\n",
      "Batch: 13 \t Epoch : 23\tNet Loss: 557.5074 \tAnswer Loss: 0.4572 \tQuestion Loss: 557.0502\n",
      "Batch: 14 \t Epoch : 23\tNet Loss: 631.3684 \tAnswer Loss: 0.3804 \tQuestion Loss: 630.9880\n",
      "Batch: 15 \t Epoch : 23\tNet Loss: 488.8040 \tAnswer Loss: 0.3790 \tQuestion Loss: 488.4249\n",
      "Batch: 16 \t Epoch : 23\tNet Loss: 592.8340 \tAnswer Loss: 0.3791 \tQuestion Loss: 592.4548\n",
      "Batch: 17 \t Epoch : 23\tNet Loss: 596.5696 \tAnswer Loss: 0.3137 \tQuestion Loss: 596.2559\n",
      "Batch: 18 \t Epoch : 23\tNet Loss: 546.0710 \tAnswer Loss: 0.5071 \tQuestion Loss: 545.5638\n",
      "Batch: 19 \t Epoch : 23\tNet Loss: 525.6132 \tAnswer Loss: 0.3792 \tQuestion Loss: 525.2341\n",
      "Batch: 20 \t Epoch : 23\tNet Loss: 513.9957 \tAnswer Loss: 0.4085 \tQuestion Loss: 513.5873\n",
      "Batch: 21 \t Epoch : 23\tNet Loss: 576.7019 \tAnswer Loss: 0.3166 \tQuestion Loss: 576.3853\n",
      "Batch: 22 \t Epoch : 23\tNet Loss: 545.2347 \tAnswer Loss: 0.5291 \tQuestion Loss: 544.7056\n",
      "Batch: 23 \t Epoch : 23\tNet Loss: 645.1462 \tAnswer Loss: 0.3875 \tQuestion Loss: 644.7587\n",
      "Batch: 24 \t Epoch : 23\tNet Loss: 628.7643 \tAnswer Loss: 0.4830 \tQuestion Loss: 628.2813\n",
      "Batch: 25 \t Epoch : 23\tNet Loss: 630.6550 \tAnswer Loss: 0.3589 \tQuestion Loss: 630.2961\n",
      "Batch: 26 \t Epoch : 23\tNet Loss: 695.8160 \tAnswer Loss: 0.3697 \tQuestion Loss: 695.4463\n",
      "Batch: 27 \t Epoch : 23\tNet Loss: 629.1541 \tAnswer Loss: 0.4237 \tQuestion Loss: 628.7304\n",
      "Batch: 28 \t Epoch : 23\tNet Loss: 637.2693 \tAnswer Loss: 0.3858 \tQuestion Loss: 636.8835\n",
      "Batch: 29 \t Epoch : 23\tNet Loss: 585.0128 \tAnswer Loss: 0.2953 \tQuestion Loss: 584.7175\n",
      "Batch: 30 \t Epoch : 23\tNet Loss: 541.2584 \tAnswer Loss: 0.3243 \tQuestion Loss: 540.9341\n",
      "Average Loss after Epoch 23 : 572.5823\n",
      "Batch: 0 \t Epoch : 24\tNet Loss: 577.7734 \tAnswer Loss: 0.0934 \tQuestion Loss: 577.6800\n",
      "Batch: 1 \t Epoch : 24\tNet Loss: 520.3852 \tAnswer Loss: 0.3457 \tQuestion Loss: 520.0394\n",
      "Batch: 2 \t Epoch : 24\tNet Loss: 593.5971 \tAnswer Loss: 0.4673 \tQuestion Loss: 593.1298\n",
      "Batch: 3 \t Epoch : 24\tNet Loss: 492.5635 \tAnswer Loss: 0.6444 \tQuestion Loss: 491.9191\n",
      "Batch: 4 \t Epoch : 24\tNet Loss: 551.4540 \tAnswer Loss: 0.2855 \tQuestion Loss: 551.1685\n",
      "Batch: 5 \t Epoch : 24\tNet Loss: 636.4241 \tAnswer Loss: 0.4539 \tQuestion Loss: 635.9703\n",
      "Batch: 6 \t Epoch : 24\tNet Loss: 628.5287 \tAnswer Loss: 0.4822 \tQuestion Loss: 628.0464\n",
      "Batch: 7 \t Epoch : 24\tNet Loss: 629.3069 \tAnswer Loss: 0.2994 \tQuestion Loss: 629.0075\n",
      "Batch: 8 \t Epoch : 24\tNet Loss: 599.4982 \tAnswer Loss: 0.2622 \tQuestion Loss: 599.2360\n",
      "Batch: 9 \t Epoch : 24\tNet Loss: 533.9620 \tAnswer Loss: 0.3445 \tQuestion Loss: 533.6176\n",
      "Batch: 10 \t Epoch : 24\tNet Loss: 553.9295 \tAnswer Loss: 0.4015 \tQuestion Loss: 553.5280\n",
      "Batch: 11 \t Epoch : 24\tNet Loss: 641.8936 \tAnswer Loss: 0.3067 \tQuestion Loss: 641.5870\n",
      "Batch: 12 \t Epoch : 24\tNet Loss: 545.0885 \tAnswer Loss: 0.4306 \tQuestion Loss: 544.6579\n",
      "Batch: 13 \t Epoch : 24\tNet Loss: 538.8563 \tAnswer Loss: 0.4571 \tQuestion Loss: 538.3992\n",
      "Batch: 14 \t Epoch : 24\tNet Loss: 612.7137 \tAnswer Loss: 0.3804 \tQuestion Loss: 612.3333\n",
      "Batch: 15 \t Epoch : 24\tNet Loss: 473.5740 \tAnswer Loss: 0.3790 \tQuestion Loss: 473.1951\n",
      "Batch: 16 \t Epoch : 24\tNet Loss: 574.7859 \tAnswer Loss: 0.3790 \tQuestion Loss: 574.4069\n",
      "Batch: 17 \t Epoch : 24\tNet Loss: 578.3879 \tAnswer Loss: 0.3136 \tQuestion Loss: 578.0743\n",
      "Batch: 18 \t Epoch : 24\tNet Loss: 529.0265 \tAnswer Loss: 0.5070 \tQuestion Loss: 528.5195\n",
      "Batch: 19 \t Epoch : 24\tNet Loss: 508.9635 \tAnswer Loss: 0.3791 \tQuestion Loss: 508.5844\n",
      "Batch: 20 \t Epoch : 24\tNet Loss: 497.1666 \tAnswer Loss: 0.4084 \tQuestion Loss: 496.7581\n",
      "Batch: 21 \t Epoch : 24\tNet Loss: 556.6744 \tAnswer Loss: 0.3166 \tQuestion Loss: 556.3578\n",
      "Batch: 22 \t Epoch : 24\tNet Loss: 526.8923 \tAnswer Loss: 0.5290 \tQuestion Loss: 526.3633\n",
      "Batch: 23 \t Epoch : 24\tNet Loss: 624.1853 \tAnswer Loss: 0.3874 \tQuestion Loss: 623.7979\n",
      "Batch: 24 \t Epoch : 24\tNet Loss: 608.5864 \tAnswer Loss: 0.4830 \tQuestion Loss: 608.1035\n",
      "Batch: 25 \t Epoch : 24\tNet Loss: 610.7206 \tAnswer Loss: 0.3588 \tQuestion Loss: 610.3618\n",
      "Batch: 26 \t Epoch : 24\tNet Loss: 675.1920 \tAnswer Loss: 0.3697 \tQuestion Loss: 674.8223\n",
      "Batch: 27 \t Epoch : 24\tNet Loss: 607.0229 \tAnswer Loss: 0.4237 \tQuestion Loss: 606.5992\n",
      "Batch: 28 \t Epoch : 24\tNet Loss: 615.9542 \tAnswer Loss: 0.3857 \tQuestion Loss: 615.5685\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 29 \t Epoch : 24\tNet Loss: 566.4982 \tAnswer Loss: 0.2952 \tQuestion Loss: 566.2029\n",
      "Batch: 30 \t Epoch : 24\tNet Loss: 523.3387 \tAnswer Loss: 0.3242 \tQuestion Loss: 523.0145\n",
      "Average Loss after Epoch 24 : 554.1545\n",
      "Batch: 0 \t Epoch : 25\tNet Loss: 558.4915 \tAnswer Loss: 0.0934 \tQuestion Loss: 558.3981\n",
      "Batch: 1 \t Epoch : 25\tNet Loss: 502.0269 \tAnswer Loss: 0.3457 \tQuestion Loss: 501.6813\n",
      "Batch: 2 \t Epoch : 25\tNet Loss: 574.8529 \tAnswer Loss: 0.4673 \tQuestion Loss: 574.3856\n",
      "Batch: 3 \t Epoch : 25\tNet Loss: 474.4221 \tAnswer Loss: 0.6443 \tQuestion Loss: 473.7777\n",
      "Batch: 4 \t Epoch : 25\tNet Loss: 533.2161 \tAnswer Loss: 0.2855 \tQuestion Loss: 532.9307\n",
      "Batch: 5 \t Epoch : 25\tNet Loss: 616.5552 \tAnswer Loss: 0.4538 \tQuestion Loss: 616.1014\n",
      "Batch: 6 \t Epoch : 25\tNet Loss: 609.3422 \tAnswer Loss: 0.4822 \tQuestion Loss: 608.8600\n",
      "Batch: 7 \t Epoch : 25\tNet Loss: 608.4489 \tAnswer Loss: 0.2994 \tQuestion Loss: 608.1495\n",
      "Batch: 8 \t Epoch : 25\tNet Loss: 580.9918 \tAnswer Loss: 0.2622 \tQuestion Loss: 580.7297\n",
      "Batch: 9 \t Epoch : 25\tNet Loss: 516.1320 \tAnswer Loss: 0.3444 \tQuestion Loss: 515.7876\n",
      "Batch: 10 \t Epoch : 25\tNet Loss: 536.9706 \tAnswer Loss: 0.4014 \tQuestion Loss: 536.5692\n",
      "Batch: 11 \t Epoch : 25\tNet Loss: 621.6247 \tAnswer Loss: 0.3066 \tQuestion Loss: 621.3181\n",
      "Batch: 12 \t Epoch : 25\tNet Loss: 527.3134 \tAnswer Loss: 0.4305 \tQuestion Loss: 526.8828\n",
      "Batch: 13 \t Epoch : 25\tNet Loss: 520.6353 \tAnswer Loss: 0.4570 \tQuestion Loss: 520.1783\n",
      "Batch: 14 \t Epoch : 25\tNet Loss: 594.3947 \tAnswer Loss: 0.3804 \tQuestion Loss: 594.0143\n",
      "Batch: 15 \t Epoch : 25\tNet Loss: 458.6120 \tAnswer Loss: 0.3789 \tQuestion Loss: 458.2331\n",
      "Batch: 16 \t Epoch : 25\tNet Loss: 557.2230 \tAnswer Loss: 0.3790 \tQuestion Loss: 556.8440\n",
      "Batch: 17 \t Epoch : 25\tNet Loss: 560.6314 \tAnswer Loss: 0.3135 \tQuestion Loss: 560.3179\n",
      "Batch: 18 \t Epoch : 25\tNet Loss: 512.4556 \tAnswer Loss: 0.5069 \tQuestion Loss: 511.9486\n",
      "Batch: 19 \t Epoch : 25\tNet Loss: 492.6886 \tAnswer Loss: 0.3790 \tQuestion Loss: 492.3096\n",
      "Batch: 20 \t Epoch : 25\tNet Loss: 480.7088 \tAnswer Loss: 0.4084 \tQuestion Loss: 480.3004\n",
      "Batch: 21 \t Epoch : 25\tNet Loss: 537.2979 \tAnswer Loss: 0.3165 \tQuestion Loss: 536.9814\n",
      "Batch: 22 \t Epoch : 25\tNet Loss: 508.9571 \tAnswer Loss: 0.5289 \tQuestion Loss: 508.4282\n",
      "Batch: 23 \t Epoch : 25\tNet Loss: 603.7203 \tAnswer Loss: 0.3874 \tQuestion Loss: 603.3329\n",
      "Batch: 24 \t Epoch : 25\tNet Loss: 588.8255 \tAnswer Loss: 0.4829 \tQuestion Loss: 588.3426\n",
      "Batch: 25 \t Epoch : 25\tNet Loss: 591.2729 \tAnswer Loss: 0.3588 \tQuestion Loss: 590.9141\n",
      "Batch: 26 \t Epoch : 25\tNet Loss: 654.9888 \tAnswer Loss: 0.3696 \tQuestion Loss: 654.6192\n",
      "Batch: 27 \t Epoch : 25\tNet Loss: 585.4913 \tAnswer Loss: 0.4236 \tQuestion Loss: 585.0677\n",
      "Batch: 28 \t Epoch : 25\tNet Loss: 595.2074 \tAnswer Loss: 0.3856 \tQuestion Loss: 594.8218\n",
      "Batch: 29 \t Epoch : 25\tNet Loss: 548.4061 \tAnswer Loss: 0.2952 \tQuestion Loss: 548.1110\n",
      "Batch: 30 \t Epoch : 25\tNet Loss: 505.9045 \tAnswer Loss: 0.3241 \tQuestion Loss: 505.5804\n",
      "Average Loss after Epoch 25 : 536.1815\n",
      "Batch: 0 \t Epoch : 26\tNet Loss: 539.6952 \tAnswer Loss: 0.0934 \tQuestion Loss: 539.6017\n",
      "Batch: 1 \t Epoch : 26\tNet Loss: 484.2256 \tAnswer Loss: 0.3456 \tQuestion Loss: 483.8800\n",
      "Batch: 2 \t Epoch : 26\tNet Loss: 556.6637 \tAnswer Loss: 0.4673 \tQuestion Loss: 556.1965\n",
      "Batch: 3 \t Epoch : 26\tNet Loss: 456.7594 \tAnswer Loss: 0.6443 \tQuestion Loss: 456.1152\n",
      "Batch: 4 \t Epoch : 26\tNet Loss: 515.4144 \tAnswer Loss: 0.2854 \tQuestion Loss: 515.1290\n",
      "Batch: 5 \t Epoch : 26\tNet Loss: 597.2009 \tAnswer Loss: 0.4537 \tQuestion Loss: 596.7472\n",
      "Batch: 6 \t Epoch : 26\tNet Loss: 590.5680 \tAnswer Loss: 0.4821 \tQuestion Loss: 590.0859\n",
      "Batch: 7 \t Epoch : 26\tNet Loss: 588.1078 \tAnswer Loss: 0.2993 \tQuestion Loss: 587.8085\n",
      "Batch: 8 \t Epoch : 26\tNet Loss: 562.8913 \tAnswer Loss: 0.2621 \tQuestion Loss: 562.6292\n",
      "Batch: 9 \t Epoch : 26\tNet Loss: 498.7602 \tAnswer Loss: 0.3443 \tQuestion Loss: 498.4158\n",
      "Batch: 10 \t Epoch : 26\tNet Loss: 520.4058 \tAnswer Loss: 0.4014 \tQuestion Loss: 520.0044\n",
      "Batch: 11 \t Epoch : 26\tNet Loss: 601.8251 \tAnswer Loss: 0.3065 \tQuestion Loss: 601.5186\n",
      "Batch: 12 \t Epoch : 26\tNet Loss: 510.0620 \tAnswer Loss: 0.4305 \tQuestion Loss: 509.6315\n",
      "Batch: 13 \t Epoch : 26\tNet Loss: 502.8531 \tAnswer Loss: 0.4569 \tQuestion Loss: 502.3961\n",
      "Batch: 14 \t Epoch : 26\tNet Loss: 576.3869 \tAnswer Loss: 0.3803 \tQuestion Loss: 576.0066\n",
      "Batch: 15 \t Epoch : 26\tNet Loss: 443.9150 \tAnswer Loss: 0.3788 \tQuestion Loss: 443.5362\n",
      "Batch: 16 \t Epoch : 26\tNet Loss: 540.1299 \tAnswer Loss: 0.3789 \tQuestion Loss: 539.7510\n",
      "Batch: 17 \t Epoch : 26\tNet Loss: 543.2956 \tAnswer Loss: 0.3135 \tQuestion Loss: 542.9821\n",
      "Batch: 18 \t Epoch : 26\tNet Loss: 496.3364 \tAnswer Loss: 0.5069 \tQuestion Loss: 495.8295\n",
      "Batch: 19 \t Epoch : 26\tNet Loss: 476.7841 \tAnswer Loss: 0.3790 \tQuestion Loss: 476.4052\n",
      "Batch: 20 \t Epoch : 26\tNet Loss: 464.6217 \tAnswer Loss: 0.4083 \tQuestion Loss: 464.2134\n",
      "Batch: 21 \t Epoch : 26\tNet Loss: 518.5339 \tAnswer Loss: 0.3164 \tQuestion Loss: 518.2175\n",
      "Batch: 22 \t Epoch : 26\tNet Loss: 491.4259 \tAnswer Loss: 0.5288 \tQuestion Loss: 490.8970\n",
      "Batch: 23 \t Epoch : 26\tNet Loss: 583.7388 \tAnswer Loss: 0.3873 \tQuestion Loss: 583.3515\n",
      "Batch: 24 \t Epoch : 26\tNet Loss: 569.4758 \tAnswer Loss: 0.4828 \tQuestion Loss: 568.9930\n",
      "Batch: 25 \t Epoch : 26\tNet Loss: 572.2869 \tAnswer Loss: 0.3587 \tQuestion Loss: 571.9282\n",
      "Batch: 26 \t Epoch : 26\tNet Loss: 635.1814 \tAnswer Loss: 0.3695 \tQuestion Loss: 634.8119\n",
      "Batch: 27 \t Epoch : 26\tNet Loss: 564.5497 \tAnswer Loss: 0.4236 \tQuestion Loss: 564.1262\n",
      "Batch: 28 \t Epoch : 26\tNet Loss: 575.0099 \tAnswer Loss: 0.3855 \tQuestion Loss: 574.6243\n",
      "Batch: 29 \t Epoch : 26\tNet Loss: 530.7111 \tAnswer Loss: 0.2951 \tQuestion Loss: 530.4160\n",
      "Batch: 30 \t Epoch : 26\tNet Loss: 488.9359 \tAnswer Loss: 0.3240 \tQuestion Loss: 488.6118\n",
      "Average Loss after Epoch 26 : 518.6485\n",
      "Batch: 0 \t Epoch : 27\tNet Loss: 521.3745 \tAnswer Loss: 0.0934 \tQuestion Loss: 521.2811\n",
      "Batch: 1 \t Epoch : 27\tNet Loss: 466.9607 \tAnswer Loss: 0.3456 \tQuestion Loss: 466.6151\n",
      "Batch: 2 \t Epoch : 27\tNet Loss: 538.9958 \tAnswer Loss: 0.4672 \tQuestion Loss: 538.5286\n",
      "Batch: 3 \t Epoch : 27\tNet Loss: 439.5714 \tAnswer Loss: 0.6442 \tQuestion Loss: 438.9272\n",
      "Batch: 4 \t Epoch : 27\tNet Loss: 498.0398 \tAnswer Loss: 0.2854 \tQuestion Loss: 497.7545\n",
      "Batch: 5 \t Epoch : 27\tNet Loss: 578.3482 \tAnswer Loss: 0.4537 \tQuestion Loss: 577.8945\n",
      "Batch: 6 \t Epoch : 27\tNet Loss: 572.1928 \tAnswer Loss: 0.4820 \tQuestion Loss: 571.7108\n",
      "Batch: 7 \t Epoch : 27\tNet Loss: 568.2669 \tAnswer Loss: 0.2992 \tQuestion Loss: 567.9677\n",
      "Batch: 8 \t Epoch : 27\tNet Loss: 545.1942 \tAnswer Loss: 0.2621 \tQuestion Loss: 544.9321\n",
      "Batch: 9 \t Epoch : 27\tNet Loss: 481.8215 \tAnswer Loss: 0.3443 \tQuestion Loss: 481.4772\n",
      "Batch: 10 \t Epoch : 27\tNet Loss: 504.2155 \tAnswer Loss: 0.4013 \tQuestion Loss: 503.8141\n",
      "Batch: 11 \t Epoch : 27\tNet Loss: 582.4835 \tAnswer Loss: 0.3065 \tQuestion Loss: 582.1771\n",
      "Batch: 12 \t Epoch : 27\tNet Loss: 493.3009 \tAnswer Loss: 0.4304 \tQuestion Loss: 492.8705\n",
      "Batch: 13 \t Epoch : 27\tNet Loss: 485.5135 \tAnswer Loss: 0.4569 \tQuestion Loss: 485.0566\n",
      "Batch: 14 \t Epoch : 27\tNet Loss: 558.6728 \tAnswer Loss: 0.3803 \tQuestion Loss: 558.2925\n",
      "Batch: 15 \t Epoch : 27\tNet Loss: 429.4860 \tAnswer Loss: 0.3787 \tQuestion Loss: 429.1072\n",
      "Batch: 16 \t Epoch : 27\tNet Loss: 523.4877 \tAnswer Loss: 0.3788 \tQuestion Loss: 523.1089\n",
      "Batch: 17 \t Epoch : 27\tNet Loss: 526.3713 \tAnswer Loss: 0.3134 \tQuestion Loss: 526.0579\n",
      "Batch: 18 \t Epoch : 27\tNet Loss: 480.6518 \tAnswer Loss: 0.5068 \tQuestion Loss: 480.1451\n",
      "Batch: 19 \t Epoch : 27\tNet Loss: 461.2439 \tAnswer Loss: 0.3789 \tQuestion Loss: 460.8650\n",
      "Batch: 20 \t Epoch : 27\tNet Loss: 448.9047 \tAnswer Loss: 0.4082 \tQuestion Loss: 448.4964\n",
      "Batch: 21 \t Epoch : 27\tNet Loss: 500.3421 \tAnswer Loss: 0.3164 \tQuestion Loss: 500.0258\n",
      "Batch: 22 \t Epoch : 27\tNet Loss: 474.2941 \tAnswer Loss: 0.5288 \tQuestion Loss: 473.7653\n",
      "Batch: 23 \t Epoch : 27\tNet Loss: 564.2237 \tAnswer Loss: 0.3872 \tQuestion Loss: 563.8365\n",
      "Batch: 24 \t Epoch : 27\tNet Loss: 550.5300 \tAnswer Loss: 0.4827 \tQuestion Loss: 550.0472\n",
      "Batch: 25 \t Epoch : 27\tNet Loss: 553.7413 \tAnswer Loss: 0.3587 \tQuestion Loss: 553.3826\n",
      "Batch: 26 \t Epoch : 27\tNet Loss: 615.7474 \tAnswer Loss: 0.3695 \tQuestion Loss: 615.3779\n",
      "Batch: 27 \t Epoch : 27\tNet Loss: 544.1890 \tAnswer Loss: 0.4235 \tQuestion Loss: 543.7655\n",
      "Batch: 28 \t Epoch : 27\tNet Loss: 555.3453 \tAnswer Loss: 0.3855 \tQuestion Loss: 554.9599\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 29 \t Epoch : 27\tNet Loss: 513.3903 \tAnswer Loss: 0.2950 \tQuestion Loss: 513.0953\n",
      "Batch: 30 \t Epoch : 27\tNet Loss: 472.4104 \tAnswer Loss: 0.3239 \tQuestion Loss: 472.0865\n",
      "Average Loss after Epoch 27 : 501.5410\n",
      "Batch: 0 \t Epoch : 28\tNet Loss: 503.5187 \tAnswer Loss: 0.0934 \tQuestion Loss: 503.4253\n",
      "Batch: 1 \t Epoch : 28\tNet Loss: 450.2137 \tAnswer Loss: 0.3455 \tQuestion Loss: 449.8682\n",
      "Batch: 2 \t Epoch : 28\tNet Loss: 521.8163 \tAnswer Loss: 0.4672 \tQuestion Loss: 521.3491\n",
      "Batch: 3 \t Epoch : 28\tNet Loss: 422.8539 \tAnswer Loss: 0.6441 \tQuestion Loss: 422.2098\n",
      "Batch: 4 \t Epoch : 28\tNet Loss: 481.0826 \tAnswer Loss: 0.2853 \tQuestion Loss: 480.7972\n",
      "Batch: 5 \t Epoch : 28\tNet Loss: 559.9806 \tAnswer Loss: 0.4536 \tQuestion Loss: 559.5270\n",
      "Batch: 6 \t Epoch : 28\tNet Loss: 554.2052 \tAnswer Loss: 0.4820 \tQuestion Loss: 553.7232\n",
      "Batch: 7 \t Epoch : 28\tNet Loss: 548.9105 \tAnswer Loss: 0.2992 \tQuestion Loss: 548.6113\n",
      "Batch: 8 \t Epoch : 28\tNet Loss: 527.8994 \tAnswer Loss: 0.2620 \tQuestion Loss: 527.6374\n",
      "Batch: 9 \t Epoch : 28\tNet Loss: 465.2958 \tAnswer Loss: 0.3442 \tQuestion Loss: 464.9517\n",
      "Batch: 10 \t Epoch : 28\tNet Loss: 488.3832 \tAnswer Loss: 0.4013 \tQuestion Loss: 487.9819\n",
      "Batch: 11 \t Epoch : 28\tNet Loss: 563.5912 \tAnswer Loss: 0.3064 \tQuestion Loss: 563.2848\n",
      "Batch: 12 \t Epoch : 28\tNet Loss: 476.9960 \tAnswer Loss: 0.4303 \tQuestion Loss: 476.5657\n",
      "Batch: 13 \t Epoch : 28\tNet Loss: 468.6174 \tAnswer Loss: 0.4568 \tQuestion Loss: 468.1606\n",
      "Batch: 14 \t Epoch : 28\tNet Loss: 541.2374 \tAnswer Loss: 0.3802 \tQuestion Loss: 540.8572\n",
      "Batch: 15 \t Epoch : 28\tNet Loss: 415.3281 \tAnswer Loss: 0.3787 \tQuestion Loss: 414.9494\n",
      "Batch: 16 \t Epoch : 28\tNet Loss: 507.2733 \tAnswer Loss: 0.3787 \tQuestion Loss: 506.8946\n",
      "Batch: 17 \t Epoch : 28\tNet Loss: 509.8466 \tAnswer Loss: 0.3133 \tQuestion Loss: 509.5332\n",
      "Batch: 18 \t Epoch : 28\tNet Loss: 465.3849 \tAnswer Loss: 0.5067 \tQuestion Loss: 464.8782\n",
      "Batch: 19 \t Epoch : 28\tNet Loss: 446.0632 \tAnswer Loss: 0.3788 \tQuestion Loss: 445.6844\n",
      "Batch: 20 \t Epoch : 28\tNet Loss: 433.5573 \tAnswer Loss: 0.4082 \tQuestion Loss: 433.1492\n",
      "Batch: 21 \t Epoch : 28\tNet Loss: 482.6851 \tAnswer Loss: 0.3163 \tQuestion Loss: 482.3688\n",
      "Batch: 22 \t Epoch : 28\tNet Loss: 457.5576 \tAnswer Loss: 0.5287 \tQuestion Loss: 457.0289\n",
      "Batch: 23 \t Epoch : 28\tNet Loss: 545.1609 \tAnswer Loss: 0.3871 \tQuestion Loss: 544.7738\n",
      "Batch: 24 \t Epoch : 28\tNet Loss: 531.9830 \tAnswer Loss: 0.4826 \tQuestion Loss: 531.5004\n",
      "Batch: 25 \t Epoch : 28\tNet Loss: 535.6158 \tAnswer Loss: 0.3586 \tQuestion Loss: 535.2572\n",
      "Batch: 26 \t Epoch : 28\tNet Loss: 596.6663 \tAnswer Loss: 0.3694 \tQuestion Loss: 596.2969\n",
      "Batch: 27 \t Epoch : 28\tNet Loss: 524.3972 \tAnswer Loss: 0.4234 \tQuestion Loss: 523.9738\n",
      "Batch: 28 \t Epoch : 28\tNet Loss: 536.2012 \tAnswer Loss: 0.3854 \tQuestion Loss: 535.8158\n",
      "Batch: 29 \t Epoch : 28\tNet Loss: 496.4247 \tAnswer Loss: 0.2950 \tQuestion Loss: 496.1298\n",
      "Batch: 30 \t Epoch : 28\tNet Loss: 456.3087 \tAnswer Loss: 0.3239 \tQuestion Loss: 455.9848\n",
      "Average Loss after Epoch 28 : 484.8455\n",
      "Batch: 0 \t Epoch : 29\tNet Loss: 486.1148 \tAnswer Loss: 0.0934 \tQuestion Loss: 486.0214\n",
      "Batch: 1 \t Epoch : 29\tNet Loss: 433.9678 \tAnswer Loss: 0.3454 \tQuestion Loss: 433.6224\n",
      "Batch: 2 \t Epoch : 29\tNet Loss: 505.0916 \tAnswer Loss: 0.4671 \tQuestion Loss: 504.6245\n",
      "Batch: 3 \t Epoch : 29\tNet Loss: 406.6026 \tAnswer Loss: 0.6440 \tQuestion Loss: 405.9586\n",
      "Batch: 4 \t Epoch : 29\tNet Loss: 464.5335 \tAnswer Loss: 0.2853 \tQuestion Loss: 464.2482\n",
      "Batch: 5 \t Epoch : 29\tNet Loss: 542.0827 \tAnswer Loss: 0.4535 \tQuestion Loss: 541.6292\n",
      "Batch: 6 \t Epoch : 29\tNet Loss: 536.5961 \tAnswer Loss: 0.4819 \tQuestion Loss: 536.1141\n",
      "Batch: 7 \t Epoch : 29\tNet Loss: 530.0269 \tAnswer Loss: 0.2991 \tQuestion Loss: 529.7278\n",
      "Batch: 8 \t Epoch : 29\tNet Loss: 511.0070 \tAnswer Loss: 0.2620 \tQuestion Loss: 510.7450\n",
      "Batch: 9 \t Epoch : 29\tNet Loss: 449.1657 \tAnswer Loss: 0.3441 \tQuestion Loss: 448.8216\n",
      "Batch: 10 \t Epoch : 29\tNet Loss: 472.8992 \tAnswer Loss: 0.4012 \tQuestion Loss: 472.4980\n",
      "Batch: 11 \t Epoch : 29\tNet Loss: 545.1402 \tAnswer Loss: 0.3063 \tQuestion Loss: 544.8339\n",
      "Batch: 12 \t Epoch : 29\tNet Loss: 461.1185 \tAnswer Loss: 0.4302 \tQuestion Loss: 460.6883\n",
      "Batch: 13 \t Epoch : 29\tNet Loss: 452.1637 \tAnswer Loss: 0.4567 \tQuestion Loss: 451.7070\n",
      "Batch: 14 \t Epoch : 29\tNet Loss: 524.0712 \tAnswer Loss: 0.3802 \tQuestion Loss: 523.6910\n",
      "Batch: 15 \t Epoch : 29\tNet Loss: 401.4456 \tAnswer Loss: 0.3786 \tQuestion Loss: 401.0670\n",
      "Batch: 16 \t Epoch : 29\tNet Loss: 491.4657 \tAnswer Loss: 0.3787 \tQuestion Loss: 491.0871\n",
      "Batch: 17 \t Epoch : 29\tNet Loss: 493.7095 \tAnswer Loss: 0.3133 \tQuestion Loss: 493.3963\n",
      "Batch: 18 \t Epoch : 29\tNet Loss: 450.5224 \tAnswer Loss: 0.5066 \tQuestion Loss: 450.0159\n",
      "Batch: 19 \t Epoch : 29\tNet Loss: 431.2370 \tAnswer Loss: 0.3787 \tQuestion Loss: 430.8582\n",
      "Batch: 20 \t Epoch : 29\tNet Loss: 418.5817 \tAnswer Loss: 0.4081 \tQuestion Loss: 418.1736\n",
      "Batch: 21 \t Epoch : 29\tNet Loss: 465.5306 \tAnswer Loss: 0.3162 \tQuestion Loss: 465.2144\n",
      "Batch: 22 \t Epoch : 29\tNet Loss: 441.2148 \tAnswer Loss: 0.5286 \tQuestion Loss: 440.6862\n",
      "Batch: 23 \t Epoch : 29\tNet Loss: 526.5398 \tAnswer Loss: 0.3871 \tQuestion Loss: 526.1527\n",
      "Batch: 24 \t Epoch : 29\tNet Loss: 513.8298 \tAnswer Loss: 0.4826 \tQuestion Loss: 513.3473\n",
      "Batch: 25 \t Epoch : 29\tNet Loss: 517.8926 \tAnswer Loss: 0.3586 \tQuestion Loss: 517.5341\n",
      "Batch: 26 \t Epoch : 29\tNet Loss: 577.9222 \tAnswer Loss: 0.3693 \tQuestion Loss: 577.5529\n",
      "Batch: 27 \t Epoch : 29\tNet Loss: 505.1628 \tAnswer Loss: 0.4234 \tQuestion Loss: 504.7394\n",
      "Batch: 28 \t Epoch : 29\tNet Loss: 517.5643 \tAnswer Loss: 0.3853 \tQuestion Loss: 517.1790\n",
      "Batch: 29 \t Epoch : 29\tNet Loss: 479.8000 \tAnswer Loss: 0.2949 \tQuestion Loss: 479.5052\n",
      "Batch: 30 \t Epoch : 29\tNet Loss: 440.6151 \tAnswer Loss: 0.3238 \tQuestion Loss: 440.2914\n",
      "Average Loss after Epoch 29 : 468.5505\n",
      "Batch: 0 \t Epoch : 30\tNet Loss: 469.1474 \tAnswer Loss: 0.0934 \tQuestion Loss: 469.0540\n",
      "Batch: 1 \t Epoch : 30\tNet Loss: 418.2081 \tAnswer Loss: 0.3454 \tQuestion Loss: 417.8628\n",
      "Batch: 2 \t Epoch : 30\tNet Loss: 488.7925 \tAnswer Loss: 0.4671 \tQuestion Loss: 488.3255\n",
      "Batch: 3 \t Epoch : 30\tNet Loss: 390.8144 \tAnswer Loss: 0.6440 \tQuestion Loss: 390.1704\n",
      "Batch: 4 \t Epoch : 30\tNet Loss: 448.3813 \tAnswer Loss: 0.2852 \tQuestion Loss: 448.0961\n",
      "Batch: 5 \t Epoch : 30\tNet Loss: 524.6357 \tAnswer Loss: 0.4535 \tQuestion Loss: 524.1823\n",
      "Batch: 6 \t Epoch : 30\tNet Loss: 519.3597 \tAnswer Loss: 0.4819 \tQuestion Loss: 518.8779\n",
      "Batch: 7 \t Epoch : 30\tNet Loss: 511.6078 \tAnswer Loss: 0.2990 \tQuestion Loss: 511.3087\n",
      "Batch: 8 \t Epoch : 30\tNet Loss: 494.5147 \tAnswer Loss: 0.2620 \tQuestion Loss: 494.2527\n",
      "Batch: 9 \t Epoch : 30\tNet Loss: 433.4183 \tAnswer Loss: 0.3440 \tQuestion Loss: 433.0743\n",
      "Batch: 10 \t Epoch : 30\tNet Loss: 457.7573 \tAnswer Loss: 0.4012 \tQuestion Loss: 457.3561\n",
      "Batch: 11 \t Epoch : 30\tNet Loss: 527.1270 \tAnswer Loss: 0.3063 \tQuestion Loss: 526.8207\n",
      "Batch: 12 \t Epoch : 30\tNet Loss: 445.6415 \tAnswer Loss: 0.4301 \tQuestion Loss: 445.2114\n",
      "Batch: 13 \t Epoch : 30\tNet Loss: 436.1497 \tAnswer Loss: 0.4567 \tQuestion Loss: 435.6931\n",
      "Batch: 14 \t Epoch : 30\tNet Loss: 507.1687 \tAnswer Loss: 0.3802 \tQuestion Loss: 506.7885\n",
      "Batch: 15 \t Epoch : 30\tNet Loss: 387.8406 \tAnswer Loss: 0.3785 \tQuestion Loss: 387.4620\n",
      "Batch: 16 \t Epoch : 30\tNet Loss: 476.0450 \tAnswer Loss: 0.3786 \tQuestion Loss: 475.6664\n",
      "Batch: 17 \t Epoch : 30\tNet Loss: 477.9499 \tAnswer Loss: 0.3132 \tQuestion Loss: 477.6367\n",
      "Batch: 18 \t Epoch : 30\tNet Loss: 436.0532 \tAnswer Loss: 0.5065 \tQuestion Loss: 435.5467\n",
      "Batch: 19 \t Epoch : 30\tNet Loss: 416.7616 \tAnswer Loss: 0.3787 \tQuestion Loss: 416.3829\n",
      "Batch: 20 \t Epoch : 30\tNet Loss: 403.9805 \tAnswer Loss: 0.4081 \tQuestion Loss: 403.5724\n",
      "Batch: 21 \t Epoch : 30\tNet Loss: 448.8546 \tAnswer Loss: 0.3161 \tQuestion Loss: 448.5385\n",
      "Batch: 22 \t Epoch : 30\tNet Loss: 425.2682 \tAnswer Loss: 0.5285 \tQuestion Loss: 424.7397\n",
      "Batch: 23 \t Epoch : 30\tNet Loss: 508.3528 \tAnswer Loss: 0.3870 \tQuestion Loss: 507.9658\n",
      "Batch: 24 \t Epoch : 30\tNet Loss: 496.0683 \tAnswer Loss: 0.4825 \tQuestion Loss: 495.5858\n",
      "Batch: 25 \t Epoch : 30\tNet Loss: 500.5545 \tAnswer Loss: 0.3585 \tQuestion Loss: 500.1960\n",
      "Batch: 26 \t Epoch : 30\tNet Loss: 559.5047 \tAnswer Loss: 0.3692 \tQuestion Loss: 559.1354\n",
      "Batch: 27 \t Epoch : 30\tNet Loss: 486.4755 \tAnswer Loss: 0.4233 \tQuestion Loss: 486.0522\n",
      "Batch: 28 \t Epoch : 30\tNet Loss: 499.4239 \tAnswer Loss: 0.3852 \tQuestion Loss: 499.0387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 29 \t Epoch : 30\tNet Loss: 463.5055 \tAnswer Loss: 0.2948 \tQuestion Loss: 463.2107\n",
      "Batch: 30 \t Epoch : 30\tNet Loss: 425.3171 \tAnswer Loss: 0.3237 \tQuestion Loss: 424.9934\n",
      "Average Loss after Epoch 30 : 452.6463\n",
      "Batch: 0 \t Epoch : 31\tNet Loss: 452.6005 \tAnswer Loss: 0.0933 \tQuestion Loss: 452.5072\n",
      "Batch: 1 \t Epoch : 31\tNet Loss: 402.9200 \tAnswer Loss: 0.3453 \tQuestion Loss: 402.5747\n",
      "Batch: 2 \t Epoch : 31\tNet Loss: 472.8937 \tAnswer Loss: 0.4670 \tQuestion Loss: 472.4266\n",
      "Batch: 3 \t Epoch : 31\tNet Loss: 375.4872 \tAnswer Loss: 0.6439 \tQuestion Loss: 374.8433\n",
      "Batch: 4 \t Epoch : 31\tNet Loss: 432.6158 \tAnswer Loss: 0.2852 \tQuestion Loss: 432.3306\n",
      "Batch: 5 \t Epoch : 31\tNet Loss: 507.6220 \tAnswer Loss: 0.4534 \tQuestion Loss: 507.1686\n",
      "Batch: 6 \t Epoch : 31\tNet Loss: 502.4950 \tAnswer Loss: 0.4818 \tQuestion Loss: 502.0132\n",
      "Batch: 7 \t Epoch : 31\tNet Loss: 493.6481 \tAnswer Loss: 0.2990 \tQuestion Loss: 493.3491\n",
      "Batch: 8 \t Epoch : 31\tNet Loss: 478.4209 \tAnswer Loss: 0.2619 \tQuestion Loss: 478.1590\n",
      "Batch: 9 \t Epoch : 31\tNet Loss: 418.0464 \tAnswer Loss: 0.3440 \tQuestion Loss: 417.7024\n",
      "Batch: 10 \t Epoch : 31\tNet Loss: 442.9531 \tAnswer Loss: 0.4012 \tQuestion Loss: 442.5519\n",
      "Batch: 11 \t Epoch : 31\tNet Loss: 509.5488 \tAnswer Loss: 0.3062 \tQuestion Loss: 509.2426\n",
      "Batch: 12 \t Epoch : 31\tNet Loss: 430.5434 \tAnswer Loss: 0.4301 \tQuestion Loss: 430.1133\n",
      "Batch: 13 \t Epoch : 31\tNet Loss: 420.5700 \tAnswer Loss: 0.4566 \tQuestion Loss: 420.1135\n",
      "Batch: 14 \t Epoch : 31\tNet Loss: 490.5279 \tAnswer Loss: 0.3801 \tQuestion Loss: 490.1478\n",
      "Batch: 15 \t Epoch : 31\tNet Loss: 374.5146 \tAnswer Loss: 0.3785 \tQuestion Loss: 374.1362\n",
      "Batch: 16 \t Epoch : 31\tNet Loss: 460.9919 \tAnswer Loss: 0.3785 \tQuestion Loss: 460.6134\n",
      "Batch: 17 \t Epoch : 31\tNet Loss: 462.5616 \tAnswer Loss: 0.3131 \tQuestion Loss: 462.2485\n",
      "Batch: 18 \t Epoch : 31\tNet Loss: 421.9671 \tAnswer Loss: 0.5064 \tQuestion Loss: 421.4608\n",
      "Batch: 19 \t Epoch : 31\tNet Loss: 402.6348 \tAnswer Loss: 0.3786 \tQuestion Loss: 402.2562\n",
      "Batch: 20 \t Epoch : 31\tNet Loss: 389.7563 \tAnswer Loss: 0.4080 \tQuestion Loss: 389.3484\n",
      "Batch: 21 \t Epoch : 31\tNet Loss: 432.6400 \tAnswer Loss: 0.3161 \tQuestion Loss: 432.3239\n",
      "Batch: 22 \t Epoch : 31\tNet Loss: 409.7188 \tAnswer Loss: 0.5284 \tQuestion Loss: 409.1903\n",
      "Batch: 23 \t Epoch : 31\tNet Loss: 490.5927 \tAnswer Loss: 0.3869 \tQuestion Loss: 490.2058\n",
      "Batch: 24 \t Epoch : 31\tNet Loss: 478.6956 \tAnswer Loss: 0.4824 \tQuestion Loss: 478.2133\n",
      "Batch: 25 \t Epoch : 31\tNet Loss: 483.5875 \tAnswer Loss: 0.3585 \tQuestion Loss: 483.2290\n",
      "Batch: 26 \t Epoch : 31\tNet Loss: 541.4050 \tAnswer Loss: 0.3692 \tQuestion Loss: 541.0358\n",
      "Batch: 27 \t Epoch : 31\tNet Loss: 468.3257 \tAnswer Loss: 0.4233 \tQuestion Loss: 467.9025\n",
      "Batch: 28 \t Epoch : 31\tNet Loss: 481.7660 \tAnswer Loss: 0.3851 \tQuestion Loss: 481.3808\n",
      "Batch: 29 \t Epoch : 31\tNet Loss: 447.5328 \tAnswer Loss: 0.2948 \tQuestion Loss: 447.2380\n",
      "Batch: 30 \t Epoch : 31\tNet Loss: 410.4065 \tAnswer Loss: 0.3236 \tQuestion Loss: 410.0829\n",
      "Average Loss after Epoch 31 : 437.1247\n",
      "Batch: 0 \t Epoch : 32\tNet Loss: 436.4587 \tAnswer Loss: 0.0933 \tQuestion Loss: 436.3654\n",
      "Batch: 1 \t Epoch : 32\tNet Loss: 388.0878 \tAnswer Loss: 0.3452 \tQuestion Loss: 387.7426\n",
      "Batch: 2 \t Epoch : 32\tNet Loss: 457.3740 \tAnswer Loss: 0.4670 \tQuestion Loss: 456.9070\n",
      "Batch: 3 \t Epoch : 32\tNet Loss: 360.6200 \tAnswer Loss: 0.6438 \tQuestion Loss: 359.9762\n",
      "Batch: 4 \t Epoch : 32\tNet Loss: 417.2274 \tAnswer Loss: 0.2851 \tQuestion Loss: 416.9423\n",
      "Batch: 5 \t Epoch : 32\tNet Loss: 491.0216 \tAnswer Loss: 0.4533 \tQuestion Loss: 490.5683\n",
      "Batch: 6 \t Epoch : 32\tNet Loss: 486.0024 \tAnswer Loss: 0.4817 \tQuestion Loss: 485.5206\n",
      "Batch: 7 \t Epoch : 32\tNet Loss: 476.1446 \tAnswer Loss: 0.2989 \tQuestion Loss: 475.8457\n",
      "Batch: 8 \t Epoch : 32\tNet Loss: 462.7201 \tAnswer Loss: 0.2619 \tQuestion Loss: 462.4582\n",
      "Batch: 9 \t Epoch : 32\tNet Loss: 403.0438 \tAnswer Loss: 0.3439 \tQuestion Loss: 402.6999\n",
      "Batch: 10 \t Epoch : 32\tNet Loss: 428.4834 \tAnswer Loss: 0.4011 \tQuestion Loss: 428.0823\n",
      "Batch: 11 \t Epoch : 32\tNet Loss: 492.4004 \tAnswer Loss: 0.3062 \tQuestion Loss: 492.0942\n",
      "Batch: 12 \t Epoch : 32\tNet Loss: 415.8036 \tAnswer Loss: 0.4300 \tQuestion Loss: 415.3736\n",
      "Batch: 13 \t Epoch : 32\tNet Loss: 405.4176 \tAnswer Loss: 0.4565 \tQuestion Loss: 404.9611\n",
      "Batch: 14 \t Epoch : 32\tNet Loss: 474.1485 \tAnswer Loss: 0.3801 \tQuestion Loss: 473.7684\n",
      "Batch: 15 \t Epoch : 32\tNet Loss: 361.4688 \tAnswer Loss: 0.3784 \tQuestion Loss: 361.0905\n",
      "Batch: 16 \t Epoch : 32\tNet Loss: 446.2861 \tAnswer Loss: 0.3784 \tQuestion Loss: 445.9077\n",
      "Batch: 17 \t Epoch : 32\tNet Loss: 447.5383 \tAnswer Loss: 0.3131 \tQuestion Loss: 447.2252\n",
      "Batch: 18 \t Epoch : 32\tNet Loss: 408.2550 \tAnswer Loss: 0.5063 \tQuestion Loss: 407.7487\n",
      "Batch: 19 \t Epoch : 32\tNet Loss: 388.8537 \tAnswer Loss: 0.3785 \tQuestion Loss: 388.4752\n",
      "Batch: 20 \t Epoch : 32\tNet Loss: 375.9095 \tAnswer Loss: 0.4079 \tQuestion Loss: 375.5016\n",
      "Batch: 21 \t Epoch : 32\tNet Loss: 416.8728 \tAnswer Loss: 0.3160 \tQuestion Loss: 416.5568\n",
      "Batch: 22 \t Epoch : 32\tNet Loss: 394.5636 \tAnswer Loss: 0.5283 \tQuestion Loss: 394.0352\n",
      "Batch: 23 \t Epoch : 32\tNet Loss: 473.2550 \tAnswer Loss: 0.3868 \tQuestion Loss: 472.8682\n",
      "Batch: 24 \t Epoch : 32\tNet Loss: 461.7091 \tAnswer Loss: 0.4823 \tQuestion Loss: 461.2268\n",
      "Batch: 25 \t Epoch : 32\tNet Loss: 466.9800 \tAnswer Loss: 0.3584 \tQuestion Loss: 466.6216\n",
      "Batch: 26 \t Epoch : 32\tNet Loss: 523.6171 \tAnswer Loss: 0.3691 \tQuestion Loss: 523.2480\n",
      "Batch: 27 \t Epoch : 32\tNet Loss: 450.7063 \tAnswer Loss: 0.4232 \tQuestion Loss: 450.2831\n",
      "Batch: 28 \t Epoch : 32\tNet Loss: 464.5760 \tAnswer Loss: 0.3851 \tQuestion Loss: 464.1910\n",
      "Batch: 29 \t Epoch : 32\tNet Loss: 431.8790 \tAnswer Loss: 0.2947 \tQuestion Loss: 431.5843\n",
      "Batch: 30 \t Epoch : 32\tNet Loss: 395.8772 \tAnswer Loss: 0.3235 \tQuestion Loss: 395.5537\n",
      "Average Loss after Epoch 32 : 421.9782\n",
      "Batch: 0 \t Epoch : 33\tNet Loss: 420.7092 \tAnswer Loss: 0.0933 \tQuestion Loss: 420.6159\n",
      "Batch: 1 \t Epoch : 33\tNet Loss: 373.6957 \tAnswer Loss: 0.3452 \tQuestion Loss: 373.3505\n",
      "Batch: 2 \t Epoch : 33\tNet Loss: 442.2180 \tAnswer Loss: 0.4669 \tQuestion Loss: 441.7511\n",
      "Batch: 3 \t Epoch : 33\tNet Loss: 346.2110 \tAnswer Loss: 0.6437 \tQuestion Loss: 345.5673\n",
      "Batch: 4 \t Epoch : 33\tNet Loss: 402.2082 \tAnswer Loss: 0.2851 \tQuestion Loss: 401.9231\n",
      "Batch: 5 \t Epoch : 33\tNet Loss: 474.8163 \tAnswer Loss: 0.4532 \tQuestion Loss: 474.3631\n",
      "Batch: 6 \t Epoch : 33\tNet Loss: 469.8853 \tAnswer Loss: 0.4817 \tQuestion Loss: 469.4037\n",
      "Batch: 7 \t Epoch : 33\tNet Loss: 459.0953 \tAnswer Loss: 0.2989 \tQuestion Loss: 458.7964\n",
      "Batch: 8 \t Epoch : 33\tNet Loss: 447.4060 \tAnswer Loss: 0.2618 \tQuestion Loss: 447.1442\n",
      "Batch: 9 \t Epoch : 33\tNet Loss: 388.4095 \tAnswer Loss: 0.3438 \tQuestion Loss: 388.0657\n",
      "Batch: 10 \t Epoch : 33\tNet Loss: 414.3430 \tAnswer Loss: 0.4011 \tQuestion Loss: 413.9419\n",
      "Batch: 11 \t Epoch : 33\tNet Loss: 475.6757 \tAnswer Loss: 0.3061 \tQuestion Loss: 475.3696\n",
      "Batch: 12 \t Epoch : 33\tNet Loss: 401.4025 \tAnswer Loss: 0.4299 \tQuestion Loss: 400.9726\n",
      "Batch: 13 \t Epoch : 33\tNet Loss: 390.6827 \tAnswer Loss: 0.4564 \tQuestion Loss: 390.2263\n",
      "Batch: 14 \t Epoch : 33\tNet Loss: 458.0343 \tAnswer Loss: 0.3800 \tQuestion Loss: 457.6543\n",
      "Batch: 15 \t Epoch : 33\tNet Loss: 348.7044 \tAnswer Loss: 0.3783 \tQuestion Loss: 348.3261\n",
      "Batch: 16 \t Epoch : 33\tNet Loss: 431.9099 \tAnswer Loss: 0.3783 \tQuestion Loss: 431.5315\n",
      "Batch: 17 \t Epoch : 33\tNet Loss: 432.8740 \tAnswer Loss: 0.3130 \tQuestion Loss: 432.5610\n",
      "Batch: 18 \t Epoch : 33\tNet Loss: 394.9089 \tAnswer Loss: 0.5062 \tQuestion Loss: 394.4027\n",
      "Batch: 19 \t Epoch : 33\tNet Loss: 375.4162 \tAnswer Loss: 0.3785 \tQuestion Loss: 375.0378\n",
      "Batch: 20 \t Epoch : 33\tNet Loss: 362.4368 \tAnswer Loss: 0.4079 \tQuestion Loss: 362.0289\n",
      "Batch: 21 \t Epoch : 33\tNet Loss: 401.5405 \tAnswer Loss: 0.3159 \tQuestion Loss: 401.2245\n",
      "Batch: 22 \t Epoch : 33\tNet Loss: 379.7970 \tAnswer Loss: 0.5283 \tQuestion Loss: 379.2687\n",
      "Batch: 23 \t Epoch : 33\tNet Loss: 456.3344 \tAnswer Loss: 0.3868 \tQuestion Loss: 455.9477\n",
      "Batch: 24 \t Epoch : 33\tNet Loss: 445.1067 \tAnswer Loss: 0.4822 \tQuestion Loss: 444.6244\n",
      "Batch: 25 \t Epoch : 33\tNet Loss: 450.7227 \tAnswer Loss: 0.3584 \tQuestion Loss: 450.3643\n",
      "Batch: 26 \t Epoch : 33\tNet Loss: 506.1377 \tAnswer Loss: 0.3690 \tQuestion Loss: 505.7687\n",
      "Batch: 27 \t Epoch : 33\tNet Loss: 433.6090 \tAnswer Loss: 0.4232 \tQuestion Loss: 433.1858\n",
      "Batch: 28 \t Epoch : 33\tNet Loss: 447.8381 \tAnswer Loss: 0.3850 \tQuestion Loss: 447.4532\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 29 \t Epoch : 33\tNet Loss: 416.5462 \tAnswer Loss: 0.2946 \tQuestion Loss: 416.2516\n",
      "Batch: 30 \t Epoch : 33\tNet Loss: 381.7248 \tAnswer Loss: 0.3234 \tQuestion Loss: 381.4013\n",
      "Average Loss after Epoch 33 : 407.2000\n",
      "Batch: 0 \t Epoch : 34\tNet Loss: 405.3403 \tAnswer Loss: 0.0933 \tQuestion Loss: 405.2471\n",
      "Batch: 1 \t Epoch : 34\tNet Loss: 359.7278 \tAnswer Loss: 0.3451 \tQuestion Loss: 359.3827\n",
      "Batch: 2 \t Epoch : 34\tNet Loss: 427.4138 \tAnswer Loss: 0.4669 \tQuestion Loss: 426.9469\n",
      "Batch: 3 \t Epoch : 34\tNet Loss: 332.2564 \tAnswer Loss: 0.6436 \tQuestion Loss: 331.6127\n",
      "Batch: 4 \t Epoch : 34\tNet Loss: 387.5523 \tAnswer Loss: 0.2850 \tQuestion Loss: 387.2673\n",
      "Batch: 5 \t Epoch : 34\tNet Loss: 458.9917 \tAnswer Loss: 0.4532 \tQuestion Loss: 458.5385\n",
      "Batch: 6 \t Epoch : 34\tNet Loss: 454.1475 \tAnswer Loss: 0.4816 \tQuestion Loss: 453.6659\n",
      "Batch: 7 \t Epoch : 34\tNet Loss: 442.4993 \tAnswer Loss: 0.2988 \tQuestion Loss: 442.2005\n",
      "Batch: 8 \t Epoch : 34\tNet Loss: 432.4718 \tAnswer Loss: 0.2618 \tQuestion Loss: 432.2100\n",
      "Batch: 9 \t Epoch : 34\tNet Loss: 374.1434 \tAnswer Loss: 0.3437 \tQuestion Loss: 373.7997\n",
      "Batch: 10 \t Epoch : 34\tNet Loss: 400.5253 \tAnswer Loss: 0.4010 \tQuestion Loss: 400.1243\n",
      "Batch: 11 \t Epoch : 34\tNet Loss: 459.3691 \tAnswer Loss: 0.3061 \tQuestion Loss: 459.0630\n",
      "Batch: 12 \t Epoch : 34\tNet Loss: 387.3252 \tAnswer Loss: 0.4298 \tQuestion Loss: 386.8954\n",
      "Batch: 13 \t Epoch : 34\tNet Loss: 376.3541 \tAnswer Loss: 0.4564 \tQuestion Loss: 375.8978\n",
      "Batch: 14 \t Epoch : 34\tNet Loss: 442.1899 \tAnswer Loss: 0.3800 \tQuestion Loss: 441.8099\n",
      "Batch: 15 \t Epoch : 34\tNet Loss: 336.2233 \tAnswer Loss: 0.3782 \tQuestion Loss: 335.8450\n",
      "Batch: 16 \t Epoch : 34\tNet Loss: 417.8455 \tAnswer Loss: 0.3782 \tQuestion Loss: 417.4672\n",
      "Batch: 17 \t Epoch : 34\tNet Loss: 418.5624 \tAnswer Loss: 0.3129 \tQuestion Loss: 418.2495\n",
      "Batch: 18 \t Epoch : 34\tNet Loss: 381.9189 \tAnswer Loss: 0.5061 \tQuestion Loss: 381.4128\n",
      "Batch: 19 \t Epoch : 34\tNet Loss: 362.3219 \tAnswer Loss: 0.3784 \tQuestion Loss: 361.9435\n",
      "Batch: 20 \t Epoch : 34\tNet Loss: 349.3351 \tAnswer Loss: 0.4078 \tQuestion Loss: 348.9272\n",
      "Batch: 21 \t Epoch : 34\tNet Loss: 386.6323 \tAnswer Loss: 0.3158 \tQuestion Loss: 386.3165\n",
      "Batch: 22 \t Epoch : 34\tNet Loss: 365.4119 \tAnswer Loss: 0.5282 \tQuestion Loss: 364.8838\n",
      "Batch: 23 \t Epoch : 34\tNet Loss: 439.8260 \tAnswer Loss: 0.3867 \tQuestion Loss: 439.4393\n",
      "Batch: 24 \t Epoch : 34\tNet Loss: 428.8862 \tAnswer Loss: 0.4821 \tQuestion Loss: 428.4040\n",
      "Batch: 25 \t Epoch : 34\tNet Loss: 434.8061 \tAnswer Loss: 0.3583 \tQuestion Loss: 434.4478\n",
      "Batch: 26 \t Epoch : 34\tNet Loss: 488.9663 \tAnswer Loss: 0.3689 \tQuestion Loss: 488.5974\n",
      "Batch: 27 \t Epoch : 34\tNet Loss: 417.0261 \tAnswer Loss: 0.4231 \tQuestion Loss: 416.6030\n",
      "Batch: 28 \t Epoch : 34\tNet Loss: 431.5367 \tAnswer Loss: 0.3849 \tQuestion Loss: 431.1518\n",
      "Batch: 29 \t Epoch : 34\tNet Loss: 401.5388 \tAnswer Loss: 0.2946 \tQuestion Loss: 401.2443\n",
      "Batch: 30 \t Epoch : 34\tNet Loss: 367.9470 \tAnswer Loss: 0.3233 \tQuestion Loss: 367.6237\n",
      "Average Loss after Epoch 34 : 392.7841\n",
      "Batch: 0 \t Epoch : 35\tNet Loss: 390.3438 \tAnswer Loss: 0.0933 \tQuestion Loss: 390.2505\n",
      "Batch: 1 \t Epoch : 35\tNet Loss: 346.1678 \tAnswer Loss: 0.3451 \tQuestion Loss: 345.8228\n",
      "Batch: 2 \t Epoch : 35\tNet Loss: 412.9510 \tAnswer Loss: 0.4669 \tQuestion Loss: 412.4842\n",
      "Batch: 3 \t Epoch : 35\tNet Loss: 318.7517 \tAnswer Loss: 0.6436 \tQuestion Loss: 318.1081\n",
      "Batch: 4 \t Epoch : 35\tNet Loss: 373.2558 \tAnswer Loss: 0.2850 \tQuestion Loss: 372.9709\n",
      "Batch: 5 \t Epoch : 35\tNet Loss: 443.5368 \tAnswer Loss: 0.4531 \tQuestion Loss: 443.0837\n",
      "Batch: 6 \t Epoch : 35\tNet Loss: 438.7903 \tAnswer Loss: 0.4816 \tQuestion Loss: 438.3087\n",
      "Batch: 7 \t Epoch : 35\tNet Loss: 426.3550 \tAnswer Loss: 0.2987 \tQuestion Loss: 426.0563\n",
      "Batch: 8 \t Epoch : 35\tNet Loss: 417.9112 \tAnswer Loss: 0.2618 \tQuestion Loss: 417.6494\n",
      "Batch: 9 \t Epoch : 35\tNet Loss: 360.2461 \tAnswer Loss: 0.3436 \tQuestion Loss: 359.9025\n",
      "Batch: 10 \t Epoch : 35\tNet Loss: 387.0233 \tAnswer Loss: 0.4010 \tQuestion Loss: 386.6223\n",
      "Batch: 11 \t Epoch : 35\tNet Loss: 443.4764 \tAnswer Loss: 0.3060 \tQuestion Loss: 443.1704\n",
      "Batch: 12 \t Epoch : 35\tNet Loss: 373.5601 \tAnswer Loss: 0.4297 \tQuestion Loss: 373.1303\n",
      "Batch: 13 \t Epoch : 35\tNet Loss: 362.4208 \tAnswer Loss: 0.4563 \tQuestion Loss: 361.9645\n",
      "Batch: 14 \t Epoch : 35\tNet Loss: 426.6231 \tAnswer Loss: 0.3800 \tQuestion Loss: 426.2432\n",
      "Batch: 15 \t Epoch : 35\tNet Loss: 324.0263 \tAnswer Loss: 0.3782 \tQuestion Loss: 323.6481\n",
      "Batch: 16 \t Epoch : 35\tNet Loss: 404.0817 \tAnswer Loss: 0.3782 \tQuestion Loss: 403.7035\n",
      "Batch: 17 \t Epoch : 35\tNet Loss: 404.5988 \tAnswer Loss: 0.3129 \tQuestion Loss: 404.2859\n",
      "Batch: 18 \t Epoch : 35\tNet Loss: 369.2751 \tAnswer Loss: 0.5060 \tQuestion Loss: 368.7692\n",
      "Batch: 19 \t Epoch : 35\tNet Loss: 349.5707 \tAnswer Loss: 0.3783 \tQuestion Loss: 349.1924\n",
      "Batch: 20 \t Epoch : 35\tNet Loss: 336.6024 \tAnswer Loss: 0.4078 \tQuestion Loss: 336.1946\n",
      "Batch: 21 \t Epoch : 35\tNet Loss: 372.1380 \tAnswer Loss: 0.3157 \tQuestion Loss: 371.8223\n",
      "Batch: 22 \t Epoch : 35\tNet Loss: 351.4011 \tAnswer Loss: 0.5281 \tQuestion Loss: 350.8730\n",
      "Batch: 23 \t Epoch : 35\tNet Loss: 423.7276 \tAnswer Loss: 0.3866 \tQuestion Loss: 423.3410\n",
      "Batch: 24 \t Epoch : 35\tNet Loss: 413.0484 \tAnswer Loss: 0.4821 \tQuestion Loss: 412.5663\n",
      "Batch: 25 \t Epoch : 35\tNet Loss: 419.2233 \tAnswer Loss: 0.3583 \tQuestion Loss: 418.8650\n",
      "Batch: 26 \t Epoch : 35\tNet Loss: 472.1064 \tAnswer Loss: 0.3689 \tQuestion Loss: 471.7376\n",
      "Batch: 27 \t Epoch : 35\tNet Loss: 400.9497 \tAnswer Loss: 0.4231 \tQuestion Loss: 400.5266\n",
      "Batch: 28 \t Epoch : 35\tNet Loss: 415.6573 \tAnswer Loss: 0.3848 \tQuestion Loss: 415.2725\n",
      "Batch: 29 \t Epoch : 35\tNet Loss: 386.8633 \tAnswer Loss: 0.2945 \tQuestion Loss: 386.5687\n",
      "Batch: 30 \t Epoch : 35\tNet Loss: 354.5431 \tAnswer Loss: 0.3232 \tQuestion Loss: 354.2198\n",
      "Average Loss after Epoch 35 : 378.7258\n",
      "Batch: 0 \t Epoch : 36\tNet Loss: 375.7125 \tAnswer Loss: 0.0932 \tQuestion Loss: 375.6193\n",
      "Batch: 1 \t Epoch : 36\tNet Loss: 333.0024 \tAnswer Loss: 0.3450 \tQuestion Loss: 332.6574\n",
      "Batch: 2 \t Epoch : 36\tNet Loss: 398.8223 \tAnswer Loss: 0.4668 \tQuestion Loss: 398.3555\n",
      "Batch: 3 \t Epoch : 36\tNet Loss: 305.6905 \tAnswer Loss: 0.6435 \tQuestion Loss: 305.0470\n",
      "Batch: 4 \t Epoch : 36\tNet Loss: 359.3157 \tAnswer Loss: 0.2849 \tQuestion Loss: 359.0307\n",
      "Batch: 5 \t Epoch : 36\tNet Loss: 428.4447 \tAnswer Loss: 0.4530 \tQuestion Loss: 427.9917\n",
      "Batch: 6 \t Epoch : 36\tNet Loss: 423.8159 \tAnswer Loss: 0.4815 \tQuestion Loss: 423.3344\n",
      "Batch: 7 \t Epoch : 36\tNet Loss: 410.6600 \tAnswer Loss: 0.2987 \tQuestion Loss: 410.3614\n",
      "Batch: 8 \t Epoch : 36\tNet Loss: 403.7183 \tAnswer Loss: 0.2617 \tQuestion Loss: 403.4566\n",
      "Batch: 9 \t Epoch : 36\tNet Loss: 346.7188 \tAnswer Loss: 0.3436 \tQuestion Loss: 346.3753\n",
      "Batch: 10 \t Epoch : 36\tNet Loss: 373.8327 \tAnswer Loss: 0.4009 \tQuestion Loss: 373.4318\n",
      "Batch: 11 \t Epoch : 36\tNet Loss: 427.9955 \tAnswer Loss: 0.3059 \tQuestion Loss: 427.6895\n",
      "Batch: 12 \t Epoch : 36\tNet Loss: 360.0996 \tAnswer Loss: 0.4297 \tQuestion Loss: 359.6699\n",
      "Batch: 13 \t Epoch : 36\tNet Loss: 348.8718 \tAnswer Loss: 0.4562 \tQuestion Loss: 348.4156\n",
      "Batch: 14 \t Epoch : 36\tNet Loss: 411.3425 \tAnswer Loss: 0.3799 \tQuestion Loss: 410.9626\n",
      "Batch: 15 \t Epoch : 36\tNet Loss: 312.1153 \tAnswer Loss: 0.3781 \tQuestion Loss: 311.7372\n",
      "Batch: 16 \t Epoch : 36\tNet Loss: 390.6111 \tAnswer Loss: 0.3781 \tQuestion Loss: 390.2330\n",
      "Batch: 17 \t Epoch : 36\tNet Loss: 390.9783 \tAnswer Loss: 0.3128 \tQuestion Loss: 390.6655\n",
      "Batch: 18 \t Epoch : 36\tNet Loss: 356.9673 \tAnswer Loss: 0.5059 \tQuestion Loss: 356.4614\n",
      "Batch: 19 \t Epoch : 36\tNet Loss: 337.1635 \tAnswer Loss: 0.3782 \tQuestion Loss: 336.7852\n",
      "Batch: 20 \t Epoch : 36\tNet Loss: 324.2361 \tAnswer Loss: 0.4077 \tQuestion Loss: 323.8284\n",
      "Batch: 21 \t Epoch : 36\tNet Loss: 358.0503 \tAnswer Loss: 0.3157 \tQuestion Loss: 357.7346\n",
      "Batch: 22 \t Epoch : 36\tNet Loss: 337.7587 \tAnswer Loss: 0.5280 \tQuestion Loss: 337.2307\n",
      "Batch: 23 \t Epoch : 36\tNet Loss: 408.0366 \tAnswer Loss: 0.3865 \tQuestion Loss: 407.6501\n",
      "Batch: 24 \t Epoch : 36\tNet Loss: 397.5939 \tAnswer Loss: 0.4820 \tQuestion Loss: 397.1119\n",
      "Batch: 25 \t Epoch : 36\tNet Loss: 403.9700 \tAnswer Loss: 0.3582 \tQuestion Loss: 403.6117\n",
      "Batch: 26 \t Epoch : 36\tNet Loss: 455.5652 \tAnswer Loss: 0.3688 \tQuestion Loss: 455.1964\n",
      "Batch: 27 \t Epoch : 36\tNet Loss: 385.3709 \tAnswer Loss: 0.4230 \tQuestion Loss: 384.9479\n",
      "Batch: 28 \t Epoch : 36\tNet Loss: 400.1888 \tAnswer Loss: 0.3847 \tQuestion Loss: 399.8040\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 29 \t Epoch : 36\tNet Loss: 372.5251 \tAnswer Loss: 0.2945 \tQuestion Loss: 372.2306\n",
      "Batch: 30 \t Epoch : 36\tNet Loss: 341.5122 \tAnswer Loss: 0.3232 \tQuestion Loss: 341.1890\n",
      "Average Loss after Epoch 36 : 365.0214\n",
      "Batch: 0 \t Epoch : 37\tNet Loss: 361.4427 \tAnswer Loss: 0.0932 \tQuestion Loss: 361.3495\n",
      "Batch: 1 \t Epoch : 37\tNet Loss: 320.2202 \tAnswer Loss: 0.3449 \tQuestion Loss: 319.8752\n",
      "Batch: 2 \t Epoch : 37\tNet Loss: 385.0233 \tAnswer Loss: 0.4668 \tQuestion Loss: 384.5565\n",
      "Batch: 3 \t Epoch : 37\tNet Loss: 293.0675 \tAnswer Loss: 0.6434 \tQuestion Loss: 292.4241\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-505ebc335b53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mnet_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mquestion_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mnet_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/ra2630/miniconda3/envs/qgen/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/ra2630/miniconda3/envs/qgen/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "verboseBatchPrinting = True\n",
    "averageBatchLossPrinting = True\n",
    "\n",
    "num_epochs = 100\n",
    "answer_encoder_hidden = answerEncoder.initHidden()\n",
    "question_encoder_hidden = questionEncoder.initHidden()\n",
    "question_decoder_hidden = None\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    avg_loss = 0\n",
    "    for batch_num in range(len(batch_input[0])):\n",
    "\n",
    "        current_batch_size = len(batch_input[0][batch_num])\n",
    "        if current_batch_size != batch_size:\n",
    "            continue\n",
    "\n",
    "\n",
    "        maxDocLenForBatch = int(max(batch_input[1][batch_num]))\n",
    "        mask = np.zeros((current_batch_size, maxDocLenForBatch))\n",
    "        for i in range(current_batch_size):\n",
    "            mask[i][0:batch_input[1][0][i]] = 1\n",
    "\n",
    "        inp = Variable(torch.from_numpy(batch_input[0][batch_num]).long())\n",
    "\n",
    "        labels = Variable(torch.from_numpy(batch_input[2][batch_num])).long()\n",
    "        if use_cuda:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embedded_inp = embedder(inp).cuda()\n",
    "        answer_encoder_hidden = repackage_hidden(answer_encoder_hidden)\n",
    "        answer_tags, answer_outputs, answer_encoder_hidden = answerEncoder(embedded_inp, answer_encoder_hidden)\n",
    "\n",
    "\n",
    "        if use_cuda:\n",
    "            answer_outputs = answer_outputs.cuda()\n",
    "            answer_tags = answer_tags.cuda()\n",
    "\n",
    "\n",
    "        t_document_mask = Variable(torch.from_numpy(mask)).float()\n",
    "        if use_cuda:\n",
    "            t_document_mask = t_document_mask.cuda()\n",
    "        outputs = torch.mul(answer_tags.squeeze(-1),t_document_mask)\n",
    "\n",
    "\n",
    "        answer_loss = criterion1(outputs, labels.float())\n",
    "\n",
    "\n",
    "        t_answer_mask = Variable(torch.from_numpy(batch_input[3][batch_num])).float()\n",
    "        if use_cuda:\n",
    "            t_answer_mask = t_answer_mask.cuda()\n",
    "\n",
    "        question_encoder_input = torch.matmul(t_answer_mask, answer_outputs.float())\n",
    "        question_encoder_hidden_batch = Variable(torch.zeros(1,current_batch_size,questionEncoder.hidden_size))\n",
    "        if use_cuda:\n",
    "            question_encoder_hidden_batch = question_encoder_hidden_batch.cuda()\n",
    "\n",
    "        question_encoder_hidden = repackage_hidden(question_encoder_hidden)\n",
    "        for i in range(current_batch_size):\n",
    "            _ , question_encoder_hidden = questionEncoder(question_encoder_input[i:i+1,0:batch_input[4][batch_num][i],:], question_encoder_hidden)\n",
    "            question_encoder_hidden_batch[:,i:i+1,:] = question_encoder_hidden\n",
    "\n",
    "        if type(question_decoder_hidden) == Variable:\n",
    "            question_decoder_hidden = repackage_hidden(question_decoder_hidden)\n",
    "        question_loss = 0\n",
    "        for i in range(current_batch_size):\n",
    "            question_decoder_hidden = question_encoder_hidden_batch[:,i:i+1,:].clone()\n",
    "            embedded_inputs = embedder(torch.from_numpy(batch_input[5][batch_num][i]).long()).cuda()\n",
    "            output_labels = Variable(torch.from_numpy(batch_input[5][batch_num][i]).long())\n",
    "            if use_cuda:\n",
    "                output_labels = output_labels.cuda()\n",
    "\n",
    "            for quesL in range(batch_input[7][batch_num][i]):\n",
    "                decoder_output, question_decoder_hidden = questionDecoder(\n",
    "                    embedded_inputs[quesL:quesL+1].unsqueeze(1),\n",
    "                    question_decoder_hidden)\n",
    "\n",
    "                final_output = questionGenerator(decoder_output)\n",
    "                output_label = Variable(torch.zeros(1,2000))\n",
    "                if use_cuda:\n",
    "                    output_label = output_label.cuda()\n",
    "                output_label[:,batch_input[5][batch_num][i][quesL]] = 1\n",
    "                question_loss += criterion2(final_output.squeeze(0),\n",
    "                                           output_labels[quesL:quesL+1])\n",
    "                ##question_loss += criterion2(final_output.squeeze(0), output_label)\n",
    "\n",
    "\n",
    "        net_loss = answer_loss + question_loss\n",
    "        net_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        avg_loss+= net_loss.data[0]\n",
    "        if verboseBatchPrinting:\n",
    "            print ('Batch: %d \\t Epoch : %d\\tNet Loss: %.4f \\tAnswer Loss: %.4f \\tQuestion Loss: %.4f'\n",
    "                   %(batch_num, epoch, net_loss.data[0], answer_loss.data[0], question_loss.data[0]))\n",
    "\n",
    "\n",
    "\n",
    "    if averageBatchLossPrinting:\n",
    "        print('Average Loss after Epoch %d : %.4f'\n",
    "                   %(epoch, avg_loss/number_of_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_loss.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0\n",
       "[torch.cuda.FloatTensor of size 1 (GPU 0)]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion2(final_output.squeeze(0), output_labels[quesL:quesL+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/data/ra2630/qgen_base_40k.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-80-e1c73141275e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manswerEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answerEncoder\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mquestionEncoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"questionEncoder\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mquestionDecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"questionDecoder\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mquestionGenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"questionGenerator\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"optimizer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'checkpoint' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedAnswerOutputs = answerEncoder(embedded_inp[:,:,:], answerEncoder.initHidden())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedAnswerOutputs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "##########ONly Batch\n",
    "\n",
    "verboseBatchPrinting = True\n",
    "averageBatchLossPrinting = True\n",
    "\n",
    "num_epochs = 100\n",
    "answer_encoder_hidden = answerEncoder.initHidden()\n",
    "question_encoder_hidden = questionEncoder.initHidden()\n",
    "question_decoder_hidden = None\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    avg_loss = 0\n",
    "    for batch_num in range(len(batch_input[0])):\n",
    "        \n",
    "        current_batch_size = len(batch_input[0][batch_num])\n",
    "        if current_batch_size != batch_size:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        maxDocLenForBatch = int(max(batch_input[1][batch_num]))\n",
    "        mask = np.zeros((current_batch_size, maxDocLenForBatch))\n",
    "        for i in range(current_batch_size):\n",
    "            mask[i][0:batch_input[1][0][i]] = 1\n",
    "\n",
    "            \n",
    "        inp = Variable(torch.from_numpy(batch_input[0][batch_num]).long())\n",
    "\n",
    "        labels = Variable(torch.from_numpy(batch_input[2][batch_num])).long()\n",
    "        if use_cuda:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        ################################### Answer Encoder + Tagging    \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embedded_inp = embedder(inp)\n",
    "        if use_cuda:\n",
    "            embedded_inp = embedder(inp).cuda()\n",
    "\n",
    "            \n",
    "        \n",
    "        answer_encoder_hidden = repackage_hidden(answer_encoder_hidden)\n",
    "        answer_tags, answer_outputs, answer_encoder_hidden = answerEncoder(embedded_inp, answer_encoder_hidden)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        if use_cuda:\n",
    "            answer_outputs = answer_outputs.cuda()\n",
    "            answer_tags = answer_tags.cuda()\n",
    "\n",
    "        #answer_tags.requires_grad=False\n",
    "        #answer_outputs.requires_grad=False\n",
    "        \n",
    "        \n",
    "        t_document_mask = Variable(torch.from_numpy(mask)).float()\n",
    "        if use_cuda:\n",
    "            t_document_mask = t_document_mask.cuda()\n",
    "        #t_document_mask.requires_grade = False\n",
    "        \n",
    "        outputs = answer_tags.squeeze(-1) * t_document_mask\n",
    "        #outputs.requires_grad = False\n",
    "        \n",
    "        \n",
    "        answer_loss = criterion1(outputs, labels.float())\n",
    "        ################################### Q Encoder    \n",
    "        \n",
    "        t_answer_mask = Variable(torch.from_numpy(batch_input[3][batch_num])).float()\n",
    "        if use_cuda:\n",
    "            t_answer_mask = t_answer_mask.cuda()\n",
    "\n",
    "        #t_answer_mask.requires_gradui = False\n",
    "        \n",
    "        # masking the non-answer embeddings\n",
    "        question_encoder_input = torch.matmul(t_answer_mask, answer_outputs.float())\n",
    "        #question_encoder_input.requires_grad = False\n",
    "        question_encoder_output = Variable(torch.zeros(1,batch_size,questionEncoder.hidden_size))\n",
    "        #question_encoder_output.requires_grad = False\n",
    "        \n",
    "        \n",
    "        if use_cuda:\n",
    "            question_encoder_hidden = question_encoder_hidden_batch.cuda()\n",
    "            \n",
    "        question_encoder_hidden = repackage_hidden(question_encoder_hidden)\n",
    "        question_encoder_output, question_encoder_hidden = questionEncoder(question_encoder_input, question_encoder_hidden)\n",
    "        \n",
    "        #question_encoder_output.requires_grad = False\n",
    "\n",
    "        \n",
    "#         question_encoder_output = Variable(torch.zeros(batch_size, max_question_len, questionEncoder.hidden_size))\n",
    "#         question_encoder_hidden = Variable(torch.zeros(batch_size, max_question_len, questionEncoder.hidden_size))\n",
    "        \n",
    "#         for i in range(current_batch_size):\n",
    "#             question_encoder_output, question_encoder_hidden = questionEncoder(question_encoder_input[i:i+1,0:batch_input[4][batch_num][i],:], question_encoder_hidden)\n",
    "#             question_encoder_hidden_batch[:,i:i+1,:] = question_encoder_hidden    \n",
    "\n",
    "        ################################### Q Decoder    \n",
    "        \n",
    "        question_loss = 0\n",
    "        \n",
    "        question_decoder_hidden = question_encoder_hidden\n",
    "        question_decoder_hidden = repackage_hidden(question_decoder_hidden)\n",
    "        \n",
    "        embedded_inputs = embedder(torch.from_numpy(batch_input[5][batch_num]).long())\n",
    "        #embedded_inputs.requires_grad = False\n",
    "        \n",
    "        output_labels = Variable(torch.from_numpy(batch_input[6][batch_num]).long())\n",
    "        \n",
    "        decoder_output = Variable(torch.zeros(1,batch_size,questionDecoder.hidden_size))\n",
    "        \n",
    "        if use_cuda:\n",
    "            embedded_inputs = embedded_inputs.cuda()\n",
    "            output_labels = output_labels.cuda()\n",
    "            \n",
    "        \n",
    "        question_decoder_hidden = repackage_hidden(question_decoder_hidden)\n",
    "        for quesL in range(max(batch_input[7][batch_num])):\n",
    "            decoder_output, question_decoder_hidden = questionDecoder(\n",
    "                embedded_inputs[:,quesL:quesL+1,:],\n",
    "                question_decoder_hidden)\n",
    "\n",
    "            final_output = questionGenerator(decoder_output)\n",
    "\n",
    "            output_label = Variable(torch.zeros(32,2000))\n",
    "            if use_cuda:\n",
    "                output_label = output_label.cuda()\n",
    "            for b in range(len(batch_input[5][0])):\n",
    "                output_label[b,batch_input[5][batch_num][b][quesL]] = 1\n",
    "            #question_loss += criterion2(final_output.squeeze(1), \n",
    "            #                           output_labels[:,quesL:quesL+1])\n",
    "            question_loss += criterion2(final_output.squeeze(0), output_label)\n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "        net_loss = answer_loss + question_loss\n",
    "        net_loss.backward()\n",
    "    \n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss+= net_loss.data[0]\n",
    "        if verboseBatchPrinting:\n",
    "            print ('Batch: %d \\t Epoch : %d\\tNet Loss: %.4f \\tAnswer Loss: %.4f \\tQuestion Loss: %.4f' \n",
    "                   %(batch_num, epoch, net_loss.data[0], answer_loss.data[0], question_loss.data[0]))\n",
    "            \n",
    "    torch.cuda.empty_cache()\n",
    "          \n",
    "        \n",
    "    if averageBatchLossPrinting:\n",
    "        print('Average Loss after Epoch %d : %.4f'\n",
    "                   %(epoch, avg_loss/number_of_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_encoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_encoder_input.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_encoder_hidden.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "sss = None\n",
    "for obj in gc.get_objects():\n",
    "    try:\n",
    "        if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n",
    "            print(type(obj), obj.size())\n",
    "            sss=obj\n",
    "    except:\n",
    "        None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_tags.squeeze(-1).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_document_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLL Loss\n",
    "question_mask = tf.sequence_mask(decoder_lengths ,dtype=tf.float32)\n",
    "question_loss = seq2seq.sequence_loss(\n",
    "    logits=decoder_outputs, targets=decoder_labels, weights=question_mask,\n",
    "    name=\"question_loss\")\n",
    "question_loss = tf.Print(question_loss, [question_loss], message=\"question_loss: \")\n",
    "\n",
    "#Suppression Loss\n",
    "lambdaSuppress = 1\n",
    "\n",
    "suppression_loss = lambdaSuppress * tf.reduce_sum(tf.matmul(tf.nn.softmax(decoder_outputs), s_answer))\n",
    "suppression_loss = tf.Print(suppression_loss, [suppression_loss], message=\"suppression_loss: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expression Loss\n",
    "\n",
    "express_loss_f = tf.reduce_sum(-tf.log(tf.multiply(tf.sigmoid(decoder_outputs),e_context) + 10e-7))\n",
    "suppress_loss_f = tf.reduce_sum(-tf.log(tf.multiply((1 - tf.sigmoid(decoder_outputs)),(1 - e_context)) + 10e-7))\n",
    "expression_loss = (express_loss_f + suppress_loss_f)/(32 * 20 * wordToTake)\n",
    "expression_loss = tf.Print(expression_loss, [expression_loss], message=\"expression_loss: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Maximize Entropy Loss\n",
    "entropy_loss = tf.matmul(tf.transpose(dense_output),dense_output)\n",
    "print(dense_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.stack([question_loss,answer_loss,suppression_loss,expression_loss])\n",
    "loss = tf.reduce_sum(tf.multiply(x, lossFlags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_list(*ls):\n",
    "    l =list(zip(*ls))\n",
    "    np.random.shuffle(l)\n",
    "    return zip(*l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expression_contexts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No of features:\",len( batch_input))\n",
    "print(\"No of batches:\",len( batch_input[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "saved_vars = []\n",
    "l = len(tf.all_variables())\n",
    "for i,var in enumerate(tf.all_variables()):\n",
    "    print(i,\"/\",l)\n",
    "    saved_vars.append(var)\n",
    "        \n",
    "print(len(saved_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=3e-3).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "# session.run(tf.variables_initializer(saved_vars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 400\n",
    "loss_flag = np.array([1,1,1,1])\n",
    "\n",
    "import time\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    batch_loss = 0\n",
    "    print(\"Epoch {0}\".format(epoch))\n",
    "    start_time = time.time()\n",
    "    for batchNum in range(len(batch_input[0])):\n",
    "        print(\"Batch : \",batchNum)\n",
    "        t = session.run([optimizer, loss, question_loss, answer_loss, suppression_loss, expression_loss], {\n",
    "            d_tokens: batch_input[0][batchNum],\n",
    "            d_lengths: batch_input[1][batchNum],\n",
    "            a_labels: batch_input[2][batchNum],\n",
    "            encoder_input_mask: batch_input[3][batchNum],\n",
    "            encoder_lengths: batch_input[4][batchNum],\n",
    "            decoder_inputs: batch_input[5][batchNum],\n",
    "            decoder_labels: batch_input[6][batchNum],\n",
    "            decoder_lengths: batch_input[7][batchNum],\n",
    "            s_answer: batch_input[8][batchNum],\n",
    "            e_context: batch_input[9][batchNum],\n",
    "            e_probs: batch_input[10][batchNum],\n",
    "            lossFlags : loss_flag,\n",
    "        })\n",
    "        print(\"Loss: {0}\".format(t[1]))\n",
    "        batch_loss += t[2]\n",
    "    batch_loss /= len(batch_input[0])\n",
    "    end_time = time.time()\n",
    "    print(\"Average Batch Question Loss: {0}\".format(batch_loss))\n",
    "    print(\"Time taken to complete epoch : \" , (end_time-start_time)/60 , \" minutes\")\n",
    "    if batch_loss < minQuestionLoss:\n",
    "        print(\"Turning on all losses\")\n",
    "        #loss_flag = np.array([1,1,1,1])\n",
    "    if(epoch%5 == 0):\n",
    "        print(\"Saving model\")\n",
    "        #saver.save(session, \"qgen-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_glove.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(session, \"qgen-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[5][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "saver.restore(session, 'qgen-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = session.run(answer_tags, {\n",
    "    d_tokens: batch_input[0][0],\n",
    "    d_lengths: batch_input[1][0],\n",
    "})\n",
    "print(answers.shape)\n",
    "print(answers[0])\n",
    "answers = np.argmax(answers, 2)\n",
    "print(answers.shape)\n",
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(276):\n",
    "    print(\"Prediction\")\n",
    "    printAllAns(answers,2,0)\n",
    "    print(\"Ground Truth\")\n",
    "    printAllAns(batch_input[2][2],2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printDoc(batch,num):\n",
    "    for i in batch_input[0][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "\n",
    "def printQues(batch,num):\n",
    "    for i in batch_input[5][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "def printAnsForQuestion(batch, num):\n",
    "    for i in batch_input[5][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "def printAllAns(answers, batch, num):\n",
    "    for i,word in enumerate(batch_input[0][batch][num]):\n",
    "        if answers[num][i] == 1 :\n",
    "            print(look_up_token_reduced(word),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "batchNum = 0\n",
    "\n",
    "helper = seq2seq.GreedyEmbeddingHelper(embedding, tf.fill([batch_input[0][batchNum].shape[0]], START_TOKEN), END_TOKEN)\n",
    "decoder = seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer=projection)\n",
    "decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder, maximum_iterations=max_question_len)\n",
    "decoder_outputs = decoder_outputs.rnn_output\n",
    "\n",
    "\n",
    "questions = session.run(decoder_outputs, {\n",
    "    d_tokens: batch_input[0][batchNum],\n",
    "    d_lengths: batch_input[1][batchNum],\n",
    "    a_labels: batch_input[2][batchNum],\n",
    "    encoder_input_mask: batch_input[3][batchNum],\n",
    "    encoder_lengths: batch_input[4][batchNum],\n",
    "    e_context: batch_input[9][batchNum],\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[9][batchNum].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#questions[:,:,END_TOKEN] = 0\n",
    "qs = np.argmax(questions, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in np.argsort(questions[0][0])[-100:]:\n",
    "    print(sigmoid(questions[0][0][i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.argsort(questions[0][10])[-100:]:\n",
    "    print(sigmoid(questions[0][10][i]),look_up_token_reduced(i), sep=\" \", end= \" \")\n",
    "print(\"\")\n",
    "print(X_train_comp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = 0\n",
    "p2 = 0\n",
    "\n",
    "for i in np.where(batch_input[9][0][0][0] == 1)[0]:\n",
    "    p1 += -np.log(sigmoid(questions[0][0][i]) + 10e-7)\n",
    "    \n",
    "for i in np.where(batch_input[9][0][0][0] == 0)[0]:\n",
    "    p2 += -np.log(sigmoid(questions[0][0][i]) + 10e-7)\n",
    "\n",
    "print(p1)\n",
    "print(p2)\n",
    "print(p2-p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(batch_input[0][batchNum].shape[0]):\n",
    "    print(\"---------------------------------------------------------------------------------------------\")\n",
    "    question = itertools.takewhile(lambda t: t != END_TOKEN, qs[i])\n",
    "    print(\"Generated Question: \" + \" \".join(look_up_token_reduced(token) for token in question))\n",
    "    print(\"Ground Truth Question: \")\n",
    "    printQues(batchNum,i)\n",
    "    print(\"Ground Truth Answer: \", X_train_ans_shuffled[batch_size*batchNum + i])\n",
    "    print(\"Context:\")\n",
    "    printDoc(batchNum,i)\n",
    "    print(\"---------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def trainDecoder(batch_num, example_num, answer_encoder_hidden, question_encoder_hidden, question_decoder_hidden):\n",
    "\n",
    "batch_num = 0\n",
    "example_num = 0\n",
    "    \n",
    "document_token = batch_input[0][batch_num][example_num:example_num+1]\n",
    "document_length = batch_input[1][batch_num][example_num]\n",
    "answer_label = batch_input[2][batch_num][example_num:example_num+1]\n",
    "answer_mask = batch_input[3][batch_num][example_num:example_num+1]\n",
    "answer_length = batch_input[4][batch_num][example_num]\n",
    "question_input_token = batch_input[5][batch_num][example_num:example_num+1]\n",
    "question_output_token = batch_input[6][batch_num][example_num:example_num+1]\n",
    "question_length = batch_input[7][batch_num][example_num:example_num+1]\n",
    "suppression_answer = batch_input[8][batch_num][example_num:example_num+1]\n",
    "\n",
    "mask = np.zeros(document_length)\n",
    "mask[0:document_length] = 1\n",
    "\n",
    "inp = Variable(torch.from_numpy(document_token).long())\n",
    "\n",
    "labels = Variable(torch.from_numpy(answer_label)).long()\n",
    "if use_cuda:\n",
    "    labels = labels.cuda()\n",
    "\n",
    "################################### Answer Encoder + Tagging    \n",
    "\n",
    "optimizer.zero_grad()\n",
    "embedded_inp = embedder(inp)\n",
    "if use_cuda:\n",
    "    embedded_inp = embedder(inp).cuda()\n",
    "\n",
    "answer_tags, answer_outputs, answer_encoder_hidden = answerEncoder(embedded_inp, answer_encoder_hidden)\n",
    "\n",
    "\n",
    "if use_cuda:\n",
    "    answer_outputs = answer_outputs.cuda()\n",
    "    answer_tags = answer_tags.cuda()\n",
    "\n",
    "\n",
    "t_document_mask = Variable(torch.from_numpy(mask)).float()\n",
    "if use_cuda:\n",
    "    t_document_mask = t_document_mask.cuda()\n",
    "\n",
    "outputs = answer_tags.squeeze(-1) * t_document_mask\n",
    "\n",
    "\n",
    "answer_loss = criterion1(outputs, labels.float())\n",
    "################################### Q Encoder    \n",
    "\n",
    "t_answer_mask = Variable(torch.from_numpy(answer_mask)).float()\n",
    "if use_cuda:\n",
    "    t_answer_mask = t_answer_mask.cuda()\n",
    "\n",
    "# masking the non-answer embeddings\n",
    "question_encoder_input = torch.matmul(t_answer_mask, answer_outputs.float())\n",
    "question_encoder_input = question_encoder_input[0:answer_length]\n",
    "\n",
    "_, question_encoder_hidden = questionEncoder(question_encoder_input, question_encoder_hidden)\n",
    "\n",
    "\n",
    "question_loss = 0\n",
    "\n",
    "question_decoder_hidden = question_encoder_hidden\n",
    "embedded_inputs = embedder(torch.from_numpy(question_input_token).long())\n",
    "if use_cuda:\n",
    "    embedded_inputs = embedded_inputs.cuda()\n",
    "output_labels = Variable(torch.from_numpy(question_output_token).long())\n",
    "if use_cuda:\n",
    "    output_labels = output_labels.cuda()\n",
    "\n",
    "for quesL in range(question_length):\n",
    "    decoder_output, question_decoder_hidden = questionDecoder(\n",
    "        embedded_inputs[quesL:quesL+1].unsqueeze(1),\n",
    "        question_decoder_hidden)\n",
    "\n",
    "    final_output = questionGenerator(decoder_output)\n",
    "\n",
    "    output_label = Variable(torch.zeros(1,2000))\n",
    "    if use_cuda:\n",
    "        output_label = output_label.cuda()\n",
    "\n",
    "    output_label[:, question_input_token[quesL]] = 1\n",
    "    question_loss += criterion2(final_output.squeeze(0), \n",
    "                               output_labels[quesL:quesL+1])\n",
    "    #question_loss += criterion2(final_output.squeeze(0), output_label)\n",
    "\n",
    "\n",
    "\n",
    "net_loss = answer_loss + question_loss\n",
    "net_loss.backward()\n",
    "optimizer.step()\n",
    "    \n",
    "    #return answer_loss, question_loss, net_loss, answer_encoder_hidden, question_encoder_hidden, question_decoder_hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if epoch % 2 == 0:\n",
    "    print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verboseBatchPrinting = True\n",
    "averageBatchLossPrinting = True\n",
    "backward_called = False\n",
    "\n",
    "num_epochs = 100\n",
    "answer_encoder_hidden = answerEncoder.initHidden()\n",
    "question_encoder_hidden = questionEncoder.initHidden()\n",
    "question_decoder_hidden = None\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    avg_loss_per_epoch = 0\n",
    "    for batch_num in range(len(batch_input[0])):\n",
    "        for example in range(len(batch_input[0][0])):\n",
    "            answer_loss, question_loss, net_loss, answer_encoder_hidden, question_encoder_hidden, question_decoder_hidden = \\\n",
    "            trainDecoder(batch_num, example, answer_encoder_hidden, question_encoder_hidden, question_decoder_hidden)\n",
    "            if verboseBatchPrinting:\n",
    "                print ('Batch: %d \\t Example: %d \\tEpoch : %d\\tNet Loss: %.4f \\tAnswer Loss: %.4f \\tQuestion Loss: %.4f' \n",
    "                       %(batch_num, example, epoch, net_loss.data[0], answer_loss.data[0], question_loss.data[0]))\n",
    "            avg_loss+= net_loss.data[0]\n",
    "            \n",
    "    if averageBatchLossPrinting:\n",
    "        print('Average Loss after Epoch %d : %.4f'\n",
    "                   %(epoch, avg_loss/examples_to_take_train))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
