{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=en_US.UTF-8\n",
    "!CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "from embedding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    !pip install nltk\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('../train-v1.1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ra2630/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ra2630/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltkStopWords = stopwords.words('english')\n",
    "punctuations = [',', '?', '.', '-',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(data):\n",
    "    contexts = []\n",
    "    qas = []\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        for j in range(len(data[\"data\"][i][\"paragraphs\"])):\n",
    "            contexts.append(data[\"data\"][i][\"paragraphs\"][j][\"context\"])\n",
    "            qas.append(data[\"data\"][i][\"paragraphs\"][j][\"qas\"])\n",
    "    return (contexts,qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CapPassage = False\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "contexts,qas = extractor(data)\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind,ind+sll\n",
    "    return (-1,-1)\n",
    "\n",
    "def capPassage(passage,answer,cap_length = 30):\n",
    "    y = np.zeros(cap_length)\n",
    "    left,right = find_sub_list(answer,passage)\n",
    "    if(left==-1):\n",
    "        return passage[0:cap_length]\n",
    "    left = left - int((cap_length - len(answer))/2)\n",
    "    right = right + int((cap_length + len(answer))/2)\n",
    "    if(left < 0):\n",
    "        left = 0\n",
    "    if(right > len(passage)):\n",
    "        right = len(passage)\n",
    "    return passage[left:right]\n",
    "    \n",
    "def findAnsVec(answer,passage):\n",
    "    ans = np.zeros((len(passage)))\n",
    "    start,end = find_sub_list(answer,passage)\n",
    "    if(start==-1):\n",
    "        start = passage.index(answer[0])\n",
    "        end = start + len(answer)\n",
    "    ans[start:end] = 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_comp_all = []\n",
    "X_train_comp_ans_all = []\n",
    "X_train_ans_all = []\n",
    "Y_train_ques_all = []\n",
    "invalid = 0\n",
    "X_train_ans_label_all = []\n",
    "for i,context in enumerate(contexts):\n",
    "    passage = word_tokenize(context.lower())\n",
    "    \n",
    "    a_lab = np.zeros(len(passage))\n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "        start,end = find_sub_list(answer,passage)\n",
    "        if start == -1:\n",
    "            continue\n",
    "        a_lab[start:end+1] = 1\n",
    "            \n",
    "            \n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        try:\n",
    "            question = word_tokenize(qas[i][j]['question'].lower())\n",
    "            answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "            \n",
    "            if CapPassage:\n",
    "                cappedPassage = capPassage(passage,answer)\n",
    "            else:\n",
    "                cappedPassage = passage\n",
    "            \n",
    "            X_train_comp_ans_all.append(findAnsVec(answer,passage))\n",
    "            X_train_ans_label_all.append(a_lab)\n",
    "            X_train_comp_all.append(cappedPassage)\n",
    "            X_train_ans_all.append(answer)\n",
    "            Y_train_ques_all.append(question)\n",
    "        except Exception as e:\n",
    "            invalid = invalid+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "def findKMostFrequentWords(k):\n",
    "    ctr = Counter([item for sublist in X_train_comp_all for item in sublist] + [item for sublist in Y_train_ques_all for item in sublist])\n",
    "    sorted_ctr = sorted(ctr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return [item[0] for item in sorted_ctr[0:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordToTake = 2000\n",
    "words = findKMostFrequentWords(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 300)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_word_to_idx_reduced = {}\n",
    "_idx_to_word_reduced = []\n",
    "\n",
    "\n",
    "def _add_word_reduced(word):\n",
    "    idx = len(_idx_to_word_reduced)\n",
    "    _word_to_idx_reduced[word] = idx\n",
    "    _idx_to_word_reduced.append(word)\n",
    "    return idx\n",
    "\n",
    "\n",
    "UNKNOWN_TOKEN = _add_word_reduced(UNKNOWN_WORD)\n",
    "START_TOKEN = _add_word_reduced(START_WORD)\n",
    "END_TOKEN = _add_word_reduced(END_WORD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dimensions = glove.shape[1]\n",
    "reduced_glove = []\n",
    "reduced_glove.append(np.zeros(dimensions))\n",
    "reduced_glove.append(-np.ones(dimensions))\n",
    "reduced_glove.append(np.ones(dimensions))\n",
    "\n",
    "for word in words:\n",
    "    l = look_up_word(word)\n",
    "    if(l != UNKNOWN_TOKEN):\n",
    "        idx = _add_word_reduced(word)\n",
    "        reduced_glove.append(glove[l])\n",
    "        if(len(reduced_glove) == wordToTake):\n",
    "            break\n",
    "        \n",
    "def look_up_word_reduced(word):\n",
    "    return _word_to_idx_reduced.get(word, UNKNOWN_TOKEN)\n",
    "\n",
    "\n",
    "def look_up_token_reduced(token):\n",
    "    return _idx_to_word_reduced[token]\n",
    "\n",
    "reduced_glove = np.array(reduced_glove)\n",
    "reduced_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda2.cims.nyu.edu\n",
      "1493\n",
      "rev\n",
      ".\n",
      "john\n",
      "j.\n",
      "cavanaugh\n",
      ",\n",
      "c.s.c\n",
      ".\n",
      "served\n",
      "more\n",
      "than\n",
      "half\n",
      ",\n",
      "lobund\n",
      "institute\n",
      "for\n",
      "animal\n",
      "studies\n",
      "and\n",
      "medieval\n",
      "institute\n",
      ".\n",
      "hall\n",
      "of\n",
      "liberal\n",
      "arts\n",
      "(\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "print(invalid)\n",
    "for i in np.where(X_train_ans_label_all[110] == 1)[0]:\n",
    "    print(X_train_comp_all[110][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['architecturally', ',', 'the', 'school', 'has', 'a', 'catholic', 'character', '.', 'atop', 'the', 'main', 'building', \"'s\", 'gold', 'dome', 'is', 'a', 'golden', 'statue', 'of', 'the', 'virgin', 'mary', '.', 'immediately', 'in', 'front', 'of', 'the', 'main', 'building', 'and', 'facing', 'it', ',', 'is', 'a', 'copper', 'statue', 'of', 'christ', 'with', 'arms', 'upraised', 'with', 'the', 'legend', '``', 'ad', 'me', 'omnes', \"''\", '.', 'next', 'to', 'the', 'main', 'building', 'is', 'the', 'basilica', 'of', 'the', 'sacred', 'heart', '.', 'immediately', 'behind', 'the', 'basilica', 'is', 'the', 'grotto', ',', 'a', 'marian', 'place', 'of', 'prayer', 'and', 'reflection', '.', 'it', 'is', 'a', 'replica', 'of', 'the', 'grotto', 'at', 'lourdes', ',', 'france', 'where', 'the', 'virgin', 'mary', 'reputedly', 'appeared', 'to', 'saint', 'bernadette', 'soubirous', 'in', '1858.', 'at', 'the', 'end', 'of', 'the', 'main', 'drive', '(', 'and', 'in', 'a', 'direct', 'line', 'that', 'connects', 'through', '3', 'statues', 'and', 'the', 'gold', 'dome', ')', ',', 'is', 'a', 'simple', ',', 'modern', 'stone', 'statue', 'of', 'mary', '.']\n",
      "['saint', 'bernadette', 'soubirous']\n"
     ]
    }
   ],
   "source": [
    "print(X_train_comp_all[0])\n",
    "print(X_train_ans_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 104)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_sub_list(X_train_ans_all[0] , X_train_comp_all[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1493\n",
      "['the', 'success', 'of', 'its', 'football', 'team', 'made', 'notre', 'dame', 'a', 'household', 'name', '.', 'the', 'success', 'of', 'note', 'dame', 'reflected', 'rising', 'status', 'of', 'irish', 'americans', 'and', 'catholics', 'in', 'the', '1920s', '.', 'catholics', 'rallied', 'up', 'around', 'the', 'team', 'and', 'listen', 'to', 'the', 'games', 'on', 'the', 'radio', ',', 'especially', 'when', 'it', 'knocked', 'off', 'the', 'schools', 'that', 'symbolized', 'the', 'protestant', 'establishment', 'in', 'america', 'â€”', 'harvard', ',', 'yale', ',', 'princeton', ',', 'and', 'army', '.', 'yet', 'this', 'role', 'as', 'high-profile', 'flagship', 'institution', 'of', 'catholicism', 'made', 'it', 'an', 'easy', 'target', 'of', 'anti-catholicism', '.', 'the', 'most', 'remarkable', 'episode', 'of', 'violence', 'was', 'the', 'clash', 'between', 'notre', 'dame', 'students', 'and', 'the', 'ku', 'klux', 'klan', 'in', '1924.', 'nativism', 'and', 'anti-catholicism', ',', 'especially', 'when', 'directed', 'towards', 'immigrants', ',', 'were', 'cornerstones', 'of', 'the', 'kkk', \"'s\", 'rhetoric', ',', 'and', 'notre', 'dame', 'was', 'seen', 'as', 'a', 'symbol', 'of', 'the', 'threat', 'posed', 'by', 'the', 'catholic', 'church', '.', 'the', 'klan', 'decided', 'to', 'have', 'a', 'week-long', 'klavern', 'in', 'south', 'bend', '.', 'clashes', 'with', 'the', 'student', 'body', 'started', 'on', 'march', '17', ',', 'when', 'students', ',', 'aware', 'of', 'the', 'anti-catholic', 'animosity', ',', 'blocked', 'the', 'klansmen', 'from', 'descending', 'from', 'their', 'trains', 'in', 'the', 'south', 'bend', 'station', 'and', 'ripped', 'the', 'kkk', 'clothes', 'and', 'regalia', '.', 'on', 'may', '19', 'thousands', 'of', 'students', 'massed', 'downtown', 'protesting', 'the', 'klavern', ',', 'and', 'only', 'the', 'arrival', 'of', 'college', 'president', 'fr', '.', 'matthew', 'walsh', 'prevented', 'any', 'further', 'clashes', '.', 'the', 'next', 'day', ',', 'football', 'coach', 'knute', 'rockne', 'spoke', 'at', 'a', 'campus', 'rally', 'and', 'implored', 'the', 'students', 'to', 'obey', 'the', 'college', 'president', 'and', 'refrain', 'from', 'further', 'violence', '.', 'a', 'few', 'days', 'later', 'the', 'klavern', 'broke', 'up', ',', 'but', 'the', 'hostility', 'shown', 'by', 'the', 'students', 'was', 'an', 'omen', 'and', 'a', 'contribution', 'to', 'the', 'downfall', 'of', 'the', 'kkk', 'in', 'indiana', '.']\n",
      "['the', 'ku', 'klux', 'klan']\n",
      "['notre', 'dame', 'students', 'had', 'a', 'showdown', 'in', '1924', 'with', 'which', 'anti-catholic', 'group', '?']\n",
      "['weeks', 'after', 'ending', 'the', 'sex', 'pistols', ',', 'lydon', 'formed', 'the', 'experimental', 'group', 'public', 'image', 'ltd', 'and', 'declared', 'the', 'project', 'to', 'be', '``', 'anti', 'music', 'of', 'any', 'kind', \"''\", '.', 'public', 'image', 'and', 'other', 'acts', 'such', 'as', 'the', 'pop', 'group', 'and', 'the', 'slits', 'had', 'begun', 'experimenting', 'with', 'dance', 'music', ',', 'dub', 'production', 'techniques', 'and', 'the', 'avant-garde', ',', 'while', 'punk-indebted', 'manchester', 'acts', 'such', 'as', 'joy', 'division', ',', 'the', 'fall', 'and', 'a', 'certain', 'ratio', 'developed', 'unique', 'styles', 'which', 'drew', 'on', 'a', 'similarly', 'disparate', 'range', 'of', 'influences', 'across', 'music', 'and', 'modernist', 'literature', '.', 'bands', 'such', 'as', 'scritti', 'politti', ',', 'gang', 'of', 'four', 'and', 'this', 'heat', 'incorporated', 'leftist', 'political', 'philosophy', 'and', 'their', 'own', 'art', 'school', 'studies', 'in', 'their', 'work', '.']\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "['public', 'image', 'ltd']\n",
      "[0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "['what', 'band', 'did', 'johnny', 'rotten', 'form', 'after', 'the', 'sex', 'pistols', '?']\n"
     ]
    }
   ],
   "source": [
    "print(invalid)\n",
    "print(X_train_comp_all[101])\n",
    "print(X_train_ans_all[101])\n",
    "print(Y_train_ques_all[101])\n",
    "\n",
    "c = list(zip(X_train_comp_all,X_train_comp_ans_all, X_train_ans_all, X_train_ans_label_all,Y_train_ques_all))\n",
    "np.random.shuffle(c)\n",
    "X_train_comp_all_shuffled,X_train_comp_ans_all_shuffled, X_train_ans_shuffled, X_train_ans_label_shuffled,Y_train_ques_all_shuffled = zip(*c)\n",
    "\n",
    "print(X_train_comp_all_shuffled[101])\n",
    "print(X_train_comp_ans_all_shuffled[101])\n",
    "print(X_train_ans_shuffled[101])\n",
    "print(X_train_ans_label_shuffled[101])\n",
    "print(Y_train_ques_all_shuffled[101])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_to_take_train = 32\n",
    "\n",
    "X_train_comp = X_train_comp_all_shuffled[0:examples_to_take_train]\n",
    "X_train_comp_ans = X_train_comp_ans_all_shuffled[0:examples_to_take_train]\n",
    "X_train_ans = X_train_ans_shuffled[0:examples_to_take_train]\n",
    "X_train_ans_label = X_train_ans_label_shuffled[0:examples_to_take_train]\n",
    "Y_train_ques = Y_train_ques_all_shuffled[0:examples_to_take_train]\n",
    "answer_indices = [np.where(x==1)[0].tolist() for x in X_train_comp_ans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_document_len = len(max(X_train_comp,key=len))\n",
    "max_answer_len = len(max(X_train_ans,key=len))\n",
    "max_question_len = len(max(Y_train_ques,key=len)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "answer_masks = np.zeros((examples_to_take_train, max_answer_len, max_document_len), dtype=np.int32)\n",
    "answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "question_input_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "question_output_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "suppression_answer = np.zeros((examples_to_take_train, reduced_glove.shape[0], 1),dtype=np.int32)\n",
    "expression_contexts = np.zeros((examples_to_take_train, max_question_len,reduced_glove.shape[0]),dtype=np.int32)\n",
    "expression_probabilities = np.zeros((examples_to_take_train, max_question_len,reduced_glove.shape[0]),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 256)\n"
     ]
    }
   ],
   "source": [
    "print(answer_labels.shape)\n",
    "for i in range(examples_to_take_train):\n",
    "    answer_labels[i,0:len(X_train_ans_label[i])] = X_train_ans_label[i]\n",
    "    for j, word in enumerate(X_train_comp[i]):\n",
    "        document_tokens[i, j] = look_up_word_reduced(word)\n",
    "    document_lengths[i] = len(X_train_comp[i])\n",
    "\n",
    "    for j, index in enumerate(answer_indices[i]):\n",
    "        answer_masks[i, j, index] = 1\n",
    "    answer_lengths[i] = len(answer_indices[i])\n",
    "    \n",
    "    #print(Y_train_ques[i])\n",
    "    question_input_words = ([START_WORD] + Y_train_ques[i])\n",
    "    question_output_words = (Y_train_ques[i] + [END_WORD])\n",
    "\n",
    "    for j, word in enumerate(question_input_words):\n",
    "            question_input_tokens[i, j] = look_up_word_reduced(word)\n",
    "    for j, word in enumerate(question_output_words):\n",
    "        question_output_tokens[i, j] = look_up_word_reduced(word)\n",
    "    question_lengths[i] = len(question_input_words)\n",
    "    \n",
    "    for j, word in enumerate(X_train_ans[i]):\n",
    "        if(word not in Y_train_ques[i]):\n",
    "            suppression_answer[i, look_up_word_reduced(word),:] = 1\n",
    "            \n",
    "    words_to_consider_expression = set(X_train_comp[i] + nltkStopWords + punctuations)\n",
    "\n",
    "    for j,word in enumerate(words_to_consider_expression):\n",
    "        expression_contexts[i,:,look_up_word_reduced(word)] = 1\n",
    "        \n",
    "    for j,word in enumerate(words_to_consider_expression):\n",
    "        expression_probabilities[i,:,look_up_word_reduced(word)] = len(np.where(expression_contexts[i][0] == 1)[0]) / float(wordToTake)\n",
    "    expression_probabilities[i,:,np.where(expression_probabilities[i][0] == 0)[0]] = len(np.where(expression_contexts[i][0] == 0)[0]) / float(wordToTake)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'s\",\n",
       " '(',\n",
       " ')',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '1577',\n",
       " '1672',\n",
       " '16th',\n",
       " '1734',\n",
       " '?',\n",
       " 'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'adal',\n",
       " 'adam',\n",
       " 'afar',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'asaita',\n",
       " 'ascension',\n",
       " 'at',\n",
       " 'aussa',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'bin',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'capital',\n",
       " 'century',\n",
       " 'clan',\n",
       " 'colonial',\n",
       " 'come',\n",
       " 'conjunction',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'declined',\n",
       " 'denkel',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'din',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'dynasty',\n",
       " 'each',\n",
       " 'end',\n",
       " 'eritrea',\n",
       " 'established',\n",
       " 'existence',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'harar',\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'head',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'imam',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'jasa',\n",
       " 'just',\n",
       " 'kedafu',\n",
       " 'last',\n",
       " 'leader',\n",
       " 'll',\n",
       " 'lowlands',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'marked',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'moved',\n",
       " 'mudaito',\n",
       " 'muhammed',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'new',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'period',\n",
       " 'point',\n",
       " 'polity',\n",
       " 'power',\n",
       " 're',\n",
       " 'recorded',\n",
       " 's',\n",
       " 'same',\n",
       " 'seized',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'sophisticated',\n",
       " 'split',\n",
       " 'start',\n",
       " 'such',\n",
       " 'sultanate',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'throne',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'umar',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'would',\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_to_consider_expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1862\n",
      "138\n",
      "1862\n",
      "138\n"
     ]
    }
   ],
   "source": [
    "print(len(np.where(expression_contexts[10][0] == 0)[0]))\n",
    "print(len(np.where(expression_contexts[10][0] == 1)[0]))\n",
    "\n",
    "print(len(np.where(expression_probabilities[10][0] > 0.5)[0]))\n",
    "print(len(np.where(expression_probabilities[10][0] < 0.5)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(data):\n",
    "    flat_list = [item for sublist in data for item in sublist]\n",
    "    vocabulary = sorted(set(flat_list))\n",
    "    vocabulary.append(\"<UNK>\")\n",
    "    vocabulary.append(\"unk\")\n",
    "    vocabulary.append(\"eos\")\n",
    "    vocabulary = [\"<EOS>\"] + vocabulary\n",
    "    word_to_index = { word:i for i,word in enumerate(vocabulary) }\n",
    "    index_to_word = { i:word for i,word in enumerate(vocabulary) }\n",
    "    return (vocabulary,word_to_index,index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 300)\n",
      "1630\n",
      "95\n",
      "1580\n"
     ]
    }
   ],
   "source": [
    "print(reduced_glove.shape)\n",
    "vocabulary_comp,word_to_index_comp,index_to_word_comp = create_vocabulary(X_train_comp + Y_train_ques)\n",
    "print(len(vocabulary_comp))\n",
    "print(word_to_index_comp[\"?\"])\n",
    "print(word_to_index_comp[\"what\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_vector(data,vocabulary,word_to_index,index_to_word, maxLen):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,word in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        if(word not in word_to_index):\n",
    "            word = \"<UNK>\"\n",
    "        one_hot[i][word_to_index[word]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def create_one_hot_vector_from_indices(data,maxLen,vocabulary):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,indice in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        one_hot[i][int(indice)] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def create_one_hot_training_Set(data,maxLen,vocabulary):\n",
    "    one_hot_data = np.zeros([data.shape[0],maxLen,len(vocabulary)])\n",
    "    for i in range(data.shape[0]):\n",
    "        one_hot_data[i] = create_one_hot_vector_from_indices(data[i],maxLen,vocabulary)\n",
    "    return one_hot_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices_glove(X,max_len):\n",
    "    \n",
    "    m = len(X)                                 \n",
    "    \n",
    "    X_indices = np.full([m,max_len],look_up_word_reduced(END_WORD))\n",
    "    \n",
    "    for i in range(m):\n",
    "        j = 0\n",
    "        for w in X[i]:\n",
    "            if(j>=max_len):\n",
    "                break;\n",
    "            \n",
    "            X_indices[i, j] = look_up_word_reduced(w)\n",
    "            j = j+1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = sentences_to_indices_glove(X_train_comp, max_document_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   3,  616,  704,    5,    0,   11,    3,  262,   21,    0,  144,\n",
       "         56,  972,  456,   34,    0,   64,    0,   17,    0,    6,   30,\n",
       "          0,    3,    0,    0,    5,    3,    0,    0,  370,    0,    4,\n",
       "          0,  561,    0, 1355,    0,    0,   20,    0,    4,    0,    4,\n",
       "          0,    0,    4, 1655,    4,    0,    4, 1655,    7, 1331,    4,\n",
       "        811,    6,    3,  704,   11,   40,    3,  757,    0,    0,   17,\n",
       "         80,    0,    0,  340,    3,    0,  231,    6,    3,  704,   11,\n",
       "          3,    0,    7,    0,  428,   17,    0,    0,   17,    3,  262,\n",
       "         21,  184,  544,    5, 1437,    0,    4,    0,  245,  290,  796,\n",
       "          7,    0,    4, 1455,  238,    0,    7,  588,    3,  329, 1872,\n",
       "          0,  290,    6,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2,    2,    2,    2,    2,    2,    2,    2,    2,\n",
       "          2,    2,    2])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 256)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_tokens = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "#document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "#answer_masks = np.zeros((examples_to_take_train, max_answer_len, max_document_len), dtype=np.int32)\n",
    "#answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#question_input_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "#question_output_tokens = np.zeros((examples_to_take_train, max_question_len), dtype=np.int32)\n",
    "#question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "#suppression_answer = np.zeros((examples_to_take_train,max_answer_len),dtype=np.int32)\n",
    "#expression_contexts = np.zeros((examples_to_take_train, max_question_len,reduced_glove.shape[0]),dtype=np.int32)\n",
    "#expression_probabilities = np.zeros((examples_to_take_train, max_question_len,reduced_glove.shape[0]),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def createBatch(inputs,batch_size,shuffle=False):\n",
    "    outputs = []\n",
    "    num_batches = math.ceil(len(inputs[0])/batch_size)\n",
    "    \n",
    "    for index,inp in enumerate(inputs):\n",
    "        start = 0\n",
    "        output = []\n",
    "        for i in range(num_batches-1):\n",
    "            maxD = max(inputs[1][start:start+batch_size])\n",
    "            maxA = max(inputs[4][start:start+batch_size])\n",
    "            maxQ = max(inputs[7][start:start+batch_size])\n",
    "            if index == 0 or index == 2:\n",
    "                output.append(inp[start:start+batch_size,0:maxD]) \n",
    "            elif index==3:\n",
    "                output.append(inp[start:start+batch_size,0:maxA,0:maxD]) \n",
    "            elif index==5 or index==6:\n",
    "                output.append(inp[start:start+batch_size,0:maxQ])\n",
    "            elif index==9 or index==10:\n",
    "                output.append(inp[start:start+batch_size,0:maxQ,:])\n",
    "            else: \n",
    "                output.append(inp[start:start+batch_size])\n",
    "            start = start + batch_size\n",
    "        \n",
    "        # Remaining training sample i.e. training mod batch_size\n",
    "        maxD = max(inputs[1][start:])\n",
    "        maxA = max(inputs[4][start:])\n",
    "        maxQ = max(inputs[7][start:])\n",
    "        if index == 0 or index == 2:\n",
    "            output.append(inp[start:,0:maxD]) \n",
    "        elif index==3:\n",
    "            output.append(inp[start:,0:maxA,0:maxD]) \n",
    "        elif index==5 or index==6:\n",
    "            output.append(inp[start:,0:maxQ])\n",
    "        elif index==9 or index==10:\n",
    "            output.append(inp[start:,0:maxQ,:]) \n",
    "        else: \n",
    "            output.append(inp[start:])\n",
    "        outputs.append(output)\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "batch_input = createBatch([document_tokens,document_lengths,answer_labels,answer_masks,answer_lengths,question_input_tokens,question_output_tokens,question_lengths,suppression_answer,expression_contexts,expression_probabilities]\n",
    "                    ,batch_size)\n",
    "number_of_batches = len(batch_input[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "use_cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "use_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, output_size)\n",
    "        \n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(reduced_glove).float())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerEncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(AnswerEncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True, bidirectional=True).cuda() #Input_size = Hidden_Size\n",
    "        self.fc = nn.Linear(hidden_size*2, 1).cuda()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        final_output = self.fc(output)\n",
    "        final_output = F.sigmoid(final_output)\n",
    "        return final_output, output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(2, batch_size, self.hidden_size)) #2 for BiDirectional\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionEncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super(QuestionEncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True).cuda()\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super(QuestionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True).cuda()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result\n",
    "\n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self,input_size, output_size):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size).cuda()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "    \n",
    "    \n",
    "class QuestionGenerationFC(nn.Module):\n",
    "    def __init__(self,input_size, output_size):\n",
    "        super(QuestionGenerationFC, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.fc = nn.Linear(hidden_size, output_size).cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        output = F.log_softmax(output.view(-1,1,1)).view(1,1,-1)\n",
    "        return output\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters =  20\n"
     ]
    }
   ],
   "source": [
    "hidden_size = reduced_glove.shape[1]\n",
    "\n",
    "embedder = Embedder(input_size = reduced_glove.shape[0], output_size = reduced_glove.shape[1])\n",
    "fcLayer = FCLayer(hidden_size*2, hidden_size)\n",
    "answerEncoder = AnswerEncoderRNN(input_size = hidden_size, hidden_size=int(hidden_size/2))\n",
    "questionEncoder = QuestionEncoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionDecoder = QuestionDecoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionGenerator = QuestionGenerationFC(input_size = hidden_size, output_size=reduced_glove.shape[0])\n",
    "\n",
    "train_param = []\n",
    "\n",
    "for model in [embedder, answerEncoder, questionEncoder, questionDecoder, questionGenerator]:\n",
    "    train_param += [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "print(\"Number of trainable parameters = \", len(train_param))\n",
    "\n",
    "optimizer = torch.optim.Adam(train_param, 0.0001)\n",
    "criterion1 = nn.BCELoss()\n",
    "#criterion2 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded input :  torch.Size([4, 113, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ra2630/miniconda3/envs/qgen/lib/python3.6/site-packages/torch/nn/functional.py:1189: UserWarning: Using a target size (torch.Size([4, 113, 1])) that is different to the input size (torch.Size([4, 113])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n",
      "/data/ra2630/miniconda3/envs/qgen/lib/python3.6/site-packages/ipykernel_launcher.py:35: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 1\tNet Loss: 342.8704 \tAnswer Loss: 0.5786 \tQuestion Loss: 342.8704\n",
      "Embedded input :  torch.Size([4, 186, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ra2630/miniconda3/envs/qgen/lib/python3.6/site-packages/torch/nn/functional.py:1189: UserWarning: Using a target size (torch.Size([4, 186, 1])) that is different to the input size (torch.Size([4, 186])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1 \t Epoch : 1\tNet Loss: 334.8837 \tAnswer Loss: 1.1303 \tQuestion Loss: 334.8837\n",
      "Embedded input :  torch.Size([4, 231, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ra2630/miniconda3/envs/qgen/lib/python3.6/site-packages/torch/nn/functional.py:1189: UserWarning: Using a target size (torch.Size([4, 231, 1])) that is different to the input size (torch.Size([4, 231])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 2 \t Epoch : 1\tNet Loss: 333.1820 \tAnswer Loss: 1.0240 \tQuestion Loss: 333.1820\n",
      "Embedded input :  torch.Size([4, 256, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ra2630/miniconda3/envs/qgen/lib/python3.6/site-packages/torch/nn/functional.py:1189: UserWarning: Using a target size (torch.Size([4, 256, 1])) that is different to the input size (torch.Size([4, 256])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 3 \t Epoch : 1\tNet Loss: 454.6831 \tAnswer Loss: 0.7296 \tQuestion Loss: 454.6831\n",
      "Embedded input :  torch.Size([4, 142, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ra2630/miniconda3/envs/qgen/lib/python3.6/site-packages/torch/nn/functional.py:1189: UserWarning: Using a target size (torch.Size([4, 142, 1])) that is different to the input size (torch.Size([4, 142])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 4 \t Epoch : 1\tNet Loss: 491.2371 \tAnswer Loss: 0.7917 \tQuestion Loss: 491.2371\n",
      "Embedded input :  torch.Size([4, 175, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ra2630/miniconda3/envs/qgen/lib/python3.6/site-packages/torch/nn/functional.py:1189: UserWarning: Using a target size (torch.Size([4, 175, 1])) that is different to the input size (torch.Size([4, 175])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 5 \t Epoch : 1\tNet Loss: 429.5011 \tAnswer Loss: 0.9265 \tQuestion Loss: 429.5011\n",
      "Embedded input :  torch.Size([4, 195, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ra2630/miniconda3/envs/qgen/lib/python3.6/site-packages/torch/nn/functional.py:1189: UserWarning: Using a target size (torch.Size([4, 195, 1])) that is different to the input size (torch.Size([4, 195])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 6 \t Epoch : 1\tNet Loss: 383.4028 \tAnswer Loss: 1.0696 \tQuestion Loss: 383.4028\n",
      "Embedded input :  torch.Size([4, 191, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/ra2630/miniconda3/envs/qgen/lib/python3.6/site-packages/torch/nn/functional.py:1189: UserWarning: Using a target size (torch.Size([4, 191, 1])) that is different to the input size (torch.Size([4, 191])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 7 \t Epoch : 1\tNet Loss: 314.2456 \tAnswer Loss: 0.7292 \tQuestion Loss: 314.2456\n",
      "Average Loss after Epoch 1 : 385.5007\n",
      "Embedded input :  torch.Size([4, 113, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 0 \t Epoch : 2\tNet Loss: 335.3247 \tAnswer Loss: 0.5789 \tQuestion Loss: 335.3247\n",
      "Embedded input :  torch.Size([4, 186, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 1 \t Epoch : 2\tNet Loss: 327.7365 \tAnswer Loss: 1.1305 \tQuestion Loss: 327.7365\n",
      "Embedded input :  torch.Size([4, 231, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 2 \t Epoch : 2\tNet Loss: 325.8122 \tAnswer Loss: 1.0239 \tQuestion Loss: 325.8122\n",
      "Embedded input :  torch.Size([4, 256, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 3 \t Epoch : 2\tNet Loss: 446.1938 \tAnswer Loss: 0.7294 \tQuestion Loss: 446.1938\n",
      "Embedded input :  torch.Size([4, 142, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 4 \t Epoch : 2\tNet Loss: 482.9841 \tAnswer Loss: 0.7919 \tQuestion Loss: 482.9841\n",
      "Embedded input :  torch.Size([4, 175, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 5 \t Epoch : 2\tNet Loss: 420.8750 \tAnswer Loss: 0.9274 \tQuestion Loss: 420.8750\n",
      "Embedded input :  torch.Size([4, 195, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 6 \t Epoch : 2\tNet Loss: 374.9742 \tAnswer Loss: 1.0701 \tQuestion Loss: 374.9742\n",
      "Embedded input :  torch.Size([4, 191, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 7 \t Epoch : 2\tNet Loss: 306.5300 \tAnswer Loss: 0.7298 \tQuestion Loss: 306.5300\n",
      "Average Loss after Epoch 2 : 377.5538\n",
      "Embedded input :  torch.Size([4, 113, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 0 \t Epoch : 3\tNet Loss: 327.9172 \tAnswer Loss: 0.5807 \tQuestion Loss: 327.9172\n",
      "Embedded input :  torch.Size([4, 186, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 1 \t Epoch : 3\tNet Loss: 320.5028 \tAnswer Loss: 1.1323 \tQuestion Loss: 320.5028\n",
      "Embedded input :  torch.Size([4, 231, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 2 \t Epoch : 3\tNet Loss: 318.0855 \tAnswer Loss: 1.0249 \tQuestion Loss: 318.0855\n",
      "Embedded input :  torch.Size([4, 256, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 3 \t Epoch : 3\tNet Loss: 437.1316 \tAnswer Loss: 0.7305 \tQuestion Loss: 437.1316\n",
      "Embedded input :  torch.Size([4, 142, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 4 \t Epoch : 3\tNet Loss: 474.1309 \tAnswer Loss: 0.7940 \tQuestion Loss: 474.1309\n",
      "Embedded input :  torch.Size([4, 175, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 5 \t Epoch : 3\tNet Loss: 411.4357 \tAnswer Loss: 0.9295 \tQuestion Loss: 411.4357\n",
      "Embedded input :  torch.Size([4, 195, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 6 \t Epoch : 3\tNet Loss: 365.5209 \tAnswer Loss: 1.0716 \tQuestion Loss: 365.5209\n",
      "Embedded input :  torch.Size([4, 191, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 7 \t Epoch : 3\tNet Loss: 297.7068 \tAnswer Loss: 0.7311 \tQuestion Loss: 297.7068\n",
      "Average Loss after Epoch 3 : 369.0539\n",
      "Embedded input :  torch.Size([4, 113, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 0 \t Epoch : 4\tNet Loss: 319.5921 \tAnswer Loss: 0.5840 \tQuestion Loss: 319.5921\n",
      "Embedded input :  torch.Size([4, 186, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 1 \t Epoch : 4\tNet Loss: 312.2728 \tAnswer Loss: 1.1352 \tQuestion Loss: 312.2728\n",
      "Embedded input :  torch.Size([4, 231, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 2 \t Epoch : 4\tNet Loss: 309.0273 \tAnswer Loss: 1.0265 \tQuestion Loss: 309.0273\n",
      "Embedded input :  torch.Size([4, 256, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 3 \t Epoch : 4\tNet Loss: 426.4086 \tAnswer Loss: 0.7324 \tQuestion Loss: 426.4086\n",
      "Embedded input :  torch.Size([4, 142, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 4 \t Epoch : 4\tNet Loss: 463.6890 \tAnswer Loss: 0.7973 \tQuestion Loss: 463.6890\n",
      "Embedded input :  torch.Size([4, 175, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 5 \t Epoch : 4\tNet Loss: 400.0461 \tAnswer Loss: 0.9325 \tQuestion Loss: 400.0461\n",
      "Embedded input :  torch.Size([4, 195, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 6 \t Epoch : 4\tNet Loss: 353.7721 \tAnswer Loss: 1.0737 \tQuestion Loss: 353.7721\n",
      "Embedded input :  torch.Size([4, 191, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 7 \t Epoch : 4\tNet Loss: 286.4375 \tAnswer Loss: 0.7330 \tQuestion Loss: 286.4375\n",
      "Average Loss after Epoch 4 : 358.9057\n",
      "Embedded input :  torch.Size([4, 113, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 0 \t Epoch : 5\tNet Loss: 309.2369 \tAnswer Loss: 0.5886 \tQuestion Loss: 309.2369\n",
      "Embedded input :  torch.Size([4, 186, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 1 \t Epoch : 5\tNet Loss: 301.8354 \tAnswer Loss: 1.1394 \tQuestion Loss: 301.8354\n",
      "Embedded input :  torch.Size([4, 231, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 2 \t Epoch : 5\tNet Loss: 297.1924 \tAnswer Loss: 1.0288 \tQuestion Loss: 297.1924\n",
      "Embedded input :  torch.Size([4, 256, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 3 \t Epoch : 5\tNet Loss: 412.2441 \tAnswer Loss: 0.7353 \tQuestion Loss: 412.2441\n",
      "Embedded input :  torch.Size([4, 142, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 4 \t Epoch : 5\tNet Loss: 450.0305 \tAnswer Loss: 0.8021 \tQuestion Loss: 450.0305\n",
      "Embedded input :  torch.Size([4, 175, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 5 \t Epoch : 5\tNet Loss: 384.8380 \tAnswer Loss: 0.9368 \tQuestion Loss: 384.8380\n",
      "Embedded input :  torch.Size([4, 195, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 6 \t Epoch : 5\tNet Loss: 337.6479 \tAnswer Loss: 1.0767 \tQuestion Loss: 337.6479\n",
      "Embedded input :  torch.Size([4, 191, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 7 \t Epoch : 5\tNet Loss: 270.6904 \tAnswer Loss: 0.7357 \tQuestion Loss: 270.6904\n",
      "Average Loss after Epoch 5 : 345.4645\n",
      "Embedded input :  torch.Size([4, 113, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 0 \t Epoch : 6\tNet Loss: 295.3100 \tAnswer Loss: 0.5951 \tQuestion Loss: 295.3100\n",
      "Embedded input :  torch.Size([4, 186, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 1 \t Epoch : 6\tNet Loss: 287.6333 \tAnswer Loss: 1.1451 \tQuestion Loss: 287.6333\n",
      "Embedded input :  torch.Size([4, 231, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 2 \t Epoch : 6\tNet Loss: 281.0487 \tAnswer Loss: 1.0320 \tQuestion Loss: 281.0487\n",
      "Embedded input :  torch.Size([4, 256, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 3 \t Epoch : 6\tNet Loss: 392.6832 \tAnswer Loss: 0.7393 \tQuestion Loss: 392.6832\n",
      "Embedded input :  torch.Size([4, 142, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 4 \t Epoch : 6\tNet Loss: 431.6286 \tAnswer Loss: 0.8086 \tQuestion Loss: 431.6286\n",
      "Embedded input :  torch.Size([4, 175, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 5 \t Epoch : 6\tNet Loss: 364.1151 \tAnswer Loss: 0.9423 \tQuestion Loss: 364.1151\n",
      "Embedded input :  torch.Size([4, 195, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 6 \t Epoch : 6\tNet Loss: 315.5135 \tAnswer Loss: 1.0808 \tQuestion Loss: 315.5135\n",
      "Embedded input :  torch.Size([4, 191, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 7 \t Epoch : 6\tNet Loss: 249.0174 \tAnswer Loss: 0.7392 \tQuestion Loss: 249.0174\n",
      "Average Loss after Epoch 6 : 327.1187\n",
      "Embedded input :  torch.Size([4, 113, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 0 \t Epoch : 7\tNet Loss: 277.2129 \tAnswer Loss: 0.6031 \tQuestion Loss: 277.2129\n",
      "Embedded input :  torch.Size([4, 186, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 1 \t Epoch : 7\tNet Loss: 269.4424 \tAnswer Loss: 1.1518 \tQuestion Loss: 269.4424\n",
      "Embedded input :  torch.Size([4, 231, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 2 \t Epoch : 7\tNet Loss: 259.9136 \tAnswer Loss: 1.0358 \tQuestion Loss: 259.9136\n",
      "Embedded input :  torch.Size([4, 256, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 3 \t Epoch : 7\tNet Loss: 365.3493 \tAnswer Loss: 0.7439 \tQuestion Loss: 365.3493\n",
      "Embedded input :  torch.Size([4, 142, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 4 \t Epoch : 7\tNet Loss: 405.8630 \tAnswer Loss: 0.8159 \tQuestion Loss: 405.8630\n",
      "Embedded input :  torch.Size([4, 175, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 5 \t Epoch : 7\tNet Loss: 334.0310 \tAnswer Loss: 0.9482 \tQuestion Loss: 334.0310\n",
      "Embedded input :  torch.Size([4, 195, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 6 \t Epoch : 7\tNet Loss: 283.3575 \tAnswer Loss: 1.0852 \tQuestion Loss: 283.3575\n",
      "Embedded input :  torch.Size([4, 191, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 7 \t Epoch : 7\tNet Loss: 217.5956 \tAnswer Loss: 0.7428 \tQuestion Loss: 217.5956\n",
      "Average Loss after Epoch 7 : 301.5957\n",
      "Embedded input :  torch.Size([4, 113, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 0 \t Epoch : 8\tNet Loss: 251.6584 \tAnswer Loss: 0.6112 \tQuestion Loss: 251.6584\n",
      "Embedded input :  torch.Size([4, 186, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 1 \t Epoch : 8\tNet Loss: 245.0386 \tAnswer Loss: 1.1582 \tQuestion Loss: 245.0386\n",
      "Embedded input :  torch.Size([4, 231, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 2 \t Epoch : 8\tNet Loss: 230.2665 \tAnswer Loss: 1.0394 \tQuestion Loss: 230.2665\n",
      "Embedded input :  torch.Size([4, 256, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 3 \t Epoch : 8\tNet Loss: 324.4054 \tAnswer Loss: 0.7484 \tQuestion Loss: 324.4054\n",
      "Embedded input :  torch.Size([4, 142, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 4 \t Epoch : 8\tNet Loss: 365.3985 \tAnswer Loss: 0.8226 \tQuestion Loss: 365.3985\n",
      "Embedded input :  torch.Size([4, 175, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 5 \t Epoch : 8\tNet Loss: 288.1026 \tAnswer Loss: 0.9535 \tQuestion Loss: 288.1026\n",
      "Embedded input :  torch.Size([4, 195, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 6 \t Epoch : 8\tNet Loss: 238.2781 \tAnswer Loss: 1.0892 \tQuestion Loss: 238.2781\n",
      "Embedded input :  torch.Size([4, 191, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 7 \t Epoch : 8\tNet Loss: 178.0635 \tAnswer Loss: 0.7460 \tQuestion Loss: 178.0635\n",
      "Average Loss after Epoch 8 : 265.1514\n",
      "Embedded input :  torch.Size([4, 113, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 0 \t Epoch : 9\tNet Loss: 221.0174 \tAnswer Loss: 0.6181 \tQuestion Loss: 221.0174\n",
      "Embedded input :  torch.Size([4, 186, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 1 \t Epoch : 9\tNet Loss: 218.8413 \tAnswer Loss: 1.1634 \tQuestion Loss: 218.8413\n",
      "Embedded input :  torch.Size([4, 231, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 2 \t Epoch : 9\tNet Loss: 198.4277 \tAnswer Loss: 1.0424 \tQuestion Loss: 198.4277\n",
      "Embedded input :  torch.Size([4, 256, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 3 \t Epoch : 9\tNet Loss: 281.9633 \tAnswer Loss: 0.7521 \tQuestion Loss: 281.9633\n",
      "Embedded input :  torch.Size([4, 142, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 4 \t Epoch : 9\tNet Loss: 324.3678 \tAnswer Loss: 0.8280 \tQuestion Loss: 324.3678\n",
      "Embedded input :  torch.Size([4, 175, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 5 \t Epoch : 9\tNet Loss: 246.4914 \tAnswer Loss: 0.9577 \tQuestion Loss: 246.4914\n",
      "Embedded input :  torch.Size([4, 195, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 6 \t Epoch : 9\tNet Loss: 201.8980 \tAnswer Loss: 1.0924 \tQuestion Loss: 201.8980\n",
      "Embedded input :  torch.Size([4, 191, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 7 \t Epoch : 9\tNet Loss: 149.4776 \tAnswer Loss: 0.7485 \tQuestion Loss: 149.4776\n",
      "Average Loss after Epoch 9 : 230.3106\n",
      "Embedded input :  torch.Size([4, 113, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 0 \t Epoch : 10\tNet Loss: 200.6320 \tAnswer Loss: 0.6233 \tQuestion Loss: 200.6320\n",
      "Embedded input :  torch.Size([4, 186, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 1 \t Epoch : 10\tNet Loss: 202.9419 \tAnswer Loss: 1.1672 \tQuestion Loss: 202.9419\n",
      "Embedded input :  torch.Size([4, 231, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 2 \t Epoch : 10\tNet Loss: 180.1439 \tAnswer Loss: 1.0444 \tQuestion Loss: 180.1439\n",
      "Embedded input :  torch.Size([4, 256, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 3 \t Epoch : 10\tNet Loss: 258.4643 \tAnswer Loss: 0.7545 \tQuestion Loss: 258.4643\n",
      "Embedded input :  torch.Size([4, 142, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 4 \t Epoch : 10\tNet Loss: 303.2528 \tAnswer Loss: 0.8317 \tQuestion Loss: 303.2528\n",
      "Embedded input :  torch.Size([4, 175, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 5 \t Epoch : 10\tNet Loss: 225.7660 \tAnswer Loss: 0.9605 \tQuestion Loss: 225.7660\n",
      "Embedded input :  torch.Size([4, 195, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 6 \t Epoch : 10\tNet Loss: 183.5571 \tAnswer Loss: 1.0945 \tQuestion Loss: 183.5571\n",
      "Embedded input :  torch.Size([4, 191, 300])\n",
      "Answer encoder Hidden :  torch.Size([2, 4, 150])\n",
      "Batch: 7 \t Epoch : 10\tNet Loss: 135.5827 \tAnswer Loss: 0.7501 \tQuestion Loss: 135.5827\n",
      "Average Loss after Epoch 10 : 211.2926\n"
     ]
    }
   ],
   "source": [
    "verboseBatchPrinting = True\n",
    "averageBatchLossPrinting = True\n",
    "\n",
    "num_epochs = 10\n",
    "answer_encoder_hidden = answerEncoder.initHidden()\n",
    "question_encoder_hidden = questionEncoder.initHidden()\n",
    "question_decoder_hidden = None\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    avg_loss = 0\n",
    "    for batch_num in range(len(batch_input[0])):\n",
    "        \n",
    "        current_batch_size = len(batch_input[0][batch_num])\n",
    "        if current_batch_size != batch_size:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        maxDocLenForBatch = int(max(batch_input[1][batch_num]))\n",
    "        mask = np.zeros((current_batch_size, maxDocLenForBatch))\n",
    "        for i in range(current_batch_size):\n",
    "            mask[i][0:batch_input[1][0][i]] = 1\n",
    "            \n",
    "        inp = Variable(torch.from_numpy(batch_input[0][batch_num]).long())\n",
    "\n",
    "        labels = Variable(torch.from_numpy(batch_input[2][batch_num])).long()\n",
    "        if use_cuda:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embedded_inp = embedder(inp).cuda()\n",
    "        print(\"Embedded input : \", embedded_inp.size())\n",
    "        print(\"Answer encoder Hidden : \", answer_encoder_hidden.size())\n",
    "        answer_tags, answer_outputs, answer_encoder_hidden = answerEncoder(embedded_inp, answer_encoder_hidden)\n",
    "        \n",
    "        \n",
    "        if use_cuda:\n",
    "            answer_outputs = answer_outputs.cuda()\n",
    "            answer_tags = answer_tags.cuda()\n",
    "        \n",
    "        \n",
    "        t_document_mask = Variable(torch.from_numpy(mask)).float()\n",
    "        if use_cuda:\n",
    "            t_document_mask = t_document_mask.cuda()\n",
    "        outputs = torch.mul(answer_tags.squeeze(-1),t_document_mask)\n",
    "        \n",
    "        \n",
    "        answer_loss = criterion1(outputs, labels.unsqueeze(2).float())\n",
    "            \n",
    "        \n",
    "        t_answer_mask = Variable(torch.from_numpy(batch_input[3][batch_num])).float()\n",
    "        if use_cuda:\n",
    "            t_answer_mask = t_answer_mask.cuda()\n",
    "        \n",
    "        question_encoder_input = torch.matmul(t_answer_mask, answer_outputs.float())\n",
    "        question_encoder_hidden_batch = Variable(torch.zeros(1,current_batch_size,questionEncoder.hidden_size))\n",
    "        if use_cuda:\n",
    "            question_encoder_hidden_batch = question_encoder_hidden_batch.cuda()\n",
    "        \n",
    "        \n",
    "        for i in range(current_batch_size):\n",
    "            _ , question_encoder_hidden = questionEncoder(question_encoder_input[i:i+1,0:batch_input[4][batch_num][i],:], question_encoder_hidden)\n",
    "            question_encoder_hidden_batch[:,i:i+1,:] = question_encoder_hidden\n",
    "            \n",
    "        #question_encoder_hidden_batch = fcLayer(question_encoder_hidden_batch)\n",
    "            \n",
    "        question_loss = 0\n",
    "        for i in range(current_batch_size):\n",
    "            question_decoder_hidden = question_encoder_hidden_batch[:,i:i+1,:].clone()\n",
    "            embedded_inputs = embedder(torch.from_numpy(batch_input[5][batch_num][i]).long()).cuda()\n",
    "            output_labels = Variable(torch.from_numpy(batch_input[5][batch_num][i]).long())\n",
    "            if use_cuda:\n",
    "                output_labels = output_labels.cuda()\n",
    "                \n",
    "            for quesL in range(batch_input[7][batch_num][i]):\n",
    "                decoder_output, question_decoder_hidden = questionDecoder(\n",
    "                    embedded_inputs[quesL:quesL+1].unsqueeze(1),\n",
    "                    question_decoder_hidden)\n",
    "                \n",
    "                final_output = questionGenerator(decoder_output)\n",
    "                output_label = Variable(torch.zeros(1,2000))\n",
    "                if use_cuda:\n",
    "                    output_label = output_label.cuda()\n",
    "                output_label[:,batch_input[5][batch_num][i][quesL]] = 1\n",
    "                question_loss += criterion2(final_output.squeeze(0), \n",
    "                                           output_labels[quesL:quesL+1])\n",
    "                ##question_loss += criterion2(final_output.squeeze(0), output_label)\n",
    "        \n",
    "\n",
    "        #net_loss = answer_loss + question_loss\n",
    "        net_loss = question_loss\n",
    "        net_loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        \n",
    "        avg_loss+= net_loss.data[0]\n",
    "        if verboseBatchPrinting:\n",
    "            print ('Batch: %d \\t Epoch : %d\\tNet Loss: %.4f \\tAnswer Loss: %.4f \\tQuestion Loss: %.4f' \n",
    "                   %(batch_num, epoch, net_loss.data[0], answer_loss.data[0], question_loss.data[0]))\n",
    "        \n",
    "        \n",
    "    if averageBatchLossPrinting:\n",
    "        print('Average Loss after Epoch %d : %.4f'\n",
    "                   %(epoch, avg_loss/number_of_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 113])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_document_mask.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_tags.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion2(final_output.squeeze(0), output_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if cellFlag == 'LSTM':\n",
    "    forward_cell = tf.contrib.rnn.LSTMCell(EMBEDDING_DIMENS)\n",
    "    backward_cell = tf.contrib.rnn.LSTMCell(EMBEDDING_DIMENS)\n",
    "elif cellFlag == 'GRU':\n",
    "    forward_cell = tf.contrib.rnn.GRUCell(EMBEDDING_DIMENS)\n",
    "    backward_cell = tf.contrib.rnn.GRUCell(EMBEDDING_DIMENS)\n",
    "\n",
    "answer_outputs, states = tf.nn.bidirectional_dynamic_rnn(\n",
    "    forward_cell, backward_cell, document_emb, d_lengths, dtype=tf.float64,\n",
    "    scope=\"answer_rnn\")\n",
    "\n",
    "answer_outputs = tf.concat(answer_outputs, 2, name=\"answer_output_concat\")\n",
    "\n",
    "answer_outputs = tf.cast(answer_outputs,tf.float32, name=\"answer_output_concat\")\n",
    "\n",
    "answer_tags = tf.layers.dense(inputs=answer_outputs, units=2, name=\"answer_tags\")\n",
    "\n",
    "\n",
    "answer_mask = tf.sequence_mask(d_lengths, dtype=tf.float32, name=\"answer_mask\")\n",
    "\n",
    "answer_loss = seq2seq.sequence_loss(\n",
    "    logits=answer_tags, targets=a_labels, weights=answer_mask, name=\"answer_loss\")\n",
    "\n",
    "answer_loss = tf.Print(answer_loss, [answer_loss], message=\"answer_loss: \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_inputs = tf.matmul(encoder_input_mask, answer_outputs, name=\"encoder_inputs\")\n",
    "encoder_lengths = tf.placeholder(tf.int32, shape=[None], name=\"encoder_lengths\")\n",
    "\n",
    "if cellFlag == 'GRU':\n",
    "    encoder_cell = tf.contrib.rnn.GRUCell(forward_cell.state_size + backward_cell.state_size)\n",
    "elif cellFlag == 'LSTM':\n",
    "    encoder_cell = tf.contrib.rnn.LSTMCell(2 * EMBEDDING_DIMENS)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_, encoder_state = tf.nn.dynamic_rnn(\n",
    "    encoder_cell, encoder_inputs, encoder_lengths, dtype=tf.float32, scope=\"encoder_rnn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "decoder_emb = tf.nn.embedding_lookup(embedding, decoder_inputs,name=\"decoder_embedding\")\n",
    "decoder_emb = tf.cast(decoder_emb,tf.float32,name=\"decoder_embedding_cast\")\n",
    "\n",
    "helper = seq2seq.TrainingHelper(decoder_emb , decoder_lengths, name=\"helper\")\n",
    "\n",
    "\n",
    "projection = Dense(embedding.shape[0], use_bias=False, name=\"projection\")\n",
    "\n",
    "if cellFlag == 'GRU':\n",
    "    decoder_cell = tf.contrib.rnn.GRUCell(encoder_cell.state_size)\n",
    "elif cellFlag == \"LSTM\":\n",
    "    decoder_cell = tf.contrib.rnn.LSTMCell(2 * EMBEDDING_DIMENS)\n",
    "\n",
    "decoder = seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer=projection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder, scope=\"decoder\")\n",
    "decoder_outputs = decoder_outputs.rnn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoder_state.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question Generation Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# NLL Loss\n",
    "question_mask = tf.sequence_mask(decoder_lengths ,dtype=tf.float32)\n",
    "question_loss = seq2seq.sequence_loss(\n",
    "    logits=decoder_outputs, targets=decoder_labels, weights=question_mask,\n",
    "    name=\"question_loss\")\n",
    "question_loss = tf.Print(question_loss, [question_loss], message=\"question_loss: \")\n",
    "\n",
    "#Suppression Loss\n",
    "lambdaSuppress = 1\n",
    "\n",
    "suppression_loss = lambdaSuppress * tf.reduce_sum(tf.matmul(tf.nn.softmax(decoder_outputs), s_answer))\n",
    "suppression_loss = tf.Print(suppression_loss, [suppression_loss], message=\"suppression_loss: \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Expression Loss\n",
    "\n",
    "express_loss_f = tf.reduce_sum(-tf.log(tf.multiply(tf.sigmoid(decoder_outputs),e_context) + 10e-7))\n",
    "suppress_loss_f = tf.reduce_sum(-tf.log(tf.multiply((1 - tf.sigmoid(decoder_outputs)),(1 - e_context)) + 10e-7))\n",
    "expression_loss = (express_loss_f + suppress_loss_f)/(32 * 20 * wordToTake)\n",
    "expression_loss = tf.Print(expression_loss, [expression_loss], message=\"expression_loss: \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#Maximize Entropy Loss\n",
    "entropy_loss = tf.matmul(tf.transpose(dense_output),dense_output)\n",
    "print(dense_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.stack([question_loss,answer_loss,suppression_loss,expression_loss])\n",
    "loss = tf.reduce_sum(tf.multiply(x, lossFlags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle_list(*ls):\n",
    "    l =list(zip(*ls))\n",
    "    np.random.shuffle(l)\n",
    "    return zip(*l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "expression_contexts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"No of features:\",len( batch_input))\n",
    "print(\"No of batches:\",len( batch_input[0]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "saved_vars = []\n",
    "l = len(tf.all_variables())\n",
    "for i,var in enumerate(tf.all_variables()):\n",
    "    print(i,\"/\",l)\n",
    "    saved_vars.append(var)\n",
    "        \n",
    "print(len(saved_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=3e-3).minimize(loss)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.InteractiveSession(config=config)\n",
    "\n",
    "session.run(tf.global_variables_initializer())\n",
    "# session.run(tf.variables_initializer(saved_vars))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EPOCHS = 400\n",
    "loss_flag = np.array([1,1,1,1])\n",
    "\n",
    "import time\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    batch_loss = 0\n",
    "    print(\"Epoch {0}\".format(epoch))\n",
    "    start_time = time.time()\n",
    "    for batchNum in range(len(batch_input[0])):\n",
    "        print(\"Batch : \",batchNum)\n",
    "        t = session.run([optimizer, loss, question_loss, answer_loss, suppression_loss, expression_loss], {\n",
    "            d_tokens: batch_input[0][batchNum],\n",
    "            d_lengths: batch_input[1][batchNum],\n",
    "            a_labels: batch_input[2][batchNum],\n",
    "            encoder_input_mask: batch_input[3][batchNum],\n",
    "            encoder_lengths: batch_input[4][batchNum],\n",
    "            decoder_inputs: batch_input[5][batchNum],\n",
    "            decoder_labels: batch_input[6][batchNum],\n",
    "            decoder_lengths: batch_input[7][batchNum],\n",
    "            s_answer: batch_input[8][batchNum],\n",
    "            e_context: batch_input[9][batchNum],\n",
    "            e_probs: batch_input[10][batchNum],\n",
    "            lossFlags : loss_flag,\n",
    "        })\n",
    "        print(\"Loss: {0}\".format(t[1]))\n",
    "        batch_loss += t[2]\n",
    "    batch_loss /= len(batch_input[0])\n",
    "    end_time = time.time()\n",
    "    print(\"Average Batch Question Loss: {0}\".format(batch_loss))\n",
    "    print(\"Time taken to complete epoch : \" , (end_time-start_time)/60 , \" minutes\")\n",
    "    if batch_loss < minQuestionLoss:\n",
    "        print(\"Turning on all losses\")\n",
    "        #loss_flag = np.array([1,1,1,1])\n",
    "    if(epoch%5 == 0):\n",
    "        print(\"Saving model\")\n",
    "        #saver.save(session, \"qgen-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reduced_glove.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(session, \"qgen-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_input[5][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.Session(config=config)\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "saver.restore(session, 'qgen-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = session.run(answer_tags, {\n",
    "    d_tokens: batch_input[0][0],\n",
    "    d_lengths: batch_input[1][0],\n",
    "})\n",
    "print(answers.shape)\n",
    "print(answers[0])\n",
    "answers = np.argmax(answers, 2)\n",
    "print(answers.shape)\n",
    "print(answers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(276):\n",
    "    print(\"Prediction\")\n",
    "    printAllAns(answers,2,0)\n",
    "    print(\"Ground Truth\")\n",
    "    printAllAns(batch_input[2][2],2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printDoc(batch,num):\n",
    "    for i in batch_input[0][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "\n",
    "def printQues(batch,num):\n",
    "    for i in batch_input[5][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "def printAnsForQuestion(batch, num):\n",
    "    for i in batch_input[5][batch][num]:\n",
    "        print(look_up_token_reduced(i),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    \n",
    "def printAllAns(answers, batch, num):\n",
    "    for i,word in enumerate(batch_input[0][batch][num]):\n",
    "        if answers[num][i] == 1 :\n",
    "            print(look_up_token_reduced(word),sep=\" \", end=\" \")\n",
    "    print(\" \")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "batchNum = 0\n",
    "\n",
    "helper = seq2seq.GreedyEmbeddingHelper(embedding, tf.fill([batch_input[0][batchNum].shape[0]], START_TOKEN), END_TOKEN)\n",
    "decoder = seq2seq.BasicDecoder(decoder_cell, helper, encoder_state, output_layer=projection)\n",
    "decoder_outputs, _, _ = seq2seq.dynamic_decode(decoder, maximum_iterations=max_question_len)\n",
    "decoder_outputs = decoder_outputs.rnn_output\n",
    "\n",
    "\n",
    "questions = session.run(decoder_outputs, {\n",
    "    d_tokens: batch_input[0][batchNum],\n",
    "    d_lengths: batch_input[1][batchNum],\n",
    "    a_labels: batch_input[2][batchNum],\n",
    "    encoder_input_mask: batch_input[3][batchNum],\n",
    "    encoder_lengths: batch_input[4][batchNum],\n",
    "    e_context: batch_input[9][batchNum],\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_input[9][batchNum].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#questions[:,:,END_TOKEN] = 0\n",
    "qs = np.argmax(questions, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in np.argsort(questions[0][0])[-100:]:\n",
    "    print(sigmoid(questions[0][0][i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in np.argsort(questions[0][10])[-100:]:\n",
    "    print(sigmoid(questions[0][10][i]),look_up_token_reduced(i), sep=\" \", end= \" \")\n",
    "print(\"\")\n",
    "print(X_train_comp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p1 = 0\n",
    "p2 = 0\n",
    "\n",
    "for i in np.where(batch_input[9][0][0][0] == 1)[0]:\n",
    "    p1 += -np.log(sigmoid(questions[0][0][i]) + 10e-7)\n",
    "    \n",
    "for i in np.where(batch_input[9][0][0][0] == 0)[0]:\n",
    "    p2 += -np.log(sigmoid(questions[0][0][i]) + 10e-7)\n",
    "\n",
    "print(p1)\n",
    "print(p2)\n",
    "print(p2-p1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(batch_input[0][batchNum].shape[0]):\n",
    "    print(\"---------------------------------------------------------------------------------------------\")\n",
    "    question = itertools.takewhile(lambda t: t != END_TOKEN, qs[i])\n",
    "    print(\"Generated Question: \" + \" \".join(look_up_token_reduced(token) for token in question))\n",
    "    print(\"Ground Truth Question: \")\n",
    "    printQues(batchNum,i)\n",
    "    print(\"Ground Truth Answer: \", X_train_ans_shuffled[batch_size*batchNum + i])\n",
    "    print(\"Context:\")\n",
    "    printDoc(batchNum,i)\n",
    "    print(\"---------------------------------------------------------------------------------------------\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_input[5][18].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions[:,0:14,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a1 = tf.constant(batch_input[9][0], dtype=tf.float32)\n",
    "a2 = tf.constant(questions[:,0:14,:], dtype=tf.float32)\n",
    "l1 = tf.reduce_sum(-tf.log(tf.multiply(tf.sigmoid(a2),a1) + 10e-7)) #y = 1\n",
    "l2 = tf.reduce_sum(-tf.log(tf.multiply(tf.sigmoid(1-a2),(1 - a1)) + 10e-7)) #y= 0\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    print(l1.eval() + l2.eval())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "questions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_input[9][0][0:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in batch_input[0][0][0]:\n",
    "    print(look_up_token_reduced(i), sep = \" \", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
