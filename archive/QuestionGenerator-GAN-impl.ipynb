{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=en_US.UTF-8\n",
    "from embedding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import random\n",
    "\n",
    "import torch\n",
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    !pip install nltk\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('../train-v1.1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ra2630/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ra2630/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltkStopWords = stopwords.words('english')\n",
    "punctuations = [',', '?', '.', '-',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(data):\n",
    "    contexts = []\n",
    "    qas = []\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        for j in range(len(data[\"data\"][i][\"paragraphs\"])):\n",
    "            contexts.append(data[\"data\"][i][\"paragraphs\"][j][\"context\"])\n",
    "            qas.append(data[\"data\"][i][\"paragraphs\"][j][\"qas\"])\n",
    "    return (contexts,qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CapPassage = True\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "contexts,qas = extractor(data)\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind,ind+sll\n",
    "    return (-1,-1)\n",
    "\n",
    "def capPassage(passage,left, right,cap_length = 20):\n",
    "    y = np.zeros(cap_length)\n",
    "    left = max(0,left - int((cap_length - len(answer))/2))\n",
    "    right = min(right + int((cap_length + len(answer))/2), cap_length)\n",
    "    if(left < 0):\n",
    "        left = 0\n",
    "    if(right > len(passage)):\n",
    "        right = len(passage)\n",
    "    return passage[left:right]\n",
    "    \n",
    "def findAnsVec(answer,passage):\n",
    "    ans = np.zeros((len(passage)))\n",
    "    start,end = find_sub_list(answer,passage)\n",
    "    if(start==-1):\n",
    "        start = passage.index(answer[0])\n",
    "        end = start + len(answer)\n",
    "    ans[start:end] = 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2429\n"
     ]
    }
   ],
   "source": [
    "n_words = 10\n",
    "X_train_comp_all = []\n",
    "X_train_comp_with_answer_marked_all = []\n",
    "X_train_ans_all = []\n",
    "X_train_comp_answer_label_all = []\n",
    "X_train_sentence_label_all = []\n",
    "Y_train_ques_all = []\n",
    "invalid = 0\n",
    "for i,context in enumerate(contexts):\n",
    "    passage = word_tokenize(context.lower())\n",
    "\n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        try:\n",
    "            question = word_tokenize(qas[i][j]['question'].lower())\n",
    "            answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "            start,end = find_sub_list(answer,passage)\n",
    "            if start == -1:\n",
    "                invalid = invalid+1\n",
    "                continue\n",
    "            marked_comp = np.zeros(len(passage))\n",
    "            marked_comp[start:end] = 1\n",
    "            left = max(0,start - n_words)\n",
    "            right = min(len(passage), end + n_words)\n",
    "            \n",
    "            cappedPassage = passage[left:right]\n",
    "            marked_comp = marked_comp[left:right]\n",
    "            \n",
    "            \n",
    "            X_train_comp_all.append(cappedPassage)\n",
    "            X_train_comp_with_answer_marked_all.append(marked_comp)\n",
    "            X_train_ans_all.append(answer)\n",
    "            Y_train_ques_all.append(question)\n",
    "        except Exception as e:\n",
    "            invalid = invalid+1\n",
    "print(invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(zip(X_train_comp_all,X_train_ans_all, Y_train_ques_all,X_train_comp_with_answer_marked_all))\n",
    "np.random.shuffle(c)\n",
    "X_train_comp_all_shuffled,X_train_ans_all_shuffled, Y_train_ques_all_shuffled, X_train_comp_with_answer_marked_all_shuffled = zip(*c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_comp_with_answer_marked_all_shuffled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "def findKMostFrequentWords(k):\n",
    "    ctr = Counter([item for sublist in X_train_comp_all_shuffled for item in sublist] + [item for sublist in Y_train_ques_all_shuffled for item in sublist])\n",
    "    sorted_ctr = sorted(ctr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return [item[0] for item in sorted_ctr[0:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordToTake = 200000\n",
    "words = findKMostFrequentWords(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "_word_to_idx_reduced = {}\n",
    "_idx_to_word_reduced = []\n",
    "\n",
    "\n",
    "def _add_word_reduced(word):\n",
    "    idx = len(_idx_to_word_reduced)\n",
    "    _word_to_idx_reduced[word] = idx\n",
    "    _idx_to_word_reduced.append(word)\n",
    "    return idx\n",
    "\n",
    "\n",
    "UNKNOWN_TOKEN = _add_word_reduced(UNKNOWN_WORD)\n",
    "START_TOKEN = _add_word_reduced(START_WORD)\n",
    "END_TOKEN = _add_word_reduced(END_WORD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dimensions = glove.shape[1]\n",
    "reduced_glove = []\n",
    "reduced_glove.append(np.zeros(dimensions))\n",
    "reduced_glove.append(-np.ones(dimensions))\n",
    "reduced_glove.append(np.ones(dimensions))\n",
    "\n",
    "for word in words:\n",
    "    l = look_up_word(word)\n",
    "    if(l != UNKNOWN_TOKEN):\n",
    "        idx = _add_word_reduced(word)\n",
    "        reduced_glove.append(glove[l])\n",
    "        if(len(reduced_glove) == wordToTake):\n",
    "            break\n",
    "        \n",
    "def look_up_word_reduced(word):\n",
    "    return _word_to_idx_reduced.get(word, UNKNOWN_TOKEN)\n",
    "\n",
    "\n",
    "def look_up_token_reduced(token):\n",
    "    return _idx_to_word_reduced[token]\n",
    "\n",
    "reduced_glove = np.array(reduced_glove)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words_set_tokens = set([look_up_word_reduced(i) for i in set(stopwords.words('english'))]) - {UNKNOWN_TOKEN, START_TOKEN}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_words_set_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda2.cims.nyu.edu\n",
      "Invalid =  2429\n",
      "Glove shape =  (59528, 300)\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "print(\"Invalid = \",invalid)\n",
    "print(\"Glove shape = \", reduced_glove.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_to_take_train = 500\n",
    "\n",
    "X_train_comp = X_train_comp_all_shuffled[0:examples_to_take_train]\n",
    "X_train_ans = X_train_ans_all_shuffled[0:examples_to_take_train]\n",
    "Y_train_ques = Y_train_ques_all_shuffled[0:examples_to_take_train]\n",
    "X_train_comp_with_answer_marked = X_train_comp_with_answer_marked_all_shuffled[0:examples_to_take_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_document_len = len(max(X_train_comp,key=len))\n",
    "max_answer_len = len(max(X_train_ans,key=len))\n",
    "max_question_len = len(max(Y_train_ques,key=len)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = np.full((examples_to_take_train, max_document_len), END_TOKEN,dtype=np.int32)\n",
    "document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "\n",
    "answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "\n",
    "question_input_tokens = np.full((examples_to_take_train, max_question_len), END_TOKEN, dtype=np.int32)\n",
    "question_output_tokens = np.full((examples_to_take_train, max_question_len), END_TOKEN, dtype=np.int32)\n",
    "question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(examples_to_take_train):\n",
    "    answer_labels[i, 0:len(X_train_comp_with_answer_marked[i])] = X_train_comp_with_answer_marked[i]\n",
    "    for j, word in enumerate(X_train_comp[i]):\n",
    "        document_tokens[i, j] = look_up_word_reduced(word)\n",
    "    document_lengths[i] = len(X_train_comp[i])\n",
    "\n",
    "    answer_lengths[i] = len(X_train_ans[i])\n",
    "    \n",
    "    question_input_words = ([START_WORD] + Y_train_ques[i])\n",
    "    question_output_words = (Y_train_ques[i] + [END_WORD])\n",
    "\n",
    "    for j, word in enumerate(question_input_words):\n",
    "            question_input_tokens[i, j] = look_up_word_reduced(word)\n",
    "    for j, word in enumerate(question_output_words):\n",
    "        question_output_tokens[i, j] = look_up_word_reduced(word)\n",
    "    question_lengths[i] = len(question_input_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(data):\n",
    "    flat_list = [item for sublist in data for item in sublist]\n",
    "    vocabulary = sorted(set(flat_list))\n",
    "    vocabulary.append(\"<UNK>\")\n",
    "    vocabulary.append(\"unk\")\n",
    "    vocabulary.append(\"eos\")\n",
    "    vocabulary = [\"<EOS>\"] + vocabulary\n",
    "    word_to_index = { word:i for i,word in enumerate(vocabulary) }\n",
    "    index_to_word = { i:word for i,word in enumerate(vocabulary) }\n",
    "    return (vocabulary,word_to_index,index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59528, 300)\n",
      "4397\n",
      "282\n",
      "4302\n"
     ]
    }
   ],
   "source": [
    "print(reduced_glove.shape)\n",
    "vocabulary_comp,word_to_index_comp,index_to_word_comp = create_vocabulary(X_train_comp + Y_train_ques)\n",
    "print(len(vocabulary_comp))\n",
    "print(word_to_index_comp[\"?\"])\n",
    "print(word_to_index_comp[\"what\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_vector(data,vocabulary,word_to_index,index_to_word, maxLen):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,word in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        if(word not in word_to_index):\n",
    "            word = \"<UNK>\"\n",
    "        one_hot[i][word_to_index[word]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def create_one_hot_vector_from_indices(data,maxLen,vocabulary):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,indice in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        one_hot[i][int(indice)] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def create_one_hot_training_Set(data,maxLen,vocabulary):\n",
    "    one_hot_data = np.zeros([data.shape[0],maxLen,len(vocabulary)])\n",
    "    for i in range(data.shape[0]):\n",
    "        one_hot_data[i] = create_one_hot_vector_from_indices(data[i],maxLen,vocabulary)\n",
    "    return one_hot_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_to_indices_glove(X,max_len):\n",
    "    \n",
    "    m = len(X)                                 \n",
    "    \n",
    "    X_indices = np.full([m,max_len],look_up_word_reduced(END_WORD))\n",
    "    \n",
    "    for i in range(m):\n",
    "        j = 0\n",
    "        for w in X[i]:\n",
    "            if(j>=max_len):\n",
    "                break;\n",
    "            \n",
    "            X_indices[i, j] = look_up_word_reduced(w)\n",
    "            j = j+1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_valid_tokens_vocab(X, Y):\n",
    "    \n",
    "    m = len(X)                                 \n",
    "    \n",
    "    X_indices = np.full([m,reduced_glove.shape[0]],0)\n",
    "    lengths = np.zeros(m)\n",
    "    \n",
    "    for i in range(m):\n",
    "        #s = set(X[i]).union(common_words_set_tokens) - {UNKNOWN_TOKEN}\n",
    "        s = set(X[i]).union(set(Y[i])) - {UNKNOWN_TOKEN}\n",
    "        for w in s:\n",
    "            X_indices[i, w] = 1\n",
    "        lengths[i] = len(s)\n",
    "    return X_indices, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = context_to_indices_glove(X_train_comp, max_document_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_tokens, valid_tokens_length = generate_valid_tokens_vocab(document_tokens, question_output_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_tokens_length[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_question_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def createBatch(inputs,batch_size,shuffle=False):\n",
    "    outputs = []\n",
    "    start = 0\n",
    "    while start < len(inputs[0]):\n",
    "        end = min(len(inputs[0]), start + batch_size)\n",
    "        output = {'document_tokens':[],\n",
    "                    'document_lengths':[],\n",
    "                    'answer_labels':[],\n",
    "                    'answer_lengths': [],\n",
    "                    'question_input_tokens':[],\n",
    "                    'question_output_tokens':[],\n",
    "                    'question_lengths':[],\n",
    "                    'valid_tokens': [],\n",
    "                    'valid_tokens_length': []}\n",
    "        \n",
    "        for index,inp in enumerate(inputs):\n",
    "            #maxD = max(inputs[1][start:start+batch_size])\n",
    "            maxD = max_document_len\n",
    "            maxA = max(inputs[3][start:start+batch_size])\n",
    "            maxQ = max_question_len\n",
    "            \n",
    "            if index == 0:\n",
    "                output['document_tokens'].append(inp[start:end,0:maxD])\n",
    "            elif index==1:\n",
    "                output['document_lengths'].append(inp[start:end])\n",
    "            elif index==2:\n",
    "                output['answer_labels'].append(inp[start:end,0:maxD])\n",
    "            elif index==3:\n",
    "                output['answer_lengths'].append(inp[start:end])\n",
    "            elif index==4:\n",
    "                output['question_input_tokens'].append(inp[start:end, 0:maxQ])\n",
    "            elif index==5:\n",
    "                output['question_output_tokens'].append(inp[start:end, 0:maxQ])\n",
    "            elif index==6:\n",
    "                output['question_lengths'].append(inp[start:end])\n",
    "            elif index==7:\n",
    "                output['valid_tokens'].append(inp[start:end, 0:reduced_glove.shape[0]])\n",
    "            elif index == 8:\n",
    "                output['valid_tokens_length'].append(inp[start:end])\n",
    "        \n",
    "        output[\"document_tokens\"] = np.array(output[\"document_tokens\"])\n",
    "        output[\"document_lengths\"] = np.array(output[\"document_lengths\"])\n",
    "        output[\"answer_labels\"] = np.array(output[\"answer_labels\"])\n",
    "        output[\"answer_lengths\"] = np.array(output[\"answer_lengths\"])\n",
    "        output[\"question_input_tokens\"] = np.array(output[\"question_input_tokens\"])\n",
    "        output[\"question_output_tokens\"] = np.array(output[\"question_output_tokens\"])\n",
    "        output[\"question_lengths\"] = np.array(output[\"question_lengths\"])\n",
    "        output[\"valid_tokens\"] = np.array(output[\"valid_tokens\"])\n",
    "        output[\"valid_tokens_length\"] = np.array(output[\"valid_tokens_length\"])\n",
    "        outputs.append(output)\n",
    "        start = start + batch_size\n",
    "            \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches =  16\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "batch_input = createBatch([document_tokens,document_lengths,answer_labels,answer_lengths,question_input_tokens,question_output_tokens,question_lengths,valid_tokens, valid_tokens_length]\n",
    "                    ,batch_size)\n",
    "for b in batch_input:\n",
    "    for k, v in b.items():\n",
    "        b[k] = v.squeeze()\n",
    "number_of_batches = len(batch_input)\n",
    "print(\"Number of batches = \", number_of_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split = int(0.8 * number_of_batches)\n",
    "#batch_input_test = batch_input[split:]\n",
    "#batch_input = batch_input[0:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 59528)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input[0]['valid_tokens'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use_CUDA=True\n",
      "current_device=0\n"
     ]
    }
   ],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "#USE_CUDA = False\n",
    "print('Use_CUDA={}'.format(USE_CUDA))\n",
    "if USE_CUDA:\n",
    "    # You can change device by `torch.cuda.set_device(device_id)`\n",
    "    print('current_device={}'.format(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchwise_sample(gen, num_samples, batch_size, comp, documentEncoder, embedder):\n",
    "    \"\"\"\n",
    "    Sample num_samples samples batch_size samples at a time from gen.\n",
    "    Does not require gpu since gen.sample() takes care of that.\n",
    "    \"\"\"\n",
    "\n",
    "    samples = []\n",
    "    start = 0\n",
    "    for i in range(int(math.ceil(num_samples/float(batch_size)))):\n",
    "        batch_comp = comp[start:min(start+batch_size, num_samples)]\n",
    "        samples.append(gen.sample(batch_comp.shape[0], batch_comp, documentEncoder, embedder).transpose(1,0))\n",
    "        start+=batch_size\n",
    "\n",
    "    return torch.cat(samples, START_TOKEN)[:num_samples].transpose(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_discriminator_data(pos_samples, neg_samples, comp = None, gpu=False):\n",
    "    \"\"\"\n",
    "    Takes positive (target) samples, negative (generator) samples and prepares inp and target data for discriminator.\n",
    "    Inputs: pos_samples, neg_samples\n",
    "        - pos_samples: pos_size x seq_len\n",
    "        - neg_samples: neg_size x seq_len\n",
    "    Returns: inp, target\n",
    "        - inp: (pos_size + neg_size) x seq_len\n",
    "        - target: pos_size + neg_size (boolean 1/0)\n",
    "    \"\"\"\n",
    "\n",
    "    inp = torch.cat((pos_samples, neg_samples), 0).type(torch.LongTensor)\n",
    "    target = torch.ones(pos_samples.size()[0] + neg_samples.size()[0])\n",
    "    target[pos_samples.size()[0]:] = 0\n",
    "    \n",
    "\n",
    "    # shuffle\n",
    "    perm = torch.randperm(target.size()[0])\n",
    "    target = target[perm]\n",
    "    inp = inp[perm]\n",
    "\n",
    "    if gpu:\n",
    "        inp = inp.cuda()\n",
    "        target = target.cuda()\n",
    "    \n",
    "    if comp is not None :\n",
    "        comp = comp.repeat(2,1)\n",
    "        if gpu:\n",
    "            comp = comp[perm.cuda()]\n",
    "        else:\n",
    "            comp = comp[perm]\n",
    "        return inp,target,comp\n",
    "        \n",
    "\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_generator_batch(samples, start_letter=START_TOKEN, gpu=False):\n",
    "    \"\"\"\n",
    "    Takes samples (a batch) and returns\n",
    "    Inputs: samples, start_letter, cuda\n",
    "        - samples: batch_size x seq_len (Tensor with a sample in each row)\n",
    "    Returns: inp, target\n",
    "        - inp: batch_size x seq_len (same as target, but with start_letter prepended)\n",
    "        - target: batch_size x seq_len (Variable same as samples)\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size, seq_len = samples.size()\n",
    "\n",
    "    inp = torch.zeros(batch_size, seq_len)\n",
    "    target = samples\n",
    "    inp[:, 0] = start_letter\n",
    "    inp[:, 1:] = target[:, :seq_len-1]\n",
    "\n",
    "    inp = Variable(inp).type(torch.LongTensor)\n",
    "    target = Variable(target).type(torch.LongTensor)\n",
    "\n",
    "    if gpu:\n",
    "        inp = inp.cuda()\n",
    "        target = target.cuda()\n",
    "\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, output_size)\n",
    "        \n",
    "        # TODO: Verify\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(reduced_glove).float())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentEncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(DocumentEncoderRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers=num_layers, batch_first= True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        _, hidden = self.gru(x, hidden)\n",
    "        hidden = hidden.view(1, hidden.shape[1], hidden.shape[2]*2*self.num_layers)\n",
    "        return hidden\n",
    "    \n",
    "    def initHidden(self, num_samples):\n",
    "        result = Variable(torch.zeros(2 * self.num_layers, num_samples, self.hidden_size)) #2 for BiDirectional\n",
    "        if USE_CUDA:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentEncoderDiscRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(DocumentEncoderDiscRNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size,num_layers=num_layers, batch_first= True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        _, hidden = self.gru(x, hidden)\n",
    "        return hidden\n",
    "    \n",
    "    def initHidden(self, num_samples):\n",
    "        result = Variable(torch.zeros(2 * self.num_layers, num_samples, self.hidden_size)) #2 for BiDirectional\n",
    "        if USE_CUDA:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, embeddings,  hidden_dim, vocab_size, max_seq_len, gpu=False, oracle_init=False):\n",
    "        super(Generator, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embeddings = embeddings\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.gpu = gpu\n",
    "\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim)\n",
    "        self.gru2out = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "        # initialise oracle network with N(0,1)\n",
    "        # otherwise variance of initialisation is very small => high NLL for data sampled from the same model\n",
    "        if oracle_init:\n",
    "            for p in self.parameters():\n",
    "                init.normal(p, 0, 1)\n",
    "\n",
    "    def init_hidden(self, comp, num_examples, documentEncoder, embedder):\n",
    "        comp = embedder(comp).float()\n",
    "        h = documentEncoder(comp, documentEncoder.initHidden(num_examples))\n",
    "        #h = Variable(torch.zeros(1, batch_size, self.hidden_dim))\n",
    "        if self.gpu:\n",
    "            h = h.cuda()\n",
    "        \n",
    "        return h\n",
    "\n",
    "    def forward(self, inp, hidden): #Make sure hidden comes from the DocumentEncoder...\n",
    "        \"\"\"\n",
    "        Embeds input and applies GRU one token at a time (seq_len = 1)\n",
    "        \"\"\"\n",
    "        # input dim                                             # batch_size\n",
    "        emb = self.embeddings(inp)                              # batch_size x embedding_dim\n",
    "        emb = emb.view(1, -1, self.embedding_dim).float()       # 1 x batch_size x embedding_dim\n",
    "        \n",
    "        out, hidden = self.gru(emb, hidden)                     # 1 x batch_size x hidden_dim (out)\n",
    "        out = self.gru2out(out.view(-1, self.hidden_dim))       # batch_size x vocab_size\n",
    "        out = F.log_softmax(out, dim=-1)\n",
    "        return out, hidden\n",
    "\n",
    "    def sample(self, num_samples, comp, documentEncoder, embedder, start_letter=START_TOKEN):\n",
    "        \"\"\"\n",
    "        Samples the network and returns num_samples samples of length max_seq_len.\n",
    "        Outputs: samples, hidden\n",
    "            - samples: num_samples x max_seq_length (a sampled sequence in each row)\n",
    "        \"\"\"\n",
    "\n",
    "        samples = torch.zeros(num_samples, self.max_seq_len).type(torch.LongTensor)\n",
    "\n",
    "        h = self.init_hidden(comp, num_samples, documentEncoder, embedder)\n",
    "        inp = torch.autograd.Variable(torch.LongTensor([start_letter]*num_samples))\n",
    "\n",
    "        if self.gpu:\n",
    "            samples = samples.cuda()\n",
    "            inp = inp.cuda()\n",
    "\n",
    "        for i in range(self.max_seq_len):\n",
    "            out, h = self.forward(inp, h)               # out: num_samples x vocab_size\n",
    "            out = torch.multinomial(torch.exp(out), 1)  # num_samples x 1 (sampling from each row)\n",
    "            samples[:, i] = out.data\n",
    "\n",
    "            inp = out.view(-1)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def batchNLLLoss(self, inp, target, comp, documentEncoder, embedder, num_samples = batch_size):\n",
    "        \"\"\"\n",
    "        Returns the NLL Loss for predicting target sequence.\n",
    "        Inputs: inp, target\n",
    "            - inp: batch_size x seq_len\n",
    "            - target: batch_size x seq_len\n",
    "            inp should be target with <s> (start letter) prepended\n",
    "        \"\"\"\n",
    "\n",
    "        loss_fn = nn.NLLLoss()\n",
    "        batch_size, seq_len = inp.size()\n",
    "        inp = inp.permute(1, 0)           # seq_len x batch_size\n",
    "        target = target.permute(1, 0)     # seq_len x batch_size\n",
    "        h = self.init_hidden(comp, num_samples, documentEncoder, embedder)\n",
    "\n",
    "        loss = 0\n",
    "        for i in range(seq_len):\n",
    "            out, h = self.forward(inp[i], h)\n",
    "            loss += loss_fn(out, target[i])\n",
    "\n",
    "        return loss     # per batch\n",
    "\n",
    "    def batchSuppressionLoss(self, inp,valid_toks,valid_tok_len, comp,documentEncoder, embedder, num_samples = batch_size):\n",
    "        batch_size, seq_len = inp.size()\n",
    "        inp = inp.permute(1, 0)           # seq_len x batch_size\n",
    "        h = self.init_hidden(comp, num_samples, documentEncoder, embedder)\n",
    "\n",
    "        loss = 0\n",
    "        for i in range(seq_len):\n",
    "            out, h = self.forward(inp[i], h)\n",
    "            _, vocab_size_o = out.size()\n",
    "            out = torch.exp(out)\n",
    "            l1 = torch.sum(torch.mul(out, (1-valid_toks)).float(), dim = 1)\n",
    "            l2 = torch.sum(torch.mul(out, valid_toks.float()), dim = 1)\n",
    "            \n",
    "            l1 = torch.div(l1, (vocab_size_o - valid_tok_len))\n",
    "            l2 = torch.div(l2, valid_tok_len)\n",
    "            loss += (torch.sum(l1) - torch.sum(l2))\n",
    "        return loss\n",
    "    \n",
    "    def batchDegeneracyLoss(self, inp, comp,documentEncoder, embedder, num_samples = batch_size):\n",
    "        batch_size, seq_len = inp.size()\n",
    "        inp = inp.permute(1, 0)           # seq_len x batch_size\n",
    "        h = self.init_hidden(comp, num_samples, documentEncoder, embedder)\n",
    "\n",
    "        loss = 0\n",
    "        for i in range(seq_len):\n",
    "            out, h = self.forward(inp[i], h)\n",
    "            out = torch.exp(out)\n",
    "            out = out.unsqueeze(2)\n",
    "            a1 = out.transpose(1,2)\n",
    "            a2 = torch.log(out)\n",
    "            loss += torch.bmm(a1, a2)\n",
    "        loss = torch.sum(loss) / (batch_size * seq_len)\n",
    "        return loss\n",
    "        \n",
    "\n",
    "    def batchPGLoss(self, inp, target, reward, comp, documentEncoder, embedder):\n",
    "        \"\"\"\n",
    "        Returns a pseudo-loss that gives corresponding policy gradients (on calling .backward()).\n",
    "        Inspired by the example in http://karpathy.github.io/2016/05/31/rl/\n",
    "        Inputs: inp, target\n",
    "            - inp: batch_size x seq_len\n",
    "            - target: batch_size x seq_len\n",
    "            - reward: batch_size (discriminator reward for each sentence, applied to each token of the corresponding\n",
    "                      sentence)\n",
    "            inp should be target with <s> (start letter) prepended\n",
    "        \"\"\"\n",
    "\n",
    "        batch_size, seq_len = inp.size()\n",
    "        inp = inp.permute(1, 0)          # seq_len x batch_size\n",
    "        target = target.permute(1, 0)    # seq_len x batch_size\n",
    "        h = self.init_hidden(comp, batch_size, documentEncoder, embedder)\n",
    "\n",
    "        loss = 0\n",
    "        for i in range(seq_len):\n",
    "            out, h = self.forward(inp[i], h)\n",
    "            # TODO: should h be detached from graph (.detach())?\n",
    "            for j in range(batch_size):\n",
    "                print(target.data[i][j], out[j][target.data[i][j]])\n",
    "                loss += -out[j][target.data[i][j]]*reward[j]     # log(P(y_t|Y_1:Y_{t-1})) * Q\n",
    "\n",
    "        return loss/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder = Embedder(input_size = reduced_glove.shape[0], output_size = reduced_glove.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(embedding_dim = reduced_glove.shape[1], \n",
    "                embeddings = embedder, \n",
    "                hidden_dim = reduced_glove.shape[1], \n",
    "                vocab_size = reduced_glove.shape[0], \n",
    "                max_seq_len = max_question_len, \n",
    "                gpu=USE_CUDA)\n",
    "'''\n",
    "disc = Discriminator(embedding_dim = reduced_glove.shape[1], \n",
    "                     embeddings = embedder, \n",
    "                     hidden_dim = reduced_glove.shape[1], \n",
    "                     vocab_size = reduced_glove.shape[0], \n",
    "                     max_seq_len = max_question_len, \n",
    "                     gpu=USE_CUDA)\n",
    "'''\n",
    "\n",
    "documentEncoder = DocumentEncoderRNN(reduced_glove.shape[1], int(reduced_glove.shape[1] / 2))\n",
    "\n",
    "#documentEncoderDisc = DocumentEncoderDiscRNN(reduced_glove.shape[1], reduced_glove.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "if USE_CUDA:\n",
    "    gen= gen.cuda()\n",
    "    #disc = disc.cuda()\n",
    "    documentEncoder = documentEncoder.cuda()\n",
    "    #documentEncoderDisc = documentEncoderDisc.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator_MLE(gen, gen_opt, batch_input, epochs):\n",
    "    \"\"\"\n",
    "    Max Likelihood Pretraining for the generator\n",
    "    \"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        print('epoch %d : ' % (epoch + 1), end='')\n",
    "        sys.stdout.flush()\n",
    "        total_loss = 0\n",
    "        total_loss_2 = 0\n",
    "        total_loss_3 = 0\n",
    "        print(\" \")\n",
    "        for i in range(len(batch_input)):\n",
    "            print('batch %d/%d : ' % (i + 1, len(batch_input)), end='\\r')\n",
    "            inp, target, comp, valid_toks, valid_tok_len = batch_input[i]['question_input_tokens'], batch_input[i]['question_output_tokens'], batch_input[i]['document_tokens'], batch_input[i]['valid_tokens'], batch_input[i]['valid_tokens_length']\n",
    "            if USE_CUDA:\n",
    "                inp = torch.from_numpy(inp).long().cuda()\n",
    "                target = Variable(torch.from_numpy(target).long().cuda())\n",
    "                comp = Variable(torch.from_numpy(comp).long().cuda())\n",
    "                valid_toks = Variable(torch.from_numpy(valid_toks).float().cuda())\n",
    "                valid_tok_len = Variable(torch.from_numpy(valid_tok_len).float().cuda())\n",
    "            else:\n",
    "                inp = torch.from_numpy(inp).long()\n",
    "                target = Variable(torch.from_numpy(target).long())\n",
    "                comp = Variable(torch.from_numpy(comp).long())\n",
    "                valid_toks = Variable(torch.from_numpy(valid_toks).float())\n",
    "                valid_tok_len = Variable(torch.from_numpy(valid_tok_len).float())\n",
    "                \n",
    "            gen_opt.zero_grad()\n",
    "            \n",
    "            loss3 = gen.batchSuppressionLoss(inp,valid_toks,valid_tok_len,comp,documentEncoder, embedder, inp.shape[0])\n",
    "\n",
    "            loss2 = gen.batchDegeneracyLoss(inp, comp, documentEncoder, embedder, inp.shape[0])\n",
    "            \n",
    "            loss = gen.batchNLLLoss(inp,target,comp,documentEncoder, embedder, inp.shape[0])\n",
    "            \n",
    "            net_loss = loss2 + loss3\n",
    "            net_loss.backward()\n",
    "            gen_opt.step()\n",
    "\n",
    "            total_loss += loss.data[0]\n",
    "            total_loss_2+= loss2.data[0]\n",
    "            total_loss_3 += loss3.data[0] \n",
    "            sys.stdout.flush()\n",
    "\n",
    "\n",
    "        print(' Total Loss = %.4f, Degeneracy Loss = %.4f, Suppression Loss = %.4f' % (total_loss, total_loss_2, total_loss_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_param_gen = []\n",
    "train_param_disc = []\n",
    "\n",
    "for model in [gen, documentEncoder]:\n",
    "    train_param_gen += [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "#for model in [disc, documentEncoderDisc]:\n",
    "#    train_param_disc += [p for p in model.parameters() if p.requires_grad]\n",
    "    \n",
    "gen_optimizer = optim.Adam(train_param_gen, lr=1e-2)\n",
    "#dis_optimizer = optim.Adagrad(train_param_disc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Generator\n",
      "epoch 1 :  \n",
      " Total Loss = 3866.8150, Degeneracy Loss = -89.2424, Suppression Loss = -292.0138\n",
      "epoch 2 :  \n",
      " Total Loss = 6583.1402, Degeneracy Loss = -10.3828, Suppression Loss = -565.3411\n",
      "epoch 3 :  \n",
      " Total Loss = 7660.3583, Degeneracy Loss = -10.5928, Suppression Loss = -565.8740\n",
      "epoch 4 :  \n",
      " Total Loss = 7875.7777, Degeneracy Loss = -10.6726, Suppression Loss = -565.9584\n",
      "epoch 5 :  \n",
      " Total Loss = 7904.0829, Degeneracy Loss = -10.7712, Suppression Loss = -566.0116\n",
      "epoch 6 :  \n",
      " Total Loss = 7887.4467, Degeneracy Loss = -10.7080, Suppression Loss = -566.0754\n",
      "epoch 7 :  \n",
      " Total Loss = 7843.6722, Degeneracy Loss = -10.8187, Suppression Loss = -566.1232\n",
      "epoch 8 :  \n",
      " Total Loss = 7828.6109, Degeneracy Loss = -10.7778, Suppression Loss = -566.1671\n",
      "epoch 9 :  \n",
      " Total Loss = 7803.0675, Degeneracy Loss = -10.7172, Suppression Loss = -566.2000\n",
      "epoch 10 :  \n",
      " Total Loss = 7743.2244, Degeneracy Loss = -10.6820, Suppression Loss = -566.2265\n",
      "epoch 11 :  \n",
      " Total Loss = 7688.3817, Degeneracy Loss = -10.7215, Suppression Loss = -566.2583\n",
      "epoch 12 :  \n",
      " Total Loss = 7614.8905, Degeneracy Loss = -10.7853, Suppression Loss = -566.2718\n",
      "epoch 13 :  \n",
      " Total Loss = 7521.8783, Degeneracy Loss = -10.9900, Suppression Loss = -566.1094\n",
      "epoch 14 :  \n",
      " Total Loss = 7383.4214, Degeneracy Loss = -11.1639, Suppression Loss = -565.8509\n",
      "epoch 15 :  \n",
      " Total Loss = 7407.3366, Degeneracy Loss = -10.7568, Suppression Loss = -566.3300\n",
      "epoch 16 :  \n",
      " Total Loss = 7393.2910, Degeneracy Loss = -10.6819, Suppression Loss = -566.3879\n",
      "epoch 17 :  \n",
      " Total Loss = 7383.7916, Degeneracy Loss = -10.5951, Suppression Loss = -566.4086\n",
      "epoch 18 :  \n",
      " Total Loss = 7402.2443, Degeneracy Loss = -10.3509, Suppression Loss = -566.4411\n",
      "epoch 19 :  \n",
      " Total Loss = 7373.6956, Degeneracy Loss = -9.8942, Suppression Loss = -566.4721\n",
      "epoch 20 :  \n",
      " Total Loss = 7325.5572, Degeneracy Loss = -10.1199, Suppression Loss = -566.4249\n",
      "epoch 21 :  \n",
      " Total Loss = 7266.0073, Degeneracy Loss = -10.4427, Suppression Loss = -566.4683\n",
      "epoch 22 :  \n",
      " Total Loss = 7169.1089, Degeneracy Loss = -10.5061, Suppression Loss = -566.4413\n",
      "epoch 23 :  \n",
      " Total Loss = 6963.9341, Degeneracy Loss = -10.9610, Suppression Loss = -566.2268\n",
      "epoch 24 :  \n",
      " Total Loss = 6927.1894, Degeneracy Loss = -11.0961, Suppression Loss = -566.2603\n",
      "epoch 25 :  \n",
      " Total Loss = 6928.1378, Degeneracy Loss = -11.5064, Suppression Loss = -565.9578\n",
      "epoch 26 :  \n",
      " Total Loss = 7041.6718, Degeneracy Loss = -10.8468, Suppression Loss = -566.5107\n",
      "epoch 27 :  \n",
      "batch 4/16 : \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-174-998680c1131e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Generator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_generator_MLE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgen_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-172-14c4d838f902>\u001b[0m in \u001b[0;36mtrain_generator_MLE\u001b[0;34m(gen, gen_opt, batch_input, epochs)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mloss3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchSuppressionLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_toks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_tok_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcomp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocumentEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m             \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchDegeneracyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocumentEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcomp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocumentEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-169-12b19c670de5>\u001b[0m in \u001b[0;36mbatchDegeneracyLoss\u001b[0;34m(self, inp, comp, documentEncoder, embedder, num_samples)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0ma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Training Generator\")\n",
    "train_generator_MLE(gen,gen_optimizer, batch_input, 200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTokens(tokens):\n",
    "    for token in tokens:\n",
    "        print(look_up_token_reduced(token), sep=' ', end=' ')\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "ammunition . in addition , the prussian artillery batteries had 30 % more guns than their french counterparts . the prussian guns typically <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> the prussian artillery batteries had what percentage more of guns than the french ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "<END> <END> <END> <END> <END> ? ? <END> <END> ? <END> <END> <END> <END> <END> ? ? <END> ? ? ? <END> <END> <END> ? <END> <END> ? <END>  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "beginning with its incorporation in 1802 , detroit has had a total of 74 mayors . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> when was detroit incorporated ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "? ? ? <END> ? ? <END> ? ? <END> <END> ? ? <END> <END> <END> <END> <END> ? <END> ? ? ? <END> ? <END> ? <END> <END>  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "on january 21 , 2014 ibm announced that company executives would forgo bonuses for fiscal <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> what date did ibm announce that its executives would forgo bonuses for fiscal year 2013 ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "? <END> <END> <END> ? <END> <END> <END> ? <END> <END> <END> <END> <END> <END> ? <END> <END> ? ? ? <END> ? <END> <END> ? ? ? <END>  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "its overseas territories until april 1974 , when a bloodless left-wing military coup in lisbon , known as the carnation revolution , led the way <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> what was the carnation revolution ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "<END> <END> <END> <END> <END> <END> <END> ? ? <END> <END> <END> ? ? <END> ? <END> <END> <END> ? <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "heat intensities that northern europeans describe as <UNK> . and italian women tolerate less intense electric shock than jewish or native <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> what nationality can tolerate the least amount of electric shock ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "<END> ? <END> ? <END> <END> ? <END> <END> ? <END> <END> <END> ? ? <END> <END> <END> <END> <END> ? <END> <END> ? <END> ? ? <END> <END>  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "rural areas and small towns , as well as many poor whites . legislation included implementation of a poll tax , timing <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> who besides african americans were victims of tennessee 's late-19th century electoral reform ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "? <END> <END> <END> <END> <END> <END> <END> ? ? <END> ? ? ? <END> <END> ? ? <END> <END> ? <END> <END> <END> ? <END> ? <END> <END>  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "notable in the bible belt , where it is in church that many people get their start in public singing . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> where do a lot of people get their start in singing in the south ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "<END> ? ? <END> ? ? <END> <END> <END> ? ? <END> <END> <END> <END> <END> ? <END> ? <END> ? <END> ? ? <END> <END> ? ? <END>  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "ago , as the ice melted , sea levels rose separating ireland from great britain and also creating the isle of man . about two <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> when sea levels rose what occured in the british isles area ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "<END> ? <END> <END> <END> <END> <END> ? <END> ? <END> <END> ? <END> ? ? <END> <END> ? <END> <END> <END> ? <END> ? <END> <END> ? <END>  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "the city has a mayor and is one of the 16 cities and towns in england and wales to have a <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> how many cities or towns are there in all of england and wales with a ceremonial sheriff acting as the mayor 's deputy ? <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "? <END> <END> <END> <END> ? ? <END> ? <END> <END> ? <END> <END> <END> <END> ? <END> ? <END> <END> <END> <END> ? ? <END> ? <END> <END>  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "of lancaster . while the administrative boundaries changed in the 1970s , the county palatine boundaries remain the same as the <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> when did the administrative boundaries for the duchy of lancaster change ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "? ? <END> ? <END> <END> ? <END> <END> <END> <END> <END> ? ? <END> ? <END> ? <END> <END> <END> <END> <END> <END> <END> <END> ? <END> ?  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "the region grew based on the lucrative fur trade , in which numerous native american people had important <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> what trade was instrumental to the growth of this region ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "? ? <END> ? ? <END> <END> <END> <END> <END> ? ? ? <END> ? <END> <END> <END> <END> ? ? <END> ? ? <END> <END> ? <END> ?  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "value whatsoever  that it was a continuous variable . the <UNK> law makes close predictions for a narrow range of values <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> what rule predicted narrow range of energy values at lower temperatures ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "? ? <END> <END> <END> <END> ? <END> ? <END> <END> <END> ? <END> ? <END> ? <END> <END> ? <END> ? <END> <END> <END> <END> ? ? ?  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      ", in 1981 the cubs hired gm dallas green from philadelphia to turn around the franchise . green had managed the <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> where was gm dallas green from ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "? ? ? ? ? <END> <END> <END> <END> <END> <END> <END> <END> ? <END> <END> <END> ? <END> ? <END> ? ? ? <END> ? <END> ? ?  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "movie . his favorite reading material for relaxation were the western novels of zane grey . with his excellent memory and <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> what genre did zane grey write in ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "<END> <END> <END> <END> ? ? <END> <END> ? <END> <END> <END> <END> ? ? <END> <END> <END> <END> ? <END> ? <END> <END> ? ? ? ? ?  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "the head of the academy of sciences of moldova , ion <UNK> , described the dictionary as a politically motivated `` absurdity <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> who was the head of the academy of sciences of moldova ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "? ? <END> <END> <END> ? <END> ? ? ? ? ? ? <END> ? <END> <END> <END> ? <END> <END> ? ? ? <END> <END> <END> <END> ?  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "a small box and shipped from richmond to abolitionists in philadelphia , pennsylvania , escaping slavery . <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> to what city was henry brown shipped as freight ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "<END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> ? ? <END> <END> <END> <END> ?  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "kerry . kerry thus decided to leave , departing in 1979 with assistant da roanne sragow to set up their own <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> when did kerry leave the da 's office ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "<END> <END> ? <END> ? <END> ? <END> <END> <END> ? <END> <END> ? <END> <END> ? ? <END> <END> <END> <END> <END> ? ? ? <END> ? ?  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "japanese scholars led by <UNK> <UNK> , reacted against the bureaucratic superstate , and began searching for the historic role of the <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> what did <UNK> <UNK> act out against ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "? <END> <END> ? <END> ? ? ? ? <END> <END> <END> <END> ? ? <END> <END> <END> <END> ? <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "in 1592 , and again in 1597 , toyotomi hideyoshi , aiming to invade china ( <UNK> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> when did toyotomi hideyoshi send an army to korea a second time ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "<END> <END> ? ? <END> ? <END> ? <END> ? <END> <END> ? ? <END> <END> <END> <END> ? ? <END> <END> ? <END> <END> <END> <END> <END> ?  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "teaching of what he called `` inert ideas ''  ideas that are disconnected scraps of information , with no application to real life or culture . he opined that `` education with inert ideas is <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> how did whitehead define `` inert ideas '' ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "<END> ? <END> ? <END> <END> ? <END> <END> ? ? <END> ? ? ? ? <END> <END> <END> <END> <END> <END> <END> <END> ? <END> ? ? <END>  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "in global sea level . this effect is offset by snow falling back onto the continent . recent decades have witnessed several <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> what weather event would offset loss of ice shelves ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "? <END> <END> <END> ? <END> <END> <END> ? <END> <END> <END> <END> <END> <END> ? <END> <END> ? ? ? <END> <END> <END> ? <END> ? ? <END>  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "the distribution of contaminated smallpox vaccine and diphtheria antitoxin . the biologics control act of 1902 required that federal government grant premarket approval for every biological <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> what act allowed premarket approval for drugs ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "? <END> ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> ? <END> ? <END> ? ? <END> ? <END> <END> <END> ? ? <END> ?  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "or arab , choosing , on 1967 he started a bluffing game `` but a successful bluff means your opponent must <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> what gambit did nasser fail at in his bluster with israel ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "? <END> ? <END> ? ? ? <END> <END> <END> <END> <END> ? <END> <END> <END> ? <END> ? ? ? <END> <END> ? ? <END> <END> ? <END>  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "have similarly come to be lumped together as sephardic . jews of mixed background are increasingly common , partly because of intermarriage between ashkenazi and <UNK> , <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> are jews of mixed backgrounds more or less common today ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "<END> <END> ? ? <END> <END> ? ? <END> ? <END> <END> <END> ? ? <END> ? <END> ? <END> <END> <END> <END> ? <END> ? ? <END> ?  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "off deep in enemy territory , surrounded and slaughtered at the battle of carrhae in which crassus himself perished . the death of crassus <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> in what battle did marcus licinius crassus die ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "? <END> ? <END> <END> <END> ? <END> ? ? <END> <END> <END> <END> ? <END> <END> <END> <END> <END> ? <END> ? ? <END> ? ? <END> ?  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "major labor draft , the <UNK> , compelled thousands of indians over the colonial period to work on infrastructure to prevent <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> who helped build infrastructure around the lake ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "? <END> <END> ? <END> ? <END> <END> ? ? ? ? ? <END> <END> <END> <END> ? ? <END> <END> <END> <END> <END> ? <END> <END> <END> ?  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "until the 1430s , the gelug was not mentioned in the <UNK> or the <UNK> lu . on this , historian li <UNK> says of tsongkhapa <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> what was the gelug not mentioned in ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "? <END> ? ? ? ? ? <END> ? <END> <END> ? <END> <END> ? <END> ? ? <END> <END> <END> ? ? ? ? ? <END> <END> <END>  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "support for ambitious renewable energy goals . in 2010 , eurobarometer polled the twenty-seven eu member states about the target `` <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> what group polled the 27 eu member states in 2010 ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "<END> ? <END> <END> <END> <END> ? <END> <END> <END> <END> ? <END> ? ? <END> <END> ? ? ? <END> <END> <END> <END> <END> <END> <END> ? ?  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "encyclopdia britannica lists the same bird groups but also includes guinea fowl and squabs ( young pigeons ) . in r. d. crawford 's poultry breeding and genetics <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> what two <UNK> types of birds are listed in teh encyclopedia <UNK> as poultry ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "<END> <END> <END> ? ? ? ? ? <END> <END> <END> <END> ? <END> ? <END> <END> ? <END> <END> ? ? <END> ? <END> ? <END> ? <END>  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      ", citing  very strong personal reasons  . on april 16 , a protest was organised in delhi `` against chinese <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> when did a protest in delhi occur to protest against chinese repression in tibet ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "<END> <END> <END> ? <END> ? <END> ? ? <END> <END> <END> <END> <END> <END> ? <END> <END> ? <END> ? <END> <END> <END> ? ? ? <END> ?  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "the gorals of southern poland and northern slovakia are partially descended from <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> who descended from <UNK> vlachs ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "? ? ? <END> <END> <END> ? <END> <END> <END> <END> ? ? ? ? ? <END> <END> ? <END> <END> ? <END> ? <END> <END> ? <END> <END>  \n",
      "----------------------------------------------------------------------------------------------------\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Comprehension : \n",
      "between 1993 and 1996 , the fbi increased its counter-terrorism role in the wake <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Ground Truth Question : \n",
      "<START> when did the fbi increase it 's counter-terrorism role ? <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END> <END>  \n",
      "*****************************************************************************************************\n",
      "Generated Question : \n",
      "<END> ? <END> <END> <END> ? <END> ? ? ? ? ? <END> <END> ? ? <END> ? ? ? <END> <END> ? <END> <END> ? ? ? ?  \n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "batch_num = 13\n",
    "x = gen.sample(batch_input[batch_num]['document_tokens'].shape[0], torch.from_numpy(batch_input[batch_num]['document_tokens']).cuda(), documentEncoder, embedder)\n",
    "#loss2 = gen.batchSuppressionLoss(inp,valid_toks,comp,documentEncoder, embedder, inp.shape[0]) \n",
    "#loss = gen.batchNLLLoss(inp,target,comp,documentEncoder, embedder, inp.shape[0])\n",
    "for i in range(x.shape[0]):\n",
    "    print(\"----------------------------------------------------------------------------------------------------\")\n",
    "    print(\"Comprehension : \")\n",
    "    printTokens(batch_input[batch_num]['document_tokens'][i])\n",
    "    print(\"*****************************************************************************************************\")\n",
    "    print(\"Ground Truth Question : \")\n",
    "    printTokens(batch_input[batch_num]['question_input_tokens'][i])\n",
    "    print(\"*****************************************************************************************************\")\n",
    "    print(\"Generated Question : \")\n",
    "    printTokens(x[i])\n",
    "    print(\"----------------------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, embeddings,  hidden_dim, vocab_size, max_seq_len,  gpu=False, dropout=0.2):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.gpu = gpu\n",
    "\n",
    "        self.embeddings = embeddings\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=2, bidirectional=True, dropout=dropout)\n",
    "        self.gru2hidden = nn.Linear(2*2*hidden_dim, hidden_dim)\n",
    "        self.dropout_linear = nn.Dropout(p=dropout)\n",
    "        self.hidden2out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def init_hidden(self, comp, num_examples, documentEncoderDisc, embedder):\n",
    "        comp = embedder(comp).float()\n",
    "        h = documentEncoderDisc(comp, documentEncoderDisc.initHidden(num_examples))\n",
    "        hidden = Variable(torch.zeros(2*2*1, num_examples, self.hidden_dim))\n",
    "        if self.gpu:\n",
    "            hidden = hidden.cuda()\n",
    "        \n",
    "        #hidden[0:2,:,:] = h\n",
    "\n",
    "        return hidden\n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        # input dim                                                # batch_size x seq_len\n",
    "        emb = self.embeddings(input)                               # batch_size x seq_len x embedding_dim\n",
    "        emb = emb.permute(1, 0, 2).float()                                 # seq_len x batch_size x embedding_dim\n",
    "        _, hidden = self.gru(emb, hidden)                          # 4 x batch_size x hidden_dim\n",
    "        hidden = hidden.permute(1, 0, 2).contiguous()              # batch_size x 4 x hidden_dim\n",
    "        out = self.gru2hidden(hidden.view(-1, 4*self.hidden_dim))  # batch_size x 4*hidden_dim\n",
    "        out = F.tanh(out)\n",
    "        out = self.dropout_linear(out)\n",
    "        out = self.hidden2out(out)                                 # batch_size x 1\n",
    "        out = F.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "    def batchClassify(self, inp, num_examples,comp, documentEncoderDisc, embedder):\n",
    "        \"\"\"\n",
    "        Classifies a batch of sequences.\n",
    "        Inputs: inp\n",
    "            - inp: batch_size x seq_len\n",
    "        Returns: out\n",
    "            - out: batch_size ([0,1] score)\n",
    "        \"\"\"\n",
    "\n",
    "        h = self.init_hidden(comp, num_examples, documentEncoderDisc, embedder)\n",
    "        out = self.forward(inp, h)\n",
    "        return out.view(-1)\n",
    "\n",
    "    def batchBCELoss(self, inp, target, comp, num_examples, documentEncoderDisc, embedder):\n",
    "        \"\"\"\n",
    "        Returns Binary Cross Entropy Loss for discriminator.\n",
    "         Inputs: inp, target\n",
    "            - inp: batch_size x seq_len\n",
    "            - target: batch_size (binary 1/0)\n",
    "        \"\"\"\n",
    "\n",
    "        loss_fn = nn.BCELoss()\n",
    "        h = self.init_hidden(comp, num_examples, documentEncoderDisc, embedder)\n",
    "        out = self.forward(inp, h)\n",
    "        return loss_fn(out, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_discriminator(discriminator, dis_opt, train_param_disc, generator, d_steps, epochs):\n",
    "    \"\"\"\n",
    "    Training the discriminator on real_data_samples (positive) and generated samples from generator (negative).\n",
    "    Samples are drawn d_steps times, and the discriminator is trained for epochs epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    # generating a small validation set before training (using oracle and generator)\n",
    "    n = 128\n",
    "    l = random.sample(range(len(batch_input)), n//batch_size)\n",
    "    pos_val = None\n",
    "    neg_val = None\n",
    "    for i in l:\n",
    "        pos = Variable(torch.from_numpy(batch_input[i]['question_output_tokens']))\n",
    "        if USE_CUDA:\n",
    "            neg = gen.sample(batch_size, Variable(torch.from_numpy(batch_input[i]['document_tokens']).long()).cuda(), documentEncoder, embedder)\n",
    "        else:\n",
    "            neg = gen.sample(batch_size, Variable(torch.from_numpy(batch_input[i]['document_tokens']).long()), documentEncoder, embedder)\n",
    "        if pos_val is None:\n",
    "            pos_val = pos\n",
    "            neg_val = neg\n",
    "        else:\n",
    "            pos_val = torch.cat((pos_val, pos), dim=0)\n",
    "            neg_val = torch.cat((neg_val, neg), dim=0)\n",
    "\n",
    "    if USE_CUDA:\n",
    "        pos_val = pos_val.cuda()\n",
    "        neg_val = neg_val.cuda()\n",
    "        \n",
    "    \n",
    "    \n",
    "    val_inp, val_target = prepare_discriminator_data(pos_val, Variable(neg_val).int())\n",
    "\n",
    "    for d_step in range(d_steps):\n",
    "        comp = None\n",
    "        real_data = None\n",
    "        for batch in batch_input:\n",
    "            if comp is None:\n",
    "                comp = batch['document_tokens']\n",
    "                real_data = batch['question_output_tokens']\n",
    "            else:\n",
    "                comp = np.concatenate([comp,batch['document_tokens']], axis = 0)\n",
    "                real_data = np.concatenate([real_data,batch['question_output_tokens']], axis = 0)\n",
    "                \n",
    "        comp = Variable(torch.from_numpy(comp).long())\n",
    "        real_data = Variable(torch.from_numpy(real_data).long())\n",
    "        \n",
    "        if USE_CUDA:\n",
    "            comp = comp.cuda()\n",
    "            real_data=real_data.cuda()\n",
    "        \n",
    "        s = batchwise_sample(generator, examples_to_take_train, batch_size, comp, documentEncoder, embedder)\n",
    "        dis_inp, dis_target, comp = prepare_discriminator_data(real_data, Variable(s), comp, gpu=USE_CUDA)\n",
    "        for epoch in range(epochs):\n",
    "            print('d-step %d epoch %d : ' % (d_step + 1, epoch + 1), end='')\n",
    "            sys.stdout.flush()\n",
    "            total_loss = 0\n",
    "            total_acc = 0\n",
    "\n",
    "            for i in range(0, dis_inp.shape[0], batch_size):\n",
    "                inp, target, comp_batch = dis_inp[i:i + batch_size], dis_target[i:i + batch_size], comp[i:i + batch_size]\n",
    "                dis_opt.zero_grad()\n",
    "                #torch.nn.utils.clip_grad_norm(train_param_disc, 0.25)\n",
    "                \n",
    "                out = discriminator.batchClassify(inp, inp.shape[0], comp_batch, documentEncoderDisc, embedder)\n",
    "                \n",
    "                loss_fn = nn.BCELoss()\n",
    "\n",
    "                loss = loss_fn(out, Variable(target))\n",
    "                loss.backward()\n",
    "                dis_opt.step()\n",
    "\n",
    "                total_loss += loss.data[0]\n",
    "\n",
    "                #total_acc += np.sum((out.data>0.5)==(target>0.5))\n",
    "\n",
    "                if (i / batch_size) % math.ceil(math.ceil(2 * examples_to_take_train / float(\n",
    "                        batch_size)) / 10.) == 0:  # roughly every 10% of an epoch\n",
    "                    print('.', end='')\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "            print(\"Total_loss : \", total_loss)\n",
    "            #total_loss /= math.ceil(2 * examples_to_take_train / float(batch_size))\n",
    "            #total_acc /= float(2 * examples_to_take_train)\n",
    "\n",
    "            #val_pred = discriminator.batchClassify(val_inp)\n",
    "            #print(' average_loss = %.4f, train_acc = %.4f, val_acc = %.4f' % (\n",
    "            #    total_loss, 0, 0))\n",
    "            #torch.sum((val_pred>0.5)==(val_target>0.5)).data[0]/200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generator_PG(gen, gen_opt, dis, documentEncoder, embedder):\n",
    "    \"\"\"\n",
    "    The generator is trained using policy gradients, using the reward from the discriminator.\n",
    "    Training is done for num_batches batches.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_loss = 0\n",
    "    for batch in batch_input:\n",
    "        comp = Variable(torch.from_numpy(batch['document_tokens']))\n",
    "        if USE_CUDA:\n",
    "            comp = comp.cuda()\n",
    "            \n",
    "        s = gen.sample(batch['document_tokens'].shape[0], comp, documentEncoder, embedder)\n",
    "        inp, target = prepare_generator_batch(s, start_letter=START_TOKEN, gpu=USE_CUDA)\n",
    "        rewards = disc.batchClassify(target, target.shape[0], comp, documentEncoderDisc, embedder)\n",
    "\n",
    "        gen_opt.zero_grad()\n",
    "        pg_loss = gen.batchPGLoss(inp, target, rewards, comp, documentEncoder, embedder)\n",
    "        total_loss += pg_loss.data[0]\n",
    "        pg_loss.backward()\n",
    "        gen_opt.step()\n",
    "    \n",
    "\n",
    "    print('Total PG Loss : ',  total_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training Discriminator\")\n",
    "train_discriminator(disc, dis_optimizer, train_param_disc, gen, 2, 10)\n",
    "for i in range(100):\n",
    "    print(\"Training Generator\")\n",
    "    train_generator_PG(gen, gen_optimizer, disc, documentEncoder, embedder)\n",
    "    print(\"training Discriminator\")\n",
    "    train_discriminator(disc, dis_optimizer, train_param_disc, gen, 2, 10)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = batchwise_sample(gen, 32, 32, torch.from_numpy(batch_input[0]['document_tokens']).cuda(), documentEncoder, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x:\n",
    "    for j in i:\n",
    "        print(look_up_token_reduced(j), end=' ')\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = Variable(torch.from_numpy(batch_input[5]['document_tokens']))\n",
    "if USE_CUDA:\n",
    "    comp = comp.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = gen.sample(comp.shape[0], comp, documentEncoder, embedder)\n",
    "inp, target = prepare_generator_batch(s, start_letter=START_TOKEN, gpu=USE_CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = disc.batchClassify(target, target.shape[0], comp, documentEncoderDisc, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_loss = gen.batchPGLoss(inp, target, rewards, comp, documentEncoder, embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[0]['valid_tokens_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
