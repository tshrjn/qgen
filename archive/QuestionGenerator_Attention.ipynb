{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=en_US.UTF-8\n",
    "from embedding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    !pip install nltk\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('../train-v1.1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ra2630/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ra2630/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltkStopWords = stopwords.words('english')\n",
    "punctuations = [',', '?', '.', '-',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(data):\n",
    "    contexts = []\n",
    "    qas = []\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        for j in range(len(data[\"data\"][i][\"paragraphs\"])):\n",
    "            contexts.append(data[\"data\"][i][\"paragraphs\"][j][\"context\"])\n",
    "            qas.append(data[\"data\"][i][\"paragraphs\"][j][\"qas\"])\n",
    "    return (contexts,qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CapPassage = True\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "contexts,qas = extractor(data)\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind,ind+sll\n",
    "    return (-1,-1)\n",
    "\n",
    "def capPassage(passage,left, right,cap_length = 20):\n",
    "    y = np.zeros(cap_length)\n",
    "    left = max(0,left - int((cap_length - len(answer))/2))\n",
    "    right = min(right + int((cap_length + len(answer))/2), cap_length)\n",
    "    if(left < 0):\n",
    "        left = 0\n",
    "    if(right > len(passage)):\n",
    "        right = len(passage)\n",
    "    return passage[left:right]\n",
    "    \n",
    "def findAnsVec(answer,passage):\n",
    "    ans = np.zeros((len(passage)))\n",
    "    start,end = find_sub_list(answer,passage)\n",
    "    if(start==-1):\n",
    "        start = passage.index(answer[0])\n",
    "        end = start + len(answer)\n",
    "    ans[start:end] = 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2429\n"
     ]
    }
   ],
   "source": [
    "n_words = 10\n",
    "X_train_comp_all = []\n",
    "X_train_comp_with_answer_marked_all = []\n",
    "X_train_ans_all = []\n",
    "X_train_comp_answer_label_all = []\n",
    "X_train_sentence_label_all = []\n",
    "Y_train_ques_all = []\n",
    "invalid = 0\n",
    "for i,context in enumerate(contexts):\n",
    "    passage = word_tokenize(context.lower())\n",
    "\n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        try:\n",
    "            question = word_tokenize(qas[i][j]['question'].lower())\n",
    "            answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "            start,end = find_sub_list(answer,passage)\n",
    "            if start == -1:\n",
    "                invalid = invalid+1\n",
    "                continue\n",
    "            marked_comp = np.zeros(len(passage))\n",
    "            marked_comp[start:end] = 1\n",
    "            left = max(0,start - n_words)\n",
    "            right = min(len(passage), end + n_words)\n",
    "            \n",
    "            cappedPassage = passage[left:right]\n",
    "            marked_comp = marked_comp[left:right]\n",
    "            \n",
    "            \n",
    "            X_train_comp_all.append(cappedPassage)\n",
    "            X_train_comp_with_answer_marked_all.append(marked_comp)\n",
    "            X_train_ans_all.append(answer)\n",
    "            Y_train_ques_all.append(question)\n",
    "        except Exception as e:\n",
    "            invalid = invalid+1\n",
    "print(invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(zip(X_train_comp_all,X_train_ans_all, Y_train_ques_all,X_train_comp_with_answer_marked_all))\n",
    "np.random.shuffle(c)\n",
    "X_train_comp_all_shuffled,X_train_ans_all_shuffled, Y_train_ques_all_shuffled, X_train_comp_with_answer_marked_all_shuffled = zip(*c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_comp_with_answer_marked_all_shuffled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "def findKMostFrequentWords(k):\n",
    "    ctr = Counter([item for sublist in X_train_comp_all_shuffled for item in sublist] + [item for sublist in Y_train_ques_all_shuffled for item in sublist])\n",
    "    sorted_ctr = sorted(ctr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return [item[0] for item in sorted_ctr[0:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordToTake = 200000\n",
    "words = findKMostFrequentWords(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59528, 300)"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_word_to_idx_reduced = {}\n",
    "_idx_to_word_reduced = []\n",
    "\n",
    "\n",
    "def _add_word_reduced(word):\n",
    "    idx = len(_idx_to_word_reduced)\n",
    "    _word_to_idx_reduced[word] = idx\n",
    "    _idx_to_word_reduced.append(word)\n",
    "    return idx\n",
    "\n",
    "\n",
    "UNKNOWN_TOKEN = _add_word_reduced(UNKNOWN_WORD)\n",
    "START_TOKEN = _add_word_reduced(START_WORD)\n",
    "END_TOKEN = _add_word_reduced(END_WORD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dimensions = glove.shape[1]\n",
    "reduced_glove = []\n",
    "reduced_glove.append(np.zeros(dimensions))\n",
    "reduced_glove.append(-np.ones(dimensions))\n",
    "reduced_glove.append(np.ones(dimensions))\n",
    "\n",
    "for word in words:\n",
    "    l = look_up_word(word)\n",
    "    if(l != UNKNOWN_TOKEN):\n",
    "        idx = _add_word_reduced(word)\n",
    "        reduced_glove.append(glove[l])\n",
    "        if(len(reduced_glove) == wordToTake):\n",
    "            break\n",
    "        \n",
    "def look_up_word_reduced(word):\n",
    "    return _word_to_idx_reduced.get(word, UNKNOWN_TOKEN)\n",
    "\n",
    "\n",
    "def look_up_token_reduced(token):\n",
    "    return _idx_to_word_reduced[token]\n",
    "\n",
    "reduced_glove = np.array(reduced_glove)\n",
    "reduced_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda2.cims.nyu.edu\n",
      "2429\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "print(invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_to_take_train = 32000\n",
    "\n",
    "X_train_comp = X_train_comp_all_shuffled[0:examples_to_take_train]\n",
    "X_train_ans = X_train_ans_all_shuffled[0:examples_to_take_train]\n",
    "Y_train_ques = Y_train_ques_all_shuffled[0:examples_to_take_train]\n",
    "X_train_comp_with_answer_marked = X_train_comp_with_answer_marked_all_shuffled[0:examples_to_take_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_document_len = len(max(X_train_comp,key=len))\n",
    "max_answer_len = len(max(X_train_ans,key=len))\n",
    "max_question_len = len(max(Y_train_ques,key=len)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = np.full((examples_to_take_train, max_document_len), END_TOKEN,dtype=np.int32)\n",
    "document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "\n",
    "answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "\n",
    "question_input_tokens = np.full((examples_to_take_train, max_question_len), END_TOKEN, dtype=np.int32)\n",
    "question_output_tokens = np.full((examples_to_take_train, max_question_len), END_TOKEN, dtype=np.int32)\n",
    "question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(examples_to_take_train):\n",
    "    answer_labels[i, 0:len(X_train_comp_with_answer_marked[i])] = X_train_comp_with_answer_marked[i]\n",
    "    for j, word in enumerate(X_train_comp[i]):\n",
    "        document_tokens[i, j] = look_up_word_reduced(word)\n",
    "    document_lengths[i] = len(X_train_comp[i])\n",
    "\n",
    "    answer_lengths[i] = len(X_train_ans[i])\n",
    "    \n",
    "    question_input_words = ([START_WORD] + Y_train_ques[i])\n",
    "    question_output_words = (Y_train_ques[i] + [END_WORD])\n",
    "\n",
    "    for j, word in enumerate(question_input_words):\n",
    "            question_input_tokens[i, j] = look_up_word_reduced(word)\n",
    "    for j, word in enumerate(question_output_words):\n",
    "        question_output_tokens[i, j] = look_up_word_reduced(word)\n",
    "    question_lengths[i] = len(question_input_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(data):\n",
    "    flat_list = [item for sublist in data for item in sublist]\n",
    "    vocabulary = sorted(set(flat_list))\n",
    "    vocabulary.append(\"<UNK>\")\n",
    "    vocabulary.append(\"unk\")\n",
    "    vocabulary.append(\"eos\")\n",
    "    vocabulary = [\"<EOS>\"] + vocabulary\n",
    "    word_to_index = { word:i for i,word in enumerate(vocabulary) }\n",
    "    index_to_word = { i:word for i,word in enumerate(vocabulary) }\n",
    "    return (vocabulary,word_to_index,index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59528, 300)\n",
      "51426\n",
      "4644\n",
      "49775\n"
     ]
    }
   ],
   "source": [
    "print(reduced_glove.shape)\n",
    "vocabulary_comp,word_to_index_comp,index_to_word_comp = create_vocabulary(X_train_comp + Y_train_ques)\n",
    "print(len(vocabulary_comp))\n",
    "print(word_to_index_comp[\"?\"])\n",
    "print(word_to_index_comp[\"what\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_vector(data,vocabulary,word_to_index,index_to_word, maxLen):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,word in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        if(word not in word_to_index):\n",
    "            word = \"<UNK>\"\n",
    "        one_hot[i][word_to_index[word]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def create_one_hot_vector_from_indices(data,maxLen,vocabulary):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,indice in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        one_hot[i][int(indice)] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def create_one_hot_training_Set(data,maxLen,vocabulary):\n",
    "    one_hot_data = np.zeros([data.shape[0],maxLen,len(vocabulary)])\n",
    "    for i in range(data.shape[0]):\n",
    "        one_hot_data[i] = create_one_hot_vector_from_indices(data[i],maxLen,vocabulary)\n",
    "    return one_hot_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_to_indices_glove(X,max_len):\n",
    "    \n",
    "    m = len(X)                                 \n",
    "    \n",
    "    X_indices = np.full([m,max_len],look_up_word_reduced(END_WORD))\n",
    "    \n",
    "    for i in range(m):\n",
    "        j = 0\n",
    "        for w in X[i]:\n",
    "            if(j>=max_len):\n",
    "                break;\n",
    "            \n",
    "            X_indices[i, j] = look_up_word_reduced(w)\n",
    "            j = j+1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = context_to_indices_glove(X_train_comp, max_document_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_document_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_question_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def createBatch(inputs,batch_size,shuffle=False):\n",
    "    outputs = []\n",
    "    start = 0\n",
    "    while start < len(inputs[0]):\n",
    "        end = min(len(inputs[0]), start + batch_size)\n",
    "        output = {'document_tokens':[],\n",
    "                    'document_lengths':[],\n",
    "                    'answer_labels':[],\n",
    "                    'answer_lengths': [],\n",
    "                    'question_input_tokens':[],\n",
    "                    'question_output_tokens':[],\n",
    "                    'question_lengths':[]}\n",
    "        \n",
    "        for index,inp in enumerate(inputs):\n",
    "            #maxD = max(inputs[1][start:start+batch_size])\n",
    "            maxD = max_document_len\n",
    "            maxA = max(inputs[3][start:start+batch_size])\n",
    "            maxQ = max_question_len\n",
    "            \n",
    "            if index == 0:\n",
    "                output['document_tokens'].append(inp[start:end,0:maxD])\n",
    "            elif index==1:\n",
    "                output['document_lengths'].append(inp[start:end])\n",
    "            elif index==2:\n",
    "                output['answer_labels'].append(inp[start:end,0:maxD])\n",
    "            elif index==3:\n",
    "                output['answer_lengths'].append(inp[start:end])\n",
    "            elif index==4:\n",
    "                output['question_input_tokens'].append(inp[start:end, 0:maxQ])\n",
    "            elif index==5:\n",
    "                output['question_output_tokens'].append(inp[start:end, 0:maxQ])\n",
    "            elif index==6:\n",
    "                output['question_lengths'].append(inp[start:end])\n",
    "        \n",
    "        output[\"document_tokens\"] = np.array(output[\"document_tokens\"])\n",
    "        output[\"document_lengths\"] = np.array(output[\"document_lengths\"])\n",
    "        output[\"answer_labels\"] = np.array(output[\"answer_labels\"])\n",
    "        output[\"answer_lengths\"] = np.array(output[\"answer_lengths\"])\n",
    "        output[\"question_input_tokens\"] = np.array(output[\"question_input_tokens\"])\n",
    "        output[\"question_output_tokens\"] = np.array(output[\"question_output_tokens\"])\n",
    "        output[\"question_lengths\"] = np.array(output[\"question_lengths\"])\n",
    "        outputs.append(output)\n",
    "        start = start + batch_size\n",
    "            \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches =  250\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "batch_input = createBatch([document_tokens,document_lengths,answer_labels,answer_lengths,question_input_tokens,question_output_tokens,question_lengths]\n",
    "                    ,batch_size)\n",
    "for b in batch_input:\n",
    "    for k, v in b.items():\n",
    "        b[k] = v.squeeze()\n",
    "number_of_batches = len(batch_input)\n",
    "print(\"Number of batches = \", number_of_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(0.8 * number_of_batches)\n",
    "batch_input_test = batch_input[split:]\n",
    "batch_input = batch_input[0:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_attention = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, output_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(reduced_glove).float())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentEncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(DocumentEncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True, bidirectional=True) #Input_size = Hidden_Size\n",
    "        if use_cuda:\n",
    "            self.gru = self.gru.cuda()\n",
    "        self.fc = nn.Linear(hidden_size*2, 1)\n",
    "        if use_cuda:\n",
    "            self.fc = self.fc.cuda()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        final_output = self.fc(output)\n",
    "        final_output = F.sigmoid(final_output)\n",
    "        self.hiddenState = hidden\n",
    "        return final_output, output, hidden\n",
    "    \n",
    "    def initHidden(self, batch_size):\n",
    "        result = Variable(torch.zeros(2, batch_size, self.hidden_size)) #2 for BiDirectional\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super(QuestionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True)\n",
    "        if use_cuda:\n",
    "            self.gru = self.gru.cuda()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        self.hiddenState = hidden\n",
    "        return output, hidden\n",
    "\n",
    "class QuestionGenerationFC(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, output_size):\n",
    "        super(QuestionGenerationFC, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        if use_cuda:\n",
    "            self.fc1 = self.fc1.cuda()\n",
    "            self.fc2 = self.fc2.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc1(x)\n",
    "        output = F.tanh(output)\n",
    "        output = self.fc2(output)\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Attention,self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        if use_cuda:\n",
    "            self.fc = self.fc.cuda()\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        #hidden = self.dropout(hidden)\n",
    "\n",
    "        encoder_outputs = self.fc(encoder_outputs)\n",
    "        \n",
    "        activations = torch.bmm(encoder_outputs, hidden.unsqueeze(1).transpose(1,2))\n",
    "        activations = F.softmax(activations, dim=1)\n",
    "        output = torch.mul(encoder_outputs, activations)\n",
    "        output = torch.sum(output, dim=1)\n",
    "        output = F.relu(output)\n",
    "        return output.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters =  20\n"
     ]
    }
   ],
   "source": [
    "hidden_size = reduced_glove.shape[1]\n",
    "\n",
    "embedder = Embedder(input_size = reduced_glove.shape[0], output_size = reduced_glove.shape[1])\n",
    "documentEncoder = DocumentEncoderRNN(input_size = hidden_size+1, hidden_size = hidden_size)\n",
    "questionDecoder = QuestionDecoderRNN(input_size=hidden_size, hidden_size=2 * hidden_size)\n",
    "questionGenerator = QuestionGenerationFC(input_size = 2*questionDecoder.hidden_size, hidden_size=2*hidden_size, output_size=reduced_glove.shape[0])\n",
    "attention = Attention(2 * documentEncoder.hidden_size, questionDecoder.hidden_size)\n",
    "\n",
    "documentEncoder.train()\n",
    "questionDecoder.train()\n",
    "questionGenerator.train()\n",
    "attention.train()\n",
    "\n",
    "train_param = []\n",
    "\n",
    "for model in [documentEncoder, questionDecoder, questionGenerator, attention]:\n",
    "    train_param += [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "print(\"Number of trainable parameters = \", len(train_param))\n",
    "\n",
    "optimizer = torch.optim.Adam(train_param, 4e-3)\n",
    "criterion1 = nn.BCELoss()\n",
    "#criterion2 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.NLLLoss()\n",
    "#criterion2 = nn.MultiLabelSoftMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainEpoch(verboseBatchPrinting):\n",
    "    #verboseBatchPrinting = True\n",
    "    avg_loss = 0\n",
    "    epoch_time_start = time.time()\n",
    "    for batch_num in range(len(batch_input)):\n",
    "        batch_time_start = time.time()\n",
    "        document_encoder_hidden = documentEncoder.initHidden(batch_size)\n",
    "\n",
    "        current_batch_size = len(batch_input[batch_num][\"document_tokens\"])\n",
    "        if current_batch_size != batch_size:\n",
    "            continue\n",
    "\n",
    "        maxDocLenForBatch = max_document_len\n",
    "        mask = np.zeros((current_batch_size, maxDocLenForBatch))\n",
    "\n",
    "        for i in range(current_batch_size):\n",
    "            mask[i,0:batch_input[batch_num][\"document_lengths\"][i]] = 1\n",
    "\n",
    "\n",
    "        inp = Variable(torch.from_numpy(batch_input[batch_num][\"document_tokens\"]).long())\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embedded_inp = embedder(inp)\n",
    "\n",
    "        if use_cuda:\n",
    "            embedded_inp = embedded_inp.cuda()\n",
    "            \n",
    "        #Adding answer=0/1 dim to embedded inp  \n",
    "        answer_target = Variable(torch.from_numpy(batch_input[batch_num]['answer_labels']).float())\n",
    "        if use_cuda:\n",
    "            answer_target = answer_target.cuda()\n",
    "        embedded_inp = torch.cat((embedded_inp,answer_target.unsqueeze(-1)),dim=-1)\n",
    "\n",
    "        answer_tags, answer_outputs, document_encoder_hidden = documentEncoder(embedded_inp, document_encoder_hidden)              \n",
    "\n",
    "\n",
    "        question_decoder_hidden = document_encoder_hidden.view(1,document_encoder_hidden.shape[1],2*document_encoder_hidden.shape[2])\n",
    "        \n",
    "        embedded_inputs = embedder(torch.from_numpy(batch_input[batch_num][\"question_input_tokens\"]).long())\n",
    "        output_labels = Variable(torch.from_numpy(batch_input[batch_num][\"question_output_tokens\"]).long())\n",
    "        if use_cuda:\n",
    "            embedded_inputs = embedded_inputs.cuda()\n",
    "            output_labels = output_labels.cuda()\n",
    "\n",
    "        question_loss = 0\n",
    "        for quesL in range(batch_input[batch_num][\"question_input_tokens\"].shape[1]):\n",
    "            _, question_decoder_hidden = questionDecoder(\n",
    "                embedded_inputs[:,quesL:quesL+1,:],\n",
    "                question_decoder_hidden)\n",
    "\n",
    "\n",
    "            output = question_decoder_hidden\n",
    "            if use_attention:\n",
    "                attn_output = attention(question_decoder_hidden.squeeze(0), answer_outputs)\n",
    "                output = torch.cat((attn_output, output), dim=2)\n",
    "\n",
    "\n",
    "            final_output = questionGenerator(output.squeeze(0))\n",
    "            question_word_loss = criterion2(final_output,\n",
    "                                        output_labels[:, quesL:quesL+1].squeeze(1))\n",
    "            question_loss += question_word_loss\n",
    "\n",
    "        net_loss = question_loss\n",
    "        net_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        avg_loss+= net_loss.data[0]\n",
    "        batch_time_end = time.time()\n",
    "\n",
    "        if verboseBatchPrinting:\n",
    "            print ('Batch: %d \\t Epoch : %d\\tNet Loss: %.4f \\tQuestion Loss: %.4f \\t Time Taken: %d seconds'\n",
    "                   %(batch_num, epoch, net_loss.data[0],  question_loss.data[0], batch_time_end-batch_time_start))\n",
    "    epoch_time_end = time.time()\n",
    "    \n",
    "    return avg_loss/len(batch_input), epoch_time_end - epoch_time_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0 \t Epoch : 1\tNet Loss: 440.5264 \tQuestion Loss: 440.5264 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 1\tNet Loss: 279.1686 \tQuestion Loss: 279.1686 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 1\tNet Loss: 125.3199 \tQuestion Loss: 125.3199 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 1\tNet Loss: 141.9577 \tQuestion Loss: 141.9577 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 1\tNet Loss: 89.8704 \tQuestion Loss: 89.8704 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 1\tNet Loss: 89.7388 \tQuestion Loss: 89.7388 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 1\tNet Loss: 90.0757 \tQuestion Loss: 90.0757 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 1\tNet Loss: 93.7322 \tQuestion Loss: 93.7322 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 1\tNet Loss: 94.9184 \tQuestion Loss: 94.9184 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 1\tNet Loss: 96.0194 \tQuestion Loss: 96.0194 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 1\tNet Loss: 90.7206 \tQuestion Loss: 90.7206 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 1\tNet Loss: 94.8511 \tQuestion Loss: 94.8511 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 1\tNet Loss: 90.7217 \tQuestion Loss: 90.7217 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 1\tNet Loss: 92.9999 \tQuestion Loss: 92.9999 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 1\tNet Loss: 93.2283 \tQuestion Loss: 93.2283 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 1\tNet Loss: 95.6268 \tQuestion Loss: 95.6268 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 1\tNet Loss: 91.8072 \tQuestion Loss: 91.8072 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 1\tNet Loss: 91.6167 \tQuestion Loss: 91.6167 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 1\tNet Loss: 92.4172 \tQuestion Loss: 92.4172 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 1\tNet Loss: 87.9089 \tQuestion Loss: 87.9089 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 1\tNet Loss: 97.1836 \tQuestion Loss: 97.1836 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 1\tNet Loss: 89.5837 \tQuestion Loss: 89.5837 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 1\tNet Loss: 92.4276 \tQuestion Loss: 92.4276 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 1\tNet Loss: 95.9627 \tQuestion Loss: 95.9627 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 1\tNet Loss: 89.7289 \tQuestion Loss: 89.7289 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 1\tNet Loss: 93.7196 \tQuestion Loss: 93.7196 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 1\tNet Loss: 98.6679 \tQuestion Loss: 98.6679 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 1\tNet Loss: 91.5286 \tQuestion Loss: 91.5286 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 1\tNet Loss: 86.9909 \tQuestion Loss: 86.9909 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 1\tNet Loss: 89.3913 \tQuestion Loss: 89.3913 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 1\tNet Loss: 90.0902 \tQuestion Loss: 90.0902 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 1\tNet Loss: 90.0200 \tQuestion Loss: 90.0200 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 1\tNet Loss: 87.6870 \tQuestion Loss: 87.6870 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 1\tNet Loss: 91.9299 \tQuestion Loss: 91.9299 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 1\tNet Loss: 86.0971 \tQuestion Loss: 86.0971 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 1\tNet Loss: 89.0378 \tQuestion Loss: 89.0378 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 1\tNet Loss: 92.4991 \tQuestion Loss: 92.4991 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 1\tNet Loss: 94.0297 \tQuestion Loss: 94.0297 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 1\tNet Loss: 89.9744 \tQuestion Loss: 89.9744 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 1\tNet Loss: 91.9615 \tQuestion Loss: 91.9615 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 1\tNet Loss: 90.1401 \tQuestion Loss: 90.1401 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 1\tNet Loss: 89.4429 \tQuestion Loss: 89.4429 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 1\tNet Loss: 90.9853 \tQuestion Loss: 90.9853 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 1\tNet Loss: 90.9270 \tQuestion Loss: 90.9270 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 1\tNet Loss: 89.0396 \tQuestion Loss: 89.0396 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 1\tNet Loss: 91.4556 \tQuestion Loss: 91.4556 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 1\tNet Loss: 90.1516 \tQuestion Loss: 90.1516 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 1\tNet Loss: 95.3674 \tQuestion Loss: 95.3674 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 1\tNet Loss: 84.8393 \tQuestion Loss: 84.8393 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 1\tNet Loss: 91.3012 \tQuestion Loss: 91.3012 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 1\tNet Loss: 90.7289 \tQuestion Loss: 90.7289 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 1\tNet Loss: 86.6718 \tQuestion Loss: 86.6718 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 1\tNet Loss: 86.8523 \tQuestion Loss: 86.8523 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 1\tNet Loss: 88.6720 \tQuestion Loss: 88.6720 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 1\tNet Loss: 94.2428 \tQuestion Loss: 94.2428 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 1\tNet Loss: 90.4301 \tQuestion Loss: 90.4301 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 1\tNet Loss: 92.1900 \tQuestion Loss: 92.1900 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 1\tNet Loss: 93.7099 \tQuestion Loss: 93.7099 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 1\tNet Loss: 85.4510 \tQuestion Loss: 85.4510 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 1\tNet Loss: 89.3115 \tQuestion Loss: 89.3115 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 1\tNet Loss: 87.0071 \tQuestion Loss: 87.0071 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 1\tNet Loss: 92.7014 \tQuestion Loss: 92.7014 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 1\tNet Loss: 86.3932 \tQuestion Loss: 86.3932 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 1\tNet Loss: 91.2605 \tQuestion Loss: 91.2605 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 1\tNet Loss: 89.4715 \tQuestion Loss: 89.4715 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 1\tNet Loss: 87.4738 \tQuestion Loss: 87.4738 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 1\tNet Loss: 89.3364 \tQuestion Loss: 89.3364 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 1\tNet Loss: 89.4506 \tQuestion Loss: 89.4506 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 1\tNet Loss: 88.4069 \tQuestion Loss: 88.4069 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 1\tNet Loss: 93.3109 \tQuestion Loss: 93.3109 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 1\tNet Loss: 91.2943 \tQuestion Loss: 91.2943 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 1\tNet Loss: 89.8909 \tQuestion Loss: 89.8909 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 1\tNet Loss: 92.9607 \tQuestion Loss: 92.9607 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 1\tNet Loss: 91.2587 \tQuestion Loss: 91.2587 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 1\tNet Loss: 91.6008 \tQuestion Loss: 91.6008 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 1\tNet Loss: 91.5835 \tQuestion Loss: 91.5835 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 1\tNet Loss: 83.6201 \tQuestion Loss: 83.6201 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 1\tNet Loss: 89.0582 \tQuestion Loss: 89.0582 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 1\tNet Loss: 93.4225 \tQuestion Loss: 93.4225 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 1\tNet Loss: 89.5125 \tQuestion Loss: 89.5125 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 1\tNet Loss: 91.1711 \tQuestion Loss: 91.1711 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 1\tNet Loss: 88.8114 \tQuestion Loss: 88.8114 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 1\tNet Loss: 87.1400 \tQuestion Loss: 87.1400 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 1\tNet Loss: 85.7111 \tQuestion Loss: 85.7111 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 1\tNet Loss: 89.1720 \tQuestion Loss: 89.1720 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 1\tNet Loss: 87.6990 \tQuestion Loss: 87.6990 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 1\tNet Loss: 89.4667 \tQuestion Loss: 89.4667 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 1\tNet Loss: 89.1050 \tQuestion Loss: 89.1050 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 1\tNet Loss: 91.3475 \tQuestion Loss: 91.3475 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 1\tNet Loss: 86.7913 \tQuestion Loss: 86.7913 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 1\tNet Loss: 92.3388 \tQuestion Loss: 92.3388 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 1\tNet Loss: 90.3236 \tQuestion Loss: 90.3236 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 1\tNet Loss: 89.2791 \tQuestion Loss: 89.2791 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 1\tNet Loss: 88.9093 \tQuestion Loss: 88.9093 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 94 \t Epoch : 1\tNet Loss: 90.4320 \tQuestion Loss: 90.4320 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 1\tNet Loss: 90.3221 \tQuestion Loss: 90.3221 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 1\tNet Loss: 88.7474 \tQuestion Loss: 88.7474 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 1\tNet Loss: 93.0445 \tQuestion Loss: 93.0445 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 1\tNet Loss: 89.3115 \tQuestion Loss: 89.3115 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 1\tNet Loss: 89.9095 \tQuestion Loss: 89.9095 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 1\tNet Loss: 89.7517 \tQuestion Loss: 89.7517 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 1\tNet Loss: 92.4366 \tQuestion Loss: 92.4366 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 1\tNet Loss: 89.8584 \tQuestion Loss: 89.8584 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 1\tNet Loss: 89.8544 \tQuestion Loss: 89.8544 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 1\tNet Loss: 92.7573 \tQuestion Loss: 92.7573 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 1\tNet Loss: 93.7889 \tQuestion Loss: 93.7889 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 1\tNet Loss: 88.9369 \tQuestion Loss: 88.9369 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 1\tNet Loss: 86.9455 \tQuestion Loss: 86.9455 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 1\tNet Loss: 86.0363 \tQuestion Loss: 86.0363 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 1\tNet Loss: 90.2086 \tQuestion Loss: 90.2086 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 1\tNet Loss: 90.8137 \tQuestion Loss: 90.8137 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 1\tNet Loss: 85.0161 \tQuestion Loss: 85.0161 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 1\tNet Loss: 86.6251 \tQuestion Loss: 86.6251 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 1\tNet Loss: 93.9347 \tQuestion Loss: 93.9347 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 1\tNet Loss: 91.1305 \tQuestion Loss: 91.1305 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 1\tNet Loss: 87.5889 \tQuestion Loss: 87.5889 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 1\tNet Loss: 85.2609 \tQuestion Loss: 85.2609 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 1\tNet Loss: 87.5142 \tQuestion Loss: 87.5142 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 1\tNet Loss: 90.6470 \tQuestion Loss: 90.6470 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 1\tNet Loss: 86.3295 \tQuestion Loss: 86.3295 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 1\tNet Loss: 88.9154 \tQuestion Loss: 88.9154 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 1\tNet Loss: 90.2434 \tQuestion Loss: 90.2434 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 1\tNet Loss: 94.2934 \tQuestion Loss: 94.2934 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 1\tNet Loss: 89.8075 \tQuestion Loss: 89.8075 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 1\tNet Loss: 92.1161 \tQuestion Loss: 92.1161 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 1\tNet Loss: 89.2652 \tQuestion Loss: 89.2652 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 1\tNet Loss: 93.2927 \tQuestion Loss: 93.2927 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 1\tNet Loss: 85.1226 \tQuestion Loss: 85.1226 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 1\tNet Loss: 89.3926 \tQuestion Loss: 89.3926 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 1\tNet Loss: 90.2638 \tQuestion Loss: 90.2638 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 1\tNet Loss: 96.6270 \tQuestion Loss: 96.6270 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 1\tNet Loss: 91.7472 \tQuestion Loss: 91.7472 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 1\tNet Loss: 88.0135 \tQuestion Loss: 88.0135 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 1\tNet Loss: 89.3786 \tQuestion Loss: 89.3786 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 1\tNet Loss: 90.0356 \tQuestion Loss: 90.0356 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 1\tNet Loss: 82.8353 \tQuestion Loss: 82.8353 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 1\tNet Loss: 93.7831 \tQuestion Loss: 93.7831 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 1\tNet Loss: 91.0268 \tQuestion Loss: 91.0268 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 1\tNet Loss: 89.7410 \tQuestion Loss: 89.7410 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 1\tNet Loss: 88.0350 \tQuestion Loss: 88.0350 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 1\tNet Loss: 85.7965 \tQuestion Loss: 85.7965 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 1\tNet Loss: 86.5374 \tQuestion Loss: 86.5374 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 1\tNet Loss: 90.9759 \tQuestion Loss: 90.9759 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 1\tNet Loss: 85.6753 \tQuestion Loss: 85.6753 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 1\tNet Loss: 83.6559 \tQuestion Loss: 83.6559 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 1\tNet Loss: 91.2915 \tQuestion Loss: 91.2915 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 1\tNet Loss: 103.5021 \tQuestion Loss: 103.5021 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 1\tNet Loss: 116.4520 \tQuestion Loss: 116.4520 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 1\tNet Loss: 109.3392 \tQuestion Loss: 109.3392 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 1\tNet Loss: 92.1309 \tQuestion Loss: 92.1309 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 1\tNet Loss: 97.5167 \tQuestion Loss: 97.5167 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 1\tNet Loss: 101.1891 \tQuestion Loss: 101.1891 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 1\tNet Loss: 96.9128 \tQuestion Loss: 96.9128 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 1\tNet Loss: 96.2542 \tQuestion Loss: 96.2542 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 1\tNet Loss: 87.4450 \tQuestion Loss: 87.4450 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 1\tNet Loss: 92.5993 \tQuestion Loss: 92.5993 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 1\tNet Loss: 94.8233 \tQuestion Loss: 94.8233 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 1\tNet Loss: 89.9035 \tQuestion Loss: 89.9035 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 1\tNet Loss: 93.9712 \tQuestion Loss: 93.9712 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 1\tNet Loss: 94.7505 \tQuestion Loss: 94.7505 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 1\tNet Loss: 96.5040 \tQuestion Loss: 96.5040 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 1\tNet Loss: 94.4313 \tQuestion Loss: 94.4313 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 1\tNet Loss: 96.6067 \tQuestion Loss: 96.6067 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 1\tNet Loss: 89.6917 \tQuestion Loss: 89.6917 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 1\tNet Loss: 88.3224 \tQuestion Loss: 88.3224 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 1\tNet Loss: 89.3963 \tQuestion Loss: 89.3963 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 1\tNet Loss: 87.3240 \tQuestion Loss: 87.3240 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 1\tNet Loss: 92.6250 \tQuestion Loss: 92.6250 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 1\tNet Loss: 91.4311 \tQuestion Loss: 91.4311 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 1\tNet Loss: 95.4142 \tQuestion Loss: 95.4142 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 1\tNet Loss: 93.5282 \tQuestion Loss: 93.5282 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 1\tNet Loss: 91.6581 \tQuestion Loss: 91.6581 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 1\tNet Loss: 88.2191 \tQuestion Loss: 88.2191 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 1\tNet Loss: 90.4973 \tQuestion Loss: 90.4973 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 1\tNet Loss: 94.5559 \tQuestion Loss: 94.5559 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 1\tNet Loss: 97.1533 \tQuestion Loss: 97.1533 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 1\tNet Loss: 90.0037 \tQuestion Loss: 90.0037 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 1\tNet Loss: 91.4483 \tQuestion Loss: 91.4483 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 1\tNet Loss: 90.1214 \tQuestion Loss: 90.1214 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 1\tNet Loss: 91.0056 \tQuestion Loss: 91.0056 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 1\tNet Loss: 89.7619 \tQuestion Loss: 89.7619 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 1\tNet Loss: 92.4693 \tQuestion Loss: 92.4693 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 1\tNet Loss: 94.4628 \tQuestion Loss: 94.4628 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 1\tNet Loss: 89.9747 \tQuestion Loss: 89.9747 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 1\tNet Loss: 92.9585 \tQuestion Loss: 92.9585 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 1\tNet Loss: 91.9609 \tQuestion Loss: 91.9609 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 1\tNet Loss: 93.7285 \tQuestion Loss: 93.7285 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 187 \t Epoch : 1\tNet Loss: 94.6967 \tQuestion Loss: 94.6967 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 1\tNet Loss: 89.6742 \tQuestion Loss: 89.6742 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 1\tNet Loss: 90.1229 \tQuestion Loss: 90.1229 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 1\tNet Loss: 91.3001 \tQuestion Loss: 91.3001 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 1\tNet Loss: 90.0968 \tQuestion Loss: 90.0968 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 1\tNet Loss: 95.0068 \tQuestion Loss: 95.0068 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 1\tNet Loss: 93.3493 \tQuestion Loss: 93.3493 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 1\tNet Loss: 95.1813 \tQuestion Loss: 95.1813 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 1\tNet Loss: 87.8377 \tQuestion Loss: 87.8377 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 1\tNet Loss: 88.9911 \tQuestion Loss: 88.9911 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 1\tNet Loss: 94.8126 \tQuestion Loss: 94.8126 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 1\tNet Loss: 87.8808 \tQuestion Loss: 87.8808 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 1\tNet Loss: 92.7551 \tQuestion Loss: 92.7551 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 1 : 94.1937 Time Taken: 150 seconds\n",
      "Batch: 0 \t Epoch : 2\tNet Loss: 92.5957 \tQuestion Loss: 92.5957 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 2\tNet Loss: 88.1319 \tQuestion Loss: 88.1319 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 2\tNet Loss: 91.7014 \tQuestion Loss: 91.7014 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 2\tNet Loss: 92.8834 \tQuestion Loss: 92.8834 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 2\tNet Loss: 90.7217 \tQuestion Loss: 90.7217 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 2\tNet Loss: 91.2902 \tQuestion Loss: 91.2902 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 2\tNet Loss: 86.9443 \tQuestion Loss: 86.9443 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 2\tNet Loss: 92.0628 \tQuestion Loss: 92.0628 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 2\tNet Loss: 90.7953 \tQuestion Loss: 90.7953 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 2\tNet Loss: 92.6609 \tQuestion Loss: 92.6609 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 2\tNet Loss: 88.7393 \tQuestion Loss: 88.7393 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 2\tNet Loss: 93.1713 \tQuestion Loss: 93.1713 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 2\tNet Loss: 88.1359 \tQuestion Loss: 88.1359 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 2\tNet Loss: 90.2941 \tQuestion Loss: 90.2941 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 2\tNet Loss: 91.1105 \tQuestion Loss: 91.1105 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 2\tNet Loss: 93.1955 \tQuestion Loss: 93.1955 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 2\tNet Loss: 89.0488 \tQuestion Loss: 89.0488 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 2\tNet Loss: 90.8849 \tQuestion Loss: 90.8849 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 2\tNet Loss: 89.1741 \tQuestion Loss: 89.1741 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 2\tNet Loss: 85.1249 \tQuestion Loss: 85.1249 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 2\tNet Loss: 94.1201 \tQuestion Loss: 94.1201 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 2\tNet Loss: 86.3863 \tQuestion Loss: 86.3863 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 2\tNet Loss: 90.5167 \tQuestion Loss: 90.5167 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 2\tNet Loss: 93.7330 \tQuestion Loss: 93.7330 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 2\tNet Loss: 87.9279 \tQuestion Loss: 87.9279 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 2\tNet Loss: 91.6818 \tQuestion Loss: 91.6818 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 2\tNet Loss: 96.2218 \tQuestion Loss: 96.2218 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 2\tNet Loss: 90.1440 \tQuestion Loss: 90.1440 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 2\tNet Loss: 85.3264 \tQuestion Loss: 85.3264 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 2\tNet Loss: 88.1876 \tQuestion Loss: 88.1876 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 2\tNet Loss: 90.2360 \tQuestion Loss: 90.2360 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 2\tNet Loss: 88.9323 \tQuestion Loss: 88.9323 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 2\tNet Loss: 87.5134 \tQuestion Loss: 87.5134 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 2\tNet Loss: 91.4717 \tQuestion Loss: 91.4717 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 2\tNet Loss: 85.9573 \tQuestion Loss: 85.9573 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 2\tNet Loss: 88.6267 \tQuestion Loss: 88.6267 \t Time Taken: 1 seconds\n",
      "Batch: 36 \t Epoch : 2\tNet Loss: 91.8975 \tQuestion Loss: 91.8975 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 2\tNet Loss: 93.8276 \tQuestion Loss: 93.8276 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 2\tNet Loss: 90.1263 \tQuestion Loss: 90.1263 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 2\tNet Loss: 92.1585 \tQuestion Loss: 92.1585 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 2\tNet Loss: 90.2358 \tQuestion Loss: 90.2358 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 2\tNet Loss: 89.2494 \tQuestion Loss: 89.2494 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 2\tNet Loss: 90.5683 \tQuestion Loss: 90.5683 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 2\tNet Loss: 91.8151 \tQuestion Loss: 91.8151 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 2\tNet Loss: 89.6602 \tQuestion Loss: 89.6602 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 2\tNet Loss: 92.1021 \tQuestion Loss: 92.1021 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 2\tNet Loss: 90.7830 \tQuestion Loss: 90.7830 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 2\tNet Loss: 95.7585 \tQuestion Loss: 95.7585 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 2\tNet Loss: 85.3594 \tQuestion Loss: 85.3594 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 2\tNet Loss: 93.1107 \tQuestion Loss: 93.1107 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 2\tNet Loss: 91.8255 \tQuestion Loss: 91.8255 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 2\tNet Loss: 86.9942 \tQuestion Loss: 86.9942 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 2\tNet Loss: 88.3396 \tQuestion Loss: 88.3396 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 2\tNet Loss: 89.2590 \tQuestion Loss: 89.2590 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 2\tNet Loss: 94.9653 \tQuestion Loss: 94.9653 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 2\tNet Loss: 91.8253 \tQuestion Loss: 91.8253 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 2\tNet Loss: 93.5130 \tQuestion Loss: 93.5130 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 2\tNet Loss: 95.1640 \tQuestion Loss: 95.1640 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 2\tNet Loss: 86.6678 \tQuestion Loss: 86.6678 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 2\tNet Loss: 90.9035 \tQuestion Loss: 90.9035 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 2\tNet Loss: 88.0340 \tQuestion Loss: 88.0340 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 2\tNet Loss: 94.0575 \tQuestion Loss: 94.0575 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 2\tNet Loss: 87.8033 \tQuestion Loss: 87.8033 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 2\tNet Loss: 92.6115 \tQuestion Loss: 92.6115 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 2\tNet Loss: 90.8431 \tQuestion Loss: 90.8431 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 2\tNet Loss: 89.1238 \tQuestion Loss: 89.1238 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 2\tNet Loss: 91.2788 \tQuestion Loss: 91.2788 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 2\tNet Loss: 90.8388 \tQuestion Loss: 90.8388 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 2\tNet Loss: 90.0787 \tQuestion Loss: 90.0787 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 2\tNet Loss: 94.7926 \tQuestion Loss: 94.7926 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 2\tNet Loss: 92.4573 \tQuestion Loss: 92.4573 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 2\tNet Loss: 91.7211 \tQuestion Loss: 91.7211 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 2\tNet Loss: 94.6004 \tQuestion Loss: 94.6004 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 2\tNet Loss: 92.8830 \tQuestion Loss: 92.8830 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 2\tNet Loss: 94.0316 \tQuestion Loss: 94.0316 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 2\tNet Loss: 93.0871 \tQuestion Loss: 93.0871 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 2\tNet Loss: 86.0885 \tQuestion Loss: 86.0885 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 2\tNet Loss: 90.6211 \tQuestion Loss: 90.6211 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 2\tNet Loss: 95.5252 \tQuestion Loss: 95.5252 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 2\tNet Loss: 91.8264 \tQuestion Loss: 91.8264 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 80 \t Epoch : 2\tNet Loss: 93.3429 \tQuestion Loss: 93.3429 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 2\tNet Loss: 91.1479 \tQuestion Loss: 91.1479 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 2\tNet Loss: 88.9256 \tQuestion Loss: 88.9256 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 2\tNet Loss: 87.9771 \tQuestion Loss: 87.9771 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 2\tNet Loss: 91.4272 \tQuestion Loss: 91.4272 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 2\tNet Loss: 89.9016 \tQuestion Loss: 89.9016 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 2\tNet Loss: 91.8575 \tQuestion Loss: 91.8575 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 2\tNet Loss: 91.0882 \tQuestion Loss: 91.0882 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 2\tNet Loss: 93.8736 \tQuestion Loss: 93.8736 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 2\tNet Loss: 89.6873 \tQuestion Loss: 89.6873 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 2\tNet Loss: 94.6565 \tQuestion Loss: 94.6565 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 2\tNet Loss: 92.4948 \tQuestion Loss: 92.4948 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 2\tNet Loss: 90.8768 \tQuestion Loss: 90.8768 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 2\tNet Loss: 91.3639 \tQuestion Loss: 91.3639 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 2\tNet Loss: 92.2781 \tQuestion Loss: 92.2781 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 2\tNet Loss: 92.8738 \tQuestion Loss: 92.8738 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 2\tNet Loss: 91.3232 \tQuestion Loss: 91.3232 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 2\tNet Loss: 95.2024 \tQuestion Loss: 95.2024 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 2\tNet Loss: 91.9826 \tQuestion Loss: 91.9826 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 2\tNet Loss: 92.7574 \tQuestion Loss: 92.7574 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 2\tNet Loss: 91.9884 \tQuestion Loss: 91.9884 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 2\tNet Loss: 95.1173 \tQuestion Loss: 95.1173 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 2\tNet Loss: 92.9826 \tQuestion Loss: 92.9826 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 2\tNet Loss: 92.2123 \tQuestion Loss: 92.2123 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 2\tNet Loss: 95.0890 \tQuestion Loss: 95.0890 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 2\tNet Loss: 96.4138 \tQuestion Loss: 96.4138 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 2\tNet Loss: 91.9608 \tQuestion Loss: 91.9608 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 2\tNet Loss: 89.1182 \tQuestion Loss: 89.1182 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 2\tNet Loss: 88.3051 \tQuestion Loss: 88.3051 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 2\tNet Loss: 92.7797 \tQuestion Loss: 92.7797 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 2\tNet Loss: 93.1841 \tQuestion Loss: 93.1841 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 2\tNet Loss: 87.5564 \tQuestion Loss: 87.5564 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 2\tNet Loss: 89.5104 \tQuestion Loss: 89.5104 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 2\tNet Loss: 96.9986 \tQuestion Loss: 96.9986 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 2\tNet Loss: 94.1689 \tQuestion Loss: 94.1689 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 2\tNet Loss: 91.0170 \tQuestion Loss: 91.0170 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 2\tNet Loss: 88.1899 \tQuestion Loss: 88.1899 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 2\tNet Loss: 90.6945 \tQuestion Loss: 90.6945 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 2\tNet Loss: 93.5106 \tQuestion Loss: 93.5106 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 2\tNet Loss: 89.4797 \tQuestion Loss: 89.4797 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 2\tNet Loss: 91.5267 \tQuestion Loss: 91.5267 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 2\tNet Loss: 93.4530 \tQuestion Loss: 93.4530 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 2\tNet Loss: 97.4067 \tQuestion Loss: 97.4067 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 2\tNet Loss: 93.2741 \tQuestion Loss: 93.2741 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 2\tNet Loss: 94.8152 \tQuestion Loss: 94.8152 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 2\tNet Loss: 92.5854 \tQuestion Loss: 92.5854 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 2\tNet Loss: 96.1644 \tQuestion Loss: 96.1644 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 2\tNet Loss: 87.9062 \tQuestion Loss: 87.9062 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 2\tNet Loss: 92.9465 \tQuestion Loss: 92.9465 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 2\tNet Loss: 93.9917 \tQuestion Loss: 93.9917 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 2\tNet Loss: 100.2423 \tQuestion Loss: 100.2423 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 2\tNet Loss: 95.0090 \tQuestion Loss: 95.0090 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 2\tNet Loss: 91.4214 \tQuestion Loss: 91.4214 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 2\tNet Loss: 92.6744 \tQuestion Loss: 92.6744 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 2\tNet Loss: 93.3241 \tQuestion Loss: 93.3241 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 2\tNet Loss: 86.3219 \tQuestion Loss: 86.3219 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 2\tNet Loss: 97.5331 \tQuestion Loss: 97.5331 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 2\tNet Loss: 95.3962 \tQuestion Loss: 95.3962 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 2\tNet Loss: 93.4820 \tQuestion Loss: 93.4820 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 2\tNet Loss: 92.4047 \tQuestion Loss: 92.4047 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 2\tNet Loss: 89.4922 \tQuestion Loss: 89.4922 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 2\tNet Loss: 90.6404 \tQuestion Loss: 90.6404 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 2\tNet Loss: 94.7499 \tQuestion Loss: 94.7499 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 2\tNet Loss: 88.8776 \tQuestion Loss: 88.8776 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 2\tNet Loss: 87.2106 \tQuestion Loss: 87.2106 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 2\tNet Loss: 95.7814 \tQuestion Loss: 95.7814 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 2\tNet Loss: 99.2840 \tQuestion Loss: 99.2840 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 2\tNet Loss: 86.3427 \tQuestion Loss: 86.3427 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 2\tNet Loss: 96.6026 \tQuestion Loss: 96.6026 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 2\tNet Loss: 90.1151 \tQuestion Loss: 90.1151 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 2\tNet Loss: 91.3165 \tQuestion Loss: 91.3165 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 2\tNet Loss: 98.3025 \tQuestion Loss: 98.3025 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 2\tNet Loss: 94.3359 \tQuestion Loss: 94.3359 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 2\tNet Loss: 94.5805 \tQuestion Loss: 94.5805 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 2\tNet Loss: 85.7170 \tQuestion Loss: 85.7170 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 2\tNet Loss: 91.0638 \tQuestion Loss: 91.0638 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 2\tNet Loss: 94.5347 \tQuestion Loss: 94.5347 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 2\tNet Loss: 89.1376 \tQuestion Loss: 89.1376 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 2\tNet Loss: 94.1576 \tQuestion Loss: 94.1576 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 2\tNet Loss: 95.1959 \tQuestion Loss: 95.1959 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 2\tNet Loss: 96.3685 \tQuestion Loss: 96.3685 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 2\tNet Loss: 95.1046 \tQuestion Loss: 95.1046 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 2\tNet Loss: 96.5418 \tQuestion Loss: 96.5418 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 2\tNet Loss: 90.3706 \tQuestion Loss: 90.3706 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 2\tNet Loss: 89.1896 \tQuestion Loss: 89.1896 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 2\tNet Loss: 90.6005 \tQuestion Loss: 90.6005 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 2\tNet Loss: 88.2373 \tQuestion Loss: 88.2373 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 2\tNet Loss: 93.7165 \tQuestion Loss: 93.7165 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 2\tNet Loss: 92.2933 \tQuestion Loss: 92.2933 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 2\tNet Loss: 96.7374 \tQuestion Loss: 96.7374 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 2\tNet Loss: 94.0139 \tQuestion Loss: 94.0139 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 2\tNet Loss: 92.9401 \tQuestion Loss: 92.9401 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 2\tNet Loss: 89.9180 \tQuestion Loss: 89.9180 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 173 \t Epoch : 2\tNet Loss: 90.8101 \tQuestion Loss: 90.8101 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 2\tNet Loss: 95.6715 \tQuestion Loss: 95.6715 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 2\tNet Loss: 98.9076 \tQuestion Loss: 98.9076 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 2\tNet Loss: 90.8460 \tQuestion Loss: 90.8460 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 2\tNet Loss: 92.5631 \tQuestion Loss: 92.5631 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 2\tNet Loss: 91.0652 \tQuestion Loss: 91.0652 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 2\tNet Loss: 92.4652 \tQuestion Loss: 92.4652 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 2\tNet Loss: 90.6703 \tQuestion Loss: 90.6703 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 2\tNet Loss: 93.3036 \tQuestion Loss: 93.3036 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 2\tNet Loss: 94.6529 \tQuestion Loss: 94.6529 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 2\tNet Loss: 90.2578 \tQuestion Loss: 90.2578 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 2\tNet Loss: 94.5346 \tQuestion Loss: 94.5346 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 2\tNet Loss: 92.5871 \tQuestion Loss: 92.5871 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 2\tNet Loss: 95.4827 \tQuestion Loss: 95.4827 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 2\tNet Loss: 96.1744 \tQuestion Loss: 96.1744 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 2\tNet Loss: 91.0313 \tQuestion Loss: 91.0313 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 2\tNet Loss: 91.0095 \tQuestion Loss: 91.0095 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 2\tNet Loss: 91.8645 \tQuestion Loss: 91.8645 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 2\tNet Loss: 90.6264 \tQuestion Loss: 90.6264 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 2\tNet Loss: 95.7276 \tQuestion Loss: 95.7276 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 2\tNet Loss: 94.5570 \tQuestion Loss: 94.5570 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 2\tNet Loss: 96.1364 \tQuestion Loss: 96.1364 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 2\tNet Loss: 88.5152 \tQuestion Loss: 88.5152 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 2\tNet Loss: 90.5938 \tQuestion Loss: 90.5938 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 2\tNet Loss: 96.1475 \tQuestion Loss: 96.1475 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 2\tNet Loss: 88.4847 \tQuestion Loss: 88.4847 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 2\tNet Loss: 93.4966 \tQuestion Loss: 93.4966 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 2 : 91.8646 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 3\tNet Loss: 93.7236 \tQuestion Loss: 93.7236 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 3\tNet Loss: 89.8847 \tQuestion Loss: 89.8847 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 3\tNet Loss: 94.6775 \tQuestion Loss: 94.6775 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 3\tNet Loss: 94.0422 \tQuestion Loss: 94.0422 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 3\tNet Loss: 93.0730 \tQuestion Loss: 93.0730 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 3\tNet Loss: 92.9899 \tQuestion Loss: 92.9899 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 3\tNet Loss: 88.3725 \tQuestion Loss: 88.3725 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 3\tNet Loss: 94.3432 \tQuestion Loss: 94.3432 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 3\tNet Loss: 92.9529 \tQuestion Loss: 92.9529 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 3\tNet Loss: 94.6341 \tQuestion Loss: 94.6341 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 3\tNet Loss: 90.6203 \tQuestion Loss: 90.6203 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 3\tNet Loss: 95.2887 \tQuestion Loss: 95.2887 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 3\tNet Loss: 89.8974 \tQuestion Loss: 89.8974 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 3\tNet Loss: 92.2490 \tQuestion Loss: 92.2490 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 3\tNet Loss: 92.6982 \tQuestion Loss: 92.6982 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 3\tNet Loss: 94.9467 \tQuestion Loss: 94.9467 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 3\tNet Loss: 90.1049 \tQuestion Loss: 90.1049 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 3\tNet Loss: 91.9207 \tQuestion Loss: 91.9207 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 3\tNet Loss: 90.3467 \tQuestion Loss: 90.3467 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 3\tNet Loss: 86.6154 \tQuestion Loss: 86.6154 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 3\tNet Loss: 95.7958 \tQuestion Loss: 95.7958 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 3\tNet Loss: 87.8495 \tQuestion Loss: 87.8495 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 3\tNet Loss: 92.5102 \tQuestion Loss: 92.5102 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 3\tNet Loss: 95.7276 \tQuestion Loss: 95.7276 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 3\tNet Loss: 89.7845 \tQuestion Loss: 89.7845 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 3\tNet Loss: 93.1762 \tQuestion Loss: 93.1762 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 3\tNet Loss: 98.2905 \tQuestion Loss: 98.2905 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 3\tNet Loss: 91.7143 \tQuestion Loss: 91.7143 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 3\tNet Loss: 86.6658 \tQuestion Loss: 86.6658 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 3\tNet Loss: 89.5596 \tQuestion Loss: 89.5596 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 3\tNet Loss: 91.0917 \tQuestion Loss: 91.0917 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 3\tNet Loss: 89.8893 \tQuestion Loss: 89.8893 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 3\tNet Loss: 88.4264 \tQuestion Loss: 88.4264 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 3\tNet Loss: 93.0166 \tQuestion Loss: 93.0166 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 3\tNet Loss: 87.0738 \tQuestion Loss: 87.0738 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 3\tNet Loss: 89.6848 \tQuestion Loss: 89.6848 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 3\tNet Loss: 93.4603 \tQuestion Loss: 93.4603 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 3\tNet Loss: 95.1031 \tQuestion Loss: 95.1031 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 3\tNet Loss: 91.0830 \tQuestion Loss: 91.0830 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 3\tNet Loss: 93.8028 \tQuestion Loss: 93.8028 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 3\tNet Loss: 91.3523 \tQuestion Loss: 91.3523 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 3\tNet Loss: 90.0200 \tQuestion Loss: 90.0200 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 3\tNet Loss: 91.5523 \tQuestion Loss: 91.5523 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 3\tNet Loss: 92.7480 \tQuestion Loss: 92.7480 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 3\tNet Loss: 90.0647 \tQuestion Loss: 90.0647 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 3\tNet Loss: 92.9204 \tQuestion Loss: 92.9204 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 3\tNet Loss: 91.6362 \tQuestion Loss: 91.6362 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 3\tNet Loss: 96.8066 \tQuestion Loss: 96.8066 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 3\tNet Loss: 86.2426 \tQuestion Loss: 86.2426 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 3\tNet Loss: 93.9969 \tQuestion Loss: 93.9969 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 3\tNet Loss: 92.7676 \tQuestion Loss: 92.7676 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 3\tNet Loss: 88.0537 \tQuestion Loss: 88.0537 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 3\tNet Loss: 89.1778 \tQuestion Loss: 89.1778 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 3\tNet Loss: 89.8027 \tQuestion Loss: 89.8027 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 3\tNet Loss: 95.8883 \tQuestion Loss: 95.8883 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 3\tNet Loss: 92.8462 \tQuestion Loss: 92.8462 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 3\tNet Loss: 94.1263 \tQuestion Loss: 94.1263 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 3\tNet Loss: 95.9082 \tQuestion Loss: 95.9082 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 3\tNet Loss: 87.3127 \tQuestion Loss: 87.3127 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 3\tNet Loss: 91.6288 \tQuestion Loss: 91.6288 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 3\tNet Loss: 88.6422 \tQuestion Loss: 88.6422 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 3\tNet Loss: 94.7429 \tQuestion Loss: 94.7429 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 3\tNet Loss: 88.4454 \tQuestion Loss: 88.4454 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 3\tNet Loss: 92.9055 \tQuestion Loss: 92.9055 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 3\tNet Loss: 91.6302 \tQuestion Loss: 91.6302 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 3\tNet Loss: 90.0598 \tQuestion Loss: 90.0598 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 66 \t Epoch : 3\tNet Loss: 91.9683 \tQuestion Loss: 91.9683 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 3\tNet Loss: 91.2318 \tQuestion Loss: 91.2318 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 3\tNet Loss: 90.5237 \tQuestion Loss: 90.5237 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 3\tNet Loss: 94.9114 \tQuestion Loss: 94.9114 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 3\tNet Loss: 92.6645 \tQuestion Loss: 92.6645 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 3\tNet Loss: 91.8762 \tQuestion Loss: 91.8762 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 3\tNet Loss: 94.8517 \tQuestion Loss: 94.8517 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 3\tNet Loss: 93.2218 \tQuestion Loss: 93.2218 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 3\tNet Loss: 94.1496 \tQuestion Loss: 94.1496 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 3\tNet Loss: 93.1877 \tQuestion Loss: 93.1877 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 3\tNet Loss: 86.2303 \tQuestion Loss: 86.2303 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 3\tNet Loss: 91.0156 \tQuestion Loss: 91.0156 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 3\tNet Loss: 95.7819 \tQuestion Loss: 95.7819 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 3\tNet Loss: 92.1073 \tQuestion Loss: 92.1073 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 3\tNet Loss: 93.3825 \tQuestion Loss: 93.3825 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 3\tNet Loss: 91.3595 \tQuestion Loss: 91.3595 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 3\tNet Loss: 89.1387 \tQuestion Loss: 89.1387 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 3\tNet Loss: 87.9692 \tQuestion Loss: 87.9692 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 3\tNet Loss: 91.4195 \tQuestion Loss: 91.4195 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 3\tNet Loss: 89.6365 \tQuestion Loss: 89.6365 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 3\tNet Loss: 91.6664 \tQuestion Loss: 91.6664 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 3\tNet Loss: 90.9795 \tQuestion Loss: 90.9795 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 3\tNet Loss: 93.7347 \tQuestion Loss: 93.7347 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 3\tNet Loss: 89.1842 \tQuestion Loss: 89.1842 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 3\tNet Loss: 94.3475 \tQuestion Loss: 94.3475 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 3\tNet Loss: 92.3514 \tQuestion Loss: 92.3514 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 3\tNet Loss: 90.8776 \tQuestion Loss: 90.8776 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 3\tNet Loss: 91.4190 \tQuestion Loss: 91.4190 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 3\tNet Loss: 92.3578 \tQuestion Loss: 92.3578 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 3\tNet Loss: 93.0449 \tQuestion Loss: 93.0449 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 3\tNet Loss: 91.4539 \tQuestion Loss: 91.4539 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 3\tNet Loss: 95.1711 \tQuestion Loss: 95.1711 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 3\tNet Loss: 91.8729 \tQuestion Loss: 91.8729 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 3\tNet Loss: 92.3862 \tQuestion Loss: 92.3862 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 3\tNet Loss: 91.8533 \tQuestion Loss: 91.8533 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 3\tNet Loss: 94.6985 \tQuestion Loss: 94.6985 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 3\tNet Loss: 92.3766 \tQuestion Loss: 92.3766 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 3\tNet Loss: 92.1227 \tQuestion Loss: 92.1227 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 3\tNet Loss: 94.4395 \tQuestion Loss: 94.4395 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 3\tNet Loss: 96.0061 \tQuestion Loss: 96.0061 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 3\tNet Loss: 91.5621 \tQuestion Loss: 91.5621 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 3\tNet Loss: 88.8178 \tQuestion Loss: 88.8178 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 3\tNet Loss: 88.1642 \tQuestion Loss: 88.1642 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 3\tNet Loss: 92.3621 \tQuestion Loss: 92.3621 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 3\tNet Loss: 92.2188 \tQuestion Loss: 92.2188 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 3\tNet Loss: 87.1761 \tQuestion Loss: 87.1761 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 3\tNet Loss: 89.3162 \tQuestion Loss: 89.3162 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 3\tNet Loss: 96.1674 \tQuestion Loss: 96.1674 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 3\tNet Loss: 93.2277 \tQuestion Loss: 93.2277 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 3\tNet Loss: 90.1818 \tQuestion Loss: 90.1818 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 3\tNet Loss: 87.5768 \tQuestion Loss: 87.5768 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 3\tNet Loss: 89.9767 \tQuestion Loss: 89.9767 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 3\tNet Loss: 93.0530 \tQuestion Loss: 93.0530 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 3\tNet Loss: 89.2690 \tQuestion Loss: 89.2690 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 3\tNet Loss: 91.0552 \tQuestion Loss: 91.0552 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 3\tNet Loss: 92.9581 \tQuestion Loss: 92.9581 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 3\tNet Loss: 96.4602 \tQuestion Loss: 96.4602 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 3\tNet Loss: 92.1012 \tQuestion Loss: 92.1012 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 3\tNet Loss: 94.1422 \tQuestion Loss: 94.1422 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 3\tNet Loss: 92.1493 \tQuestion Loss: 92.1493 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 3\tNet Loss: 95.8874 \tQuestion Loss: 95.8874 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 3\tNet Loss: 87.8938 \tQuestion Loss: 87.8938 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 3\tNet Loss: 92.0480 \tQuestion Loss: 92.0480 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 3\tNet Loss: 93.1309 \tQuestion Loss: 93.1309 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 3\tNet Loss: 99.1741 \tQuestion Loss: 99.1741 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 3\tNet Loss: 94.1687 \tQuestion Loss: 94.1687 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 3\tNet Loss: 90.5829 \tQuestion Loss: 90.5829 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 3\tNet Loss: 92.3552 \tQuestion Loss: 92.3552 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 3\tNet Loss: 92.6186 \tQuestion Loss: 92.6186 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 3\tNet Loss: 85.4836 \tQuestion Loss: 85.4836 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 3\tNet Loss: 96.6100 \tQuestion Loss: 96.6100 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 3\tNet Loss: 94.2097 \tQuestion Loss: 94.2097 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 3\tNet Loss: 92.3094 \tQuestion Loss: 92.3094 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 3\tNet Loss: 91.2702 \tQuestion Loss: 91.2702 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 3\tNet Loss: 88.9442 \tQuestion Loss: 88.9442 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 3\tNet Loss: 88.3214 \tQuestion Loss: 88.3214 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 3\tNet Loss: 93.5004 \tQuestion Loss: 93.5004 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 3\tNet Loss: 89.9795 \tQuestion Loss: 89.9795 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 3\tNet Loss: 87.0106 \tQuestion Loss: 87.0106 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 3\tNet Loss: 94.8770 \tQuestion Loss: 94.8770 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 3\tNet Loss: 98.5215 \tQuestion Loss: 98.5215 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 3\tNet Loss: 87.5282 \tQuestion Loss: 87.5282 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 3\tNet Loss: 96.3416 \tQuestion Loss: 96.3416 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 3\tNet Loss: 89.6135 \tQuestion Loss: 89.6135 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 3\tNet Loss: 90.5938 \tQuestion Loss: 90.5938 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 3\tNet Loss: 97.4363 \tQuestion Loss: 97.4363 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 3\tNet Loss: 93.4121 \tQuestion Loss: 93.4121 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 3\tNet Loss: 93.7629 \tQuestion Loss: 93.7629 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 3\tNet Loss: 84.4639 \tQuestion Loss: 84.4639 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 3\tNet Loss: 89.9468 \tQuestion Loss: 89.9468 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 3\tNet Loss: 92.7825 \tQuestion Loss: 92.7825 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 3\tNet Loss: 87.9048 \tQuestion Loss: 87.9048 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 3\tNet Loss: 93.2849 \tQuestion Loss: 93.2849 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 159 \t Epoch : 3\tNet Loss: 94.1877 \tQuestion Loss: 94.1877 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 3\tNet Loss: 95.1523 \tQuestion Loss: 95.1523 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 3\tNet Loss: 93.5522 \tQuestion Loss: 93.5522 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 3\tNet Loss: 95.0770 \tQuestion Loss: 95.0770 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 3\tNet Loss: 88.7709 \tQuestion Loss: 88.7709 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 3\tNet Loss: 87.8754 \tQuestion Loss: 87.8754 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 3\tNet Loss: 89.4287 \tQuestion Loss: 89.4287 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 3\tNet Loss: 86.8897 \tQuestion Loss: 86.8897 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 3\tNet Loss: 92.5448 \tQuestion Loss: 92.5448 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 3\tNet Loss: 90.6010 \tQuestion Loss: 90.6010 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 3\tNet Loss: 94.8453 \tQuestion Loss: 94.8453 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 3\tNet Loss: 92.9644 \tQuestion Loss: 92.9644 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 3\tNet Loss: 91.7586 \tQuestion Loss: 91.7586 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 3\tNet Loss: 88.3700 \tQuestion Loss: 88.3700 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 3\tNet Loss: 90.3024 \tQuestion Loss: 90.3024 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 3\tNet Loss: 94.3066 \tQuestion Loss: 94.3066 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 3\tNet Loss: 96.7934 \tQuestion Loss: 96.7934 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 3\tNet Loss: 89.5986 \tQuestion Loss: 89.5986 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 3\tNet Loss: 90.9446 \tQuestion Loss: 90.9446 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 3\tNet Loss: 89.8990 \tQuestion Loss: 89.8990 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 3\tNet Loss: 91.1816 \tQuestion Loss: 91.1816 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 3\tNet Loss: 89.6045 \tQuestion Loss: 89.6045 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 3\tNet Loss: 92.0256 \tQuestion Loss: 92.0256 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 3\tNet Loss: 93.9885 \tQuestion Loss: 93.9885 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 3\tNet Loss: 89.3749 \tQuestion Loss: 89.3749 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 3\tNet Loss: 92.8376 \tQuestion Loss: 92.8376 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 3\tNet Loss: 91.1756 \tQuestion Loss: 91.1756 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 3\tNet Loss: 93.5887 \tQuestion Loss: 93.5887 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 3\tNet Loss: 94.3611 \tQuestion Loss: 94.3611 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 3\tNet Loss: 89.1163 \tQuestion Loss: 89.1163 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 3\tNet Loss: 89.3469 \tQuestion Loss: 89.3469 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 3\tNet Loss: 90.4631 \tQuestion Loss: 90.4631 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 3\tNet Loss: 89.5882 \tQuestion Loss: 89.5882 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 3\tNet Loss: 94.9350 \tQuestion Loss: 94.9350 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 3\tNet Loss: 93.1001 \tQuestion Loss: 93.1001 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 3\tNet Loss: 94.7884 \tQuestion Loss: 94.7884 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 3\tNet Loss: 87.7035 \tQuestion Loss: 87.7035 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 3\tNet Loss: 88.6814 \tQuestion Loss: 88.6814 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 3\tNet Loss: 94.1099 \tQuestion Loss: 94.1099 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 3\tNet Loss: 87.5718 \tQuestion Loss: 87.5718 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 3\tNet Loss: 92.2909 \tQuestion Loss: 92.2909 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 3 : 91.8247 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 4\tNet Loss: 92.2025 \tQuestion Loss: 92.2025 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 4\tNet Loss: 88.3503 \tQuestion Loss: 88.3503 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 4\tNet Loss: 93.0163 \tQuestion Loss: 93.0163 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 4\tNet Loss: 92.8598 \tQuestion Loss: 92.8598 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 4\tNet Loss: 92.9074 \tQuestion Loss: 92.9074 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 4\tNet Loss: 93.0644 \tQuestion Loss: 93.0644 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 4\tNet Loss: 88.4001 \tQuestion Loss: 88.4001 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 4\tNet Loss: 94.3624 \tQuestion Loss: 94.3624 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 4\tNet Loss: 92.7407 \tQuestion Loss: 92.7407 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 4\tNet Loss: 94.2237 \tQuestion Loss: 94.2237 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 4\tNet Loss: 90.4625 \tQuestion Loss: 90.4625 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 4\tNet Loss: 95.0592 \tQuestion Loss: 95.0592 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 4\tNet Loss: 89.7921 \tQuestion Loss: 89.7921 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 4\tNet Loss: 92.1858 \tQuestion Loss: 92.1858 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 4\tNet Loss: 92.5485 \tQuestion Loss: 92.5485 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 4\tNet Loss: 94.8938 \tQuestion Loss: 94.8938 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 4\tNet Loss: 90.0475 \tQuestion Loss: 90.0475 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 4\tNet Loss: 92.0283 \tQuestion Loss: 92.0283 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 4\tNet Loss: 90.3040 \tQuestion Loss: 90.3040 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 4\tNet Loss: 86.2733 \tQuestion Loss: 86.2733 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 4\tNet Loss: 95.4959 \tQuestion Loss: 95.4959 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 4\tNet Loss: 87.9315 \tQuestion Loss: 87.9315 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 4\tNet Loss: 92.4638 \tQuestion Loss: 92.4638 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 4\tNet Loss: 95.4197 \tQuestion Loss: 95.4197 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 4\tNet Loss: 89.1480 \tQuestion Loss: 89.1480 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 4\tNet Loss: 92.7372 \tQuestion Loss: 92.7372 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 4\tNet Loss: 97.8793 \tQuestion Loss: 97.8793 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 4\tNet Loss: 91.2885 \tQuestion Loss: 91.2885 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 4\tNet Loss: 86.5879 \tQuestion Loss: 86.5879 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 4\tNet Loss: 89.4727 \tQuestion Loss: 89.4727 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 4\tNet Loss: 91.3410 \tQuestion Loss: 91.3410 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 4\tNet Loss: 90.0766 \tQuestion Loss: 90.0766 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 4\tNet Loss: 88.5926 \tQuestion Loss: 88.5926 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 4\tNet Loss: 92.9179 \tQuestion Loss: 92.9179 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 4\tNet Loss: 87.0836 \tQuestion Loss: 87.0836 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 4\tNet Loss: 89.7936 \tQuestion Loss: 89.7936 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 4\tNet Loss: 93.6925 \tQuestion Loss: 93.6925 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 4\tNet Loss: 95.0085 \tQuestion Loss: 95.0085 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 4\tNet Loss: 90.7624 \tQuestion Loss: 90.7624 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 4\tNet Loss: 93.5808 \tQuestion Loss: 93.5808 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 4\tNet Loss: 91.1055 \tQuestion Loss: 91.1055 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 4\tNet Loss: 89.9168 \tQuestion Loss: 89.9168 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 4\tNet Loss: 91.7469 \tQuestion Loss: 91.7469 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 4\tNet Loss: 93.1097 \tQuestion Loss: 93.1097 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 4\tNet Loss: 90.4594 \tQuestion Loss: 90.4594 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 4\tNet Loss: 93.5072 \tQuestion Loss: 93.5072 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 4\tNet Loss: 91.9749 \tQuestion Loss: 91.9749 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 4\tNet Loss: 97.0676 \tQuestion Loss: 97.0676 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 4\tNet Loss: 86.2509 \tQuestion Loss: 86.2509 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 4\tNet Loss: 93.7467 \tQuestion Loss: 93.7467 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 4\tNet Loss: 92.7161 \tQuestion Loss: 92.7161 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 4\tNet Loss: 88.4141 \tQuestion Loss: 88.4141 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 52 \t Epoch : 4\tNet Loss: 89.4859 \tQuestion Loss: 89.4859 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 4\tNet Loss: 89.7675 \tQuestion Loss: 89.7675 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 4\tNet Loss: 96.0703 \tQuestion Loss: 96.0703 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 4\tNet Loss: 93.0282 \tQuestion Loss: 93.0282 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 4\tNet Loss: 94.3586 \tQuestion Loss: 94.3586 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 4\tNet Loss: 96.5919 \tQuestion Loss: 96.5919 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 4\tNet Loss: 87.4921 \tQuestion Loss: 87.4921 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 4\tNet Loss: 92.2042 \tQuestion Loss: 92.2042 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 4\tNet Loss: 89.1059 \tQuestion Loss: 89.1059 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 4\tNet Loss: 95.1478 \tQuestion Loss: 95.1478 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 4\tNet Loss: 88.6544 \tQuestion Loss: 88.6544 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 4\tNet Loss: 93.3900 \tQuestion Loss: 93.3900 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 4\tNet Loss: 91.9739 \tQuestion Loss: 91.9739 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 4\tNet Loss: 90.3371 \tQuestion Loss: 90.3371 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 4\tNet Loss: 92.1522 \tQuestion Loss: 92.1522 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 4\tNet Loss: 91.3672 \tQuestion Loss: 91.3672 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 4\tNet Loss: 90.8658 \tQuestion Loss: 90.8658 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 4\tNet Loss: 95.7968 \tQuestion Loss: 95.7968 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 4\tNet Loss: 93.2685 \tQuestion Loss: 93.2685 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 4\tNet Loss: 92.6164 \tQuestion Loss: 92.6164 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 4\tNet Loss: 95.5402 \tQuestion Loss: 95.5402 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 4\tNet Loss: 93.5365 \tQuestion Loss: 93.5365 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 4\tNet Loss: 95.0150 \tQuestion Loss: 95.0150 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 4\tNet Loss: 93.6674 \tQuestion Loss: 93.6674 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 4\tNet Loss: 86.8887 \tQuestion Loss: 86.8887 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 4\tNet Loss: 91.5770 \tQuestion Loss: 91.5770 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 4\tNet Loss: 96.4193 \tQuestion Loss: 96.4193 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 4\tNet Loss: 92.8293 \tQuestion Loss: 92.8293 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 4\tNet Loss: 94.0989 \tQuestion Loss: 94.0989 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 4\tNet Loss: 92.0201 \tQuestion Loss: 92.0201 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 4\tNet Loss: 89.6150 \tQuestion Loss: 89.6150 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 4\tNet Loss: 88.6555 \tQuestion Loss: 88.6555 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 4\tNet Loss: 91.9416 \tQuestion Loss: 91.9416 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 4\tNet Loss: 90.1455 \tQuestion Loss: 90.1455 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 4\tNet Loss: 92.2685 \tQuestion Loss: 92.2685 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 4\tNet Loss: 91.6674 \tQuestion Loss: 91.6674 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 4\tNet Loss: 94.6155 \tQuestion Loss: 94.6155 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 4\tNet Loss: 90.1946 \tQuestion Loss: 90.1946 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 4\tNet Loss: 95.2460 \tQuestion Loss: 95.2460 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 4\tNet Loss: 92.8575 \tQuestion Loss: 92.8575 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 4\tNet Loss: 91.2423 \tQuestion Loss: 91.2423 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 4\tNet Loss: 92.0666 \tQuestion Loss: 92.0666 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 4\tNet Loss: 92.6197 \tQuestion Loss: 92.6197 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 4\tNet Loss: 93.4512 \tQuestion Loss: 93.4512 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 4\tNet Loss: 91.9347 \tQuestion Loss: 91.9347 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 4\tNet Loss: 95.7988 \tQuestion Loss: 95.7988 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 4\tNet Loss: 92.8192 \tQuestion Loss: 92.8192 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 4\tNet Loss: 92.9563 \tQuestion Loss: 92.9563 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 4\tNet Loss: 92.5340 \tQuestion Loss: 92.5340 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 4\tNet Loss: 95.6281 \tQuestion Loss: 95.6281 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 4\tNet Loss: 93.3486 \tQuestion Loss: 93.3486 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 4\tNet Loss: 92.4222 \tQuestion Loss: 92.4222 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 4\tNet Loss: 95.0950 \tQuestion Loss: 95.0950 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 4\tNet Loss: 96.6758 \tQuestion Loss: 96.6758 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 4\tNet Loss: 92.3365 \tQuestion Loss: 92.3365 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 4\tNet Loss: 89.3513 \tQuestion Loss: 89.3513 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 4\tNet Loss: 88.4425 \tQuestion Loss: 88.4425 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 4\tNet Loss: 92.9616 \tQuestion Loss: 92.9616 \t Time Taken: 1 seconds\n",
      "Batch: 110 \t Epoch : 4\tNet Loss: 93.5326 \tQuestion Loss: 93.5326 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 4\tNet Loss: 87.9597 \tQuestion Loss: 87.9597 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 4\tNet Loss: 89.8542 \tQuestion Loss: 89.8542 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 4\tNet Loss: 97.2824 \tQuestion Loss: 97.2824 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 4\tNet Loss: 94.5892 \tQuestion Loss: 94.5892 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 4\tNet Loss: 91.5123 \tQuestion Loss: 91.5123 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 4\tNet Loss: 88.5962 \tQuestion Loss: 88.5962 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 4\tNet Loss: 90.7948 \tQuestion Loss: 90.7948 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 4\tNet Loss: 93.8224 \tQuestion Loss: 93.8224 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 4\tNet Loss: 89.6004 \tQuestion Loss: 89.6004 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 4\tNet Loss: 91.5443 \tQuestion Loss: 91.5443 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 4\tNet Loss: 93.7117 \tQuestion Loss: 93.7117 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 4\tNet Loss: 97.6141 \tQuestion Loss: 97.6141 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 4\tNet Loss: 93.2090 \tQuestion Loss: 93.2090 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 4\tNet Loss: 94.9754 \tQuestion Loss: 94.9754 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 4\tNet Loss: 92.7592 \tQuestion Loss: 92.7592 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 4\tNet Loss: 96.4455 \tQuestion Loss: 96.4455 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 4\tNet Loss: 87.9695 \tQuestion Loss: 87.9695 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 4\tNet Loss: 93.0475 \tQuestion Loss: 93.0475 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 4\tNet Loss: 94.4260 \tQuestion Loss: 94.4260 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 4\tNet Loss: 100.4050 \tQuestion Loss: 100.4050 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 4\tNet Loss: 95.0931 \tQuestion Loss: 95.0931 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 4\tNet Loss: 91.3969 \tQuestion Loss: 91.3969 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 4\tNet Loss: 92.7564 \tQuestion Loss: 92.7564 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 4\tNet Loss: 93.5682 \tQuestion Loss: 93.5682 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 4\tNet Loss: 86.4951 \tQuestion Loss: 86.4951 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 4\tNet Loss: 97.3542 \tQuestion Loss: 97.3542 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 4\tNet Loss: 95.0442 \tQuestion Loss: 95.0442 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 4\tNet Loss: 93.4689 \tQuestion Loss: 93.4689 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 4\tNet Loss: 92.2937 \tQuestion Loss: 92.2937 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 4\tNet Loss: 89.4282 \tQuestion Loss: 89.4282 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 4\tNet Loss: 90.0624 \tQuestion Loss: 90.0624 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 4\tNet Loss: 94.6256 \tQuestion Loss: 94.6256 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 4\tNet Loss: 88.8512 \tQuestion Loss: 88.8512 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 4\tNet Loss: 87.0049 \tQuestion Loss: 87.0049 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 145 \t Epoch : 4\tNet Loss: 95.6080 \tQuestion Loss: 95.6080 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 4\tNet Loss: 99.0331 \tQuestion Loss: 99.0331 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 4\tNet Loss: 86.7048 \tQuestion Loss: 86.7048 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 4\tNet Loss: 96.5671 \tQuestion Loss: 96.5671 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 4\tNet Loss: 89.9592 \tQuestion Loss: 89.9592 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 4\tNet Loss: 91.1883 \tQuestion Loss: 91.1883 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 4\tNet Loss: 98.0955 \tQuestion Loss: 98.0955 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 4\tNet Loss: 93.9208 \tQuestion Loss: 93.9208 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 4\tNet Loss: 94.4409 \tQuestion Loss: 94.4409 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 4\tNet Loss: 85.6377 \tQuestion Loss: 85.6377 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 4\tNet Loss: 91.0195 \tQuestion Loss: 91.0195 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 4\tNet Loss: 93.8417 \tQuestion Loss: 93.8417 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 4\tNet Loss: 88.5635 \tQuestion Loss: 88.5635 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 4\tNet Loss: 93.5893 \tQuestion Loss: 93.5893 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 4\tNet Loss: 94.8027 \tQuestion Loss: 94.8027 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 4\tNet Loss: 96.2075 \tQuestion Loss: 96.2075 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 4\tNet Loss: 94.6461 \tQuestion Loss: 94.6461 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 4\tNet Loss: 96.1767 \tQuestion Loss: 96.1767 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 4\tNet Loss: 89.8411 \tQuestion Loss: 89.8411 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 4\tNet Loss: 88.6954 \tQuestion Loss: 88.6954 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 4\tNet Loss: 90.0483 \tQuestion Loss: 90.0483 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 4\tNet Loss: 87.7559 \tQuestion Loss: 87.7559 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 4\tNet Loss: 93.3184 \tQuestion Loss: 93.3184 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 4\tNet Loss: 91.7895 \tQuestion Loss: 91.7895 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 4\tNet Loss: 96.1608 \tQuestion Loss: 96.1608 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 4\tNet Loss: 93.6674 \tQuestion Loss: 93.6674 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 4\tNet Loss: 92.4743 \tQuestion Loss: 92.4743 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 4\tNet Loss: 89.2629 \tQuestion Loss: 89.2629 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 4\tNet Loss: 90.6778 \tQuestion Loss: 90.6778 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 4\tNet Loss: 95.2630 \tQuestion Loss: 95.2630 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 4\tNet Loss: 98.3551 \tQuestion Loss: 98.3551 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 4\tNet Loss: 90.2874 \tQuestion Loss: 90.2874 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 4\tNet Loss: 91.6947 \tQuestion Loss: 91.6947 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 4\tNet Loss: 90.7149 \tQuestion Loss: 90.7149 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 4\tNet Loss: 92.0928 \tQuestion Loss: 92.0928 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 4\tNet Loss: 90.3508 \tQuestion Loss: 90.3508 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 4\tNet Loss: 92.6689 \tQuestion Loss: 92.6689 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 4\tNet Loss: 94.2545 \tQuestion Loss: 94.2545 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 4\tNet Loss: 89.4589 \tQuestion Loss: 89.4589 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 4\tNet Loss: 93.3649 \tQuestion Loss: 93.3649 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 4\tNet Loss: 92.0180 \tQuestion Loss: 92.0180 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 4\tNet Loss: 94.6093 \tQuestion Loss: 94.6093 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 4\tNet Loss: 95.6596 \tQuestion Loss: 95.6596 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 4\tNet Loss: 90.4869 \tQuestion Loss: 90.4869 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 4\tNet Loss: 90.2771 \tQuestion Loss: 90.2771 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 4\tNet Loss: 91.4602 \tQuestion Loss: 91.4602 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 4\tNet Loss: 90.0416 \tQuestion Loss: 90.0416 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 4\tNet Loss: 95.0124 \tQuestion Loss: 95.0124 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 4\tNet Loss: 93.4125 \tQuestion Loss: 93.4125 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 4\tNet Loss: 95.2930 \tQuestion Loss: 95.2930 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 4\tNet Loss: 87.9874 \tQuestion Loss: 87.9874 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 4\tNet Loss: 89.5496 \tQuestion Loss: 89.5496 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 4\tNet Loss: 95.0628 \tQuestion Loss: 95.0628 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 4\tNet Loss: 87.8663 \tQuestion Loss: 87.8663 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 4\tNet Loss: 92.7349 \tQuestion Loss: 92.7349 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 4 : 92.2797 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 5\tNet Loss: 93.0749 \tQuestion Loss: 93.0749 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 5\tNet Loss: 89.3960 \tQuestion Loss: 89.3960 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 5\tNet Loss: 94.2947 \tQuestion Loss: 94.2947 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 5\tNet Loss: 93.4953 \tQuestion Loss: 93.4953 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 5\tNet Loss: 93.3764 \tQuestion Loss: 93.3764 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 5\tNet Loss: 93.2110 \tQuestion Loss: 93.2110 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 5\tNet Loss: 88.4310 \tQuestion Loss: 88.4310 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 5\tNet Loss: 94.3900 \tQuestion Loss: 94.3900 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 5\tNet Loss: 93.0165 \tQuestion Loss: 93.0165 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 5\tNet Loss: 94.7503 \tQuestion Loss: 94.7503 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 5\tNet Loss: 90.7645 \tQuestion Loss: 90.7645 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 5\tNet Loss: 95.1472 \tQuestion Loss: 95.1472 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 5\tNet Loss: 90.0801 \tQuestion Loss: 90.0801 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 5\tNet Loss: 92.2704 \tQuestion Loss: 92.2704 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 5\tNet Loss: 92.6603 \tQuestion Loss: 92.6603 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 5\tNet Loss: 95.0556 \tQuestion Loss: 95.0556 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 5\tNet Loss: 90.1187 \tQuestion Loss: 90.1187 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 5\tNet Loss: 92.0291 \tQuestion Loss: 92.0291 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 5\tNet Loss: 90.6432 \tQuestion Loss: 90.6432 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 5\tNet Loss: 86.6214 \tQuestion Loss: 86.6214 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 5\tNet Loss: 95.5234 \tQuestion Loss: 95.5234 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 5\tNet Loss: 87.7009 \tQuestion Loss: 87.7009 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 5\tNet Loss: 92.4176 \tQuestion Loss: 92.4176 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 5\tNet Loss: 95.5627 \tQuestion Loss: 95.5627 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 5\tNet Loss: 89.6510 \tQuestion Loss: 89.6510 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 5\tNet Loss: 93.0563 \tQuestion Loss: 93.0563 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 5\tNet Loss: 98.0971 \tQuestion Loss: 98.0971 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 5\tNet Loss: 91.6344 \tQuestion Loss: 91.6344 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 5\tNet Loss: 86.6928 \tQuestion Loss: 86.6928 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 5\tNet Loss: 89.5608 \tQuestion Loss: 89.5608 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 5\tNet Loss: 91.2141 \tQuestion Loss: 91.2141 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 5\tNet Loss: 89.9719 \tQuestion Loss: 89.9719 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 5\tNet Loss: 88.4639 \tQuestion Loss: 88.4639 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 5\tNet Loss: 92.9945 \tQuestion Loss: 92.9945 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 5\tNet Loss: 86.9647 \tQuestion Loss: 86.9647 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 5\tNet Loss: 89.7554 \tQuestion Loss: 89.7554 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 5\tNet Loss: 93.4470 \tQuestion Loss: 93.4470 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 37 \t Epoch : 5\tNet Loss: 94.9902 \tQuestion Loss: 94.9902 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 5\tNet Loss: 90.9997 \tQuestion Loss: 90.9997 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 5\tNet Loss: 93.7334 \tQuestion Loss: 93.7334 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 5\tNet Loss: 91.2253 \tQuestion Loss: 91.2253 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 5\tNet Loss: 90.0246 \tQuestion Loss: 90.0246 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 5\tNet Loss: 91.6151 \tQuestion Loss: 91.6151 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 5\tNet Loss: 92.9253 \tQuestion Loss: 92.9253 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 5\tNet Loss: 90.2485 \tQuestion Loss: 90.2485 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 5\tNet Loss: 93.1966 \tQuestion Loss: 93.1966 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 5\tNet Loss: 91.6619 \tQuestion Loss: 91.6619 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 5\tNet Loss: 96.8599 \tQuestion Loss: 96.8599 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 5\tNet Loss: 86.3511 \tQuestion Loss: 86.3511 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 5\tNet Loss: 93.9934 \tQuestion Loss: 93.9934 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 5\tNet Loss: 92.6513 \tQuestion Loss: 92.6513 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 5\tNet Loss: 88.0560 \tQuestion Loss: 88.0560 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 5\tNet Loss: 89.2057 \tQuestion Loss: 89.2057 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 5\tNet Loss: 89.7237 \tQuestion Loss: 89.7237 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 5\tNet Loss: 95.8943 \tQuestion Loss: 95.8943 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 5\tNet Loss: 92.8195 \tQuestion Loss: 92.8195 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 5\tNet Loss: 94.0693 \tQuestion Loss: 94.0693 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 5\tNet Loss: 96.1540 \tQuestion Loss: 96.1540 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 5\tNet Loss: 87.3812 \tQuestion Loss: 87.3812 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 5\tNet Loss: 91.7522 \tQuestion Loss: 91.7522 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 5\tNet Loss: 88.6024 \tQuestion Loss: 88.6024 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 5\tNet Loss: 94.8659 \tQuestion Loss: 94.8659 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 5\tNet Loss: 88.4431 \tQuestion Loss: 88.4431 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 5\tNet Loss: 92.9722 \tQuestion Loss: 92.9722 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 5\tNet Loss: 91.7819 \tQuestion Loss: 91.7819 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 5\tNet Loss: 90.2641 \tQuestion Loss: 90.2641 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 5\tNet Loss: 92.0981 \tQuestion Loss: 92.0981 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 5\tNet Loss: 91.2766 \tQuestion Loss: 91.2766 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 5\tNet Loss: 90.5094 \tQuestion Loss: 90.5094 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 5\tNet Loss: 95.2817 \tQuestion Loss: 95.2817 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 5\tNet Loss: 92.8707 \tQuestion Loss: 92.8707 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 5\tNet Loss: 92.1725 \tQuestion Loss: 92.1725 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 5\tNet Loss: 94.9359 \tQuestion Loss: 94.9359 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 5\tNet Loss: 93.3419 \tQuestion Loss: 93.3419 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 5\tNet Loss: 94.4428 \tQuestion Loss: 94.4428 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 5\tNet Loss: 93.3468 \tQuestion Loss: 93.3468 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 5\tNet Loss: 86.3400 \tQuestion Loss: 86.3400 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 5\tNet Loss: 91.0829 \tQuestion Loss: 91.0829 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 5\tNet Loss: 96.0536 \tQuestion Loss: 96.0536 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 5\tNet Loss: 92.2851 \tQuestion Loss: 92.2851 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 5\tNet Loss: 93.7128 \tQuestion Loss: 93.7128 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 5\tNet Loss: 91.7253 \tQuestion Loss: 91.7253 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 5\tNet Loss: 89.1602 \tQuestion Loss: 89.1602 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 5\tNet Loss: 88.0656 \tQuestion Loss: 88.0656 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 5\tNet Loss: 91.5066 \tQuestion Loss: 91.5066 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 5\tNet Loss: 89.9174 \tQuestion Loss: 89.9174 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 5\tNet Loss: 91.7798 \tQuestion Loss: 91.7798 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 5\tNet Loss: 91.2100 \tQuestion Loss: 91.2100 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 5\tNet Loss: 93.9825 \tQuestion Loss: 93.9825 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 5\tNet Loss: 89.3967 \tQuestion Loss: 89.3967 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 5\tNet Loss: 94.7064 \tQuestion Loss: 94.7064 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 5\tNet Loss: 92.5657 \tQuestion Loss: 92.5657 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 5\tNet Loss: 91.0315 \tQuestion Loss: 91.0315 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 5\tNet Loss: 91.5116 \tQuestion Loss: 91.5116 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 5\tNet Loss: 92.4943 \tQuestion Loss: 92.4943 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 5\tNet Loss: 93.0447 \tQuestion Loss: 93.0447 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 5\tNet Loss: 91.6195 \tQuestion Loss: 91.6195 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 5\tNet Loss: 95.3072 \tQuestion Loss: 95.3072 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 5\tNet Loss: 92.0768 \tQuestion Loss: 92.0768 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 5\tNet Loss: 92.5938 \tQuestion Loss: 92.5938 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 5\tNet Loss: 92.0560 \tQuestion Loss: 92.0560 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 5\tNet Loss: 94.9601 \tQuestion Loss: 94.9601 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 5\tNet Loss: 92.5894 \tQuestion Loss: 92.5894 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 5\tNet Loss: 92.2744 \tQuestion Loss: 92.2744 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 5\tNet Loss: 94.7696 \tQuestion Loss: 94.7696 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 5\tNet Loss: 96.1088 \tQuestion Loss: 96.1088 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 5\tNet Loss: 91.7484 \tQuestion Loss: 91.7484 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 5\tNet Loss: 88.9517 \tQuestion Loss: 88.9517 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 5\tNet Loss: 88.3408 \tQuestion Loss: 88.3408 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 5\tNet Loss: 92.5963 \tQuestion Loss: 92.5963 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 5\tNet Loss: 92.6182 \tQuestion Loss: 92.6182 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 5\tNet Loss: 87.5302 \tQuestion Loss: 87.5302 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 5\tNet Loss: 89.5673 \tQuestion Loss: 89.5673 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 5\tNet Loss: 96.5861 \tQuestion Loss: 96.5861 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 5\tNet Loss: 93.5367 \tQuestion Loss: 93.5367 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 5\tNet Loss: 90.4143 \tQuestion Loss: 90.4143 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 5\tNet Loss: 87.7866 \tQuestion Loss: 87.7866 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 5\tNet Loss: 90.3427 \tQuestion Loss: 90.3427 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 5\tNet Loss: 93.4588 \tQuestion Loss: 93.4588 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 5\tNet Loss: 89.4647 \tQuestion Loss: 89.4647 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 5\tNet Loss: 91.2182 \tQuestion Loss: 91.2182 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 5\tNet Loss: 93.1879 \tQuestion Loss: 93.1879 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 5\tNet Loss: 96.6722 \tQuestion Loss: 96.6722 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 5\tNet Loss: 92.3542 \tQuestion Loss: 92.3542 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 5\tNet Loss: 94.3898 \tQuestion Loss: 94.3898 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 5\tNet Loss: 92.2914 \tQuestion Loss: 92.2914 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 5\tNet Loss: 96.1508 \tQuestion Loss: 96.1508 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 5\tNet Loss: 88.1026 \tQuestion Loss: 88.1026 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 5\tNet Loss: 92.3760 \tQuestion Loss: 92.3760 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 5\tNet Loss: 93.1391 \tQuestion Loss: 93.1391 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 130 \t Epoch : 5\tNet Loss: 99.4157 \tQuestion Loss: 99.4157 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 5\tNet Loss: 94.4266 \tQuestion Loss: 94.4266 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 5\tNet Loss: 91.0226 \tQuestion Loss: 91.0226 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 5\tNet Loss: 92.5104 \tQuestion Loss: 92.5104 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 5\tNet Loss: 92.7155 \tQuestion Loss: 92.7155 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 5\tNet Loss: 85.6152 \tQuestion Loss: 85.6152 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 5\tNet Loss: 97.0789 \tQuestion Loss: 97.0789 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 5\tNet Loss: 94.7179 \tQuestion Loss: 94.7179 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 5\tNet Loss: 92.7722 \tQuestion Loss: 92.7722 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 5\tNet Loss: 91.7677 \tQuestion Loss: 91.7677 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 5\tNet Loss: 89.2221 \tQuestion Loss: 89.2221 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 5\tNet Loss: 89.2569 \tQuestion Loss: 89.2569 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 5\tNet Loss: 93.9729 \tQuestion Loss: 93.9729 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 5\tNet Loss: 89.8215 \tQuestion Loss: 89.8215 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 5\tNet Loss: 86.9836 \tQuestion Loss: 86.9836 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 5\tNet Loss: 95.1343 \tQuestion Loss: 95.1343 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 5\tNet Loss: 98.5504 \tQuestion Loss: 98.5504 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 5\tNet Loss: 87.2470 \tQuestion Loss: 87.2470 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 5\tNet Loss: 96.5629 \tQuestion Loss: 96.5629 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 5\tNet Loss: 89.8837 \tQuestion Loss: 89.8837 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 5\tNet Loss: 90.8964 \tQuestion Loss: 90.8964 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 5\tNet Loss: 97.6471 \tQuestion Loss: 97.6471 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 5\tNet Loss: 93.6258 \tQuestion Loss: 93.6258 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 5\tNet Loss: 93.9221 \tQuestion Loss: 93.9221 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 5\tNet Loss: 84.8065 \tQuestion Loss: 84.8065 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 5\tNet Loss: 90.0606 \tQuestion Loss: 90.0606 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 5\tNet Loss: 93.0032 \tQuestion Loss: 93.0032 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 5\tNet Loss: 88.2717 \tQuestion Loss: 88.2717 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 5\tNet Loss: 93.6995 \tQuestion Loss: 93.6995 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 5\tNet Loss: 94.4860 \tQuestion Loss: 94.4860 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 5\tNet Loss: 95.4332 \tQuestion Loss: 95.4332 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 5\tNet Loss: 93.8028 \tQuestion Loss: 93.8028 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 5\tNet Loss: 95.4378 \tQuestion Loss: 95.4378 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 5\tNet Loss: 89.1449 \tQuestion Loss: 89.1449 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 5\tNet Loss: 88.3007 \tQuestion Loss: 88.3007 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 5\tNet Loss: 89.6225 \tQuestion Loss: 89.6225 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 5\tNet Loss: 87.2237 \tQuestion Loss: 87.2237 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 5\tNet Loss: 92.4734 \tQuestion Loss: 92.4734 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 5\tNet Loss: 90.9700 \tQuestion Loss: 90.9700 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 5\tNet Loss: 95.3360 \tQuestion Loss: 95.3360 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 5\tNet Loss: 93.3911 \tQuestion Loss: 93.3911 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 5\tNet Loss: 92.3289 \tQuestion Loss: 92.3289 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 5\tNet Loss: 88.9007 \tQuestion Loss: 88.9007 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 5\tNet Loss: 90.6408 \tQuestion Loss: 90.6408 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 5\tNet Loss: 94.5422 \tQuestion Loss: 94.5422 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 5\tNet Loss: 97.1852 \tQuestion Loss: 97.1852 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 5\tNet Loss: 89.8784 \tQuestion Loss: 89.8784 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 5\tNet Loss: 91.5035 \tQuestion Loss: 91.5035 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 5\tNet Loss: 90.4239 \tQuestion Loss: 90.4239 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 5\tNet Loss: 91.5350 \tQuestion Loss: 91.5350 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 5\tNet Loss: 89.8783 \tQuestion Loss: 89.8783 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 5\tNet Loss: 92.3461 \tQuestion Loss: 92.3461 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 5\tNet Loss: 94.2223 \tQuestion Loss: 94.2223 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 5\tNet Loss: 89.5785 \tQuestion Loss: 89.5785 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 5\tNet Loss: 93.4036 \tQuestion Loss: 93.4036 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 5\tNet Loss: 91.6144 \tQuestion Loss: 91.6144 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 5\tNet Loss: 93.9885 \tQuestion Loss: 93.9885 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 5\tNet Loss: 94.7685 \tQuestion Loss: 94.7685 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 5\tNet Loss: 89.4296 \tQuestion Loss: 89.4296 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 5\tNet Loss: 89.5251 \tQuestion Loss: 89.5251 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 5\tNet Loss: 90.7143 \tQuestion Loss: 90.7143 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 5\tNet Loss: 89.8566 \tQuestion Loss: 89.8566 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 5\tNet Loss: 95.0559 \tQuestion Loss: 95.0559 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 5\tNet Loss: 93.5270 \tQuestion Loss: 93.5270 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 5\tNet Loss: 95.2258 \tQuestion Loss: 95.2258 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 5\tNet Loss: 88.0255 \tQuestion Loss: 88.0255 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 5\tNet Loss: 89.2632 \tQuestion Loss: 89.2632 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 5\tNet Loss: 94.7355 \tQuestion Loss: 94.7355 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 5\tNet Loss: 87.8750 \tQuestion Loss: 87.8750 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 5\tNet Loss: 92.4262 \tQuestion Loss: 92.4262 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 5 : 91.9983 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 6\tNet Loss: 92.4557 \tQuestion Loss: 92.4557 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 6\tNet Loss: 88.6565 \tQuestion Loss: 88.6565 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 6\tNet Loss: 93.7655 \tQuestion Loss: 93.7655 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 6\tNet Loss: 93.2459 \tQuestion Loss: 93.2459 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 6\tNet Loss: 93.5384 \tQuestion Loss: 93.5384 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 6\tNet Loss: 93.5432 \tQuestion Loss: 93.5432 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 6\tNet Loss: 88.8211 \tQuestion Loss: 88.8211 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 6\tNet Loss: 95.0257 \tQuestion Loss: 95.0257 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 6\tNet Loss: 93.4204 \tQuestion Loss: 93.4204 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 6\tNet Loss: 94.8584 \tQuestion Loss: 94.8584 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 6\tNet Loss: 90.9974 \tQuestion Loss: 90.9974 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 6\tNet Loss: 95.6448 \tQuestion Loss: 95.6448 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 6\tNet Loss: 90.2489 \tQuestion Loss: 90.2489 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 6\tNet Loss: 92.6557 \tQuestion Loss: 92.6557 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 6\tNet Loss: 93.0071 \tQuestion Loss: 93.0071 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 6\tNet Loss: 95.3119 \tQuestion Loss: 95.3119 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 6\tNet Loss: 90.4885 \tQuestion Loss: 90.4885 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 6\tNet Loss: 92.4951 \tQuestion Loss: 92.4951 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 6\tNet Loss: 90.6593 \tQuestion Loss: 90.6593 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 6\tNet Loss: 86.6864 \tQuestion Loss: 86.6864 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 6\tNet Loss: 96.0894 \tQuestion Loss: 96.0894 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 6\tNet Loss: 88.2596 \tQuestion Loss: 88.2596 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 22 \t Epoch : 6\tNet Loss: 92.8743 \tQuestion Loss: 92.8743 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 6\tNet Loss: 95.8225 \tQuestion Loss: 95.8225 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 6\tNet Loss: 89.6776 \tQuestion Loss: 89.6776 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 6\tNet Loss: 93.1153 \tQuestion Loss: 93.1153 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 6\tNet Loss: 98.3711 \tQuestion Loss: 98.3711 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 6\tNet Loss: 91.7613 \tQuestion Loss: 91.7613 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 6\tNet Loss: 86.9827 \tQuestion Loss: 86.9827 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 6\tNet Loss: 89.8230 \tQuestion Loss: 89.8230 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 6\tNet Loss: 91.5417 \tQuestion Loss: 91.5417 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 6\tNet Loss: 90.3652 \tQuestion Loss: 90.3652 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 6\tNet Loss: 89.0068 \tQuestion Loss: 89.0068 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 6\tNet Loss: 93.2890 \tQuestion Loss: 93.2890 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 6\tNet Loss: 87.3803 \tQuestion Loss: 87.3803 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 6\tNet Loss: 90.1693 \tQuestion Loss: 90.1693 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 6\tNet Loss: 94.0376 \tQuestion Loss: 94.0376 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 6\tNet Loss: 95.2358 \tQuestion Loss: 95.2358 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 6\tNet Loss: 91.0650 \tQuestion Loss: 91.0650 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 6\tNet Loss: 93.8452 \tQuestion Loss: 93.8452 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 6\tNet Loss: 91.4420 \tQuestion Loss: 91.4420 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 6\tNet Loss: 90.2672 \tQuestion Loss: 90.2672 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 6\tNet Loss: 92.1463 \tQuestion Loss: 92.1463 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 6\tNet Loss: 93.4505 \tQuestion Loss: 93.4505 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 6\tNet Loss: 90.7004 \tQuestion Loss: 90.7004 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 6\tNet Loss: 93.6755 \tQuestion Loss: 93.6755 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 6\tNet Loss: 92.1973 \tQuestion Loss: 92.1973 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 6\tNet Loss: 97.4375 \tQuestion Loss: 97.4375 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 6\tNet Loss: 86.5451 \tQuestion Loss: 86.5451 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 6\tNet Loss: 93.9356 \tQuestion Loss: 93.9356 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 6\tNet Loss: 92.9389 \tQuestion Loss: 92.9389 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 6\tNet Loss: 88.6285 \tQuestion Loss: 88.6285 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 6\tNet Loss: 89.7240 \tQuestion Loss: 89.7240 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 6\tNet Loss: 89.9200 \tQuestion Loss: 89.9200 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 6\tNet Loss: 96.3367 \tQuestion Loss: 96.3367 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 6\tNet Loss: 93.2632 \tQuestion Loss: 93.2632 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 6\tNet Loss: 94.5853 \tQuestion Loss: 94.5853 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 6\tNet Loss: 96.6522 \tQuestion Loss: 96.6522 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 6\tNet Loss: 87.6466 \tQuestion Loss: 87.6466 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 6\tNet Loss: 92.2383 \tQuestion Loss: 92.2383 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 6\tNet Loss: 89.2209 \tQuestion Loss: 89.2209 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 6\tNet Loss: 95.2901 \tQuestion Loss: 95.2901 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 6\tNet Loss: 88.8496 \tQuestion Loss: 88.8496 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 6\tNet Loss: 93.5079 \tQuestion Loss: 93.5079 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 6\tNet Loss: 92.2182 \tQuestion Loss: 92.2182 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 6\tNet Loss: 90.6531 \tQuestion Loss: 90.6531 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 6\tNet Loss: 92.3496 \tQuestion Loss: 92.3496 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 6\tNet Loss: 91.6173 \tQuestion Loss: 91.6173 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 6\tNet Loss: 91.0287 \tQuestion Loss: 91.0287 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 6\tNet Loss: 95.8353 \tQuestion Loss: 95.8353 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 6\tNet Loss: 93.2223 \tQuestion Loss: 93.2223 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 6\tNet Loss: 92.6971 \tQuestion Loss: 92.6971 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 6\tNet Loss: 95.7105 \tQuestion Loss: 95.7105 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 6\tNet Loss: 93.6884 \tQuestion Loss: 93.6884 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 6\tNet Loss: 95.0583 \tQuestion Loss: 95.0583 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 6\tNet Loss: 93.6198 \tQuestion Loss: 93.6198 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 6\tNet Loss: 86.9750 \tQuestion Loss: 86.9750 \t Time Taken: 1 seconds\n",
      "Batch: 77 \t Epoch : 6\tNet Loss: 91.6069 \tQuestion Loss: 91.6069 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 6\tNet Loss: 96.8293 \tQuestion Loss: 96.8293 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 6\tNet Loss: 93.0723 \tQuestion Loss: 93.0723 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 6\tNet Loss: 94.2485 \tQuestion Loss: 94.2485 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 6\tNet Loss: 92.0924 \tQuestion Loss: 92.0924 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 6\tNet Loss: 89.6776 \tQuestion Loss: 89.6776 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 6\tNet Loss: 88.9159 \tQuestion Loss: 88.9159 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 6\tNet Loss: 92.0882 \tQuestion Loss: 92.0882 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 6\tNet Loss: 90.2544 \tQuestion Loss: 90.2544 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 6\tNet Loss: 92.3949 \tQuestion Loss: 92.3949 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 6\tNet Loss: 91.7741 \tQuestion Loss: 91.7741 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 6\tNet Loss: 94.6027 \tQuestion Loss: 94.6027 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 6\tNet Loss: 90.2208 \tQuestion Loss: 90.2208 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 6\tNet Loss: 95.2808 \tQuestion Loss: 95.2808 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 6\tNet Loss: 92.8134 \tQuestion Loss: 92.8134 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 6\tNet Loss: 91.4621 \tQuestion Loss: 91.4621 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 6\tNet Loss: 92.1530 \tQuestion Loss: 92.1530 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 6\tNet Loss: 92.7277 \tQuestion Loss: 92.7277 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 6\tNet Loss: 93.6673 \tQuestion Loss: 93.6673 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 6\tNet Loss: 91.9620 \tQuestion Loss: 91.9620 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 6\tNet Loss: 95.8526 \tQuestion Loss: 95.8526 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 6\tNet Loss: 92.9729 \tQuestion Loss: 92.9729 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 6\tNet Loss: 92.9076 \tQuestion Loss: 92.9076 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 6\tNet Loss: 92.4738 \tQuestion Loss: 92.4738 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 6\tNet Loss: 95.6264 \tQuestion Loss: 95.6264 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 6\tNet Loss: 93.2866 \tQuestion Loss: 93.2866 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 6\tNet Loss: 92.3721 \tQuestion Loss: 92.3721 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 6\tNet Loss: 95.0846 \tQuestion Loss: 95.0846 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 6\tNet Loss: 96.7460 \tQuestion Loss: 96.7460 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 6\tNet Loss: 92.3658 \tQuestion Loss: 92.3658 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 6\tNet Loss: 89.2585 \tQuestion Loss: 89.2585 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 6\tNet Loss: 88.3773 \tQuestion Loss: 88.3773 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 6\tNet Loss: 92.9401 \tQuestion Loss: 92.9401 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 6\tNet Loss: 93.5577 \tQuestion Loss: 93.5577 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 6\tNet Loss: 87.7818 \tQuestion Loss: 87.7818 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 6\tNet Loss: 89.7549 \tQuestion Loss: 89.7549 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 6\tNet Loss: 97.1564 \tQuestion Loss: 97.1564 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 6\tNet Loss: 94.5423 \tQuestion Loss: 94.5423 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 115 \t Epoch : 6\tNet Loss: 91.5789 \tQuestion Loss: 91.5789 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 6\tNet Loss: 88.5726 \tQuestion Loss: 88.5726 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 6\tNet Loss: 90.6240 \tQuestion Loss: 90.6240 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 6\tNet Loss: 93.7491 \tQuestion Loss: 93.7491 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 6\tNet Loss: 89.5817 \tQuestion Loss: 89.5817 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 6\tNet Loss: 91.5797 \tQuestion Loss: 91.5797 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 6\tNet Loss: 93.6998 \tQuestion Loss: 93.6998 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 6\tNet Loss: 97.6055 \tQuestion Loss: 97.6055 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 6\tNet Loss: 92.9725 \tQuestion Loss: 92.9725 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 6\tNet Loss: 94.8158 \tQuestion Loss: 94.8158 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 6\tNet Loss: 92.7876 \tQuestion Loss: 92.7876 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 6\tNet Loss: 96.2239 \tQuestion Loss: 96.2239 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 6\tNet Loss: 87.8479 \tQuestion Loss: 87.8479 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 6\tNet Loss: 92.9803 \tQuestion Loss: 92.9803 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 6\tNet Loss: 94.4820 \tQuestion Loss: 94.4820 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 6\tNet Loss: 100.2993 \tQuestion Loss: 100.2993 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 6\tNet Loss: 94.8343 \tQuestion Loss: 94.8343 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 6\tNet Loss: 91.3399 \tQuestion Loss: 91.3399 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 6\tNet Loss: 92.7832 \tQuestion Loss: 92.7832 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 6\tNet Loss: 93.5774 \tQuestion Loss: 93.5774 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 6\tNet Loss: 86.3018 \tQuestion Loss: 86.3018 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 6\tNet Loss: 97.1247 \tQuestion Loss: 97.1247 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 6\tNet Loss: 95.0327 \tQuestion Loss: 95.0327 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 6\tNet Loss: 93.4628 \tQuestion Loss: 93.4628 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 6\tNet Loss: 91.9966 \tQuestion Loss: 91.9966 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 6\tNet Loss: 89.3360 \tQuestion Loss: 89.3360 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 6\tNet Loss: 89.8563 \tQuestion Loss: 89.8563 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 6\tNet Loss: 94.4802 \tQuestion Loss: 94.4802 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 6\tNet Loss: 88.8775 \tQuestion Loss: 88.8775 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 6\tNet Loss: 86.9113 \tQuestion Loss: 86.9113 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 6\tNet Loss: 95.5735 \tQuestion Loss: 95.5735 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 6\tNet Loss: 98.8733 \tQuestion Loss: 98.8733 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 6\tNet Loss: 86.7279 \tQuestion Loss: 86.7279 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 6\tNet Loss: 96.5022 \tQuestion Loss: 96.5022 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 6\tNet Loss: 89.9108 \tQuestion Loss: 89.9108 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 6\tNet Loss: 91.1701 \tQuestion Loss: 91.1701 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 6\tNet Loss: 97.8197 \tQuestion Loss: 97.8197 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 6\tNet Loss: 93.6173 \tQuestion Loss: 93.6173 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 6\tNet Loss: 94.2961 \tQuestion Loss: 94.2961 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 6\tNet Loss: 85.7293 \tQuestion Loss: 85.7293 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 6\tNet Loss: 90.7402 \tQuestion Loss: 90.7402 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 6\tNet Loss: 93.4771 \tQuestion Loss: 93.4771 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 6\tNet Loss: 88.3513 \tQuestion Loss: 88.3513 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 6\tNet Loss: 93.4999 \tQuestion Loss: 93.4999 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 6\tNet Loss: 94.6928 \tQuestion Loss: 94.6928 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 6\tNet Loss: 95.9916 \tQuestion Loss: 95.9916 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 6\tNet Loss: 94.3414 \tQuestion Loss: 94.3414 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 6\tNet Loss: 96.0097 \tQuestion Loss: 96.0097 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 6\tNet Loss: 89.5665 \tQuestion Loss: 89.5665 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 6\tNet Loss: 88.5616 \tQuestion Loss: 88.5616 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 6\tNet Loss: 89.7689 \tQuestion Loss: 89.7689 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 6\tNet Loss: 87.4404 \tQuestion Loss: 87.4404 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 6\tNet Loss: 93.0991 \tQuestion Loss: 93.0991 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 6\tNet Loss: 91.6618 \tQuestion Loss: 91.6618 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 6\tNet Loss: 95.8678 \tQuestion Loss: 95.8678 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 6\tNet Loss: 93.6081 \tQuestion Loss: 93.6081 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 6\tNet Loss: 92.2620 \tQuestion Loss: 92.2620 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 6\tNet Loss: 89.0776 \tQuestion Loss: 89.0776 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 6\tNet Loss: 90.5608 \tQuestion Loss: 90.5608 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 6\tNet Loss: 94.9603 \tQuestion Loss: 94.9603 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 6\tNet Loss: 97.8309 \tQuestion Loss: 97.8309 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 6\tNet Loss: 90.0998 \tQuestion Loss: 90.0998 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 6\tNet Loss: 91.4005 \tQuestion Loss: 91.4005 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 6\tNet Loss: 90.5259 \tQuestion Loss: 90.5259 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 6\tNet Loss: 91.9236 \tQuestion Loss: 91.9236 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 6\tNet Loss: 90.0257 \tQuestion Loss: 90.0257 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 6\tNet Loss: 92.4482 \tQuestion Loss: 92.4482 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 6\tNet Loss: 94.1472 \tQuestion Loss: 94.1472 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 6\tNet Loss: 89.2418 \tQuestion Loss: 89.2418 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 6\tNet Loss: 93.1354 \tQuestion Loss: 93.1354 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 6\tNet Loss: 91.6635 \tQuestion Loss: 91.6635 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 6\tNet Loss: 94.3796 \tQuestion Loss: 94.3796 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 6\tNet Loss: 95.3048 \tQuestion Loss: 95.3048 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 6\tNet Loss: 90.0981 \tQuestion Loss: 90.0981 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 6\tNet Loss: 90.0965 \tQuestion Loss: 90.0965 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 6\tNet Loss: 91.2070 \tQuestion Loss: 91.2070 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 6\tNet Loss: 89.9877 \tQuestion Loss: 89.9877 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 6\tNet Loss: 94.7653 \tQuestion Loss: 94.7653 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 6\tNet Loss: 93.2216 \tQuestion Loss: 93.2216 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 6\tNet Loss: 94.9601 \tQuestion Loss: 94.9601 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 6\tNet Loss: 87.9091 \tQuestion Loss: 87.9091 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 6\tNet Loss: 89.1309 \tQuestion Loss: 89.1309 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 6\tNet Loss: 94.6607 \tQuestion Loss: 94.6607 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 6\tNet Loss: 87.7323 \tQuestion Loss: 87.7323 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 6\tNet Loss: 92.4239 \tQuestion Loss: 92.4239 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 6 : 92.3390 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 7\tNet Loss: 92.7113 \tQuestion Loss: 92.7113 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 7\tNet Loss: 89.0699 \tQuestion Loss: 89.0699 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 7\tNet Loss: 94.1210 \tQuestion Loss: 94.1210 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 7\tNet Loss: 93.5809 \tQuestion Loss: 93.5809 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 7\tNet Loss: 93.4125 \tQuestion Loss: 93.4125 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 7\tNet Loss: 93.2851 \tQuestion Loss: 93.2851 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 7\tNet Loss: 88.5727 \tQuestion Loss: 88.5727 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 7 \t Epoch : 7\tNet Loss: 94.2893 \tQuestion Loss: 94.2893 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 7\tNet Loss: 93.0135 \tQuestion Loss: 93.0135 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 7\tNet Loss: 94.8071 \tQuestion Loss: 94.8071 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 7\tNet Loss: 90.7882 \tQuestion Loss: 90.7882 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 7\tNet Loss: 95.3052 \tQuestion Loss: 95.3052 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 7\tNet Loss: 90.0167 \tQuestion Loss: 90.0167 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 7\tNet Loss: 92.3261 \tQuestion Loss: 92.3261 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 7\tNet Loss: 92.6739 \tQuestion Loss: 92.6739 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 7\tNet Loss: 95.1078 \tQuestion Loss: 95.1078 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 7\tNet Loss: 90.1827 \tQuestion Loss: 90.1827 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 7\tNet Loss: 92.1234 \tQuestion Loss: 92.1234 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 7\tNet Loss: 90.5744 \tQuestion Loss: 90.5744 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 7\tNet Loss: 86.6249 \tQuestion Loss: 86.6249 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 7\tNet Loss: 95.6799 \tQuestion Loss: 95.6799 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 7\tNet Loss: 87.8018 \tQuestion Loss: 87.8018 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 7\tNet Loss: 92.4155 \tQuestion Loss: 92.4155 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 7\tNet Loss: 95.4845 \tQuestion Loss: 95.4845 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 7\tNet Loss: 89.5753 \tQuestion Loss: 89.5753 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 7\tNet Loss: 93.0233 \tQuestion Loss: 93.0233 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 7\tNet Loss: 98.1932 \tQuestion Loss: 98.1932 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 7\tNet Loss: 91.6374 \tQuestion Loss: 91.6374 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 7\tNet Loss: 86.7903 \tQuestion Loss: 86.7903 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 7\tNet Loss: 89.6579 \tQuestion Loss: 89.6579 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 7\tNet Loss: 91.3071 \tQuestion Loss: 91.3071 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 7\tNet Loss: 90.0567 \tQuestion Loss: 90.0567 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 7\tNet Loss: 88.5047 \tQuestion Loss: 88.5047 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 7\tNet Loss: 93.1151 \tQuestion Loss: 93.1151 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 7\tNet Loss: 87.1288 \tQuestion Loss: 87.1288 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 7\tNet Loss: 89.8314 \tQuestion Loss: 89.8314 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 7\tNet Loss: 93.4553 \tQuestion Loss: 93.4553 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 7\tNet Loss: 94.9875 \tQuestion Loss: 94.9875 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 7\tNet Loss: 90.9635 \tQuestion Loss: 90.9635 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 7\tNet Loss: 93.8064 \tQuestion Loss: 93.8064 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 7\tNet Loss: 91.2674 \tQuestion Loss: 91.2674 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 7\tNet Loss: 90.0343 \tQuestion Loss: 90.0343 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 7\tNet Loss: 91.7073 \tQuestion Loss: 91.7073 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 7\tNet Loss: 93.0581 \tQuestion Loss: 93.0581 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 7\tNet Loss: 90.3752 \tQuestion Loss: 90.3752 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 7\tNet Loss: 93.4102 \tQuestion Loss: 93.4102 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 7\tNet Loss: 91.9244 \tQuestion Loss: 91.9244 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 7\tNet Loss: 96.9977 \tQuestion Loss: 96.9977 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 7\tNet Loss: 86.3017 \tQuestion Loss: 86.3017 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 7\tNet Loss: 94.0130 \tQuestion Loss: 94.0130 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 7\tNet Loss: 92.7734 \tQuestion Loss: 92.7734 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 7\tNet Loss: 88.2306 \tQuestion Loss: 88.2306 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 7\tNet Loss: 89.1920 \tQuestion Loss: 89.1920 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 7\tNet Loss: 89.6781 \tQuestion Loss: 89.6781 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 7\tNet Loss: 96.0866 \tQuestion Loss: 96.0866 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 7\tNet Loss: 92.9066 \tQuestion Loss: 92.9066 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 7\tNet Loss: 94.2029 \tQuestion Loss: 94.2029 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 7\tNet Loss: 96.3269 \tQuestion Loss: 96.3269 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 7\tNet Loss: 87.4484 \tQuestion Loss: 87.4484 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 7\tNet Loss: 91.8946 \tQuestion Loss: 91.8946 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 7\tNet Loss: 88.7306 \tQuestion Loss: 88.7306 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 7\tNet Loss: 94.8591 \tQuestion Loss: 94.8591 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 7\tNet Loss: 88.5239 \tQuestion Loss: 88.5239 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 7\tNet Loss: 93.2497 \tQuestion Loss: 93.2497 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 7\tNet Loss: 91.9081 \tQuestion Loss: 91.9081 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 7\tNet Loss: 90.2353 \tQuestion Loss: 90.2353 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 7\tNet Loss: 92.1438 \tQuestion Loss: 92.1438 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 7\tNet Loss: 91.2695 \tQuestion Loss: 91.2695 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 7\tNet Loss: 90.5991 \tQuestion Loss: 90.5991 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 7\tNet Loss: 95.4135 \tQuestion Loss: 95.4135 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 7\tNet Loss: 93.0750 \tQuestion Loss: 93.0750 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 7\tNet Loss: 92.4264 \tQuestion Loss: 92.4264 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 7\tNet Loss: 95.1392 \tQuestion Loss: 95.1392 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 7\tNet Loss: 93.4469 \tQuestion Loss: 93.4469 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 7\tNet Loss: 94.7406 \tQuestion Loss: 94.7406 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 7\tNet Loss: 93.5530 \tQuestion Loss: 93.5530 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 7\tNet Loss: 86.5373 \tQuestion Loss: 86.5373 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 7\tNet Loss: 91.1985 \tQuestion Loss: 91.1985 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 7\tNet Loss: 96.1299 \tQuestion Loss: 96.1299 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 7\tNet Loss: 92.5165 \tQuestion Loss: 92.5165 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 7\tNet Loss: 93.9986 \tQuestion Loss: 93.9986 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 7\tNet Loss: 92.0395 \tQuestion Loss: 92.0395 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 7\tNet Loss: 89.3269 \tQuestion Loss: 89.3269 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 7\tNet Loss: 88.2266 \tQuestion Loss: 88.2266 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 7\tNet Loss: 91.6997 \tQuestion Loss: 91.6997 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 7\tNet Loss: 90.1257 \tQuestion Loss: 90.1257 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 7\tNet Loss: 91.8372 \tQuestion Loss: 91.8372 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 7\tNet Loss: 91.3555 \tQuestion Loss: 91.3555 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 7\tNet Loss: 94.2165 \tQuestion Loss: 94.2165 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 7\tNet Loss: 89.6915 \tQuestion Loss: 89.6915 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 7\tNet Loss: 95.0837 \tQuestion Loss: 95.0837 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 7\tNet Loss: 92.7820 \tQuestion Loss: 92.7820 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 7\tNet Loss: 91.2807 \tQuestion Loss: 91.2807 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 7\tNet Loss: 91.6807 \tQuestion Loss: 91.6807 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 7\tNet Loss: 92.5167 \tQuestion Loss: 92.5167 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 7\tNet Loss: 93.2237 \tQuestion Loss: 93.2237 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 7\tNet Loss: 91.7754 \tQuestion Loss: 91.7754 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 7\tNet Loss: 95.4616 \tQuestion Loss: 95.4616 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 7\tNet Loss: 92.2062 \tQuestion Loss: 92.2062 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 7\tNet Loss: 92.8409 \tQuestion Loss: 92.8409 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 7\tNet Loss: 92.3486 \tQuestion Loss: 92.3486 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 101 \t Epoch : 7\tNet Loss: 95.2701 \tQuestion Loss: 95.2701 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 7\tNet Loss: 92.7628 \tQuestion Loss: 92.7628 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 7\tNet Loss: 92.4415 \tQuestion Loss: 92.4415 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 7\tNet Loss: 95.0557 \tQuestion Loss: 95.0557 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 7\tNet Loss: 96.1440 \tQuestion Loss: 96.1440 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 7\tNet Loss: 91.8510 \tQuestion Loss: 91.8510 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 7\tNet Loss: 89.0898 \tQuestion Loss: 89.0898 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 7\tNet Loss: 88.4683 \tQuestion Loss: 88.4683 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 7\tNet Loss: 92.7889 \tQuestion Loss: 92.7889 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 7\tNet Loss: 92.7648 \tQuestion Loss: 92.7648 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 7\tNet Loss: 87.6793 \tQuestion Loss: 87.6793 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 7\tNet Loss: 89.8002 \tQuestion Loss: 89.8002 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 7\tNet Loss: 96.9732 \tQuestion Loss: 96.9732 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 7\tNet Loss: 93.6995 \tQuestion Loss: 93.6995 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 7\tNet Loss: 90.5680 \tQuestion Loss: 90.5680 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 7\tNet Loss: 87.9806 \tQuestion Loss: 87.9806 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 7\tNet Loss: 90.5549 \tQuestion Loss: 90.5549 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 7\tNet Loss: 93.7380 \tQuestion Loss: 93.7380 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 7\tNet Loss: 89.6377 \tQuestion Loss: 89.6377 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 7\tNet Loss: 91.3929 \tQuestion Loss: 91.3929 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 7\tNet Loss: 93.4995 \tQuestion Loss: 93.4995 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 7\tNet Loss: 96.8760 \tQuestion Loss: 96.8760 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 7\tNet Loss: 92.5839 \tQuestion Loss: 92.5839 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 7\tNet Loss: 94.6326 \tQuestion Loss: 94.6326 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 7\tNet Loss: 92.5451 \tQuestion Loss: 92.5451 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 7\tNet Loss: 96.3188 \tQuestion Loss: 96.3188 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 7\tNet Loss: 88.1085 \tQuestion Loss: 88.1085 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 7\tNet Loss: 92.6054 \tQuestion Loss: 92.6054 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 7\tNet Loss: 93.2853 \tQuestion Loss: 93.2853 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 7\tNet Loss: 99.7434 \tQuestion Loss: 99.7434 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 7\tNet Loss: 94.7070 \tQuestion Loss: 94.7070 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 7\tNet Loss: 91.2098 \tQuestion Loss: 91.2098 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 7\tNet Loss: 92.7218 \tQuestion Loss: 92.7218 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 7\tNet Loss: 92.6761 \tQuestion Loss: 92.6761 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 7\tNet Loss: 85.6836 \tQuestion Loss: 85.6836 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 7\tNet Loss: 97.3909 \tQuestion Loss: 97.3909 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 7\tNet Loss: 95.0871 \tQuestion Loss: 95.0871 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 7\tNet Loss: 92.9573 \tQuestion Loss: 92.9573 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 7\tNet Loss: 91.9949 \tQuestion Loss: 91.9949 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 7\tNet Loss: 89.3710 \tQuestion Loss: 89.3710 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 7\tNet Loss: 89.5471 \tQuestion Loss: 89.5471 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 7\tNet Loss: 94.2851 \tQuestion Loss: 94.2851 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 7\tNet Loss: 89.6497 \tQuestion Loss: 89.6497 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 7\tNet Loss: 87.0759 \tQuestion Loss: 87.0759 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 7\tNet Loss: 95.3596 \tQuestion Loss: 95.3596 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 7\tNet Loss: 98.6743 \tQuestion Loss: 98.6743 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 7\tNet Loss: 87.1901 \tQuestion Loss: 87.1901 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 7\tNet Loss: 96.6653 \tQuestion Loss: 96.6653 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 7\tNet Loss: 90.0990 \tQuestion Loss: 90.0990 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 7\tNet Loss: 91.1347 \tQuestion Loss: 91.1347 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 7\tNet Loss: 97.9632 \tQuestion Loss: 97.9632 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 7\tNet Loss: 93.9425 \tQuestion Loss: 93.9425 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 7\tNet Loss: 94.1848 \tQuestion Loss: 94.1848 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 7\tNet Loss: 84.9663 \tQuestion Loss: 84.9663 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 7\tNet Loss: 90.1879 \tQuestion Loss: 90.1879 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 7\tNet Loss: 93.3867 \tQuestion Loss: 93.3867 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 7\tNet Loss: 88.4440 \tQuestion Loss: 88.4440 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 7\tNet Loss: 93.8494 \tQuestion Loss: 93.8494 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 7\tNet Loss: 94.6937 \tQuestion Loss: 94.6937 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 7\tNet Loss: 95.7316 \tQuestion Loss: 95.7316 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 7\tNet Loss: 94.1729 \tQuestion Loss: 94.1729 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 7\tNet Loss: 95.6931 \tQuestion Loss: 95.6931 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 7\tNet Loss: 89.4127 \tQuestion Loss: 89.4127 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 7\tNet Loss: 88.5632 \tQuestion Loss: 88.5632 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 7\tNet Loss: 89.7729 \tQuestion Loss: 89.7729 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 7\tNet Loss: 87.4945 \tQuestion Loss: 87.4945 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 7\tNet Loss: 92.6380 \tQuestion Loss: 92.6380 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 7\tNet Loss: 91.2535 \tQuestion Loss: 91.2535 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 7\tNet Loss: 95.7175 \tQuestion Loss: 95.7175 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 7\tNet Loss: 93.5839 \tQuestion Loss: 93.5839 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 7\tNet Loss: 92.5760 \tQuestion Loss: 92.5760 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 7\tNet Loss: 89.0583 \tQuestion Loss: 89.0583 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 7\tNet Loss: 90.7781 \tQuestion Loss: 90.7781 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 7\tNet Loss: 94.6926 \tQuestion Loss: 94.6926 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 7\tNet Loss: 97.4872 \tQuestion Loss: 97.4872 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 7\tNet Loss: 90.0504 \tQuestion Loss: 90.0504 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 7\tNet Loss: 91.7517 \tQuestion Loss: 91.7517 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 7\tNet Loss: 90.5969 \tQuestion Loss: 90.5969 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 7\tNet Loss: 91.6640 \tQuestion Loss: 91.6640 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 7\tNet Loss: 90.0478 \tQuestion Loss: 90.0478 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 7\tNet Loss: 92.4497 \tQuestion Loss: 92.4497 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 7\tNet Loss: 94.3430 \tQuestion Loss: 94.3430 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 7\tNet Loss: 89.6447 \tQuestion Loss: 89.6447 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 7\tNet Loss: 93.5596 \tQuestion Loss: 93.5596 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 7\tNet Loss: 91.7898 \tQuestion Loss: 91.7898 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 7\tNet Loss: 94.1128 \tQuestion Loss: 94.1128 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 7\tNet Loss: 94.9709 \tQuestion Loss: 94.9709 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 7\tNet Loss: 89.7881 \tQuestion Loss: 89.7881 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 7\tNet Loss: 89.6534 \tQuestion Loss: 89.6534 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 7\tNet Loss: 90.9139 \tQuestion Loss: 90.9139 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 7\tNet Loss: 90.0102 \tQuestion Loss: 90.0102 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 7\tNet Loss: 95.0927 \tQuestion Loss: 95.0927 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 7\tNet Loss: 93.4935 \tQuestion Loss: 93.4935 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 194 \t Epoch : 7\tNet Loss: 95.3183 \tQuestion Loss: 95.3183 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 7\tNet Loss: 88.0260 \tQuestion Loss: 88.0260 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 7\tNet Loss: 89.4674 \tQuestion Loss: 89.4674 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 7\tNet Loss: 94.8981 \tQuestion Loss: 94.8981 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 7\tNet Loss: 88.0124 \tQuestion Loss: 88.0124 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 7\tNet Loss: 92.5037 \tQuestion Loss: 92.5037 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 7 : 92.1399 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 8\tNet Loss: 92.6815 \tQuestion Loss: 92.6815 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 8\tNet Loss: 88.8484 \tQuestion Loss: 88.8484 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 8\tNet Loss: 94.1913 \tQuestion Loss: 94.1913 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 8\tNet Loss: 93.3677 \tQuestion Loss: 93.3677 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 8\tNet Loss: 93.8129 \tQuestion Loss: 93.8129 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 8\tNet Loss: 93.6561 \tQuestion Loss: 93.6561 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 8\tNet Loss: 88.8965 \tQuestion Loss: 88.8965 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 8\tNet Loss: 95.1976 \tQuestion Loss: 95.1976 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 8\tNet Loss: 93.6158 \tQuestion Loss: 93.6158 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 8\tNet Loss: 95.0315 \tQuestion Loss: 95.0315 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 8\tNet Loss: 91.1861 \tQuestion Loss: 91.1861 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 8\tNet Loss: 95.6802 \tQuestion Loss: 95.6802 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 8\tNet Loss: 90.4229 \tQuestion Loss: 90.4229 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 8\tNet Loss: 92.7670 \tQuestion Loss: 92.7670 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 8\tNet Loss: 93.1457 \tQuestion Loss: 93.1457 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 8\tNet Loss: 95.4660 \tQuestion Loss: 95.4660 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 8\tNet Loss: 90.5025 \tQuestion Loss: 90.5025 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 8\tNet Loss: 92.6697 \tQuestion Loss: 92.6697 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 8\tNet Loss: 90.8510 \tQuestion Loss: 90.8510 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 8\tNet Loss: 86.8849 \tQuestion Loss: 86.8849 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 8\tNet Loss: 96.1264 \tQuestion Loss: 96.1264 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 8\tNet Loss: 88.2755 \tQuestion Loss: 88.2755 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 8\tNet Loss: 92.9612 \tQuestion Loss: 92.9612 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 8\tNet Loss: 95.9930 \tQuestion Loss: 95.9930 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 8\tNet Loss: 89.8537 \tQuestion Loss: 89.8537 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 8\tNet Loss: 93.2492 \tQuestion Loss: 93.2492 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 8\tNet Loss: 98.4546 \tQuestion Loss: 98.4546 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 8\tNet Loss: 91.8989 \tQuestion Loss: 91.8989 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 8\tNet Loss: 87.0338 \tQuestion Loss: 87.0338 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 8\tNet Loss: 89.8435 \tQuestion Loss: 89.8435 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 8\tNet Loss: 91.5927 \tQuestion Loss: 91.5927 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 8\tNet Loss: 90.4388 \tQuestion Loss: 90.4388 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 8\tNet Loss: 89.1227 \tQuestion Loss: 89.1227 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 8\tNet Loss: 93.3431 \tQuestion Loss: 93.3431 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 8\tNet Loss: 87.4215 \tQuestion Loss: 87.4215 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 8\tNet Loss: 90.2298 \tQuestion Loss: 90.2298 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 8\tNet Loss: 94.1292 \tQuestion Loss: 94.1292 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 8\tNet Loss: 95.3088 \tQuestion Loss: 95.3088 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 8\tNet Loss: 91.1925 \tQuestion Loss: 91.1925 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 8\tNet Loss: 94.0075 \tQuestion Loss: 94.0075 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 8\tNet Loss: 91.5223 \tQuestion Loss: 91.5223 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 8\tNet Loss: 90.3965 \tQuestion Loss: 90.3965 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 8\tNet Loss: 92.2442 \tQuestion Loss: 92.2442 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 8\tNet Loss: 93.4878 \tQuestion Loss: 93.4878 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 8\tNet Loss: 90.6740 \tQuestion Loss: 90.6740 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 8\tNet Loss: 93.6412 \tQuestion Loss: 93.6412 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 8\tNet Loss: 92.1712 \tQuestion Loss: 92.1712 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 8\tNet Loss: 97.4434 \tQuestion Loss: 97.4434 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 8\tNet Loss: 86.7014 \tQuestion Loss: 86.7014 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 8\tNet Loss: 94.0564 \tQuestion Loss: 94.0564 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 8\tNet Loss: 92.9201 \tQuestion Loss: 92.9201 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 8\tNet Loss: 88.5838 \tQuestion Loss: 88.5838 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 8\tNet Loss: 89.7155 \tQuestion Loss: 89.7155 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 8\tNet Loss: 89.9991 \tQuestion Loss: 89.9991 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 8\tNet Loss: 96.4042 \tQuestion Loss: 96.4042 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 8\tNet Loss: 93.2507 \tQuestion Loss: 93.2507 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 8\tNet Loss: 94.5847 \tQuestion Loss: 94.5847 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 8\tNet Loss: 96.7102 \tQuestion Loss: 96.7102 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 8\tNet Loss: 87.6962 \tQuestion Loss: 87.6962 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 8\tNet Loss: 92.1879 \tQuestion Loss: 92.1879 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 8\tNet Loss: 89.1293 \tQuestion Loss: 89.1293 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 8\tNet Loss: 95.3041 \tQuestion Loss: 95.3041 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 8\tNet Loss: 88.8559 \tQuestion Loss: 88.8559 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 8\tNet Loss: 93.4871 \tQuestion Loss: 93.4871 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 8\tNet Loss: 92.2300 \tQuestion Loss: 92.2300 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 8\tNet Loss: 90.7420 \tQuestion Loss: 90.7420 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 8\tNet Loss: 92.4455 \tQuestion Loss: 92.4455 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 8\tNet Loss: 91.7629 \tQuestion Loss: 91.7629 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 8\tNet Loss: 90.9828 \tQuestion Loss: 90.9828 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 8\tNet Loss: 95.7953 \tQuestion Loss: 95.7953 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 8\tNet Loss: 93.2544 \tQuestion Loss: 93.2544 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 8\tNet Loss: 92.6993 \tQuestion Loss: 92.6993 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 8\tNet Loss: 95.6709 \tQuestion Loss: 95.6709 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 8\tNet Loss: 93.6863 \tQuestion Loss: 93.6863 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 8\tNet Loss: 94.9482 \tQuestion Loss: 94.9482 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 8\tNet Loss: 93.5156 \tQuestion Loss: 93.5156 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 8\tNet Loss: 86.9074 \tQuestion Loss: 86.9074 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 8\tNet Loss: 91.5853 \tQuestion Loss: 91.5853 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 8\tNet Loss: 96.9839 \tQuestion Loss: 96.9839 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 8\tNet Loss: 93.0018 \tQuestion Loss: 93.0018 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 8\tNet Loss: 94.2304 \tQuestion Loss: 94.2304 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 8\tNet Loss: 92.0912 \tQuestion Loss: 92.0912 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 8\tNet Loss: 89.6651 \tQuestion Loss: 89.6651 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 8\tNet Loss: 88.6415 \tQuestion Loss: 88.6415 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 8\tNet Loss: 92.0691 \tQuestion Loss: 92.0691 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 8\tNet Loss: 90.2386 \tQuestion Loss: 90.2386 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 8\tNet Loss: 92.4072 \tQuestion Loss: 92.4072 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 87 \t Epoch : 8\tNet Loss: 91.7192 \tQuestion Loss: 91.7192 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 8\tNet Loss: 94.5208 \tQuestion Loss: 94.5208 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 8\tNet Loss: 90.2326 \tQuestion Loss: 90.2326 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 8\tNet Loss: 95.2327 \tQuestion Loss: 95.2327 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 8\tNet Loss: 92.7341 \tQuestion Loss: 92.7341 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 8\tNet Loss: 91.2228 \tQuestion Loss: 91.2228 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 8\tNet Loss: 92.0586 \tQuestion Loss: 92.0586 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 8\tNet Loss: 92.6366 \tQuestion Loss: 92.6366 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 8\tNet Loss: 93.3994 \tQuestion Loss: 93.3994 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 8\tNet Loss: 91.8953 \tQuestion Loss: 91.8953 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 8\tNet Loss: 95.8026 \tQuestion Loss: 95.8026 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 8\tNet Loss: 92.8652 \tQuestion Loss: 92.8652 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 8\tNet Loss: 92.8964 \tQuestion Loss: 92.8964 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 8\tNet Loss: 92.4260 \tQuestion Loss: 92.4260 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 8\tNet Loss: 95.5859 \tQuestion Loss: 95.5859 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 8\tNet Loss: 93.1718 \tQuestion Loss: 93.1718 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 8\tNet Loss: 92.4273 \tQuestion Loss: 92.4273 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 8\tNet Loss: 95.0028 \tQuestion Loss: 95.0028 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 8\tNet Loss: 96.7184 \tQuestion Loss: 96.7184 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 8\tNet Loss: 92.3031 \tQuestion Loss: 92.3031 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 8\tNet Loss: 89.2049 \tQuestion Loss: 89.2049 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 8\tNet Loss: 88.3528 \tQuestion Loss: 88.3528 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 8\tNet Loss: 92.8827 \tQuestion Loss: 92.8827 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 8\tNet Loss: 93.5484 \tQuestion Loss: 93.5484 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 8\tNet Loss: 87.7071 \tQuestion Loss: 87.7071 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 8\tNet Loss: 89.6434 \tQuestion Loss: 89.6434 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 8\tNet Loss: 97.0189 \tQuestion Loss: 97.0189 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 8\tNet Loss: 94.4374 \tQuestion Loss: 94.4374 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 8\tNet Loss: 91.5410 \tQuestion Loss: 91.5410 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 8\tNet Loss: 88.4465 \tQuestion Loss: 88.4465 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 8\tNet Loss: 90.4804 \tQuestion Loss: 90.4804 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 8\tNet Loss: 93.8012 \tQuestion Loss: 93.8012 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 8\tNet Loss: 89.5664 \tQuestion Loss: 89.5664 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 8\tNet Loss: 91.5214 \tQuestion Loss: 91.5214 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 8\tNet Loss: 93.5878 \tQuestion Loss: 93.5878 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 8\tNet Loss: 97.5188 \tQuestion Loss: 97.5188 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 8\tNet Loss: 92.8077 \tQuestion Loss: 92.8077 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 8\tNet Loss: 94.7433 \tQuestion Loss: 94.7433 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 8\tNet Loss: 92.6505 \tQuestion Loss: 92.6505 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 8\tNet Loss: 96.0440 \tQuestion Loss: 96.0440 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 8\tNet Loss: 87.8163 \tQuestion Loss: 87.8163 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 8\tNet Loss: 92.8640 \tQuestion Loss: 92.8640 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 8\tNet Loss: 94.5706 \tQuestion Loss: 94.5706 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 8\tNet Loss: 100.1518 \tQuestion Loss: 100.1518 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 8\tNet Loss: 94.7435 \tQuestion Loss: 94.7435 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 8\tNet Loss: 91.2784 \tQuestion Loss: 91.2784 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 8\tNet Loss: 92.6087 \tQuestion Loss: 92.6087 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 8\tNet Loss: 93.4851 \tQuestion Loss: 93.4851 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 8\tNet Loss: 86.1047 \tQuestion Loss: 86.1047 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 8\tNet Loss: 97.1090 \tQuestion Loss: 97.1090 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 8\tNet Loss: 94.9754 \tQuestion Loss: 94.9754 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 8\tNet Loss: 93.4689 \tQuestion Loss: 93.4689 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 8\tNet Loss: 92.0333 \tQuestion Loss: 92.0333 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 8\tNet Loss: 89.2376 \tQuestion Loss: 89.2376 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 8\tNet Loss: 89.6821 \tQuestion Loss: 89.6821 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 8\tNet Loss: 94.3173 \tQuestion Loss: 94.3173 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 8\tNet Loss: 89.0228 \tQuestion Loss: 89.0228 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 8\tNet Loss: 86.8499 \tQuestion Loss: 86.8499 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 8\tNet Loss: 95.6261 \tQuestion Loss: 95.6261 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 8\tNet Loss: 98.7469 \tQuestion Loss: 98.7469 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 8\tNet Loss: 86.7634 \tQuestion Loss: 86.7634 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 8\tNet Loss: 96.4342 \tQuestion Loss: 96.4342 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 8\tNet Loss: 89.8156 \tQuestion Loss: 89.8156 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 8\tNet Loss: 91.0670 \tQuestion Loss: 91.0670 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 8\tNet Loss: 97.7546 \tQuestion Loss: 97.7546 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 8\tNet Loss: 93.4728 \tQuestion Loss: 93.4728 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 8\tNet Loss: 94.2012 \tQuestion Loss: 94.2012 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 8\tNet Loss: 85.6296 \tQuestion Loss: 85.6296 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 8\tNet Loss: 90.6280 \tQuestion Loss: 90.6280 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 8\tNet Loss: 93.3593 \tQuestion Loss: 93.3593 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 8\tNet Loss: 88.3115 \tQuestion Loss: 88.3115 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 8\tNet Loss: 93.4938 \tQuestion Loss: 93.4938 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 8\tNet Loss: 94.6041 \tQuestion Loss: 94.6041 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 8\tNet Loss: 95.8459 \tQuestion Loss: 95.8459 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 8\tNet Loss: 94.1950 \tQuestion Loss: 94.1950 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 8\tNet Loss: 95.9725 \tQuestion Loss: 95.9725 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 8\tNet Loss: 89.4996 \tQuestion Loss: 89.4996 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 8\tNet Loss: 88.4558 \tQuestion Loss: 88.4558 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 8\tNet Loss: 89.6231 \tQuestion Loss: 89.6231 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 8\tNet Loss: 87.3318 \tQuestion Loss: 87.3318 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 8\tNet Loss: 92.9769 \tQuestion Loss: 92.9769 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 8\tNet Loss: 91.5455 \tQuestion Loss: 91.5455 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 8\tNet Loss: 95.7746 \tQuestion Loss: 95.7746 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 8\tNet Loss: 93.5459 \tQuestion Loss: 93.5459 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 8\tNet Loss: 92.2407 \tQuestion Loss: 92.2407 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 8\tNet Loss: 89.1188 \tQuestion Loss: 89.1188 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 8\tNet Loss: 90.4342 \tQuestion Loss: 90.4342 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 8\tNet Loss: 94.8662 \tQuestion Loss: 94.8662 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 8\tNet Loss: 97.6662 \tQuestion Loss: 97.6662 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 8\tNet Loss: 90.1299 \tQuestion Loss: 90.1299 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 8\tNet Loss: 91.5051 \tQuestion Loss: 91.5051 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 8\tNet Loss: 90.4242 \tQuestion Loss: 90.4242 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 8\tNet Loss: 91.7903 \tQuestion Loss: 91.7903 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 180 \t Epoch : 8\tNet Loss: 89.9197 \tQuestion Loss: 89.9197 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 8\tNet Loss: 92.3644 \tQuestion Loss: 92.3644 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 8\tNet Loss: 94.2076 \tQuestion Loss: 94.2076 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 8\tNet Loss: 89.3278 \tQuestion Loss: 89.3278 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 8\tNet Loss: 93.1461 \tQuestion Loss: 93.1461 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 8\tNet Loss: 91.4873 \tQuestion Loss: 91.4873 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 8\tNet Loss: 94.0886 \tQuestion Loss: 94.0886 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 8\tNet Loss: 95.1565 \tQuestion Loss: 95.1565 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 8\tNet Loss: 89.9606 \tQuestion Loss: 89.9606 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 8\tNet Loss: 89.9610 \tQuestion Loss: 89.9610 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 8\tNet Loss: 91.1086 \tQuestion Loss: 91.1086 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 8\tNet Loss: 89.9548 \tQuestion Loss: 89.9548 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 8\tNet Loss: 94.7518 \tQuestion Loss: 94.7518 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 8\tNet Loss: 93.2008 \tQuestion Loss: 93.2008 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 8\tNet Loss: 94.9128 \tQuestion Loss: 94.9128 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 8\tNet Loss: 87.8614 \tQuestion Loss: 87.8614 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 8\tNet Loss: 89.0565 \tQuestion Loss: 89.0565 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 8\tNet Loss: 94.5490 \tQuestion Loss: 94.5490 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 8\tNet Loss: 87.6613 \tQuestion Loss: 87.6613 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 8\tNet Loss: 92.3047 \tQuestion Loss: 92.3047 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 8 : 92.3229 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 9\tNet Loss: 92.6135 \tQuestion Loss: 92.6135 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 9\tNet Loss: 89.0005 \tQuestion Loss: 89.0005 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 9\tNet Loss: 94.1436 \tQuestion Loss: 94.1436 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 9\tNet Loss: 93.5030 \tQuestion Loss: 93.5030 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 9\tNet Loss: 93.4494 \tQuestion Loss: 93.4494 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 9\tNet Loss: 93.4463 \tQuestion Loss: 93.4463 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 9\tNet Loss: 88.6893 \tQuestion Loss: 88.6893 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 9\tNet Loss: 94.3719 \tQuestion Loss: 94.3719 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 9\tNet Loss: 93.0591 \tQuestion Loss: 93.0591 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 9\tNet Loss: 94.8311 \tQuestion Loss: 94.8311 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 9\tNet Loss: 90.8263 \tQuestion Loss: 90.8263 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 9\tNet Loss: 95.5071 \tQuestion Loss: 95.5071 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 9\tNet Loss: 90.0777 \tQuestion Loss: 90.0777 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 9\tNet Loss: 92.4681 \tQuestion Loss: 92.4681 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 9\tNet Loss: 92.7665 \tQuestion Loss: 92.7665 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 9\tNet Loss: 95.1792 \tQuestion Loss: 95.1792 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 9\tNet Loss: 90.3049 \tQuestion Loss: 90.3049 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 9\tNet Loss: 92.1825 \tQuestion Loss: 92.1825 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 9\tNet Loss: 90.5685 \tQuestion Loss: 90.5685 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 9\tNet Loss: 86.6811 \tQuestion Loss: 86.6811 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 9\tNet Loss: 95.8797 \tQuestion Loss: 95.8797 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 9\tNet Loss: 87.9722 \tQuestion Loss: 87.9722 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 9\tNet Loss: 92.5302 \tQuestion Loss: 92.5302 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 9\tNet Loss: 95.5133 \tQuestion Loss: 95.5133 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 9\tNet Loss: 89.6205 \tQuestion Loss: 89.6205 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 9\tNet Loss: 93.0586 \tQuestion Loss: 93.0586 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 9\tNet Loss: 98.2387 \tQuestion Loss: 98.2387 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 9\tNet Loss: 91.6588 \tQuestion Loss: 91.6588 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 9\tNet Loss: 86.8243 \tQuestion Loss: 86.8243 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 9\tNet Loss: 89.7648 \tQuestion Loss: 89.7648 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 9\tNet Loss: 91.4214 \tQuestion Loss: 91.4214 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 9\tNet Loss: 90.1483 \tQuestion Loss: 90.1483 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 9\tNet Loss: 88.5765 \tQuestion Loss: 88.5765 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 9\tNet Loss: 93.2211 \tQuestion Loss: 93.2211 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 9\tNet Loss: 87.2417 \tQuestion Loss: 87.2417 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 9\tNet Loss: 89.9156 \tQuestion Loss: 89.9156 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 9\tNet Loss: 93.5188 \tQuestion Loss: 93.5188 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 9\tNet Loss: 95.1365 \tQuestion Loss: 95.1365 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 9\tNet Loss: 91.0301 \tQuestion Loss: 91.0301 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 9\tNet Loss: 93.8527 \tQuestion Loss: 93.8527 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 9\tNet Loss: 91.3815 \tQuestion Loss: 91.3815 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 9\tNet Loss: 90.0504 \tQuestion Loss: 90.0504 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 9\tNet Loss: 91.8660 \tQuestion Loss: 91.8660 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 9\tNet Loss: 93.1607 \tQuestion Loss: 93.1607 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 9\tNet Loss: 90.4781 \tQuestion Loss: 90.4781 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 9\tNet Loss: 93.5972 \tQuestion Loss: 93.5972 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 9\tNet Loss: 92.1175 \tQuestion Loss: 92.1175 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 9\tNet Loss: 97.1912 \tQuestion Loss: 97.1912 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 9\tNet Loss: 86.2703 \tQuestion Loss: 86.2703 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 9\tNet Loss: 94.0425 \tQuestion Loss: 94.0425 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 9\tNet Loss: 92.8653 \tQuestion Loss: 92.8653 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 9\tNet Loss: 88.3552 \tQuestion Loss: 88.3552 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 9\tNet Loss: 89.2444 \tQuestion Loss: 89.2444 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 9\tNet Loss: 89.6916 \tQuestion Loss: 89.6916 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 9\tNet Loss: 96.4503 \tQuestion Loss: 96.4503 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 9\tNet Loss: 93.0725 \tQuestion Loss: 93.0725 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 9\tNet Loss: 94.3006 \tQuestion Loss: 94.3006 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 9\tNet Loss: 96.6023 \tQuestion Loss: 96.6023 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 9\tNet Loss: 87.4904 \tQuestion Loss: 87.4904 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 9\tNet Loss: 92.0331 \tQuestion Loss: 92.0331 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 9\tNet Loss: 88.8748 \tQuestion Loss: 88.8748 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 9\tNet Loss: 95.0362 \tQuestion Loss: 95.0362 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 9\tNet Loss: 88.6661 \tQuestion Loss: 88.6661 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 9\tNet Loss: 93.4348 \tQuestion Loss: 93.4348 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 9\tNet Loss: 92.0137 \tQuestion Loss: 92.0137 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 9\tNet Loss: 90.3546 \tQuestion Loss: 90.3546 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 9\tNet Loss: 92.1054 \tQuestion Loss: 92.1054 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 9\tNet Loss: 91.2694 \tQuestion Loss: 91.2694 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 9\tNet Loss: 90.6412 \tQuestion Loss: 90.6412 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 9\tNet Loss: 95.5515 \tQuestion Loss: 95.5515 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 9\tNet Loss: 93.1282 \tQuestion Loss: 93.1282 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 9\tNet Loss: 92.6133 \tQuestion Loss: 92.6133 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 9\tNet Loss: 95.3159 \tQuestion Loss: 95.3159 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 73 \t Epoch : 9\tNet Loss: 93.5363 \tQuestion Loss: 93.5363 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 9\tNet Loss: 94.9101 \tQuestion Loss: 94.9101 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 9\tNet Loss: 93.6131 \tQuestion Loss: 93.6131 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 9\tNet Loss: 86.6798 \tQuestion Loss: 86.6798 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 9\tNet Loss: 91.3196 \tQuestion Loss: 91.3196 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 9\tNet Loss: 96.1307 \tQuestion Loss: 96.1307 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 9\tNet Loss: 92.6944 \tQuestion Loss: 92.6944 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 9\tNet Loss: 94.2030 \tQuestion Loss: 94.2030 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 9\tNet Loss: 92.1405 \tQuestion Loss: 92.1405 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 9\tNet Loss: 89.5131 \tQuestion Loss: 89.5131 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 9\tNet Loss: 88.4708 \tQuestion Loss: 88.4708 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 9\tNet Loss: 91.8606 \tQuestion Loss: 91.8606 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 9\tNet Loss: 90.2318 \tQuestion Loss: 90.2318 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 9\tNet Loss: 91.9212 \tQuestion Loss: 91.9212 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 9\tNet Loss: 91.5053 \tQuestion Loss: 91.5053 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 9\tNet Loss: 94.3833 \tQuestion Loss: 94.3833 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 9\tNet Loss: 89.9132 \tQuestion Loss: 89.9132 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 9\tNet Loss: 95.2498 \tQuestion Loss: 95.2498 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 9\tNet Loss: 92.8867 \tQuestion Loss: 92.8867 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 9\tNet Loss: 91.4637 \tQuestion Loss: 91.4637 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 9\tNet Loss: 91.8182 \tQuestion Loss: 91.8182 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 9\tNet Loss: 92.5767 \tQuestion Loss: 92.5767 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 9\tNet Loss: 93.4586 \tQuestion Loss: 93.4586 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 9\tNet Loss: 91.9140 \tQuestion Loss: 91.9140 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 9\tNet Loss: 95.5819 \tQuestion Loss: 95.5819 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 9\tNet Loss: 92.3780 \tQuestion Loss: 92.3780 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 9\tNet Loss: 93.0668 \tQuestion Loss: 93.0668 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 9\tNet Loss: 92.4925 \tQuestion Loss: 92.4925 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 9\tNet Loss: 95.4832 \tQuestion Loss: 95.4832 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 9\tNet Loss: 92.9420 \tQuestion Loss: 92.9420 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 9\tNet Loss: 92.4695 \tQuestion Loss: 92.4695 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 9\tNet Loss: 95.1966 \tQuestion Loss: 95.1966 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 9\tNet Loss: 96.2709 \tQuestion Loss: 96.2709 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 9\tNet Loss: 91.9826 \tQuestion Loss: 91.9826 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 9\tNet Loss: 89.2236 \tQuestion Loss: 89.2236 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 9\tNet Loss: 88.5341 \tQuestion Loss: 88.5341 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 9\tNet Loss: 92.8790 \tQuestion Loss: 92.8790 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 9\tNet Loss: 92.9829 \tQuestion Loss: 92.9829 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 9\tNet Loss: 87.7877 \tQuestion Loss: 87.7877 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 9\tNet Loss: 89.8427 \tQuestion Loss: 89.8427 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 9\tNet Loss: 97.1391 \tQuestion Loss: 97.1391 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 9\tNet Loss: 93.8487 \tQuestion Loss: 93.8487 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 9\tNet Loss: 90.7865 \tQuestion Loss: 90.7865 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 9\tNet Loss: 88.1174 \tQuestion Loss: 88.1174 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 9\tNet Loss: 90.6799 \tQuestion Loss: 90.6799 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 9\tNet Loss: 93.9221 \tQuestion Loss: 93.9221 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 9\tNet Loss: 89.6601 \tQuestion Loss: 89.6601 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 9\tNet Loss: 91.5487 \tQuestion Loss: 91.5487 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 9\tNet Loss: 93.6892 \tQuestion Loss: 93.6892 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 9\tNet Loss: 96.9910 \tQuestion Loss: 96.9910 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 9\tNet Loss: 92.7478 \tQuestion Loss: 92.7478 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 9\tNet Loss: 94.7320 \tQuestion Loss: 94.7320 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 9\tNet Loss: 92.7285 \tQuestion Loss: 92.7285 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 9\tNet Loss: 96.4576 \tQuestion Loss: 96.4576 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 9\tNet Loss: 88.0520 \tQuestion Loss: 88.0520 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 9\tNet Loss: 92.7605 \tQuestion Loss: 92.7605 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 9\tNet Loss: 93.4233 \tQuestion Loss: 93.4233 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 9\tNet Loss: 99.9665 \tQuestion Loss: 99.9665 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 9\tNet Loss: 94.8437 \tQuestion Loss: 94.8437 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 9\tNet Loss: 91.2293 \tQuestion Loss: 91.2293 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 9\tNet Loss: 92.8227 \tQuestion Loss: 92.8227 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 9\tNet Loss: 92.6951 \tQuestion Loss: 92.6951 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 9\tNet Loss: 85.7724 \tQuestion Loss: 85.7724 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 9\tNet Loss: 97.5090 \tQuestion Loss: 97.5090 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 9\tNet Loss: 95.2937 \tQuestion Loss: 95.2937 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 9\tNet Loss: 93.0400 \tQuestion Loss: 93.0400 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 9\tNet Loss: 92.0893 \tQuestion Loss: 92.0893 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 9\tNet Loss: 89.3765 \tQuestion Loss: 89.3765 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 9\tNet Loss: 89.6781 \tQuestion Loss: 89.6781 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 9\tNet Loss: 94.4436 \tQuestion Loss: 94.4436 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 9\tNet Loss: 89.6159 \tQuestion Loss: 89.6159 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 9\tNet Loss: 87.0794 \tQuestion Loss: 87.0794 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 9\tNet Loss: 95.4832 \tQuestion Loss: 95.4832 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 9\tNet Loss: 98.7679 \tQuestion Loss: 98.7679 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 9\tNet Loss: 87.0891 \tQuestion Loss: 87.0891 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 9\tNet Loss: 96.6343 \tQuestion Loss: 96.6343 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 9\tNet Loss: 90.1399 \tQuestion Loss: 90.1399 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 9\tNet Loss: 91.2597 \tQuestion Loss: 91.2597 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 9\tNet Loss: 98.1230 \tQuestion Loss: 98.1230 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 9\tNet Loss: 94.1041 \tQuestion Loss: 94.1041 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 9\tNet Loss: 94.2134 \tQuestion Loss: 94.2134 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 9\tNet Loss: 85.0184 \tQuestion Loss: 85.0184 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 9\tNet Loss: 90.3715 \tQuestion Loss: 90.3715 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 9\tNet Loss: 93.5429 \tQuestion Loss: 93.5429 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 9\tNet Loss: 88.3998 \tQuestion Loss: 88.3998 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 9\tNet Loss: 93.8418 \tQuestion Loss: 93.8418 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 9\tNet Loss: 94.8057 \tQuestion Loss: 94.8057 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 9\tNet Loss: 95.9155 \tQuestion Loss: 95.9155 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 9\tNet Loss: 94.3503 \tQuestion Loss: 94.3503 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 9\tNet Loss: 95.7759 \tQuestion Loss: 95.7759 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 9\tNet Loss: 89.4907 \tQuestion Loss: 89.4907 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 9\tNet Loss: 88.5985 \tQuestion Loss: 88.5985 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 9\tNet Loss: 89.8568 \tQuestion Loss: 89.8568 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 166 \t Epoch : 9\tNet Loss: 87.6597 \tQuestion Loss: 87.6597 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 9\tNet Loss: 92.8227 \tQuestion Loss: 92.8227 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 9\tNet Loss: 91.4286 \tQuestion Loss: 91.4286 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 9\tNet Loss: 95.8949 \tQuestion Loss: 95.8949 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 9\tNet Loss: 93.6455 \tQuestion Loss: 93.6455 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 9\tNet Loss: 92.6078 \tQuestion Loss: 92.6078 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 9\tNet Loss: 89.0343 \tQuestion Loss: 89.0343 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 9\tNet Loss: 90.7744 \tQuestion Loss: 90.7744 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 9\tNet Loss: 94.8142 \tQuestion Loss: 94.8142 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 9\tNet Loss: 97.7460 \tQuestion Loss: 97.7460 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 9\tNet Loss: 90.1552 \tQuestion Loss: 90.1552 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 9\tNet Loss: 91.7592 \tQuestion Loss: 91.7592 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 9\tNet Loss: 90.6043 \tQuestion Loss: 90.6043 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 9\tNet Loss: 91.7963 \tQuestion Loss: 91.7963 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 9\tNet Loss: 90.1322 \tQuestion Loss: 90.1322 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 9\tNet Loss: 92.4954 \tQuestion Loss: 92.4954 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 9\tNet Loss: 94.4647 \tQuestion Loss: 94.4647 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 9\tNet Loss: 89.5768 \tQuestion Loss: 89.5768 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 9\tNet Loss: 93.6056 \tQuestion Loss: 93.6056 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 9\tNet Loss: 91.8239 \tQuestion Loss: 91.8239 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 9\tNet Loss: 94.2599 \tQuestion Loss: 94.2599 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 9\tNet Loss: 95.1367 \tQuestion Loss: 95.1367 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 9\tNet Loss: 89.9231 \tQuestion Loss: 89.9231 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 9\tNet Loss: 89.7527 \tQuestion Loss: 89.7527 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 9\tNet Loss: 91.1795 \tQuestion Loss: 91.1795 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 9\tNet Loss: 90.1106 \tQuestion Loss: 90.1106 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 9\tNet Loss: 95.0521 \tQuestion Loss: 95.0521 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 9\tNet Loss: 93.4392 \tQuestion Loss: 93.4392 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 9\tNet Loss: 95.2441 \tQuestion Loss: 95.2441 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 9\tNet Loss: 88.0125 \tQuestion Loss: 88.0125 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 9\tNet Loss: 89.4530 \tQuestion Loss: 89.4530 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 9\tNet Loss: 94.9225 \tQuestion Loss: 94.9225 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 9\tNet Loss: 88.0091 \tQuestion Loss: 88.0091 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 9\tNet Loss: 92.5183 \tQuestion Loss: 92.5183 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 9 : 92.2397 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 10\tNet Loss: 92.7301 \tQuestion Loss: 92.7301 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 10\tNet Loss: 89.0062 \tQuestion Loss: 89.0062 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 10\tNet Loss: 94.2709 \tQuestion Loss: 94.2709 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 10\tNet Loss: 93.4703 \tQuestion Loss: 93.4703 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 10\tNet Loss: 93.8841 \tQuestion Loss: 93.8841 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 10\tNet Loss: 93.6655 \tQuestion Loss: 93.6655 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 10\tNet Loss: 88.8473 \tQuestion Loss: 88.8473 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 10\tNet Loss: 95.0887 \tQuestion Loss: 95.0887 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 10\tNet Loss: 93.6409 \tQuestion Loss: 93.6409 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 10\tNet Loss: 95.1241 \tQuestion Loss: 95.1241 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 10\tNet Loss: 91.2317 \tQuestion Loss: 91.2317 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 10\tNet Loss: 95.6525 \tQuestion Loss: 95.6525 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 10\tNet Loss: 90.4707 \tQuestion Loss: 90.4707 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 10\tNet Loss: 92.7440 \tQuestion Loss: 92.7440 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 10\tNet Loss: 93.1651 \tQuestion Loss: 93.1651 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 10\tNet Loss: 95.4993 \tQuestion Loss: 95.4993 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 10\tNet Loss: 90.4535 \tQuestion Loss: 90.4535 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 10\tNet Loss: 92.6714 \tQuestion Loss: 92.6714 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 10\tNet Loss: 90.9462 \tQuestion Loss: 90.9462 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 10\tNet Loss: 86.9279 \tQuestion Loss: 86.9279 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 10\tNet Loss: 96.1002 \tQuestion Loss: 96.1002 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 10\tNet Loss: 88.2131 \tQuestion Loss: 88.2131 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 10\tNet Loss: 92.9074 \tQuestion Loss: 92.9074 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 10\tNet Loss: 96.0037 \tQuestion Loss: 96.0037 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 10\tNet Loss: 89.8818 \tQuestion Loss: 89.8818 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 10\tNet Loss: 93.3105 \tQuestion Loss: 93.3105 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 10\tNet Loss: 98.4927 \tQuestion Loss: 98.4927 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 10\tNet Loss: 91.9640 \tQuestion Loss: 91.9640 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 10\tNet Loss: 87.0242 \tQuestion Loss: 87.0242 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 10\tNet Loss: 89.8082 \tQuestion Loss: 89.8082 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 10\tNet Loss: 91.6138 \tQuestion Loss: 91.6138 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 10\tNet Loss: 90.4361 \tQuestion Loss: 90.4361 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 10\tNet Loss: 89.1839 \tQuestion Loss: 89.1839 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 10\tNet Loss: 93.3686 \tQuestion Loss: 93.3686 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 10\tNet Loss: 87.4062 \tQuestion Loss: 87.4062 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 10\tNet Loss: 90.1678 \tQuestion Loss: 90.1678 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 10\tNet Loss: 94.0476 \tQuestion Loss: 94.0476 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 10\tNet Loss: 95.2307 \tQuestion Loss: 95.2307 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 10\tNet Loss: 91.1818 \tQuestion Loss: 91.1818 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 10\tNet Loss: 94.0789 \tQuestion Loss: 94.0789 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 10\tNet Loss: 91.5489 \tQuestion Loss: 91.5489 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 10\tNet Loss: 90.4383 \tQuestion Loss: 90.4383 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 10\tNet Loss: 92.2205 \tQuestion Loss: 92.2205 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 10\tNet Loss: 93.4314 \tQuestion Loss: 93.4314 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 10\tNet Loss: 90.6652 \tQuestion Loss: 90.6652 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 10\tNet Loss: 93.6134 \tQuestion Loss: 93.6134 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 10\tNet Loss: 92.1627 \tQuestion Loss: 92.1627 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 10\tNet Loss: 97.4202 \tQuestion Loss: 97.4202 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 10\tNet Loss: 86.7754 \tQuestion Loss: 86.7754 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 10\tNet Loss: 94.1164 \tQuestion Loss: 94.1164 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 10\tNet Loss: 92.8737 \tQuestion Loss: 92.8737 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 10\tNet Loss: 88.5130 \tQuestion Loss: 88.5130 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 10\tNet Loss: 89.6756 \tQuestion Loss: 89.6756 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 10\tNet Loss: 90.0915 \tQuestion Loss: 90.0915 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 10\tNet Loss: 96.3880 \tQuestion Loss: 96.3880 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 10\tNet Loss: 93.1418 \tQuestion Loss: 93.1418 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 10\tNet Loss: 94.5095 \tQuestion Loss: 94.5095 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 10\tNet Loss: 96.6127 \tQuestion Loss: 96.6127 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 58 \t Epoch : 10\tNet Loss: 87.6175 \tQuestion Loss: 87.6175 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 10\tNet Loss: 92.1098 \tQuestion Loss: 92.1098 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 10\tNet Loss: 89.1108 \tQuestion Loss: 89.1108 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 10\tNet Loss: 95.3091 \tQuestion Loss: 95.3091 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 10\tNet Loss: 88.8374 \tQuestion Loss: 88.8374 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 10\tNet Loss: 93.4210 \tQuestion Loss: 93.4210 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 10\tNet Loss: 92.1379 \tQuestion Loss: 92.1379 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 10\tNet Loss: 90.6240 \tQuestion Loss: 90.6240 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 10\tNet Loss: 92.4890 \tQuestion Loss: 92.4890 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 10\tNet Loss: 91.9049 \tQuestion Loss: 91.9049 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 10\tNet Loss: 90.9923 \tQuestion Loss: 90.9923 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 10\tNet Loss: 95.7880 \tQuestion Loss: 95.7880 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 10\tNet Loss: 93.2480 \tQuestion Loss: 93.2480 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 10\tNet Loss: 92.6788 \tQuestion Loss: 92.6788 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 10\tNet Loss: 95.5865 \tQuestion Loss: 95.5865 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 10\tNet Loss: 93.6477 \tQuestion Loss: 93.6477 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 10\tNet Loss: 94.8523 \tQuestion Loss: 94.8523 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 10\tNet Loss: 93.4887 \tQuestion Loss: 93.4887 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 10\tNet Loss: 86.8568 \tQuestion Loss: 86.8568 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 10\tNet Loss: 91.5009 \tQuestion Loss: 91.5009 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 10\tNet Loss: 96.9134 \tQuestion Loss: 96.9134 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 10\tNet Loss: 92.9238 \tQuestion Loss: 92.9238 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 10\tNet Loss: 94.1871 \tQuestion Loss: 94.1871 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 10\tNet Loss: 92.0478 \tQuestion Loss: 92.0478 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 10\tNet Loss: 89.6533 \tQuestion Loss: 89.6533 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 10\tNet Loss: 88.6387 \tQuestion Loss: 88.6387 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 10\tNet Loss: 92.0039 \tQuestion Loss: 92.0039 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 10\tNet Loss: 90.2292 \tQuestion Loss: 90.2292 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 10\tNet Loss: 92.4268 \tQuestion Loss: 92.4268 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 10\tNet Loss: 91.6834 \tQuestion Loss: 91.6834 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 10\tNet Loss: 94.5123 \tQuestion Loss: 94.5123 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 10\tNet Loss: 90.1581 \tQuestion Loss: 90.1581 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 10\tNet Loss: 95.1746 \tQuestion Loss: 95.1746 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 10\tNet Loss: 92.7046 \tQuestion Loss: 92.7046 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 10\tNet Loss: 91.2149 \tQuestion Loss: 91.2149 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 10\tNet Loss: 91.9673 \tQuestion Loss: 91.9673 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 10\tNet Loss: 92.6428 \tQuestion Loss: 92.6428 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 10\tNet Loss: 93.3899 \tQuestion Loss: 93.3899 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 10\tNet Loss: 91.7826 \tQuestion Loss: 91.7826 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 10\tNet Loss: 95.7202 \tQuestion Loss: 95.7202 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 10\tNet Loss: 92.8095 \tQuestion Loss: 92.8095 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 10\tNet Loss: 92.9188 \tQuestion Loss: 92.9188 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 10\tNet Loss: 92.4492 \tQuestion Loss: 92.4492 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 10\tNet Loss: 95.5646 \tQuestion Loss: 95.5646 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 10\tNet Loss: 93.0626 \tQuestion Loss: 93.0626 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 10\tNet Loss: 92.4094 \tQuestion Loss: 92.4094 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 10\tNet Loss: 94.8973 \tQuestion Loss: 94.8973 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 10\tNet Loss: 96.6353 \tQuestion Loss: 96.6353 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 10\tNet Loss: 92.2601 \tQuestion Loss: 92.2601 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 10\tNet Loss: 89.1968 \tQuestion Loss: 89.1968 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 10\tNet Loss: 88.3530 \tQuestion Loss: 88.3530 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 10\tNet Loss: 92.8776 \tQuestion Loss: 92.8776 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 10\tNet Loss: 93.3806 \tQuestion Loss: 93.3806 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 10\tNet Loss: 87.5867 \tQuestion Loss: 87.5867 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 10\tNet Loss: 89.6451 \tQuestion Loss: 89.6451 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 10\tNet Loss: 97.0009 \tQuestion Loss: 97.0009 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 10\tNet Loss: 94.4324 \tQuestion Loss: 94.4324 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 10\tNet Loss: 91.5451 \tQuestion Loss: 91.5451 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 10\tNet Loss: 88.3905 \tQuestion Loss: 88.3905 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 10\tNet Loss: 90.4374 \tQuestion Loss: 90.4374 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 10\tNet Loss: 93.7550 \tQuestion Loss: 93.7550 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 10\tNet Loss: 89.6001 \tQuestion Loss: 89.6001 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 10\tNet Loss: 91.5145 \tQuestion Loss: 91.5145 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 10\tNet Loss: 93.5302 \tQuestion Loss: 93.5302 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 10\tNet Loss: 97.5233 \tQuestion Loss: 97.5233 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 10\tNet Loss: 92.8118 \tQuestion Loss: 92.8118 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 10\tNet Loss: 94.6933 \tQuestion Loss: 94.6933 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 10\tNet Loss: 92.5392 \tQuestion Loss: 92.5392 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 10\tNet Loss: 96.0017 \tQuestion Loss: 96.0017 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 10\tNet Loss: 87.8628 \tQuestion Loss: 87.8628 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 10\tNet Loss: 92.8027 \tQuestion Loss: 92.8027 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 10\tNet Loss: 94.5318 \tQuestion Loss: 94.5318 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 10\tNet Loss: 100.0263 \tQuestion Loss: 100.0263 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 10\tNet Loss: 94.7332 \tQuestion Loss: 94.7332 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 10\tNet Loss: 91.2401 \tQuestion Loss: 91.2401 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 10\tNet Loss: 92.4592 \tQuestion Loss: 92.4592 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 10\tNet Loss: 93.5007 \tQuestion Loss: 93.5007 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 10\tNet Loss: 86.0005 \tQuestion Loss: 86.0005 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 10\tNet Loss: 97.1958 \tQuestion Loss: 97.1958 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 10\tNet Loss: 94.9069 \tQuestion Loss: 94.9069 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 10\tNet Loss: 93.5456 \tQuestion Loss: 93.5456 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 10\tNet Loss: 92.1045 \tQuestion Loss: 92.1045 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 10\tNet Loss: 89.1553 \tQuestion Loss: 89.1553 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 10\tNet Loss: 89.6260 \tQuestion Loss: 89.6260 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 10\tNet Loss: 94.3379 \tQuestion Loss: 94.3379 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 10\tNet Loss: 89.1583 \tQuestion Loss: 89.1583 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 10\tNet Loss: 86.8899 \tQuestion Loss: 86.8899 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 10\tNet Loss: 95.6189 \tQuestion Loss: 95.6189 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 10\tNet Loss: 98.6122 \tQuestion Loss: 98.6122 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 10\tNet Loss: 86.7353 \tQuestion Loss: 86.7353 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 10\tNet Loss: 96.4293 \tQuestion Loss: 96.4293 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 10\tNet Loss: 89.8992 \tQuestion Loss: 89.8992 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 150 \t Epoch : 10\tNet Loss: 91.0808 \tQuestion Loss: 91.0808 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 10\tNet Loss: 97.7782 \tQuestion Loss: 97.7782 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 10\tNet Loss: 93.4243 \tQuestion Loss: 93.4243 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 10\tNet Loss: 94.1315 \tQuestion Loss: 94.1315 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 10\tNet Loss: 85.4931 \tQuestion Loss: 85.4931 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 10\tNet Loss: 90.5315 \tQuestion Loss: 90.5315 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 10\tNet Loss: 93.4150 \tQuestion Loss: 93.4150 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 10\tNet Loss: 88.4180 \tQuestion Loss: 88.4180 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 10\tNet Loss: 93.6196 \tQuestion Loss: 93.6196 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 10\tNet Loss: 94.5518 \tQuestion Loss: 94.5518 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 10\tNet Loss: 95.8060 \tQuestion Loss: 95.8060 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 10\tNet Loss: 94.0826 \tQuestion Loss: 94.0826 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 10\tNet Loss: 95.8248 \tQuestion Loss: 95.8248 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 10\tNet Loss: 89.4315 \tQuestion Loss: 89.4315 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 10\tNet Loss: 88.4502 \tQuestion Loss: 88.4502 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 10\tNet Loss: 89.6440 \tQuestion Loss: 89.6440 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 10\tNet Loss: 87.3262 \tQuestion Loss: 87.3262 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 10\tNet Loss: 92.9303 \tQuestion Loss: 92.9303 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 10\tNet Loss: 91.5296 \tQuestion Loss: 91.5296 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 10\tNet Loss: 95.7920 \tQuestion Loss: 95.7920 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 10\tNet Loss: 93.5267 \tQuestion Loss: 93.5267 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 10\tNet Loss: 92.2977 \tQuestion Loss: 92.2977 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 10\tNet Loss: 89.1764 \tQuestion Loss: 89.1764 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 10\tNet Loss: 90.4685 \tQuestion Loss: 90.4685 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 10\tNet Loss: 94.8500 \tQuestion Loss: 94.8500 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 10\tNet Loss: 97.6476 \tQuestion Loss: 97.6476 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 10\tNet Loss: 90.1295 \tQuestion Loss: 90.1295 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 10\tNet Loss: 91.4713 \tQuestion Loss: 91.4713 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 10\tNet Loss: 90.3895 \tQuestion Loss: 90.3895 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 10\tNet Loss: 91.6550 \tQuestion Loss: 91.6550 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 10\tNet Loss: 90.0280 \tQuestion Loss: 90.0280 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 10\tNet Loss: 92.3628 \tQuestion Loss: 92.3628 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 10\tNet Loss: 94.2985 \tQuestion Loss: 94.2985 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 10\tNet Loss: 89.3430 \tQuestion Loss: 89.3430 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 10\tNet Loss: 93.1399 \tQuestion Loss: 93.1399 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 10\tNet Loss: 91.4514 \tQuestion Loss: 91.4514 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 10\tNet Loss: 94.0088 \tQuestion Loss: 94.0088 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 10\tNet Loss: 95.0974 \tQuestion Loss: 95.0974 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 10\tNet Loss: 89.9037 \tQuestion Loss: 89.9037 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 10\tNet Loss: 89.9365 \tQuestion Loss: 89.9365 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 10\tNet Loss: 91.0913 \tQuestion Loss: 91.0913 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 10\tNet Loss: 89.9687 \tQuestion Loss: 89.9687 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 10\tNet Loss: 94.7363 \tQuestion Loss: 94.7363 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 10\tNet Loss: 93.3109 \tQuestion Loss: 93.3109 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 10\tNet Loss: 94.9618 \tQuestion Loss: 94.9618 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 10\tNet Loss: 87.9122 \tQuestion Loss: 87.9122 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 10\tNet Loss: 89.1163 \tQuestion Loss: 89.1163 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 10\tNet Loss: 94.5390 \tQuestion Loss: 94.5390 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 10\tNet Loss: 87.6159 \tQuestion Loss: 87.6159 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 10\tNet Loss: 92.2494 \tQuestion Loss: 92.2494 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 10 : 92.3071 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 11\tNet Loss: 92.6195 \tQuestion Loss: 92.6195 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 11\tNet Loss: 89.0617 \tQuestion Loss: 89.0617 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 11\tNet Loss: 94.2119 \tQuestion Loss: 94.2119 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 11\tNet Loss: 93.5597 \tQuestion Loss: 93.5597 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 11\tNet Loss: 93.5400 \tQuestion Loss: 93.5400 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 11\tNet Loss: 93.5581 \tQuestion Loss: 93.5581 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 11\tNet Loss: 88.7110 \tQuestion Loss: 88.7110 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 11\tNet Loss: 94.5612 \tQuestion Loss: 94.5612 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 11\tNet Loss: 93.1431 \tQuestion Loss: 93.1431 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 11\tNet Loss: 94.8257 \tQuestion Loss: 94.8257 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 11\tNet Loss: 90.8981 \tQuestion Loss: 90.8981 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 11\tNet Loss: 95.6045 \tQuestion Loss: 95.6045 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 11\tNet Loss: 90.1405 \tQuestion Loss: 90.1405 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 11\tNet Loss: 92.5677 \tQuestion Loss: 92.5677 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 11\tNet Loss: 92.8785 \tQuestion Loss: 92.8785 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 11\tNet Loss: 95.3080 \tQuestion Loss: 95.3080 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 11\tNet Loss: 90.4116 \tQuestion Loss: 90.4116 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 11\tNet Loss: 92.2443 \tQuestion Loss: 92.2443 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 11\tNet Loss: 90.5121 \tQuestion Loss: 90.5121 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 11\tNet Loss: 86.7068 \tQuestion Loss: 86.7068 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 11\tNet Loss: 95.9810 \tQuestion Loss: 95.9810 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 11\tNet Loss: 88.0822 \tQuestion Loss: 88.0822 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 11\tNet Loss: 92.6983 \tQuestion Loss: 92.6983 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 11\tNet Loss: 95.6364 \tQuestion Loss: 95.6364 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 11\tNet Loss: 89.7293 \tQuestion Loss: 89.7293 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 11\tNet Loss: 93.1183 \tQuestion Loss: 93.1183 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 11\tNet Loss: 98.2684 \tQuestion Loss: 98.2684 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 11\tNet Loss: 91.7327 \tQuestion Loss: 91.7327 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 11\tNet Loss: 86.8894 \tQuestion Loss: 86.8894 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 11\tNet Loss: 89.8796 \tQuestion Loss: 89.8796 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 11\tNet Loss: 91.5335 \tQuestion Loss: 91.5335 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 11\tNet Loss: 90.2471 \tQuestion Loss: 90.2471 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 11\tNet Loss: 88.6164 \tQuestion Loss: 88.6164 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 11\tNet Loss: 93.2569 \tQuestion Loss: 93.2569 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 11\tNet Loss: 87.2936 \tQuestion Loss: 87.2936 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 11\tNet Loss: 90.0000 \tQuestion Loss: 90.0000 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 11\tNet Loss: 93.6654 \tQuestion Loss: 93.6654 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 11\tNet Loss: 95.1035 \tQuestion Loss: 95.1035 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 11\tNet Loss: 91.1372 \tQuestion Loss: 91.1372 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 11\tNet Loss: 93.9203 \tQuestion Loss: 93.9203 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 11\tNet Loss: 91.4634 \tQuestion Loss: 91.4634 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 41 \t Epoch : 11\tNet Loss: 90.1130 \tQuestion Loss: 90.1130 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 11\tNet Loss: 91.9118 \tQuestion Loss: 91.9118 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 11\tNet Loss: 93.2450 \tQuestion Loss: 93.2450 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 11\tNet Loss: 90.5715 \tQuestion Loss: 90.5715 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 11\tNet Loss: 93.7105 \tQuestion Loss: 93.7105 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 11\tNet Loss: 92.2395 \tQuestion Loss: 92.2395 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 11\tNet Loss: 97.2784 \tQuestion Loss: 97.2784 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 11\tNet Loss: 86.1754 \tQuestion Loss: 86.1754 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 11\tNet Loss: 93.9607 \tQuestion Loss: 93.9607 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 11\tNet Loss: 92.9285 \tQuestion Loss: 92.9285 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 11\tNet Loss: 88.4817 \tQuestion Loss: 88.4817 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 11\tNet Loss: 89.3527 \tQuestion Loss: 89.3527 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 11\tNet Loss: 89.7455 \tQuestion Loss: 89.7455 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 11\tNet Loss: 96.4667 \tQuestion Loss: 96.4667 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 11\tNet Loss: 93.1392 \tQuestion Loss: 93.1392 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 11\tNet Loss: 94.3362 \tQuestion Loss: 94.3362 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 11\tNet Loss: 96.5761 \tQuestion Loss: 96.5761 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 11\tNet Loss: 87.5624 \tQuestion Loss: 87.5624 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 11\tNet Loss: 92.2095 \tQuestion Loss: 92.2095 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 11\tNet Loss: 89.0204 \tQuestion Loss: 89.0204 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 11\tNet Loss: 95.1787 \tQuestion Loss: 95.1787 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 11\tNet Loss: 88.7580 \tQuestion Loss: 88.7580 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 11\tNet Loss: 93.5242 \tQuestion Loss: 93.5242 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 11\tNet Loss: 92.1224 \tQuestion Loss: 92.1224 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 11\tNet Loss: 90.4491 \tQuestion Loss: 90.4491 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 11\tNet Loss: 92.1115 \tQuestion Loss: 92.1115 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 11\tNet Loss: 91.2788 \tQuestion Loss: 91.2788 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 11\tNet Loss: 90.7126 \tQuestion Loss: 90.7126 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 11\tNet Loss: 95.5947 \tQuestion Loss: 95.5947 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 11\tNet Loss: 93.2198 \tQuestion Loss: 93.2198 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 11\tNet Loss: 92.7120 \tQuestion Loss: 92.7120 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 11\tNet Loss: 95.4174 \tQuestion Loss: 95.4174 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 11\tNet Loss: 93.6229 \tQuestion Loss: 93.6229 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 11\tNet Loss: 94.9902 \tQuestion Loss: 94.9902 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 11\tNet Loss: 93.6454 \tQuestion Loss: 93.6454 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 11\tNet Loss: 86.7916 \tQuestion Loss: 86.7916 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 11\tNet Loss: 91.4404 \tQuestion Loss: 91.4404 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 11\tNet Loss: 96.2448 \tQuestion Loss: 96.2448 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 11\tNet Loss: 92.8294 \tQuestion Loss: 92.8294 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 11\tNet Loss: 94.3028 \tQuestion Loss: 94.3028 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 11\tNet Loss: 92.2094 \tQuestion Loss: 92.2094 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 11\tNet Loss: 89.5975 \tQuestion Loss: 89.5975 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 11\tNet Loss: 88.6459 \tQuestion Loss: 88.6459 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 11\tNet Loss: 91.8949 \tQuestion Loss: 91.8949 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 11\tNet Loss: 90.3170 \tQuestion Loss: 90.3170 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 11\tNet Loss: 92.0298 \tQuestion Loss: 92.0298 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 11\tNet Loss: 91.6446 \tQuestion Loss: 91.6446 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 11\tNet Loss: 94.4774 \tQuestion Loss: 94.4774 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 11\tNet Loss: 90.0166 \tQuestion Loss: 90.0166 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 11\tNet Loss: 95.2656 \tQuestion Loss: 95.2656 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 11\tNet Loss: 92.9603 \tQuestion Loss: 92.9603 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 11\tNet Loss: 91.5254 \tQuestion Loss: 91.5254 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 11\tNet Loss: 91.9480 \tQuestion Loss: 91.9480 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 11\tNet Loss: 92.7077 \tQuestion Loss: 92.7077 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 11\tNet Loss: 93.4874 \tQuestion Loss: 93.4874 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 11\tNet Loss: 91.9430 \tQuestion Loss: 91.9430 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 11\tNet Loss: 95.6756 \tQuestion Loss: 95.6756 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 11\tNet Loss: 92.5603 \tQuestion Loss: 92.5603 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 11\tNet Loss: 93.1421 \tQuestion Loss: 93.1421 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 11\tNet Loss: 92.5982 \tQuestion Loss: 92.5982 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 11\tNet Loss: 95.5644 \tQuestion Loss: 95.5644 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 11\tNet Loss: 92.9801 \tQuestion Loss: 92.9801 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 11\tNet Loss: 92.4186 \tQuestion Loss: 92.4186 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 11\tNet Loss: 95.2523 \tQuestion Loss: 95.2523 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 11\tNet Loss: 96.3976 \tQuestion Loss: 96.3976 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 11\tNet Loss: 92.1288 \tQuestion Loss: 92.1288 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 11\tNet Loss: 89.2900 \tQuestion Loss: 89.2900 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 11\tNet Loss: 88.5055 \tQuestion Loss: 88.5055 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 11\tNet Loss: 92.8677 \tQuestion Loss: 92.8677 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 11\tNet Loss: 93.1005 \tQuestion Loss: 93.1005 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 11\tNet Loss: 87.8309 \tQuestion Loss: 87.8309 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 11\tNet Loss: 89.8275 \tQuestion Loss: 89.8275 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 11\tNet Loss: 97.2465 \tQuestion Loss: 97.2465 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 11\tNet Loss: 93.9517 \tQuestion Loss: 93.9517 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 11\tNet Loss: 90.8327 \tQuestion Loss: 90.8327 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 11\tNet Loss: 88.2342 \tQuestion Loss: 88.2342 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 11\tNet Loss: 90.6258 \tQuestion Loss: 90.6258 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 11\tNet Loss: 93.9718 \tQuestion Loss: 93.9718 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 11\tNet Loss: 89.6653 \tQuestion Loss: 89.6653 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 11\tNet Loss: 91.5838 \tQuestion Loss: 91.5838 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 11\tNet Loss: 93.7501 \tQuestion Loss: 93.7501 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 11\tNet Loss: 96.9803 \tQuestion Loss: 96.9803 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 11\tNet Loss: 92.7498 \tQuestion Loss: 92.7498 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 11\tNet Loss: 94.7568 \tQuestion Loss: 94.7568 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 11\tNet Loss: 92.7872 \tQuestion Loss: 92.7872 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 11\tNet Loss: 96.4609 \tQuestion Loss: 96.4609 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 11\tNet Loss: 88.0623 \tQuestion Loss: 88.0623 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 11\tNet Loss: 92.8523 \tQuestion Loss: 92.8523 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 11\tNet Loss: 93.4347 \tQuestion Loss: 93.4347 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 11\tNet Loss: 100.0436 \tQuestion Loss: 100.0436 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 11\tNet Loss: 94.8540 \tQuestion Loss: 94.8540 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 11\tNet Loss: 91.1854 \tQuestion Loss: 91.1854 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 133 \t Epoch : 11\tNet Loss: 92.8516 \tQuestion Loss: 92.8516 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 11\tNet Loss: 92.7518 \tQuestion Loss: 92.7518 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 11\tNet Loss: 85.9097 \tQuestion Loss: 85.9097 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 11\tNet Loss: 97.5200 \tQuestion Loss: 97.5200 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 11\tNet Loss: 95.3994 \tQuestion Loss: 95.3994 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 11\tNet Loss: 93.2720 \tQuestion Loss: 93.2720 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 11\tNet Loss: 92.1478 \tQuestion Loss: 92.1478 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 11\tNet Loss: 89.3132 \tQuestion Loss: 89.3132 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 11\tNet Loss: 89.7603 \tQuestion Loss: 89.7603 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 11\tNet Loss: 94.5551 \tQuestion Loss: 94.5551 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 11\tNet Loss: 89.5790 \tQuestion Loss: 89.5790 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 11\tNet Loss: 87.1054 \tQuestion Loss: 87.1054 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 11\tNet Loss: 95.4461 \tQuestion Loss: 95.4461 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 11\tNet Loss: 98.7169 \tQuestion Loss: 98.7169 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 11\tNet Loss: 86.9971 \tQuestion Loss: 86.9971 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 11\tNet Loss: 96.6096 \tQuestion Loss: 96.6096 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 11\tNet Loss: 90.2513 \tQuestion Loss: 90.2513 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 11\tNet Loss: 91.2910 \tQuestion Loss: 91.2910 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 11\tNet Loss: 98.1430 \tQuestion Loss: 98.1430 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 11\tNet Loss: 94.0845 \tQuestion Loss: 94.0845 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 11\tNet Loss: 94.2833 \tQuestion Loss: 94.2833 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 11\tNet Loss: 85.0318 \tQuestion Loss: 85.0318 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 11\tNet Loss: 90.4118 \tQuestion Loss: 90.4118 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 11\tNet Loss: 93.5675 \tQuestion Loss: 93.5675 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 11\tNet Loss: 88.3927 \tQuestion Loss: 88.3927 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 11\tNet Loss: 93.7800 \tQuestion Loss: 93.7800 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 11\tNet Loss: 94.8271 \tQuestion Loss: 94.8271 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 11\tNet Loss: 95.9663 \tQuestion Loss: 95.9663 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 11\tNet Loss: 94.3802 \tQuestion Loss: 94.3802 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 11\tNet Loss: 95.9031 \tQuestion Loss: 95.9031 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 11\tNet Loss: 89.6706 \tQuestion Loss: 89.6706 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 11\tNet Loss: 88.6150 \tQuestion Loss: 88.6150 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 11\tNet Loss: 89.8390 \tQuestion Loss: 89.8390 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 11\tNet Loss: 87.6556 \tQuestion Loss: 87.6556 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 11\tNet Loss: 92.8971 \tQuestion Loss: 92.8971 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 11\tNet Loss: 91.4686 \tQuestion Loss: 91.4686 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 11\tNet Loss: 95.9427 \tQuestion Loss: 95.9427 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 11\tNet Loss: 93.6848 \tQuestion Loss: 93.6848 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 11\tNet Loss: 92.6226 \tQuestion Loss: 92.6226 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 11\tNet Loss: 89.0323 \tQuestion Loss: 89.0323 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 11\tNet Loss: 90.7415 \tQuestion Loss: 90.7415 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 11\tNet Loss: 94.8599 \tQuestion Loss: 94.8599 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 11\tNet Loss: 97.8025 \tQuestion Loss: 97.8025 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 11\tNet Loss: 90.1821 \tQuestion Loss: 90.1821 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 11\tNet Loss: 91.7267 \tQuestion Loss: 91.7267 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 11\tNet Loss: 90.5594 \tQuestion Loss: 90.5594 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 11\tNet Loss: 91.9102 \tQuestion Loss: 91.9102 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 11\tNet Loss: 90.1715 \tQuestion Loss: 90.1715 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 11\tNet Loss: 92.5078 \tQuestion Loss: 92.5078 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 11\tNet Loss: 94.4400 \tQuestion Loss: 94.4400 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 11\tNet Loss: 89.5878 \tQuestion Loss: 89.5878 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 11\tNet Loss: 93.5480 \tQuestion Loss: 93.5480 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 11\tNet Loss: 91.7996 \tQuestion Loss: 91.7996 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 11\tNet Loss: 94.3395 \tQuestion Loss: 94.3395 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 11\tNet Loss: 95.2152 \tQuestion Loss: 95.2152 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 11\tNet Loss: 90.0016 \tQuestion Loss: 90.0016 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 11\tNet Loss: 89.7991 \tQuestion Loss: 89.7991 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 11\tNet Loss: 91.1984 \tQuestion Loss: 91.1984 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 11\tNet Loss: 90.0595 \tQuestion Loss: 90.0595 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 11\tNet Loss: 94.9789 \tQuestion Loss: 94.9789 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 11\tNet Loss: 93.4198 \tQuestion Loss: 93.4198 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 11\tNet Loss: 95.2124 \tQuestion Loss: 95.2124 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 11\tNet Loss: 88.0192 \tQuestion Loss: 88.0192 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 11\tNet Loss: 89.4291 \tQuestion Loss: 89.4291 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 11\tNet Loss: 94.8661 \tQuestion Loss: 94.8661 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 11\tNet Loss: 87.9946 \tQuestion Loss: 87.9946 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 11\tNet Loss: 92.5046 \tQuestion Loss: 92.5046 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 11 : 92.2934 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 12\tNet Loss: 92.7944 \tQuestion Loss: 92.7944 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 12\tNet Loss: 89.1673 \tQuestion Loss: 89.1673 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 12\tNet Loss: 94.2975 \tQuestion Loss: 94.2975 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 12\tNet Loss: 93.5095 \tQuestion Loss: 93.5095 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 12\tNet Loss: 93.8696 \tQuestion Loss: 93.8696 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 12\tNet Loss: 93.6651 \tQuestion Loss: 93.6651 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 12\tNet Loss: 88.7919 \tQuestion Loss: 88.7919 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 12\tNet Loss: 95.0200 \tQuestion Loss: 95.0200 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 12\tNet Loss: 93.6589 \tQuestion Loss: 93.6589 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 12\tNet Loss: 95.1622 \tQuestion Loss: 95.1622 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 12\tNet Loss: 91.2502 \tQuestion Loss: 91.2502 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 12\tNet Loss: 95.6031 \tQuestion Loss: 95.6031 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 12\tNet Loss: 90.5213 \tQuestion Loss: 90.5213 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 12\tNet Loss: 92.7217 \tQuestion Loss: 92.7217 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 12\tNet Loss: 93.1764 \tQuestion Loss: 93.1764 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 12\tNet Loss: 95.4863 \tQuestion Loss: 95.4863 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 12\tNet Loss: 90.4424 \tQuestion Loss: 90.4424 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 12\tNet Loss: 92.6237 \tQuestion Loss: 92.6237 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 12\tNet Loss: 91.0313 \tQuestion Loss: 91.0313 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 12\tNet Loss: 86.9124 \tQuestion Loss: 86.9124 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 12\tNet Loss: 96.0998 \tQuestion Loss: 96.0998 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 12\tNet Loss: 88.2035 \tQuestion Loss: 88.2035 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 12\tNet Loss: 92.8744 \tQuestion Loss: 92.8744 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 12\tNet Loss: 95.9765 \tQuestion Loss: 95.9765 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 24 \t Epoch : 12\tNet Loss: 89.8414 \tQuestion Loss: 89.8414 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 12\tNet Loss: 93.3066 \tQuestion Loss: 93.3066 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 12\tNet Loss: 98.4288 \tQuestion Loss: 98.4288 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 12\tNet Loss: 91.9495 \tQuestion Loss: 91.9495 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 12\tNet Loss: 87.0384 \tQuestion Loss: 87.0384 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 12\tNet Loss: 89.8293 \tQuestion Loss: 89.8293 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 12\tNet Loss: 91.5864 \tQuestion Loss: 91.5864 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 12\tNet Loss: 90.4494 \tQuestion Loss: 90.4494 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 12\tNet Loss: 89.1747 \tQuestion Loss: 89.1747 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 12\tNet Loss: 93.3341 \tQuestion Loss: 93.3341 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 12\tNet Loss: 87.4234 \tQuestion Loss: 87.4234 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 12\tNet Loss: 90.1416 \tQuestion Loss: 90.1416 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 12\tNet Loss: 94.0119 \tQuestion Loss: 94.0119 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 12\tNet Loss: 95.3047 \tQuestion Loss: 95.3047 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 12\tNet Loss: 91.1612 \tQuestion Loss: 91.1612 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 12\tNet Loss: 94.1002 \tQuestion Loss: 94.1002 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 12\tNet Loss: 91.5080 \tQuestion Loss: 91.5080 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 12\tNet Loss: 90.3597 \tQuestion Loss: 90.3597 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 12\tNet Loss: 92.1600 \tQuestion Loss: 92.1600 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 12\tNet Loss: 93.4482 \tQuestion Loss: 93.4482 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 12\tNet Loss: 90.6625 \tQuestion Loss: 90.6625 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 12\tNet Loss: 93.6231 \tQuestion Loss: 93.6231 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 12\tNet Loss: 92.1202 \tQuestion Loss: 92.1202 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 12\tNet Loss: 97.3464 \tQuestion Loss: 97.3464 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 12\tNet Loss: 86.7636 \tQuestion Loss: 86.7636 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 12\tNet Loss: 94.1424 \tQuestion Loss: 94.1424 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 12\tNet Loss: 92.9117 \tQuestion Loss: 92.9117 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 12\tNet Loss: 88.4952 \tQuestion Loss: 88.4952 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 12\tNet Loss: 89.6341 \tQuestion Loss: 89.6341 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 12\tNet Loss: 90.0499 \tQuestion Loss: 90.0499 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 12\tNet Loss: 96.4062 \tQuestion Loss: 96.4062 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 12\tNet Loss: 93.0757 \tQuestion Loss: 93.0757 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 12\tNet Loss: 94.4626 \tQuestion Loss: 94.4626 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 12\tNet Loss: 96.6626 \tQuestion Loss: 96.6626 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 12\tNet Loss: 87.6361 \tQuestion Loss: 87.6361 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 12\tNet Loss: 92.0352 \tQuestion Loss: 92.0352 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 12\tNet Loss: 89.0883 \tQuestion Loss: 89.0883 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 12\tNet Loss: 95.2620 \tQuestion Loss: 95.2620 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 12\tNet Loss: 88.7970 \tQuestion Loss: 88.7970 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 12\tNet Loss: 93.4058 \tQuestion Loss: 93.4058 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 12\tNet Loss: 92.1009 \tQuestion Loss: 92.1009 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 12\tNet Loss: 90.5991 \tQuestion Loss: 90.5991 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 12\tNet Loss: 92.5203 \tQuestion Loss: 92.5203 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 12\tNet Loss: 91.9728 \tQuestion Loss: 91.9728 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 12\tNet Loss: 90.9077 \tQuestion Loss: 90.9077 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 12\tNet Loss: 95.8086 \tQuestion Loss: 95.8086 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 12\tNet Loss: 93.1826 \tQuestion Loss: 93.1826 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 12\tNet Loss: 92.7060 \tQuestion Loss: 92.7060 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 12\tNet Loss: 95.5399 \tQuestion Loss: 95.5399 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 12\tNet Loss: 93.6210 \tQuestion Loss: 93.6210 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 12\tNet Loss: 94.8321 \tQuestion Loss: 94.8321 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 12\tNet Loss: 93.5018 \tQuestion Loss: 93.5018 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 12\tNet Loss: 86.7801 \tQuestion Loss: 86.7801 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 12\tNet Loss: 91.4743 \tQuestion Loss: 91.4743 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 12\tNet Loss: 96.8677 \tQuestion Loss: 96.8677 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 12\tNet Loss: 92.8967 \tQuestion Loss: 92.8967 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 12\tNet Loss: 94.1670 \tQuestion Loss: 94.1670 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 12\tNet Loss: 92.0556 \tQuestion Loss: 92.0556 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 12\tNet Loss: 89.6099 \tQuestion Loss: 89.6099 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 12\tNet Loss: 88.5538 \tQuestion Loss: 88.5538 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 12\tNet Loss: 91.9493 \tQuestion Loss: 91.9493 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 12\tNet Loss: 90.2798 \tQuestion Loss: 90.2798 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 12\tNet Loss: 92.4308 \tQuestion Loss: 92.4308 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 12\tNet Loss: 91.6743 \tQuestion Loss: 91.6743 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 12\tNet Loss: 94.5081 \tQuestion Loss: 94.5081 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 12\tNet Loss: 90.0967 \tQuestion Loss: 90.0967 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 12\tNet Loss: 95.1604 \tQuestion Loss: 95.1604 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 12\tNet Loss: 92.7867 \tQuestion Loss: 92.7867 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 12\tNet Loss: 91.2335 \tQuestion Loss: 91.2335 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 12\tNet Loss: 91.9260 \tQuestion Loss: 91.9260 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 12\tNet Loss: 92.6212 \tQuestion Loss: 92.6212 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 12\tNet Loss: 93.3456 \tQuestion Loss: 93.3456 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 12\tNet Loss: 91.7044 \tQuestion Loss: 91.7044 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 12\tNet Loss: 95.7077 \tQuestion Loss: 95.7077 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 12\tNet Loss: 92.7665 \tQuestion Loss: 92.7665 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 12\tNet Loss: 92.9445 \tQuestion Loss: 92.9445 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 12\tNet Loss: 92.5418 \tQuestion Loss: 92.5418 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 12\tNet Loss: 95.4996 \tQuestion Loss: 95.4996 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 12\tNet Loss: 92.9853 \tQuestion Loss: 92.9853 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 12\tNet Loss: 92.3762 \tQuestion Loss: 92.3762 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 12\tNet Loss: 94.9533 \tQuestion Loss: 94.9533 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 12\tNet Loss: 96.6577 \tQuestion Loss: 96.6577 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 12\tNet Loss: 92.2367 \tQuestion Loss: 92.2367 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 12\tNet Loss: 89.2240 \tQuestion Loss: 89.2240 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 12\tNet Loss: 88.3681 \tQuestion Loss: 88.3681 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 12\tNet Loss: 92.8381 \tQuestion Loss: 92.8381 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 12\tNet Loss: 93.4070 \tQuestion Loss: 93.4070 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 12\tNet Loss: 87.6402 \tQuestion Loss: 87.6402 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 12\tNet Loss: 89.7051 \tQuestion Loss: 89.7051 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 12\tNet Loss: 96.9456 \tQuestion Loss: 96.9456 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 12\tNet Loss: 94.3579 \tQuestion Loss: 94.3579 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 12\tNet Loss: 91.4386 \tQuestion Loss: 91.4386 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 116 \t Epoch : 12\tNet Loss: 88.3402 \tQuestion Loss: 88.3402 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 12\tNet Loss: 90.4625 \tQuestion Loss: 90.4625 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 12\tNet Loss: 93.7775 \tQuestion Loss: 93.7775 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 12\tNet Loss: 89.6655 \tQuestion Loss: 89.6655 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 12\tNet Loss: 91.4961 \tQuestion Loss: 91.4961 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 12\tNet Loss: 93.4600 \tQuestion Loss: 93.4600 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 12\tNet Loss: 97.5079 \tQuestion Loss: 97.5079 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 12\tNet Loss: 92.8163 \tQuestion Loss: 92.8163 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 12\tNet Loss: 94.7201 \tQuestion Loss: 94.7201 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 12\tNet Loss: 92.4806 \tQuestion Loss: 92.4806 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 12\tNet Loss: 95.9962 \tQuestion Loss: 95.9962 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 12\tNet Loss: 87.9036 \tQuestion Loss: 87.9036 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 12\tNet Loss: 92.7300 \tQuestion Loss: 92.7300 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 12\tNet Loss: 94.6102 \tQuestion Loss: 94.6102 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 12\tNet Loss: 99.9882 \tQuestion Loss: 99.9882 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 12\tNet Loss: 94.7539 \tQuestion Loss: 94.7539 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 12\tNet Loss: 91.2812 \tQuestion Loss: 91.2812 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 12\tNet Loss: 92.5071 \tQuestion Loss: 92.5071 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 12\tNet Loss: 93.4602 \tQuestion Loss: 93.4602 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 12\tNet Loss: 85.9347 \tQuestion Loss: 85.9347 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 12\tNet Loss: 97.2432 \tQuestion Loss: 97.2432 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 12\tNet Loss: 94.8154 \tQuestion Loss: 94.8154 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 12\tNet Loss: 93.4860 \tQuestion Loss: 93.4860 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 12\tNet Loss: 92.0585 \tQuestion Loss: 92.0585 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 12\tNet Loss: 89.1723 \tQuestion Loss: 89.1723 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 12\tNet Loss: 89.6505 \tQuestion Loss: 89.6505 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 12\tNet Loss: 94.3798 \tQuestion Loss: 94.3798 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 12\tNet Loss: 89.2572 \tQuestion Loss: 89.2572 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 12\tNet Loss: 86.9368 \tQuestion Loss: 86.9368 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 12\tNet Loss: 95.5708 \tQuestion Loss: 95.5708 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 12\tNet Loss: 98.5551 \tQuestion Loss: 98.5551 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 12\tNet Loss: 86.8150 \tQuestion Loss: 86.8150 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 12\tNet Loss: 96.5121 \tQuestion Loss: 96.5121 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 12\tNet Loss: 89.9049 \tQuestion Loss: 89.9049 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 12\tNet Loss: 91.0918 \tQuestion Loss: 91.0918 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 12\tNet Loss: 97.7890 \tQuestion Loss: 97.7890 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 12\tNet Loss: 93.3470 \tQuestion Loss: 93.3470 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 12\tNet Loss: 94.0589 \tQuestion Loss: 94.0589 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 12\tNet Loss: 85.4458 \tQuestion Loss: 85.4458 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 12\tNet Loss: 90.6034 \tQuestion Loss: 90.6034 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 12\tNet Loss: 93.4554 \tQuestion Loss: 93.4554 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 12\tNet Loss: 88.4401 \tQuestion Loss: 88.4401 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 12\tNet Loss: 93.6276 \tQuestion Loss: 93.6276 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 12\tNet Loss: 94.5950 \tQuestion Loss: 94.5950 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 12\tNet Loss: 95.8275 \tQuestion Loss: 95.8275 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 12\tNet Loss: 94.0513 \tQuestion Loss: 94.0513 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 12\tNet Loss: 95.8249 \tQuestion Loss: 95.8249 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 12\tNet Loss: 89.6087 \tQuestion Loss: 89.6087 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 12\tNet Loss: 88.4686 \tQuestion Loss: 88.4686 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 12\tNet Loss: 89.6429 \tQuestion Loss: 89.6429 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 12\tNet Loss: 87.3259 \tQuestion Loss: 87.3259 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 12\tNet Loss: 92.9412 \tQuestion Loss: 92.9412 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 12\tNet Loss: 91.5398 \tQuestion Loss: 91.5398 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 12\tNet Loss: 95.7906 \tQuestion Loss: 95.7906 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 12\tNet Loss: 93.4841 \tQuestion Loss: 93.4841 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 12\tNet Loss: 92.4029 \tQuestion Loss: 92.4029 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 12\tNet Loss: 89.2073 \tQuestion Loss: 89.2073 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 12\tNet Loss: 90.4960 \tQuestion Loss: 90.4960 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 12\tNet Loss: 94.9482 \tQuestion Loss: 94.9482 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 12\tNet Loss: 97.6442 \tQuestion Loss: 97.6442 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 12\tNet Loss: 90.1109 \tQuestion Loss: 90.1109 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 12\tNet Loss: 91.4651 \tQuestion Loss: 91.4651 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 12\tNet Loss: 90.4226 \tQuestion Loss: 90.4226 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 12\tNet Loss: 91.6474 \tQuestion Loss: 91.6474 \t Time Taken: 1 seconds\n",
      "Batch: 180 \t Epoch : 12\tNet Loss: 90.1092 \tQuestion Loss: 90.1092 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 12\tNet Loss: 92.3638 \tQuestion Loss: 92.3638 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 12\tNet Loss: 94.3007 \tQuestion Loss: 94.3007 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 12\tNet Loss: 89.4103 \tQuestion Loss: 89.4103 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 12\tNet Loss: 93.2054 \tQuestion Loss: 93.2054 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 12\tNet Loss: 91.4874 \tQuestion Loss: 91.4874 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 12\tNet Loss: 93.9805 \tQuestion Loss: 93.9805 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 12\tNet Loss: 95.0749 \tQuestion Loss: 95.0749 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 12\tNet Loss: 89.8994 \tQuestion Loss: 89.8994 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 12\tNet Loss: 89.9000 \tQuestion Loss: 89.9000 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 12\tNet Loss: 91.0878 \tQuestion Loss: 91.0878 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 12\tNet Loss: 89.9836 \tQuestion Loss: 89.9836 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 12\tNet Loss: 94.7954 \tQuestion Loss: 94.7954 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 12\tNet Loss: 93.3920 \tQuestion Loss: 93.3920 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 12\tNet Loss: 95.0837 \tQuestion Loss: 95.0837 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 12\tNet Loss: 88.0091 \tQuestion Loss: 88.0091 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 12\tNet Loss: 89.1253 \tQuestion Loss: 89.1253 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 12\tNet Loss: 94.5277 \tQuestion Loss: 94.5277 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 12\tNet Loss: 87.6100 \tQuestion Loss: 87.6100 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 12\tNet Loss: 92.3007 \tQuestion Loss: 92.3007 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 12 : 92.3060 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 13\tNet Loss: 92.6355 \tQuestion Loss: 92.6355 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 13\tNet Loss: 89.0201 \tQuestion Loss: 89.0201 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 13\tNet Loss: 94.2425 \tQuestion Loss: 94.2425 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 13\tNet Loss: 93.5516 \tQuestion Loss: 93.5516 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 13\tNet Loss: 93.5897 \tQuestion Loss: 93.5897 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 13\tNet Loss: 93.6012 \tQuestion Loss: 93.6012 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 13\tNet Loss: 88.8169 \tQuestion Loss: 88.8169 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 7 \t Epoch : 13\tNet Loss: 94.7121 \tQuestion Loss: 94.7121 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 13\tNet Loss: 93.2075 \tQuestion Loss: 93.2075 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 13\tNet Loss: 94.8166 \tQuestion Loss: 94.8166 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 13\tNet Loss: 90.9159 \tQuestion Loss: 90.9159 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 13\tNet Loss: 95.6368 \tQuestion Loss: 95.6368 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 13\tNet Loss: 90.2094 \tQuestion Loss: 90.2094 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 13\tNet Loss: 92.6581 \tQuestion Loss: 92.6581 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 13\tNet Loss: 92.9422 \tQuestion Loss: 92.9422 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 13\tNet Loss: 95.4178 \tQuestion Loss: 95.4178 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 13\tNet Loss: 90.4694 \tQuestion Loss: 90.4694 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 13\tNet Loss: 92.2868 \tQuestion Loss: 92.2868 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 13\tNet Loss: 90.5186 \tQuestion Loss: 90.5186 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 13\tNet Loss: 86.7378 \tQuestion Loss: 86.7378 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 13\tNet Loss: 96.0378 \tQuestion Loss: 96.0378 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 13\tNet Loss: 88.1408 \tQuestion Loss: 88.1408 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 13\tNet Loss: 92.7939 \tQuestion Loss: 92.7939 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 13\tNet Loss: 95.7182 \tQuestion Loss: 95.7182 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 13\tNet Loss: 89.7871 \tQuestion Loss: 89.7871 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 13\tNet Loss: 93.1358 \tQuestion Loss: 93.1358 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 13\tNet Loss: 98.3271 \tQuestion Loss: 98.3271 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 13\tNet Loss: 91.7911 \tQuestion Loss: 91.7911 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 13\tNet Loss: 86.9369 \tQuestion Loss: 86.9369 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 13\tNet Loss: 89.9278 \tQuestion Loss: 89.9278 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 13\tNet Loss: 91.6122 \tQuestion Loss: 91.6122 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 13\tNet Loss: 90.2989 \tQuestion Loss: 90.2989 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 13\tNet Loss: 88.6644 \tQuestion Loss: 88.6644 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 13\tNet Loss: 93.2912 \tQuestion Loss: 93.2912 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 13\tNet Loss: 87.3253 \tQuestion Loss: 87.3253 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 13\tNet Loss: 90.0525 \tQuestion Loss: 90.0525 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 13\tNet Loss: 93.7605 \tQuestion Loss: 93.7605 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 13\tNet Loss: 95.1000 \tQuestion Loss: 95.1000 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 13\tNet Loss: 91.1707 \tQuestion Loss: 91.1707 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 13\tNet Loss: 93.9408 \tQuestion Loss: 93.9408 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 13\tNet Loss: 91.4837 \tQuestion Loss: 91.4837 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 13\tNet Loss: 90.1237 \tQuestion Loss: 90.1237 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 13\tNet Loss: 91.9963 \tQuestion Loss: 91.9963 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 13\tNet Loss: 93.3018 \tQuestion Loss: 93.3018 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 13\tNet Loss: 90.6464 \tQuestion Loss: 90.6464 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 13\tNet Loss: 93.7508 \tQuestion Loss: 93.7508 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 13\tNet Loss: 92.2809 \tQuestion Loss: 92.2809 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 13\tNet Loss: 97.3146 \tQuestion Loss: 97.3146 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 13\tNet Loss: 86.1255 \tQuestion Loss: 86.1255 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 13\tNet Loss: 94.0052 \tQuestion Loss: 94.0052 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 13\tNet Loss: 92.9968 \tQuestion Loss: 92.9968 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 13\tNet Loss: 88.5361 \tQuestion Loss: 88.5361 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 13\tNet Loss: 89.4192 \tQuestion Loss: 89.4192 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 13\tNet Loss: 89.7805 \tQuestion Loss: 89.7805 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 13\tNet Loss: 96.4568 \tQuestion Loss: 96.4568 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 13\tNet Loss: 93.2150 \tQuestion Loss: 93.2150 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 13\tNet Loss: 94.3574 \tQuestion Loss: 94.3574 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 13\tNet Loss: 96.7576 \tQuestion Loss: 96.7576 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 13\tNet Loss: 87.6335 \tQuestion Loss: 87.6335 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 13\tNet Loss: 92.2995 \tQuestion Loss: 92.2995 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 13\tNet Loss: 89.0921 \tQuestion Loss: 89.0921 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 13\tNet Loss: 95.2513 \tQuestion Loss: 95.2513 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 13\tNet Loss: 88.8044 \tQuestion Loss: 88.8044 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 13\tNet Loss: 93.5961 \tQuestion Loss: 93.5961 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 13\tNet Loss: 92.2012 \tQuestion Loss: 92.2012 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 13\tNet Loss: 90.5652 \tQuestion Loss: 90.5652 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 13\tNet Loss: 92.1564 \tQuestion Loss: 92.1564 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 13\tNet Loss: 91.2817 \tQuestion Loss: 91.2817 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 13\tNet Loss: 90.8302 \tQuestion Loss: 90.8302 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 13\tNet Loss: 95.6463 \tQuestion Loss: 95.6463 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 13\tNet Loss: 93.2374 \tQuestion Loss: 93.2374 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 13\tNet Loss: 92.7147 \tQuestion Loss: 92.7147 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 13\tNet Loss: 95.4854 \tQuestion Loss: 95.4854 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 13\tNet Loss: 93.6855 \tQuestion Loss: 93.6855 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 13\tNet Loss: 95.0121 \tQuestion Loss: 95.0121 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 13\tNet Loss: 93.6365 \tQuestion Loss: 93.6365 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 13\tNet Loss: 86.8378 \tQuestion Loss: 86.8378 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 13\tNet Loss: 91.4726 \tQuestion Loss: 91.4726 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 13\tNet Loss: 96.3051 \tQuestion Loss: 96.3051 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 13\tNet Loss: 92.9317 \tQuestion Loss: 92.9317 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 13\tNet Loss: 94.3380 \tQuestion Loss: 94.3380 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 13\tNet Loss: 92.1905 \tQuestion Loss: 92.1905 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 13\tNet Loss: 89.6035 \tQuestion Loss: 89.6035 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 13\tNet Loss: 88.6856 \tQuestion Loss: 88.6856 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 13\tNet Loss: 91.9557 \tQuestion Loss: 91.9557 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 13\tNet Loss: 90.3554 \tQuestion Loss: 90.3554 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 13\tNet Loss: 92.0639 \tQuestion Loss: 92.0639 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 13\tNet Loss: 91.7106 \tQuestion Loss: 91.7106 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 13\tNet Loss: 94.4381 \tQuestion Loss: 94.4381 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 13\tNet Loss: 89.9941 \tQuestion Loss: 89.9941 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 13\tNet Loss: 95.3000 \tQuestion Loss: 95.3000 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 13\tNet Loss: 93.0135 \tQuestion Loss: 93.0135 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 13\tNet Loss: 91.5407 \tQuestion Loss: 91.5407 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 13\tNet Loss: 92.0375 \tQuestion Loss: 92.0375 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 13\tNet Loss: 92.7350 \tQuestion Loss: 92.7350 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 13\tNet Loss: 93.4478 \tQuestion Loss: 93.4478 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 13\tNet Loss: 91.9620 \tQuestion Loss: 91.9620 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 13\tNet Loss: 95.7970 \tQuestion Loss: 95.7970 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 13\tNet Loss: 92.6549 \tQuestion Loss: 92.6549 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 13\tNet Loss: 93.1764 \tQuestion Loss: 93.1764 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 100 \t Epoch : 13\tNet Loss: 92.5552 \tQuestion Loss: 92.5552 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 13\tNet Loss: 95.5623 \tQuestion Loss: 95.5623 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 13\tNet Loss: 93.0204 \tQuestion Loss: 93.0204 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 13\tNet Loss: 92.4967 \tQuestion Loss: 92.4967 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 13\tNet Loss: 95.2864 \tQuestion Loss: 95.2864 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 13\tNet Loss: 96.4468 \tQuestion Loss: 96.4468 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 13\tNet Loss: 92.1406 \tQuestion Loss: 92.1406 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 13\tNet Loss: 89.2632 \tQuestion Loss: 89.2632 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 13\tNet Loss: 88.4479 \tQuestion Loss: 88.4479 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 13\tNet Loss: 92.9016 \tQuestion Loss: 92.9016 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 13\tNet Loss: 93.1953 \tQuestion Loss: 93.1953 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 13\tNet Loss: 87.8771 \tQuestion Loss: 87.8771 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 13\tNet Loss: 89.8107 \tQuestion Loss: 89.8107 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 13\tNet Loss: 97.2204 \tQuestion Loss: 97.2204 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 13\tNet Loss: 93.9840 \tQuestion Loss: 93.9840 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 13\tNet Loss: 90.8950 \tQuestion Loss: 90.8950 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 13\tNet Loss: 88.3004 \tQuestion Loss: 88.3004 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 13\tNet Loss: 90.6522 \tQuestion Loss: 90.6522 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 13\tNet Loss: 93.9341 \tQuestion Loss: 93.9341 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 13\tNet Loss: 89.6358 \tQuestion Loss: 89.6358 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 13\tNet Loss: 91.5552 \tQuestion Loss: 91.5552 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 13\tNet Loss: 93.7796 \tQuestion Loss: 93.7796 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 13\tNet Loss: 96.9751 \tQuestion Loss: 96.9751 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 13\tNet Loss: 92.7511 \tQuestion Loss: 92.7511 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 13\tNet Loss: 94.7652 \tQuestion Loss: 94.7652 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 13\tNet Loss: 92.8395 \tQuestion Loss: 92.8395 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 13\tNet Loss: 96.4337 \tQuestion Loss: 96.4337 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 13\tNet Loss: 88.0624 \tQuestion Loss: 88.0624 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 13\tNet Loss: 92.9720 \tQuestion Loss: 92.9720 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 13\tNet Loss: 93.3351 \tQuestion Loss: 93.3351 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 13\tNet Loss: 100.0972 \tQuestion Loss: 100.0972 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 13\tNet Loss: 94.8474 \tQuestion Loss: 94.8474 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 13\tNet Loss: 91.1351 \tQuestion Loss: 91.1351 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 13\tNet Loss: 92.8863 \tQuestion Loss: 92.8863 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 13\tNet Loss: 92.8112 \tQuestion Loss: 92.8112 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 13\tNet Loss: 86.0381 \tQuestion Loss: 86.0381 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 13\tNet Loss: 97.4894 \tQuestion Loss: 97.4894 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 13\tNet Loss: 95.4272 \tQuestion Loss: 95.4272 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 13\tNet Loss: 93.2020 \tQuestion Loss: 93.2020 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 13\tNet Loss: 92.1097 \tQuestion Loss: 92.1097 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 13\tNet Loss: 89.3082 \tQuestion Loss: 89.3082 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 13\tNet Loss: 89.8108 \tQuestion Loss: 89.8108 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 13\tNet Loss: 94.5446 \tQuestion Loss: 94.5446 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 13\tNet Loss: 89.5777 \tQuestion Loss: 89.5777 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 13\tNet Loss: 87.1075 \tQuestion Loss: 87.1075 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 13\tNet Loss: 95.3795 \tQuestion Loss: 95.3795 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 13\tNet Loss: 98.7380 \tQuestion Loss: 98.7380 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 13\tNet Loss: 87.0148 \tQuestion Loss: 87.0148 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 13\tNet Loss: 96.5615 \tQuestion Loss: 96.5615 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 13\tNet Loss: 90.2181 \tQuestion Loss: 90.2181 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 13\tNet Loss: 91.2618 \tQuestion Loss: 91.2618 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 13\tNet Loss: 98.1061 \tQuestion Loss: 98.1061 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 13\tNet Loss: 94.0961 \tQuestion Loss: 94.0961 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 13\tNet Loss: 94.3912 \tQuestion Loss: 94.3912 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 13\tNet Loss: 85.0892 \tQuestion Loss: 85.0892 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 13\tNet Loss: 90.5296 \tQuestion Loss: 90.5296 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 13\tNet Loss: 93.5535 \tQuestion Loss: 93.5535 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 13\tNet Loss: 88.3543 \tQuestion Loss: 88.3543 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 13\tNet Loss: 93.7062 \tQuestion Loss: 93.7062 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 13\tNet Loss: 94.8621 \tQuestion Loss: 94.8621 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 13\tNet Loss: 95.9802 \tQuestion Loss: 95.9802 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 13\tNet Loss: 94.3999 \tQuestion Loss: 94.3999 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 13\tNet Loss: 96.0129 \tQuestion Loss: 96.0129 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 13\tNet Loss: 89.7163 \tQuestion Loss: 89.7163 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 13\tNet Loss: 88.5992 \tQuestion Loss: 88.5992 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 13\tNet Loss: 89.8288 \tQuestion Loss: 89.8288 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 13\tNet Loss: 87.6113 \tQuestion Loss: 87.6113 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 13\tNet Loss: 92.9285 \tQuestion Loss: 92.9285 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 13\tNet Loss: 91.4964 \tQuestion Loss: 91.4964 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 13\tNet Loss: 95.8947 \tQuestion Loss: 95.8947 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 13\tNet Loss: 93.6353 \tQuestion Loss: 93.6353 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 13\tNet Loss: 92.6408 \tQuestion Loss: 92.6408 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 13\tNet Loss: 89.0379 \tQuestion Loss: 89.0379 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 13\tNet Loss: 90.8065 \tQuestion Loss: 90.8065 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 13\tNet Loss: 94.8896 \tQuestion Loss: 94.8896 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 13\tNet Loss: 97.8027 \tQuestion Loss: 97.8027 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 13\tNet Loss: 90.1642 \tQuestion Loss: 90.1642 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 13\tNet Loss: 91.6759 \tQuestion Loss: 91.6759 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 13\tNet Loss: 90.5307 \tQuestion Loss: 90.5307 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 13\tNet Loss: 91.9836 \tQuestion Loss: 91.9836 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 13\tNet Loss: 90.1811 \tQuestion Loss: 90.1811 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 13\tNet Loss: 92.4978 \tQuestion Loss: 92.4978 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 13\tNet Loss: 94.3913 \tQuestion Loss: 94.3913 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 13\tNet Loss: 89.6346 \tQuestion Loss: 89.6346 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 13\tNet Loss: 93.5226 \tQuestion Loss: 93.5226 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 13\tNet Loss: 91.7435 \tQuestion Loss: 91.7435 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 13\tNet Loss: 94.3763 \tQuestion Loss: 94.3763 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 13\tNet Loss: 95.2299 \tQuestion Loss: 95.2299 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 13\tNet Loss: 90.0098 \tQuestion Loss: 90.0098 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 13\tNet Loss: 89.7825 \tQuestion Loss: 89.7825 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 13\tNet Loss: 91.2080 \tQuestion Loss: 91.2080 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 13\tNet Loss: 90.0863 \tQuestion Loss: 90.0863 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 192 \t Epoch : 13\tNet Loss: 94.9886 \tQuestion Loss: 94.9886 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 13\tNet Loss: 93.4057 \tQuestion Loss: 93.4057 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 13\tNet Loss: 95.0578 \tQuestion Loss: 95.0578 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 13\tNet Loss: 87.9887 \tQuestion Loss: 87.9887 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 13\tNet Loss: 89.3669 \tQuestion Loss: 89.3669 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 13\tNet Loss: 94.8062 \tQuestion Loss: 94.8062 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 13\tNet Loss: 87.9988 \tQuestion Loss: 87.9988 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 13\tNet Loss: 92.4931 \tQuestion Loss: 92.4931 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 13 : 92.3189 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 14\tNet Loss: 92.7974 \tQuestion Loss: 92.7974 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 14\tNet Loss: 89.1448 \tQuestion Loss: 89.1448 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 14\tNet Loss: 94.3237 \tQuestion Loss: 94.3237 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 14\tNet Loss: 93.5306 \tQuestion Loss: 93.5306 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 14\tNet Loss: 93.8718 \tQuestion Loss: 93.8718 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 14\tNet Loss: 93.7720 \tQuestion Loss: 93.7720 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 14\tNet Loss: 88.8342 \tQuestion Loss: 88.8342 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 14\tNet Loss: 95.0709 \tQuestion Loss: 95.0709 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 14\tNet Loss: 93.6115 \tQuestion Loss: 93.6115 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 14\tNet Loss: 95.1722 \tQuestion Loss: 95.1722 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 14\tNet Loss: 91.2343 \tQuestion Loss: 91.2343 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 14\tNet Loss: 95.6162 \tQuestion Loss: 95.6162 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 14\tNet Loss: 90.4741 \tQuestion Loss: 90.4741 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 14\tNet Loss: 92.7534 \tQuestion Loss: 92.7534 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 14\tNet Loss: 93.1732 \tQuestion Loss: 93.1732 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 14\tNet Loss: 95.4967 \tQuestion Loss: 95.4967 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 14\tNet Loss: 90.4322 \tQuestion Loss: 90.4322 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 14\tNet Loss: 92.6096 \tQuestion Loss: 92.6096 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 14\tNet Loss: 91.0626 \tQuestion Loss: 91.0626 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 14\tNet Loss: 86.8773 \tQuestion Loss: 86.8773 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 14\tNet Loss: 96.1252 \tQuestion Loss: 96.1252 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 14\tNet Loss: 88.1931 \tQuestion Loss: 88.1931 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 14\tNet Loss: 92.8296 \tQuestion Loss: 92.8296 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 14\tNet Loss: 95.9538 \tQuestion Loss: 95.9538 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 14\tNet Loss: 89.8059 \tQuestion Loss: 89.8059 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 14\tNet Loss: 93.2912 \tQuestion Loss: 93.2912 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 14\tNet Loss: 98.4040 \tQuestion Loss: 98.4040 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 14\tNet Loss: 91.9409 \tQuestion Loss: 91.9409 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 14\tNet Loss: 87.0337 \tQuestion Loss: 87.0337 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 14\tNet Loss: 89.8457 \tQuestion Loss: 89.8457 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 14\tNet Loss: 91.5390 \tQuestion Loss: 91.5390 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 14\tNet Loss: 90.4197 \tQuestion Loss: 90.4197 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 14\tNet Loss: 89.1473 \tQuestion Loss: 89.1473 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 14\tNet Loss: 93.3125 \tQuestion Loss: 93.3125 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 14\tNet Loss: 87.4077 \tQuestion Loss: 87.4077 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 14\tNet Loss: 90.1021 \tQuestion Loss: 90.1021 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 14\tNet Loss: 93.9431 \tQuestion Loss: 93.9431 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 14\tNet Loss: 95.2646 \tQuestion Loss: 95.2646 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 14\tNet Loss: 91.1550 \tQuestion Loss: 91.1550 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 14\tNet Loss: 94.1517 \tQuestion Loss: 94.1517 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 14\tNet Loss: 91.5047 \tQuestion Loss: 91.5047 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 14\tNet Loss: 90.3524 \tQuestion Loss: 90.3524 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 14\tNet Loss: 92.1860 \tQuestion Loss: 92.1860 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 14\tNet Loss: 93.4467 \tQuestion Loss: 93.4467 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 14\tNet Loss: 90.6689 \tQuestion Loss: 90.6689 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 14\tNet Loss: 93.6038 \tQuestion Loss: 93.6038 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 14\tNet Loss: 92.1279 \tQuestion Loss: 92.1279 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 14\tNet Loss: 97.2885 \tQuestion Loss: 97.2885 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 14\tNet Loss: 86.7712 \tQuestion Loss: 86.7712 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 14\tNet Loss: 94.1978 \tQuestion Loss: 94.1978 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 14\tNet Loss: 92.9310 \tQuestion Loss: 92.9310 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 14\tNet Loss: 88.4784 \tQuestion Loss: 88.4784 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 14\tNet Loss: 89.6128 \tQuestion Loss: 89.6128 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 14\tNet Loss: 90.0272 \tQuestion Loss: 90.0272 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 14\tNet Loss: 96.3882 \tQuestion Loss: 96.3882 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 14\tNet Loss: 93.0629 \tQuestion Loss: 93.0629 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 14\tNet Loss: 94.4256 \tQuestion Loss: 94.4256 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 14\tNet Loss: 96.6218 \tQuestion Loss: 96.6218 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 14\tNet Loss: 87.6147 \tQuestion Loss: 87.6147 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 14\tNet Loss: 92.0485 \tQuestion Loss: 92.0485 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 14\tNet Loss: 89.1345 \tQuestion Loss: 89.1345 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 14\tNet Loss: 95.2547 \tQuestion Loss: 95.2547 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 14\tNet Loss: 88.8410 \tQuestion Loss: 88.8410 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 14\tNet Loss: 93.4428 \tQuestion Loss: 93.4428 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 14\tNet Loss: 92.0361 \tQuestion Loss: 92.0361 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 14\tNet Loss: 90.4951 \tQuestion Loss: 90.4951 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 14\tNet Loss: 92.5653 \tQuestion Loss: 92.5653 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 14\tNet Loss: 92.0432 \tQuestion Loss: 92.0432 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 14\tNet Loss: 90.8654 \tQuestion Loss: 90.8654 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 14\tNet Loss: 95.8325 \tQuestion Loss: 95.8325 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 14\tNet Loss: 93.1566 \tQuestion Loss: 93.1566 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 14\tNet Loss: 92.7396 \tQuestion Loss: 92.7396 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 14\tNet Loss: 95.5126 \tQuestion Loss: 95.5126 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 14\tNet Loss: 93.6134 \tQuestion Loss: 93.6134 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 14\tNet Loss: 94.8231 \tQuestion Loss: 94.8231 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 14\tNet Loss: 93.5115 \tQuestion Loss: 93.5115 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 14\tNet Loss: 86.7292 \tQuestion Loss: 86.7292 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 14\tNet Loss: 91.4565 \tQuestion Loss: 91.4565 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 14\tNet Loss: 96.8936 \tQuestion Loss: 96.8936 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 14\tNet Loss: 92.8840 \tQuestion Loss: 92.8840 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 14\tNet Loss: 94.1429 \tQuestion Loss: 94.1429 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 14\tNet Loss: 92.0992 \tQuestion Loss: 92.0992 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 14\tNet Loss: 89.5606 \tQuestion Loss: 89.5606 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 14\tNet Loss: 88.5301 \tQuestion Loss: 88.5301 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 84 \t Epoch : 14\tNet Loss: 91.9126 \tQuestion Loss: 91.9126 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 14\tNet Loss: 90.2732 \tQuestion Loss: 90.2732 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 14\tNet Loss: 92.4186 \tQuestion Loss: 92.4186 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 14\tNet Loss: 91.6631 \tQuestion Loss: 91.6631 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 14\tNet Loss: 94.4510 \tQuestion Loss: 94.4510 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 14\tNet Loss: 90.0745 \tQuestion Loss: 90.0745 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 14\tNet Loss: 95.2578 \tQuestion Loss: 95.2578 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 14\tNet Loss: 92.8158 \tQuestion Loss: 92.8158 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 14\tNet Loss: 91.2504 \tQuestion Loss: 91.2504 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 14\tNet Loss: 91.8860 \tQuestion Loss: 91.8860 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 14\tNet Loss: 92.5682 \tQuestion Loss: 92.5682 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 14\tNet Loss: 93.3865 \tQuestion Loss: 93.3865 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 14\tNet Loss: 91.7510 \tQuestion Loss: 91.7510 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 14\tNet Loss: 95.7131 \tQuestion Loss: 95.7131 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 14\tNet Loss: 92.7474 \tQuestion Loss: 92.7474 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 14\tNet Loss: 92.9439 \tQuestion Loss: 92.9439 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 14\tNet Loss: 92.4972 \tQuestion Loss: 92.4972 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 14\tNet Loss: 95.4787 \tQuestion Loss: 95.4787 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 14\tNet Loss: 93.0022 \tQuestion Loss: 93.0022 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 14\tNet Loss: 92.4566 \tQuestion Loss: 92.4566 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 14\tNet Loss: 94.9791 \tQuestion Loss: 94.9791 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 14\tNet Loss: 96.6984 \tQuestion Loss: 96.6984 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 14\tNet Loss: 92.2079 \tQuestion Loss: 92.2079 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 14\tNet Loss: 89.2059 \tQuestion Loss: 89.2059 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 14\tNet Loss: 88.4057 \tQuestion Loss: 88.4057 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 14\tNet Loss: 92.8656 \tQuestion Loss: 92.8656 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 14\tNet Loss: 93.3969 \tQuestion Loss: 93.3969 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 14\tNet Loss: 87.6636 \tQuestion Loss: 87.6636 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 14\tNet Loss: 89.7253 \tQuestion Loss: 89.7253 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 14\tNet Loss: 97.0161 \tQuestion Loss: 97.0161 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 14\tNet Loss: 94.3072 \tQuestion Loss: 94.3072 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 14\tNet Loss: 91.3761 \tQuestion Loss: 91.3761 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 14\tNet Loss: 88.3022 \tQuestion Loss: 88.3022 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 14\tNet Loss: 90.5652 \tQuestion Loss: 90.5652 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 14\tNet Loss: 93.7872 \tQuestion Loss: 93.7872 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 14\tNet Loss: 89.6645 \tQuestion Loss: 89.6645 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 14\tNet Loss: 91.5115 \tQuestion Loss: 91.5115 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 14\tNet Loss: 93.4418 \tQuestion Loss: 93.4418 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 14\tNet Loss: 97.5589 \tQuestion Loss: 97.5589 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 14\tNet Loss: 92.8451 \tQuestion Loss: 92.8451 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 14\tNet Loss: 94.7329 \tQuestion Loss: 94.7329 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 14\tNet Loss: 92.4545 \tQuestion Loss: 92.4545 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 14\tNet Loss: 96.0303 \tQuestion Loss: 96.0303 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 14\tNet Loss: 87.9576 \tQuestion Loss: 87.9576 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 14\tNet Loss: 92.6243 \tQuestion Loss: 92.6243 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 14\tNet Loss: 94.7585 \tQuestion Loss: 94.7585 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 14\tNet Loss: 99.9527 \tQuestion Loss: 99.9527 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 14\tNet Loss: 94.7731 \tQuestion Loss: 94.7731 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 14\tNet Loss: 91.2967 \tQuestion Loss: 91.2967 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 14\tNet Loss: 92.5648 \tQuestion Loss: 92.5648 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 14\tNet Loss: 93.4537 \tQuestion Loss: 93.4537 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 14\tNet Loss: 85.9379 \tQuestion Loss: 85.9379 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 14\tNet Loss: 97.2478 \tQuestion Loss: 97.2478 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 14\tNet Loss: 94.7315 \tQuestion Loss: 94.7315 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 14\tNet Loss: 93.3130 \tQuestion Loss: 93.3130 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 14\tNet Loss: 92.0598 \tQuestion Loss: 92.0598 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 14\tNet Loss: 89.2229 \tQuestion Loss: 89.2229 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 14\tNet Loss: 89.6295 \tQuestion Loss: 89.6295 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 14\tNet Loss: 94.3636 \tQuestion Loss: 94.3636 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 14\tNet Loss: 89.2863 \tQuestion Loss: 89.2863 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 14\tNet Loss: 86.9012 \tQuestion Loss: 86.9012 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 14\tNet Loss: 95.5525 \tQuestion Loss: 95.5525 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 14\tNet Loss: 98.5752 \tQuestion Loss: 98.5752 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 14\tNet Loss: 86.9092 \tQuestion Loss: 86.9092 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 14\tNet Loss: 96.5195 \tQuestion Loss: 96.5195 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 14\tNet Loss: 89.9280 \tQuestion Loss: 89.9280 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 14\tNet Loss: 91.0953 \tQuestion Loss: 91.0953 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 14\tNet Loss: 97.8529 \tQuestion Loss: 97.8529 \t Time Taken: 1 seconds\n",
      "Batch: 152 \t Epoch : 14\tNet Loss: 93.3867 \tQuestion Loss: 93.3867 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 14\tNet Loss: 93.9862 \tQuestion Loss: 93.9862 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 14\tNet Loss: 85.3826 \tQuestion Loss: 85.3826 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 14\tNet Loss: 90.5964 \tQuestion Loss: 90.5964 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 14\tNet Loss: 93.4646 \tQuestion Loss: 93.4646 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 14\tNet Loss: 88.4615 \tQuestion Loss: 88.4615 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 14\tNet Loss: 93.6908 \tQuestion Loss: 93.6908 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 14\tNet Loss: 94.6423 \tQuestion Loss: 94.6423 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 14\tNet Loss: 95.8479 \tQuestion Loss: 95.8479 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 14\tNet Loss: 94.0291 \tQuestion Loss: 94.0291 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 14\tNet Loss: 95.7726 \tQuestion Loss: 95.7726 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 14\tNet Loss: 89.6526 \tQuestion Loss: 89.6526 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 14\tNet Loss: 88.5095 \tQuestion Loss: 88.5095 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 14\tNet Loss: 89.6740 \tQuestion Loss: 89.6740 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 14\tNet Loss: 87.3531 \tQuestion Loss: 87.3531 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 14\tNet Loss: 92.9805 \tQuestion Loss: 92.9805 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 14\tNet Loss: 91.5359 \tQuestion Loss: 91.5359 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 14\tNet Loss: 95.7445 \tQuestion Loss: 95.7445 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 14\tNet Loss: 93.4990 \tQuestion Loss: 93.4990 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 14\tNet Loss: 92.4580 \tQuestion Loss: 92.4580 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 14\tNet Loss: 89.2551 \tQuestion Loss: 89.2551 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 14\tNet Loss: 90.4944 \tQuestion Loss: 90.4944 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 14\tNet Loss: 94.9256 \tQuestion Loss: 94.9256 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 14\tNet Loss: 97.6345 \tQuestion Loss: 97.6345 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 176 \t Epoch : 14\tNet Loss: 90.1087 \tQuestion Loss: 90.1087 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 14\tNet Loss: 91.4693 \tQuestion Loss: 91.4693 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 14\tNet Loss: 90.4475 \tQuestion Loss: 90.4475 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 14\tNet Loss: 91.6653 \tQuestion Loss: 91.6653 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 14\tNet Loss: 90.0938 \tQuestion Loss: 90.0938 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 14\tNet Loss: 92.3610 \tQuestion Loss: 92.3610 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 14\tNet Loss: 94.3287 \tQuestion Loss: 94.3287 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 14\tNet Loss: 89.4278 \tQuestion Loss: 89.4278 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 14\tNet Loss: 93.2385 \tQuestion Loss: 93.2385 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 14\tNet Loss: 91.5477 \tQuestion Loss: 91.5477 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 14\tNet Loss: 93.9826 \tQuestion Loss: 93.9826 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 14\tNet Loss: 95.0497 \tQuestion Loss: 95.0497 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 14\tNet Loss: 89.8904 \tQuestion Loss: 89.8904 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 14\tNet Loss: 89.8845 \tQuestion Loss: 89.8845 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 14\tNet Loss: 91.0592 \tQuestion Loss: 91.0592 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 14\tNet Loss: 90.0215 \tQuestion Loss: 90.0215 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 14\tNet Loss: 94.8256 \tQuestion Loss: 94.8256 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 14\tNet Loss: 93.4123 \tQuestion Loss: 93.4123 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 14\tNet Loss: 95.1597 \tQuestion Loss: 95.1597 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 14\tNet Loss: 87.9912 \tQuestion Loss: 87.9912 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 14\tNet Loss: 89.1440 \tQuestion Loss: 89.1440 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 14\tNet Loss: 94.5964 \tQuestion Loss: 94.5964 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 14\tNet Loss: 87.6562 \tQuestion Loss: 87.6562 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 14\tNet Loss: 92.3257 \tQuestion Loss: 92.3257 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 14 : 92.3085 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 15\tNet Loss: 92.6700 \tQuestion Loss: 92.6700 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 15\tNet Loss: 89.0695 \tQuestion Loss: 89.0695 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 15\tNet Loss: 94.3091 \tQuestion Loss: 94.3091 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 15\tNet Loss: 93.5635 \tQuestion Loss: 93.5635 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 15\tNet Loss: 93.6685 \tQuestion Loss: 93.6685 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 15\tNet Loss: 93.6069 \tQuestion Loss: 93.6069 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 15\tNet Loss: 88.8346 \tQuestion Loss: 88.8346 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 15\tNet Loss: 94.7067 \tQuestion Loss: 94.7067 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 15\tNet Loss: 93.2970 \tQuestion Loss: 93.2970 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 15\tNet Loss: 94.8566 \tQuestion Loss: 94.8566 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 15\tNet Loss: 91.0089 \tQuestion Loss: 91.0089 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 15\tNet Loss: 95.6408 \tQuestion Loss: 95.6408 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 15\tNet Loss: 90.2945 \tQuestion Loss: 90.2945 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 15\tNet Loss: 92.6947 \tQuestion Loss: 92.6947 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 15\tNet Loss: 92.9648 \tQuestion Loss: 92.9648 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 15\tNet Loss: 95.4257 \tQuestion Loss: 95.4257 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 15\tNet Loss: 90.5150 \tQuestion Loss: 90.5150 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 15\tNet Loss: 92.3305 \tQuestion Loss: 92.3305 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 15\tNet Loss: 90.5155 \tQuestion Loss: 90.5155 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 15\tNet Loss: 86.7446 \tQuestion Loss: 86.7446 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 15\tNet Loss: 96.0689 \tQuestion Loss: 96.0689 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 15\tNet Loss: 88.1535 \tQuestion Loss: 88.1535 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 15\tNet Loss: 92.8354 \tQuestion Loss: 92.8354 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 15\tNet Loss: 95.7797 \tQuestion Loss: 95.7797 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 15\tNet Loss: 89.8161 \tQuestion Loss: 89.8161 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 15\tNet Loss: 93.1332 \tQuestion Loss: 93.1332 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 15\tNet Loss: 98.4252 \tQuestion Loss: 98.4252 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 15\tNet Loss: 91.8180 \tQuestion Loss: 91.8180 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 15\tNet Loss: 86.9996 \tQuestion Loss: 86.9996 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 15\tNet Loss: 89.9254 \tQuestion Loss: 89.9254 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 15\tNet Loss: 91.6566 \tQuestion Loss: 91.6566 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 15\tNet Loss: 90.3516 \tQuestion Loss: 90.3516 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 15\tNet Loss: 88.7280 \tQuestion Loss: 88.7280 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 15\tNet Loss: 93.3455 \tQuestion Loss: 93.3455 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 15\tNet Loss: 87.3100 \tQuestion Loss: 87.3100 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 15\tNet Loss: 90.0796 \tQuestion Loss: 90.0796 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 15\tNet Loss: 93.7952 \tQuestion Loss: 93.7952 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 15\tNet Loss: 95.1326 \tQuestion Loss: 95.1326 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 15\tNet Loss: 91.1638 \tQuestion Loss: 91.1638 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 15\tNet Loss: 93.9201 \tQuestion Loss: 93.9201 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 15\tNet Loss: 91.4749 \tQuestion Loss: 91.4749 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 15\tNet Loss: 90.1605 \tQuestion Loss: 90.1605 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 15\tNet Loss: 91.9920 \tQuestion Loss: 91.9920 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 15\tNet Loss: 93.3322 \tQuestion Loss: 93.3322 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 15\tNet Loss: 90.6556 \tQuestion Loss: 90.6556 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 15\tNet Loss: 93.7568 \tQuestion Loss: 93.7568 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 15\tNet Loss: 92.2925 \tQuestion Loss: 92.2925 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 15\tNet Loss: 97.3340 \tQuestion Loss: 97.3340 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 15\tNet Loss: 86.1006 \tQuestion Loss: 86.1006 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 15\tNet Loss: 94.0064 \tQuestion Loss: 94.0064 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 15\tNet Loss: 92.9974 \tQuestion Loss: 92.9974 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 15\tNet Loss: 88.5402 \tQuestion Loss: 88.5402 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 15\tNet Loss: 89.4755 \tQuestion Loss: 89.4755 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 15\tNet Loss: 89.7960 \tQuestion Loss: 89.7960 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 15\tNet Loss: 96.5441 \tQuestion Loss: 96.5441 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 15\tNet Loss: 93.2280 \tQuestion Loss: 93.2280 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 15\tNet Loss: 94.3651 \tQuestion Loss: 94.3651 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 15\tNet Loss: 96.6466 \tQuestion Loss: 96.6466 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 15\tNet Loss: 87.6159 \tQuestion Loss: 87.6159 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 15\tNet Loss: 92.1572 \tQuestion Loss: 92.1572 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 15\tNet Loss: 89.1157 \tQuestion Loss: 89.1157 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 15\tNet Loss: 95.2598 \tQuestion Loss: 95.2598 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 15\tNet Loss: 88.8510 \tQuestion Loss: 88.8510 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 15\tNet Loss: 93.5795 \tQuestion Loss: 93.5795 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 15\tNet Loss: 92.2110 \tQuestion Loss: 92.2110 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 15\tNet Loss: 90.6982 \tQuestion Loss: 90.6982 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 15\tNet Loss: 92.1566 \tQuestion Loss: 92.1566 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 15\tNet Loss: 91.2991 \tQuestion Loss: 91.2991 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 68 \t Epoch : 15\tNet Loss: 90.8196 \tQuestion Loss: 90.8196 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 15\tNet Loss: 95.6039 \tQuestion Loss: 95.6039 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 15\tNet Loss: 93.2450 \tQuestion Loss: 93.2450 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 15\tNet Loss: 92.7388 \tQuestion Loss: 92.7388 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 15\tNet Loss: 95.5582 \tQuestion Loss: 95.5582 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 15\tNet Loss: 93.7259 \tQuestion Loss: 93.7259 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 15\tNet Loss: 95.0089 \tQuestion Loss: 95.0089 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 15\tNet Loss: 93.5975 \tQuestion Loss: 93.5975 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 15\tNet Loss: 86.8441 \tQuestion Loss: 86.8441 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 15\tNet Loss: 91.5789 \tQuestion Loss: 91.5789 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 15\tNet Loss: 96.3633 \tQuestion Loss: 96.3633 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 15\tNet Loss: 92.9296 \tQuestion Loss: 92.9296 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 15\tNet Loss: 94.3280 \tQuestion Loss: 94.3280 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 15\tNet Loss: 92.1519 \tQuestion Loss: 92.1519 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 15\tNet Loss: 89.5777 \tQuestion Loss: 89.5777 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 15\tNet Loss: 88.7146 \tQuestion Loss: 88.7146 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 15\tNet Loss: 92.0072 \tQuestion Loss: 92.0072 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 15\tNet Loss: 90.3912 \tQuestion Loss: 90.3912 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 15\tNet Loss: 92.0610 \tQuestion Loss: 92.0610 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 15\tNet Loss: 91.7064 \tQuestion Loss: 91.7064 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 15\tNet Loss: 94.3992 \tQuestion Loss: 94.3992 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 15\tNet Loss: 90.0240 \tQuestion Loss: 90.0240 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 15\tNet Loss: 95.3161 \tQuestion Loss: 95.3161 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 15\tNet Loss: 92.9845 \tQuestion Loss: 92.9845 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 15\tNet Loss: 91.5228 \tQuestion Loss: 91.5228 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 15\tNet Loss: 92.0863 \tQuestion Loss: 92.0863 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 15\tNet Loss: 92.7149 \tQuestion Loss: 92.7149 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 15\tNet Loss: 93.4872 \tQuestion Loss: 93.4872 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 15\tNet Loss: 91.9888 \tQuestion Loss: 91.9888 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 15\tNet Loss: 95.8438 \tQuestion Loss: 95.8438 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 15\tNet Loss: 92.7117 \tQuestion Loss: 92.7117 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 15\tNet Loss: 93.1608 \tQuestion Loss: 93.1608 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 15\tNet Loss: 92.5220 \tQuestion Loss: 92.5220 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 15\tNet Loss: 95.5879 \tQuestion Loss: 95.5879 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 15\tNet Loss: 93.0524 \tQuestion Loss: 93.0524 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 15\tNet Loss: 92.4899 \tQuestion Loss: 92.4899 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 15\tNet Loss: 95.2562 \tQuestion Loss: 95.2562 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 15\tNet Loss: 96.4342 \tQuestion Loss: 96.4342 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 15\tNet Loss: 92.1878 \tQuestion Loss: 92.1878 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 15\tNet Loss: 89.2660 \tQuestion Loss: 89.2660 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 15\tNet Loss: 88.4688 \tQuestion Loss: 88.4688 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 15\tNet Loss: 92.8799 \tQuestion Loss: 92.8799 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 15\tNet Loss: 93.1812 \tQuestion Loss: 93.1812 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 15\tNet Loss: 87.8169 \tQuestion Loss: 87.8169 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 15\tNet Loss: 89.7994 \tQuestion Loss: 89.7994 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 15\tNet Loss: 97.2206 \tQuestion Loss: 97.2206 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 15\tNet Loss: 94.0462 \tQuestion Loss: 94.0462 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 15\tNet Loss: 90.9434 \tQuestion Loss: 90.9434 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 15\tNet Loss: 88.3330 \tQuestion Loss: 88.3330 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 15\tNet Loss: 90.6342 \tQuestion Loss: 90.6342 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 15\tNet Loss: 93.8835 \tQuestion Loss: 93.8835 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 15\tNet Loss: 89.6306 \tQuestion Loss: 89.6306 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 15\tNet Loss: 91.5394 \tQuestion Loss: 91.5394 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 15\tNet Loss: 93.8105 \tQuestion Loss: 93.8105 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 15\tNet Loss: 96.9160 \tQuestion Loss: 96.9160 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 15\tNet Loss: 92.7547 \tQuestion Loss: 92.7547 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 15\tNet Loss: 94.7672 \tQuestion Loss: 94.7672 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 15\tNet Loss: 92.8444 \tQuestion Loss: 92.8444 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 15\tNet Loss: 96.4139 \tQuestion Loss: 96.4139 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 15\tNet Loss: 88.0685 \tQuestion Loss: 88.0685 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 15\tNet Loss: 93.1159 \tQuestion Loss: 93.1159 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 15\tNet Loss: 93.2216 \tQuestion Loss: 93.2216 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 15\tNet Loss: 100.1214 \tQuestion Loss: 100.1214 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 15\tNet Loss: 94.8055 \tQuestion Loss: 94.8055 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 15\tNet Loss: 91.1571 \tQuestion Loss: 91.1571 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 15\tNet Loss: 92.8857 \tQuestion Loss: 92.8857 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 15\tNet Loss: 92.8591 \tQuestion Loss: 92.8591 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 15\tNet Loss: 86.0855 \tQuestion Loss: 86.0855 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 15\tNet Loss: 97.4031 \tQuestion Loss: 97.4031 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 15\tNet Loss: 95.3817 \tQuestion Loss: 95.3817 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 15\tNet Loss: 93.2577 \tQuestion Loss: 93.2577 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 15\tNet Loss: 92.1415 \tQuestion Loss: 92.1415 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 15\tNet Loss: 89.3292 \tQuestion Loss: 89.3292 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 15\tNet Loss: 89.8073 \tQuestion Loss: 89.8073 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 15\tNet Loss: 94.5225 \tQuestion Loss: 94.5225 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 15\tNet Loss: 89.5281 \tQuestion Loss: 89.5281 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 15\tNet Loss: 87.0910 \tQuestion Loss: 87.0910 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 15\tNet Loss: 95.3572 \tQuestion Loss: 95.3572 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 15\tNet Loss: 98.7414 \tQuestion Loss: 98.7414 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 15\tNet Loss: 87.0187 \tQuestion Loss: 87.0187 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 15\tNet Loss: 96.5110 \tQuestion Loss: 96.5110 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 15\tNet Loss: 90.1438 \tQuestion Loss: 90.1438 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 15\tNet Loss: 91.2144 \tQuestion Loss: 91.2144 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 15\tNet Loss: 98.0812 \tQuestion Loss: 98.0812 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 15\tNet Loss: 94.1049 \tQuestion Loss: 94.1049 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 15\tNet Loss: 94.4638 \tQuestion Loss: 94.4638 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 15\tNet Loss: 85.1219 \tQuestion Loss: 85.1219 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 15\tNet Loss: 90.5050 \tQuestion Loss: 90.5050 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 15\tNet Loss: 93.5473 \tQuestion Loss: 93.5473 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 15\tNet Loss: 88.3601 \tQuestion Loss: 88.3601 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 15\tNet Loss: 93.6673 \tQuestion Loss: 93.6673 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 15\tNet Loss: 94.8363 \tQuestion Loss: 94.8363 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 160 \t Epoch : 15\tNet Loss: 96.0151 \tQuestion Loss: 96.0151 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 15\tNet Loss: 94.3692 \tQuestion Loss: 94.3692 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 15\tNet Loss: 95.9591 \tQuestion Loss: 95.9591 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 15\tNet Loss: 89.6196 \tQuestion Loss: 89.6196 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 15\tNet Loss: 88.5892 \tQuestion Loss: 88.5892 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 15\tNet Loss: 89.7812 \tQuestion Loss: 89.7812 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 15\tNet Loss: 87.5843 \tQuestion Loss: 87.5843 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 15\tNet Loss: 92.9268 \tQuestion Loss: 92.9268 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 15\tNet Loss: 91.4941 \tQuestion Loss: 91.4941 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 15\tNet Loss: 95.8859 \tQuestion Loss: 95.8859 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 15\tNet Loss: 93.6410 \tQuestion Loss: 93.6410 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 15\tNet Loss: 92.6403 \tQuestion Loss: 92.6403 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 15\tNet Loss: 89.0126 \tQuestion Loss: 89.0126 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 15\tNet Loss: 90.7115 \tQuestion Loss: 90.7115 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 15\tNet Loss: 94.9065 \tQuestion Loss: 94.9065 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 15\tNet Loss: 97.7940 \tQuestion Loss: 97.7940 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 15\tNet Loss: 90.1513 \tQuestion Loss: 90.1513 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 15\tNet Loss: 91.6485 \tQuestion Loss: 91.6485 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 15\tNet Loss: 90.5330 \tQuestion Loss: 90.5330 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 15\tNet Loss: 91.9679 \tQuestion Loss: 91.9679 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 15\tNet Loss: 90.1574 \tQuestion Loss: 90.1574 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 15\tNet Loss: 92.4898 \tQuestion Loss: 92.4898 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 15\tNet Loss: 94.4097 \tQuestion Loss: 94.4097 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 15\tNet Loss: 89.5674 \tQuestion Loss: 89.5674 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 15\tNet Loss: 93.5047 \tQuestion Loss: 93.5047 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 15\tNet Loss: 91.7127 \tQuestion Loss: 91.7127 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 15\tNet Loss: 94.3655 \tQuestion Loss: 94.3655 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 15\tNet Loss: 95.2431 \tQuestion Loss: 95.2431 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 15\tNet Loss: 89.9839 \tQuestion Loss: 89.9839 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 15\tNet Loss: 89.8087 \tQuestion Loss: 89.8087 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 15\tNet Loss: 91.2252 \tQuestion Loss: 91.2252 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 15\tNet Loss: 90.0811 \tQuestion Loss: 90.0811 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 15\tNet Loss: 94.9752 \tQuestion Loss: 94.9752 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 15\tNet Loss: 93.4050 \tQuestion Loss: 93.4050 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 15\tNet Loss: 95.1499 \tQuestion Loss: 95.1499 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 15\tNet Loss: 87.9276 \tQuestion Loss: 87.9276 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 15\tNet Loss: 89.3344 \tQuestion Loss: 89.3344 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 15\tNet Loss: 94.8111 \tQuestion Loss: 94.8111 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 15\tNet Loss: 87.9744 \tQuestion Loss: 87.9744 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 15\tNet Loss: 92.4634 \tQuestion Loss: 92.4634 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 15 : 92.3247 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 16\tNet Loss: 92.7963 \tQuestion Loss: 92.7963 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 16\tNet Loss: 89.1359 \tQuestion Loss: 89.1359 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 16\tNet Loss: 94.3289 \tQuestion Loss: 94.3289 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 16\tNet Loss: 93.5611 \tQuestion Loss: 93.5611 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 16\tNet Loss: 93.8784 \tQuestion Loss: 93.8784 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 16\tNet Loss: 93.7258 \tQuestion Loss: 93.7258 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 16\tNet Loss: 88.8294 \tQuestion Loss: 88.8294 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 16\tNet Loss: 94.9745 \tQuestion Loss: 94.9745 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 16\tNet Loss: 93.5930 \tQuestion Loss: 93.5930 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 16\tNet Loss: 95.2053 \tQuestion Loss: 95.2053 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 16\tNet Loss: 91.3051 \tQuestion Loss: 91.3051 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 16\tNet Loss: 95.6141 \tQuestion Loss: 95.6141 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 16\tNet Loss: 90.4622 \tQuestion Loss: 90.4622 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 16\tNet Loss: 92.7017 \tQuestion Loss: 92.7017 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 16\tNet Loss: 93.1293 \tQuestion Loss: 93.1293 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 16\tNet Loss: 95.5464 \tQuestion Loss: 95.5464 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 16\tNet Loss: 90.4349 \tQuestion Loss: 90.4349 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 16\tNet Loss: 92.5889 \tQuestion Loss: 92.5889 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 16\tNet Loss: 91.0628 \tQuestion Loss: 91.0628 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 16\tNet Loss: 86.8685 \tQuestion Loss: 86.8685 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 16\tNet Loss: 96.1561 \tQuestion Loss: 96.1561 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 16\tNet Loss: 88.1862 \tQuestion Loss: 88.1862 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 16\tNet Loss: 92.8063 \tQuestion Loss: 92.8063 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 16\tNet Loss: 95.9490 \tQuestion Loss: 95.9490 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 16\tNet Loss: 89.8000 \tQuestion Loss: 89.8000 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 16\tNet Loss: 93.3105 \tQuestion Loss: 93.3105 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 16\tNet Loss: 98.3902 \tQuestion Loss: 98.3902 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 16\tNet Loss: 91.9349 \tQuestion Loss: 91.9349 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 16\tNet Loss: 87.0364 \tQuestion Loss: 87.0364 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 16\tNet Loss: 89.8547 \tQuestion Loss: 89.8547 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 16\tNet Loss: 91.5033 \tQuestion Loss: 91.5033 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 16\tNet Loss: 90.4384 \tQuestion Loss: 90.4384 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 16\tNet Loss: 89.1248 \tQuestion Loss: 89.1248 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 16\tNet Loss: 93.2808 \tQuestion Loss: 93.2808 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 16\tNet Loss: 87.4311 \tQuestion Loss: 87.4311 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 16\tNet Loss: 90.1174 \tQuestion Loss: 90.1174 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 16\tNet Loss: 93.9179 \tQuestion Loss: 93.9179 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 16\tNet Loss: 95.2648 \tQuestion Loss: 95.2648 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 16\tNet Loss: 91.1303 \tQuestion Loss: 91.1303 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 16\tNet Loss: 94.1651 \tQuestion Loss: 94.1651 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 16\tNet Loss: 91.5045 \tQuestion Loss: 91.5045 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 16\tNet Loss: 90.3830 \tQuestion Loss: 90.3830 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 16\tNet Loss: 92.1607 \tQuestion Loss: 92.1607 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 16\tNet Loss: 93.4717 \tQuestion Loss: 93.4717 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 16\tNet Loss: 90.6557 \tQuestion Loss: 90.6557 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 16\tNet Loss: 93.6143 \tQuestion Loss: 93.6143 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 16\tNet Loss: 92.1261 \tQuestion Loss: 92.1261 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 16\tNet Loss: 97.2701 \tQuestion Loss: 97.2701 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 16\tNet Loss: 86.7850 \tQuestion Loss: 86.7850 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 16\tNet Loss: 94.2192 \tQuestion Loss: 94.2192 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 16\tNet Loss: 92.9242 \tQuestion Loss: 92.9242 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 16\tNet Loss: 88.4993 \tQuestion Loss: 88.4993 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 52 \t Epoch : 16\tNet Loss: 89.5839 \tQuestion Loss: 89.5839 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 16\tNet Loss: 90.0035 \tQuestion Loss: 90.0035 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 16\tNet Loss: 96.4036 \tQuestion Loss: 96.4036 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 16\tNet Loss: 93.0010 \tQuestion Loss: 93.0010 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 16\tNet Loss: 94.4064 \tQuestion Loss: 94.4064 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 16\tNet Loss: 96.6755 \tQuestion Loss: 96.6755 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 16\tNet Loss: 87.5633 \tQuestion Loss: 87.5633 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 16\tNet Loss: 92.0744 \tQuestion Loss: 92.0744 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 16\tNet Loss: 89.1047 \tQuestion Loss: 89.1047 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 16\tNet Loss: 95.2334 \tQuestion Loss: 95.2334 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 16\tNet Loss: 88.8749 \tQuestion Loss: 88.8749 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 16\tNet Loss: 93.4406 \tQuestion Loss: 93.4406 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 16\tNet Loss: 91.9923 \tQuestion Loss: 91.9923 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 16\tNet Loss: 90.4673 \tQuestion Loss: 90.4673 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 16\tNet Loss: 92.5772 \tQuestion Loss: 92.5772 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 16\tNet Loss: 92.1717 \tQuestion Loss: 92.1717 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 16\tNet Loss: 90.8622 \tQuestion Loss: 90.8622 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 16\tNet Loss: 95.9207 \tQuestion Loss: 95.9207 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 16\tNet Loss: 93.2378 \tQuestion Loss: 93.2378 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 16\tNet Loss: 92.7800 \tQuestion Loss: 92.7800 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 16\tNet Loss: 95.5204 \tQuestion Loss: 95.5204 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 16\tNet Loss: 93.5770 \tQuestion Loss: 93.5770 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 16\tNet Loss: 94.7894 \tQuestion Loss: 94.7894 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 16\tNet Loss: 93.5197 \tQuestion Loss: 93.5197 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 16\tNet Loss: 86.7415 \tQuestion Loss: 86.7415 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 16\tNet Loss: 91.3819 \tQuestion Loss: 91.3819 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 16\tNet Loss: 96.9206 \tQuestion Loss: 96.9206 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 16\tNet Loss: 92.8515 \tQuestion Loss: 92.8515 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 16\tNet Loss: 94.2154 \tQuestion Loss: 94.2154 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 16\tNet Loss: 92.0893 \tQuestion Loss: 92.0893 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 16\tNet Loss: 89.6802 \tQuestion Loss: 89.6802 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 16\tNet Loss: 88.4972 \tQuestion Loss: 88.4972 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 16\tNet Loss: 91.8333 \tQuestion Loss: 91.8333 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 16\tNet Loss: 90.2203 \tQuestion Loss: 90.2203 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 16\tNet Loss: 92.3977 \tQuestion Loss: 92.3977 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 16\tNet Loss: 91.6734 \tQuestion Loss: 91.6734 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 16\tNet Loss: 94.4609 \tQuestion Loss: 94.4609 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 16\tNet Loss: 90.0704 \tQuestion Loss: 90.0704 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 16\tNet Loss: 95.2767 \tQuestion Loss: 95.2767 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 16\tNet Loss: 92.8474 \tQuestion Loss: 92.8474 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 16\tNet Loss: 91.2633 \tQuestion Loss: 91.2633 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 16\tNet Loss: 91.8466 \tQuestion Loss: 91.8466 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 16\tNet Loss: 92.5568 \tQuestion Loss: 92.5568 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 16\tNet Loss: 93.3860 \tQuestion Loss: 93.3860 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 16\tNet Loss: 91.7572 \tQuestion Loss: 91.7572 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 16\tNet Loss: 95.6831 \tQuestion Loss: 95.6831 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 16\tNet Loss: 92.7005 \tQuestion Loss: 92.7005 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 16\tNet Loss: 92.9425 \tQuestion Loss: 92.9425 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 16\tNet Loss: 92.5341 \tQuestion Loss: 92.5341 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 16\tNet Loss: 95.4799 \tQuestion Loss: 95.4799 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 16\tNet Loss: 92.9812 \tQuestion Loss: 92.9812 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 16\tNet Loss: 92.4774 \tQuestion Loss: 92.4774 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 16\tNet Loss: 95.0120 \tQuestion Loss: 95.0120 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 16\tNet Loss: 96.6537 \tQuestion Loss: 96.6537 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 16\tNet Loss: 92.1350 \tQuestion Loss: 92.1350 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 16\tNet Loss: 89.2223 \tQuestion Loss: 89.2223 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 16\tNet Loss: 88.3983 \tQuestion Loss: 88.3983 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 16\tNet Loss: 92.9133 \tQuestion Loss: 92.9133 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 16\tNet Loss: 93.3816 \tQuestion Loss: 93.3816 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 16\tNet Loss: 87.6961 \tQuestion Loss: 87.6961 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 16\tNet Loss: 89.7561 \tQuestion Loss: 89.7561 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 16\tNet Loss: 97.0465 \tQuestion Loss: 97.0465 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 16\tNet Loss: 94.2537 \tQuestion Loss: 94.2537 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 16\tNet Loss: 91.3488 \tQuestion Loss: 91.3488 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 16\tNet Loss: 88.3140 \tQuestion Loss: 88.3140 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 16\tNet Loss: 90.5497 \tQuestion Loss: 90.5497 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 16\tNet Loss: 93.8035 \tQuestion Loss: 93.8035 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 16\tNet Loss: 89.6909 \tQuestion Loss: 89.6909 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 16\tNet Loss: 91.5143 \tQuestion Loss: 91.5143 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 16\tNet Loss: 93.4326 \tQuestion Loss: 93.4326 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 16\tNet Loss: 97.6113 \tQuestion Loss: 97.6113 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 16\tNet Loss: 92.8467 \tQuestion Loss: 92.8467 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 16\tNet Loss: 94.7601 \tQuestion Loss: 94.7601 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 16\tNet Loss: 92.4672 \tQuestion Loss: 92.4672 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 16\tNet Loss: 96.0838 \tQuestion Loss: 96.0838 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 16\tNet Loss: 87.9716 \tQuestion Loss: 87.9716 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 16\tNet Loss: 92.4487 \tQuestion Loss: 92.4487 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 16\tNet Loss: 94.8061 \tQuestion Loss: 94.8061 \t Time Taken: 1 seconds\n",
      "Batch: 130 \t Epoch : 16\tNet Loss: 99.9163 \tQuestion Loss: 99.9163 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 16\tNet Loss: 94.7816 \tQuestion Loss: 94.7816 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 16\tNet Loss: 91.3064 \tQuestion Loss: 91.3064 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 16\tNet Loss: 92.5791 \tQuestion Loss: 92.5791 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 16\tNet Loss: 93.4730 \tQuestion Loss: 93.4730 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 16\tNet Loss: 85.9749 \tQuestion Loss: 85.9749 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 16\tNet Loss: 97.2675 \tQuestion Loss: 97.2675 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 16\tNet Loss: 94.7012 \tQuestion Loss: 94.7012 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 16\tNet Loss: 93.3425 \tQuestion Loss: 93.3425 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 16\tNet Loss: 92.1088 \tQuestion Loss: 92.1088 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 16\tNet Loss: 89.2431 \tQuestion Loss: 89.2431 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 16\tNet Loss: 89.6402 \tQuestion Loss: 89.6402 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 16\tNet Loss: 94.4140 \tQuestion Loss: 94.4140 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 16\tNet Loss: 89.3375 \tQuestion Loss: 89.3375 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 144 \t Epoch : 16\tNet Loss: 86.9462 \tQuestion Loss: 86.9462 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 16\tNet Loss: 95.5501 \tQuestion Loss: 95.5501 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 16\tNet Loss: 98.5872 \tQuestion Loss: 98.5872 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 16\tNet Loss: 86.9634 \tQuestion Loss: 86.9634 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 16\tNet Loss: 96.5232 \tQuestion Loss: 96.5232 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 16\tNet Loss: 89.9480 \tQuestion Loss: 89.9480 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 16\tNet Loss: 91.0967 \tQuestion Loss: 91.0967 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 16\tNet Loss: 97.8940 \tQuestion Loss: 97.8940 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 16\tNet Loss: 93.3987 \tQuestion Loss: 93.3987 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 16\tNet Loss: 93.9381 \tQuestion Loss: 93.9381 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 16\tNet Loss: 85.3311 \tQuestion Loss: 85.3311 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 16\tNet Loss: 90.6348 \tQuestion Loss: 90.6348 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 16\tNet Loss: 93.4685 \tQuestion Loss: 93.4685 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 16\tNet Loss: 88.4169 \tQuestion Loss: 88.4169 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 16\tNet Loss: 93.7290 \tQuestion Loss: 93.7290 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 16\tNet Loss: 94.6870 \tQuestion Loss: 94.6870 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 16\tNet Loss: 95.8482 \tQuestion Loss: 95.8482 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 16\tNet Loss: 94.0331 \tQuestion Loss: 94.0331 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 16\tNet Loss: 95.8273 \tQuestion Loss: 95.8273 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 16\tNet Loss: 89.6952 \tQuestion Loss: 89.6952 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 16\tNet Loss: 88.5135 \tQuestion Loss: 88.5135 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 16\tNet Loss: 89.6809 \tQuestion Loss: 89.6809 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 16\tNet Loss: 87.4012 \tQuestion Loss: 87.4012 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 16\tNet Loss: 92.9566 \tQuestion Loss: 92.9566 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 16\tNet Loss: 91.5302 \tQuestion Loss: 91.5302 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 16\tNet Loss: 95.7400 \tQuestion Loss: 95.7400 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 16\tNet Loss: 93.4940 \tQuestion Loss: 93.4940 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 16\tNet Loss: 92.4801 \tQuestion Loss: 92.4801 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 16\tNet Loss: 89.2708 \tQuestion Loss: 89.2708 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 16\tNet Loss: 90.5556 \tQuestion Loss: 90.5556 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 16\tNet Loss: 94.8837 \tQuestion Loss: 94.8837 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 16\tNet Loss: 97.6202 \tQuestion Loss: 97.6202 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 16\tNet Loss: 90.1166 \tQuestion Loss: 90.1166 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 16\tNet Loss: 91.5070 \tQuestion Loss: 91.5070 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 16\tNet Loss: 90.4749 \tQuestion Loss: 90.4749 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 16\tNet Loss: 91.6848 \tQuestion Loss: 91.6848 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 16\tNet Loss: 90.0975 \tQuestion Loss: 90.0975 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 16\tNet Loss: 92.3628 \tQuestion Loss: 92.3628 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 16\tNet Loss: 94.3606 \tQuestion Loss: 94.3606 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 16\tNet Loss: 89.3915 \tQuestion Loss: 89.3915 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 16\tNet Loss: 93.2762 \tQuestion Loss: 93.2762 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 16\tNet Loss: 91.5843 \tQuestion Loss: 91.5843 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 16\tNet Loss: 93.9436 \tQuestion Loss: 93.9436 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 16\tNet Loss: 95.0315 \tQuestion Loss: 95.0315 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 16\tNet Loss: 89.9118 \tQuestion Loss: 89.9118 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 16\tNet Loss: 89.8654 \tQuestion Loss: 89.8654 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 16\tNet Loss: 91.0802 \tQuestion Loss: 91.0802 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 16\tNet Loss: 90.0303 \tQuestion Loss: 90.0303 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 16\tNet Loss: 94.8549 \tQuestion Loss: 94.8549 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 16\tNet Loss: 93.4329 \tQuestion Loss: 93.4329 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 16\tNet Loss: 95.1660 \tQuestion Loss: 95.1660 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 16\tNet Loss: 87.9534 \tQuestion Loss: 87.9534 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 16\tNet Loss: 89.1592 \tQuestion Loss: 89.1592 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 16\tNet Loss: 94.5970 \tQuestion Loss: 94.5970 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 16\tNet Loss: 87.7455 \tQuestion Loss: 87.7455 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 16\tNet Loss: 92.3722 \tQuestion Loss: 92.3722 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 16 : 92.3127 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 17\tNet Loss: 92.7220 \tQuestion Loss: 92.7220 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 17\tNet Loss: 89.0776 \tQuestion Loss: 89.0776 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 17\tNet Loss: 94.3541 \tQuestion Loss: 94.3541 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 17\tNet Loss: 93.5779 \tQuestion Loss: 93.5779 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 17\tNet Loss: 93.7163 \tQuestion Loss: 93.7163 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 17\tNet Loss: 93.6330 \tQuestion Loss: 93.6330 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 17\tNet Loss: 88.8118 \tQuestion Loss: 88.8118 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 17\tNet Loss: 94.7786 \tQuestion Loss: 94.7786 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 17\tNet Loss: 93.3158 \tQuestion Loss: 93.3158 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 17\tNet Loss: 94.8393 \tQuestion Loss: 94.8393 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 17\tNet Loss: 91.0064 \tQuestion Loss: 91.0064 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 17\tNet Loss: 95.6539 \tQuestion Loss: 95.6539 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 17\tNet Loss: 90.3394 \tQuestion Loss: 90.3394 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 17\tNet Loss: 92.7233 \tQuestion Loss: 92.7233 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 17\tNet Loss: 92.9831 \tQuestion Loss: 92.9831 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 17\tNet Loss: 95.4387 \tQuestion Loss: 95.4387 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 17\tNet Loss: 90.5301 \tQuestion Loss: 90.5301 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 17\tNet Loss: 92.3661 \tQuestion Loss: 92.3661 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 17\tNet Loss: 90.5942 \tQuestion Loss: 90.5942 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 17\tNet Loss: 86.7617 \tQuestion Loss: 86.7617 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 17\tNet Loss: 96.0696 \tQuestion Loss: 96.0696 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 17\tNet Loss: 88.1405 \tQuestion Loss: 88.1405 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 17\tNet Loss: 92.8800 \tQuestion Loss: 92.8800 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 17\tNet Loss: 95.8182 \tQuestion Loss: 95.8182 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 17\tNet Loss: 89.8368 \tQuestion Loss: 89.8368 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 17\tNet Loss: 93.1430 \tQuestion Loss: 93.1430 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 17\tNet Loss: 98.4148 \tQuestion Loss: 98.4148 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 17\tNet Loss: 91.8373 \tQuestion Loss: 91.8373 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 17\tNet Loss: 86.9913 \tQuestion Loss: 86.9913 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 17\tNet Loss: 89.8978 \tQuestion Loss: 89.8978 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 17\tNet Loss: 91.6890 \tQuestion Loss: 91.6890 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 17\tNet Loss: 90.3645 \tQuestion Loss: 90.3645 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 17\tNet Loss: 88.7665 \tQuestion Loss: 88.7665 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 17\tNet Loss: 93.3350 \tQuestion Loss: 93.3350 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 17\tNet Loss: 87.3192 \tQuestion Loss: 87.3192 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 35 \t Epoch : 17\tNet Loss: 90.1149 \tQuestion Loss: 90.1149 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 17\tNet Loss: 93.8290 \tQuestion Loss: 93.8290 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 17\tNet Loss: 95.2065 \tQuestion Loss: 95.2065 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 17\tNet Loss: 91.2056 \tQuestion Loss: 91.2056 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 17\tNet Loss: 93.8973 \tQuestion Loss: 93.8973 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 17\tNet Loss: 91.4772 \tQuestion Loss: 91.4772 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 17\tNet Loss: 90.2211 \tQuestion Loss: 90.2211 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 17\tNet Loss: 92.0935 \tQuestion Loss: 92.0935 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 17\tNet Loss: 93.3183 \tQuestion Loss: 93.3183 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 17\tNet Loss: 90.6252 \tQuestion Loss: 90.6252 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 17\tNet Loss: 93.7504 \tQuestion Loss: 93.7504 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 17\tNet Loss: 92.3100 \tQuestion Loss: 92.3100 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 17\tNet Loss: 97.3164 \tQuestion Loss: 97.3164 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 17\tNet Loss: 86.0884 \tQuestion Loss: 86.0884 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 17\tNet Loss: 93.9865 \tQuestion Loss: 93.9865 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 17\tNet Loss: 92.9912 \tQuestion Loss: 92.9912 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 17\tNet Loss: 88.5249 \tQuestion Loss: 88.5249 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 17\tNet Loss: 89.5026 \tQuestion Loss: 89.5026 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 17\tNet Loss: 89.7971 \tQuestion Loss: 89.7971 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 17\tNet Loss: 96.5268 \tQuestion Loss: 96.5268 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 17\tNet Loss: 93.2150 \tQuestion Loss: 93.2150 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 17\tNet Loss: 94.3685 \tQuestion Loss: 94.3685 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 17\tNet Loss: 96.5906 \tQuestion Loss: 96.5906 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 17\tNet Loss: 87.6137 \tQuestion Loss: 87.6137 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 17\tNet Loss: 92.1561 \tQuestion Loss: 92.1561 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 17\tNet Loss: 89.1046 \tQuestion Loss: 89.1046 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 17\tNet Loss: 95.2792 \tQuestion Loss: 95.2792 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 17\tNet Loss: 88.8708 \tQuestion Loss: 88.8708 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 17\tNet Loss: 93.5519 \tQuestion Loss: 93.5519 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 17\tNet Loss: 92.2258 \tQuestion Loss: 92.2258 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 17\tNet Loss: 90.6718 \tQuestion Loss: 90.6718 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 17\tNet Loss: 92.1344 \tQuestion Loss: 92.1344 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 17\tNet Loss: 91.3099 \tQuestion Loss: 91.3099 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 17\tNet Loss: 90.8629 \tQuestion Loss: 90.8629 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 17\tNet Loss: 95.5930 \tQuestion Loss: 95.5930 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 17\tNet Loss: 93.1947 \tQuestion Loss: 93.1947 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 17\tNet Loss: 92.7327 \tQuestion Loss: 92.7327 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 17\tNet Loss: 95.6250 \tQuestion Loss: 95.6250 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 17\tNet Loss: 93.7361 \tQuestion Loss: 93.7361 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 17\tNet Loss: 94.9847 \tQuestion Loss: 94.9847 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 17\tNet Loss: 93.5803 \tQuestion Loss: 93.5803 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 17\tNet Loss: 86.8691 \tQuestion Loss: 86.8691 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 17\tNet Loss: 91.5899 \tQuestion Loss: 91.5899 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 17\tNet Loss: 96.3464 \tQuestion Loss: 96.3464 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 17\tNet Loss: 92.9358 \tQuestion Loss: 92.9358 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 17\tNet Loss: 94.3308 \tQuestion Loss: 94.3308 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 17\tNet Loss: 92.1787 \tQuestion Loss: 92.1787 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 17\tNet Loss: 89.6064 \tQuestion Loss: 89.6064 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 17\tNet Loss: 88.7258 \tQuestion Loss: 88.7258 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 17\tNet Loss: 92.1084 \tQuestion Loss: 92.1084 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 17\tNet Loss: 90.4256 \tQuestion Loss: 90.4256 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 17\tNet Loss: 92.0865 \tQuestion Loss: 92.0865 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 17\tNet Loss: 91.7001 \tQuestion Loss: 91.7001 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 17\tNet Loss: 94.4138 \tQuestion Loss: 94.4138 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 17\tNet Loss: 90.0332 \tQuestion Loss: 90.0332 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 17\tNet Loss: 95.3199 \tQuestion Loss: 95.3199 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 17\tNet Loss: 92.9760 \tQuestion Loss: 92.9760 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 17\tNet Loss: 91.4745 \tQuestion Loss: 91.4745 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 17\tNet Loss: 92.0845 \tQuestion Loss: 92.0845 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 17\tNet Loss: 92.7756 \tQuestion Loss: 92.7756 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 17\tNet Loss: 93.4996 \tQuestion Loss: 93.4996 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 17\tNet Loss: 92.0003 \tQuestion Loss: 92.0003 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 17\tNet Loss: 95.8597 \tQuestion Loss: 95.8597 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 17\tNet Loss: 92.7195 \tQuestion Loss: 92.7195 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 17\tNet Loss: 93.1611 \tQuestion Loss: 93.1611 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 17\tNet Loss: 92.4960 \tQuestion Loss: 92.4960 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 17\tNet Loss: 95.5762 \tQuestion Loss: 95.5762 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 17\tNet Loss: 93.0408 \tQuestion Loss: 93.0408 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 17\tNet Loss: 92.4740 \tQuestion Loss: 92.4740 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 17\tNet Loss: 95.2565 \tQuestion Loss: 95.2565 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 17\tNet Loss: 96.4414 \tQuestion Loss: 96.4414 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 17\tNet Loss: 92.1984 \tQuestion Loss: 92.1984 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 17\tNet Loss: 89.2770 \tQuestion Loss: 89.2770 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 17\tNet Loss: 88.4137 \tQuestion Loss: 88.4137 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 17\tNet Loss: 92.8779 \tQuestion Loss: 92.8779 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 17\tNet Loss: 93.2731 \tQuestion Loss: 93.2731 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 17\tNet Loss: 87.8345 \tQuestion Loss: 87.8345 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 17\tNet Loss: 89.7645 \tQuestion Loss: 89.7645 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 17\tNet Loss: 97.2199 \tQuestion Loss: 97.2199 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 17\tNet Loss: 94.0928 \tQuestion Loss: 94.0928 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 17\tNet Loss: 91.0041 \tQuestion Loss: 91.0041 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 17\tNet Loss: 88.3499 \tQuestion Loss: 88.3499 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 17\tNet Loss: 90.6454 \tQuestion Loss: 90.6454 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 17\tNet Loss: 93.8572 \tQuestion Loss: 93.8572 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 17\tNet Loss: 89.6445 \tQuestion Loss: 89.6445 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 17\tNet Loss: 91.5413 \tQuestion Loss: 91.5413 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 17\tNet Loss: 93.8156 \tQuestion Loss: 93.8156 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 17\tNet Loss: 96.8474 \tQuestion Loss: 96.8474 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 17\tNet Loss: 92.7634 \tQuestion Loss: 92.7634 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 17\tNet Loss: 94.7629 \tQuestion Loss: 94.7629 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 17\tNet Loss: 92.8398 \tQuestion Loss: 92.8398 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 17\tNet Loss: 96.3956 \tQuestion Loss: 96.3956 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 127 \t Epoch : 17\tNet Loss: 88.0680 \tQuestion Loss: 88.0680 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 17\tNet Loss: 93.2553 \tQuestion Loss: 93.2553 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 17\tNet Loss: 93.2084 \tQuestion Loss: 93.2084 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 17\tNet Loss: 100.1271 \tQuestion Loss: 100.1271 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 17\tNet Loss: 94.7814 \tQuestion Loss: 94.7814 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 17\tNet Loss: 91.1653 \tQuestion Loss: 91.1653 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 17\tNet Loss: 92.8634 \tQuestion Loss: 92.8634 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 17\tNet Loss: 92.9076 \tQuestion Loss: 92.9076 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 17\tNet Loss: 86.0926 \tQuestion Loss: 86.0926 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 17\tNet Loss: 97.3647 \tQuestion Loss: 97.3647 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 17\tNet Loss: 95.3566 \tQuestion Loss: 95.3566 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 17\tNet Loss: 93.3076 \tQuestion Loss: 93.3076 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 17\tNet Loss: 92.1803 \tQuestion Loss: 92.1803 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 17\tNet Loss: 89.3266 \tQuestion Loss: 89.3266 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 17\tNet Loss: 89.8171 \tQuestion Loss: 89.8171 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 17\tNet Loss: 94.5033 \tQuestion Loss: 94.5033 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 17\tNet Loss: 89.5489 \tQuestion Loss: 89.5489 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 17\tNet Loss: 87.0727 \tQuestion Loss: 87.0727 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 17\tNet Loss: 95.3352 \tQuestion Loss: 95.3352 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 17\tNet Loss: 98.7594 \tQuestion Loss: 98.7594 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 17\tNet Loss: 87.0463 \tQuestion Loss: 87.0463 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 17\tNet Loss: 96.5229 \tQuestion Loss: 96.5229 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 17\tNet Loss: 90.1032 \tQuestion Loss: 90.1032 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 17\tNet Loss: 91.1881 \tQuestion Loss: 91.1881 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 17\tNet Loss: 98.0702 \tQuestion Loss: 98.0702 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 17\tNet Loss: 94.1200 \tQuestion Loss: 94.1200 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 17\tNet Loss: 94.5019 \tQuestion Loss: 94.5019 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 17\tNet Loss: 85.1674 \tQuestion Loss: 85.1674 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 17\tNet Loss: 90.5571 \tQuestion Loss: 90.5571 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 17\tNet Loss: 93.5725 \tQuestion Loss: 93.5725 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 17\tNet Loss: 88.3536 \tQuestion Loss: 88.3536 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 17\tNet Loss: 93.6957 \tQuestion Loss: 93.6957 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 17\tNet Loss: 94.8193 \tQuestion Loss: 94.8193 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 17\tNet Loss: 95.9923 \tQuestion Loss: 95.9923 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 17\tNet Loss: 94.3647 \tQuestion Loss: 94.3647 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 17\tNet Loss: 95.9749 \tQuestion Loss: 95.9749 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 17\tNet Loss: 89.6565 \tQuestion Loss: 89.6565 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 17\tNet Loss: 88.6083 \tQuestion Loss: 88.6083 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 17\tNet Loss: 89.7740 \tQuestion Loss: 89.7740 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 17\tNet Loss: 87.5904 \tQuestion Loss: 87.5904 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 17\tNet Loss: 92.9552 \tQuestion Loss: 92.9552 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 17\tNet Loss: 91.5049 \tQuestion Loss: 91.5049 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 17\tNet Loss: 95.8727 \tQuestion Loss: 95.8727 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 17\tNet Loss: 93.6336 \tQuestion Loss: 93.6336 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 17\tNet Loss: 92.6436 \tQuestion Loss: 92.6436 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 17\tNet Loss: 88.9905 \tQuestion Loss: 88.9905 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 17\tNet Loss: 90.7078 \tQuestion Loss: 90.7078 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 17\tNet Loss: 94.9258 \tQuestion Loss: 94.9258 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 17\tNet Loss: 97.7922 \tQuestion Loss: 97.7922 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 17\tNet Loss: 90.1415 \tQuestion Loss: 90.1415 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 17\tNet Loss: 91.6516 \tQuestion Loss: 91.6516 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 17\tNet Loss: 90.5201 \tQuestion Loss: 90.5201 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 17\tNet Loss: 91.9781 \tQuestion Loss: 91.9781 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 17\tNet Loss: 90.1482 \tQuestion Loss: 90.1482 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 17\tNet Loss: 92.4854 \tQuestion Loss: 92.4854 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 17\tNet Loss: 94.3877 \tQuestion Loss: 94.3877 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 17\tNet Loss: 89.6237 \tQuestion Loss: 89.6237 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 17\tNet Loss: 93.5116 \tQuestion Loss: 93.5116 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 17\tNet Loss: 91.6796 \tQuestion Loss: 91.6796 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 17\tNet Loss: 94.3965 \tQuestion Loss: 94.3965 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 17\tNet Loss: 95.2384 \tQuestion Loss: 95.2384 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 17\tNet Loss: 89.9833 \tQuestion Loss: 89.9833 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 17\tNet Loss: 89.8351 \tQuestion Loss: 89.8351 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 17\tNet Loss: 91.2160 \tQuestion Loss: 91.2160 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 17\tNet Loss: 90.0886 \tQuestion Loss: 90.0886 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 17\tNet Loss: 94.9879 \tQuestion Loss: 94.9879 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 17\tNet Loss: 93.4014 \tQuestion Loss: 93.4014 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 17\tNet Loss: 95.0817 \tQuestion Loss: 95.0817 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 17\tNet Loss: 87.9633 \tQuestion Loss: 87.9633 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 17\tNet Loss: 89.3188 \tQuestion Loss: 89.3188 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 17\tNet Loss: 94.7676 \tQuestion Loss: 94.7676 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 17\tNet Loss: 87.9565 \tQuestion Loss: 87.9565 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 17\tNet Loss: 92.4456 \tQuestion Loss: 92.4456 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 17 : 92.3320 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 18\tNet Loss: 92.7786 \tQuestion Loss: 92.7786 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 18\tNet Loss: 89.1154 \tQuestion Loss: 89.1154 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 18\tNet Loss: 94.3024 \tQuestion Loss: 94.3024 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 18\tNet Loss: 93.5479 \tQuestion Loss: 93.5479 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 18\tNet Loss: 93.8866 \tQuestion Loss: 93.8866 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 18\tNet Loss: 93.7591 \tQuestion Loss: 93.7591 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 18\tNet Loss: 88.8296 \tQuestion Loss: 88.8296 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 18\tNet Loss: 94.9799 \tQuestion Loss: 94.9799 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 18\tNet Loss: 93.5994 \tQuestion Loss: 93.5994 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 18\tNet Loss: 95.2166 \tQuestion Loss: 95.2166 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 18\tNet Loss: 91.2395 \tQuestion Loss: 91.2395 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 18\tNet Loss: 95.6387 \tQuestion Loss: 95.6387 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 18\tNet Loss: 90.4640 \tQuestion Loss: 90.4640 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 18\tNet Loss: 92.7146 \tQuestion Loss: 92.7146 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 18\tNet Loss: 93.1271 \tQuestion Loss: 93.1271 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 18\tNet Loss: 95.5068 \tQuestion Loss: 95.5068 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 18\tNet Loss: 90.4569 \tQuestion Loss: 90.4569 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 18\tNet Loss: 92.5828 \tQuestion Loss: 92.5828 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 18 \t Epoch : 18\tNet Loss: 91.0347 \tQuestion Loss: 91.0347 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 18\tNet Loss: 86.8682 \tQuestion Loss: 86.8682 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 18\tNet Loss: 96.1747 \tQuestion Loss: 96.1747 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 18\tNet Loss: 88.1913 \tQuestion Loss: 88.1913 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 18\tNet Loss: 92.8074 \tQuestion Loss: 92.8074 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 18\tNet Loss: 95.9282 \tQuestion Loss: 95.9282 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 18\tNet Loss: 89.7833 \tQuestion Loss: 89.7833 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 18\tNet Loss: 93.3229 \tQuestion Loss: 93.3229 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 18\tNet Loss: 98.4150 \tQuestion Loss: 98.4150 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 18\tNet Loss: 91.9432 \tQuestion Loss: 91.9432 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 18\tNet Loss: 87.0381 \tQuestion Loss: 87.0381 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 18\tNet Loss: 89.8731 \tQuestion Loss: 89.8731 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 18\tNet Loss: 91.5439 \tQuestion Loss: 91.5439 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 18\tNet Loss: 90.4123 \tQuestion Loss: 90.4123 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 18\tNet Loss: 89.1421 \tQuestion Loss: 89.1421 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 18\tNet Loss: 93.3351 \tQuestion Loss: 93.3351 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 18\tNet Loss: 87.4498 \tQuestion Loss: 87.4498 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 18\tNet Loss: 90.1128 \tQuestion Loss: 90.1128 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 18\tNet Loss: 93.9046 \tQuestion Loss: 93.9046 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 18\tNet Loss: 95.2260 \tQuestion Loss: 95.2260 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 18\tNet Loss: 91.0894 \tQuestion Loss: 91.0894 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 18\tNet Loss: 94.1818 \tQuestion Loss: 94.1818 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 18\tNet Loss: 91.5189 \tQuestion Loss: 91.5189 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 18\tNet Loss: 90.3553 \tQuestion Loss: 90.3553 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 18\tNet Loss: 92.1765 \tQuestion Loss: 92.1765 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 18\tNet Loss: 93.5053 \tQuestion Loss: 93.5053 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 18\tNet Loss: 90.6622 \tQuestion Loss: 90.6622 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 18\tNet Loss: 93.6122 \tQuestion Loss: 93.6122 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 18\tNet Loss: 92.1349 \tQuestion Loss: 92.1349 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 18\tNet Loss: 97.2590 \tQuestion Loss: 97.2590 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 18\tNet Loss: 86.8097 \tQuestion Loss: 86.8097 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 18\tNet Loss: 94.2949 \tQuestion Loss: 94.2949 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 18\tNet Loss: 92.9336 \tQuestion Loss: 92.9336 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 18\tNet Loss: 88.5098 \tQuestion Loss: 88.5098 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 18\tNet Loss: 89.5823 \tQuestion Loss: 89.5823 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 18\tNet Loss: 89.9649 \tQuestion Loss: 89.9649 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 18\tNet Loss: 96.3887 \tQuestion Loss: 96.3887 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 18\tNet Loss: 92.9603 \tQuestion Loss: 92.9603 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 18\tNet Loss: 94.3985 \tQuestion Loss: 94.3985 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 18\tNet Loss: 96.7051 \tQuestion Loss: 96.7051 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 18\tNet Loss: 87.5458 \tQuestion Loss: 87.5458 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 18\tNet Loss: 92.1202 \tQuestion Loss: 92.1202 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 18\tNet Loss: 89.0769 \tQuestion Loss: 89.0769 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 18\tNet Loss: 95.2322 \tQuestion Loss: 95.2322 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 18\tNet Loss: 88.8570 \tQuestion Loss: 88.8570 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 18\tNet Loss: 93.4172 \tQuestion Loss: 93.4172 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 18\tNet Loss: 91.9481 \tQuestion Loss: 91.9481 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 18\tNet Loss: 90.4182 \tQuestion Loss: 90.4182 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 18\tNet Loss: 92.5795 \tQuestion Loss: 92.5795 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 18\tNet Loss: 92.1270 \tQuestion Loss: 92.1270 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 18\tNet Loss: 90.8426 \tQuestion Loss: 90.8426 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 18\tNet Loss: 95.8923 \tQuestion Loss: 95.8923 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 18\tNet Loss: 93.2406 \tQuestion Loss: 93.2406 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 18\tNet Loss: 92.8360 \tQuestion Loss: 92.8360 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 18\tNet Loss: 95.4892 \tQuestion Loss: 95.4892 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 18\tNet Loss: 93.5435 \tQuestion Loss: 93.5435 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 18\tNet Loss: 94.8166 \tQuestion Loss: 94.8166 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 18\tNet Loss: 93.5356 \tQuestion Loss: 93.5356 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 18\tNet Loss: 86.7932 \tQuestion Loss: 86.7932 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 18\tNet Loss: 91.4182 \tQuestion Loss: 91.4182 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 18\tNet Loss: 96.9269 \tQuestion Loss: 96.9269 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 18\tNet Loss: 92.8309 \tQuestion Loss: 92.8309 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 18\tNet Loss: 94.1940 \tQuestion Loss: 94.1940 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 18\tNet Loss: 92.0887 \tQuestion Loss: 92.0887 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 18\tNet Loss: 89.6078 \tQuestion Loss: 89.6078 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 18\tNet Loss: 88.5037 \tQuestion Loss: 88.5037 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 18\tNet Loss: 91.7775 \tQuestion Loss: 91.7775 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 18\tNet Loss: 90.1554 \tQuestion Loss: 90.1554 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 18\tNet Loss: 92.3740 \tQuestion Loss: 92.3740 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 18\tNet Loss: 91.6516 \tQuestion Loss: 91.6516 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 18\tNet Loss: 94.4849 \tQuestion Loss: 94.4849 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 18\tNet Loss: 90.0438 \tQuestion Loss: 90.0438 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 18\tNet Loss: 95.2965 \tQuestion Loss: 95.2965 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 18\tNet Loss: 92.9201 \tQuestion Loss: 92.9201 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 18\tNet Loss: 91.3070 \tQuestion Loss: 91.3070 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 18\tNet Loss: 91.8065 \tQuestion Loss: 91.8065 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 18\tNet Loss: 92.5431 \tQuestion Loss: 92.5431 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 18\tNet Loss: 93.4214 \tQuestion Loss: 93.4214 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 18\tNet Loss: 91.7859 \tQuestion Loss: 91.7859 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 18\tNet Loss: 95.6991 \tQuestion Loss: 95.6991 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 18\tNet Loss: 92.6790 \tQuestion Loss: 92.6790 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 18\tNet Loss: 92.9557 \tQuestion Loss: 92.9557 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 18\tNet Loss: 92.5470 \tQuestion Loss: 92.5470 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 18\tNet Loss: 95.5026 \tQuestion Loss: 95.5026 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 18\tNet Loss: 93.0159 \tQuestion Loss: 93.0159 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 18\tNet Loss: 92.5039 \tQuestion Loss: 92.5039 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 18\tNet Loss: 94.9903 \tQuestion Loss: 94.9903 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 18\tNet Loss: 96.6500 \tQuestion Loss: 96.6500 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 18\tNet Loss: 92.1537 \tQuestion Loss: 92.1537 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 18\tNet Loss: 89.2320 \tQuestion Loss: 89.2320 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 18\tNet Loss: 88.4387 \tQuestion Loss: 88.4387 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 18\tNet Loss: 92.8835 \tQuestion Loss: 92.8835 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 110 \t Epoch : 18\tNet Loss: 93.3110 \tQuestion Loss: 93.3110 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 18\tNet Loss: 87.6913 \tQuestion Loss: 87.6913 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 18\tNet Loss: 89.7718 \tQuestion Loss: 89.7718 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 18\tNet Loss: 97.1529 \tQuestion Loss: 97.1529 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 18\tNet Loss: 94.2142 \tQuestion Loss: 94.2142 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 18\tNet Loss: 91.2808 \tQuestion Loss: 91.2808 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 18\tNet Loss: 88.3087 \tQuestion Loss: 88.3087 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 18\tNet Loss: 90.5480 \tQuestion Loss: 90.5480 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 18\tNet Loss: 93.8173 \tQuestion Loss: 93.8173 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 18\tNet Loss: 89.6958 \tQuestion Loss: 89.6958 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 18\tNet Loss: 91.4995 \tQuestion Loss: 91.4995 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 18\tNet Loss: 93.4335 \tQuestion Loss: 93.4335 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 18\tNet Loss: 97.6591 \tQuestion Loss: 97.6591 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 18\tNet Loss: 92.8834 \tQuestion Loss: 92.8834 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 18\tNet Loss: 94.7754 \tQuestion Loss: 94.7754 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 18\tNet Loss: 92.5481 \tQuestion Loss: 92.5481 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 18\tNet Loss: 96.0819 \tQuestion Loss: 96.0819 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 18\tNet Loss: 87.9875 \tQuestion Loss: 87.9875 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 18\tNet Loss: 92.3172 \tQuestion Loss: 92.3172 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 18\tNet Loss: 94.8002 \tQuestion Loss: 94.8002 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 18\tNet Loss: 99.8959 \tQuestion Loss: 99.8959 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 18\tNet Loss: 94.7953 \tQuestion Loss: 94.7953 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 18\tNet Loss: 91.3118 \tQuestion Loss: 91.3118 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 18\tNet Loss: 92.6000 \tQuestion Loss: 92.6000 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 18\tNet Loss: 93.4794 \tQuestion Loss: 93.4794 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 18\tNet Loss: 86.0027 \tQuestion Loss: 86.0027 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 18\tNet Loss: 97.2762 \tQuestion Loss: 97.2762 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 18\tNet Loss: 94.7109 \tQuestion Loss: 94.7109 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 18\tNet Loss: 93.3370 \tQuestion Loss: 93.3370 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 18\tNet Loss: 92.1222 \tQuestion Loss: 92.1222 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 18\tNet Loss: 89.2226 \tQuestion Loss: 89.2226 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 18\tNet Loss: 89.6390 \tQuestion Loss: 89.6390 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 18\tNet Loss: 94.4463 \tQuestion Loss: 94.4463 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 18\tNet Loss: 89.3678 \tQuestion Loss: 89.3678 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 18\tNet Loss: 86.9057 \tQuestion Loss: 86.9057 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 18\tNet Loss: 95.6065 \tQuestion Loss: 95.6065 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 18\tNet Loss: 98.5881 \tQuestion Loss: 98.5881 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 18\tNet Loss: 87.0583 \tQuestion Loss: 87.0583 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 18\tNet Loss: 96.5044 \tQuestion Loss: 96.5044 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 18\tNet Loss: 89.9678 \tQuestion Loss: 89.9678 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 18\tNet Loss: 91.1485 \tQuestion Loss: 91.1485 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 18\tNet Loss: 97.9329 \tQuestion Loss: 97.9329 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 18\tNet Loss: 93.4620 \tQuestion Loss: 93.4620 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 18\tNet Loss: 93.9002 \tQuestion Loss: 93.9002 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 18\tNet Loss: 85.2724 \tQuestion Loss: 85.2724 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 18\tNet Loss: 90.6233 \tQuestion Loss: 90.6233 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 18\tNet Loss: 93.4893 \tQuestion Loss: 93.4893 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 18\tNet Loss: 88.4626 \tQuestion Loss: 88.4626 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 18\tNet Loss: 93.7448 \tQuestion Loss: 93.7448 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 18\tNet Loss: 94.6636 \tQuestion Loss: 94.6636 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 18\tNet Loss: 95.8342 \tQuestion Loss: 95.8342 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 18\tNet Loss: 94.0374 \tQuestion Loss: 94.0374 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 18\tNet Loss: 95.8448 \tQuestion Loss: 95.8448 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 18\tNet Loss: 89.6633 \tQuestion Loss: 89.6633 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 18\tNet Loss: 88.5005 \tQuestion Loss: 88.5005 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 18\tNet Loss: 89.7022 \tQuestion Loss: 89.7022 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 18\tNet Loss: 87.4417 \tQuestion Loss: 87.4417 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 18\tNet Loss: 92.9243 \tQuestion Loss: 92.9243 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 18\tNet Loss: 91.5064 \tQuestion Loss: 91.5064 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 18\tNet Loss: 95.7207 \tQuestion Loss: 95.7207 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 18\tNet Loss: 93.5243 \tQuestion Loss: 93.5243 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 18\tNet Loss: 92.4708 \tQuestion Loss: 92.4708 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 18\tNet Loss: 89.2703 \tQuestion Loss: 89.2703 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 18\tNet Loss: 90.5819 \tQuestion Loss: 90.5819 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 18\tNet Loss: 94.8242 \tQuestion Loss: 94.8242 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 18\tNet Loss: 97.6122 \tQuestion Loss: 97.6122 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 18\tNet Loss: 90.1235 \tQuestion Loss: 90.1235 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 18\tNet Loss: 91.5125 \tQuestion Loss: 91.5125 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 18\tNet Loss: 90.5054 \tQuestion Loss: 90.5054 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 18\tNet Loss: 91.6812 \tQuestion Loss: 91.6812 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 18\tNet Loss: 90.0830 \tQuestion Loss: 90.0830 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 18\tNet Loss: 92.3665 \tQuestion Loss: 92.3665 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 18\tNet Loss: 94.3579 \tQuestion Loss: 94.3579 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 18\tNet Loss: 89.3323 \tQuestion Loss: 89.3323 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 18\tNet Loss: 93.2928 \tQuestion Loss: 93.2928 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 18\tNet Loss: 91.6070 \tQuestion Loss: 91.6070 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 18\tNet Loss: 93.9068 \tQuestion Loss: 93.9068 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 18\tNet Loss: 95.0188 \tQuestion Loss: 95.0188 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 18\tNet Loss: 89.9323 \tQuestion Loss: 89.9323 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 18\tNet Loss: 89.8583 \tQuestion Loss: 89.8583 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 18\tNet Loss: 91.0714 \tQuestion Loss: 91.0714 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 18\tNet Loss: 90.0288 \tQuestion Loss: 90.0288 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 18\tNet Loss: 94.8807 \tQuestion Loss: 94.8807 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 18\tNet Loss: 93.4512 \tQuestion Loss: 93.4512 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 18\tNet Loss: 95.1977 \tQuestion Loss: 95.1977 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 18\tNet Loss: 87.9447 \tQuestion Loss: 87.9447 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 18\tNet Loss: 89.1650 \tQuestion Loss: 89.1650 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 18\tNet Loss: 94.6033 \tQuestion Loss: 94.6033 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 18\tNet Loss: 87.7315 \tQuestion Loss: 87.7315 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 18\tNet Loss: 92.3804 \tQuestion Loss: 92.3804 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 18 : 92.3140 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 19\tNet Loss: 92.7021 \tQuestion Loss: 92.7021 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1 \t Epoch : 19\tNet Loss: 89.0731 \tQuestion Loss: 89.0731 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 19\tNet Loss: 94.3162 \tQuestion Loss: 94.3162 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 19\tNet Loss: 93.5862 \tQuestion Loss: 93.5862 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 19\tNet Loss: 93.7463 \tQuestion Loss: 93.7463 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 19\tNet Loss: 93.6201 \tQuestion Loss: 93.6201 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 19\tNet Loss: 88.8134 \tQuestion Loss: 88.8134 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 19\tNet Loss: 94.8284 \tQuestion Loss: 94.8284 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 19\tNet Loss: 93.3378 \tQuestion Loss: 93.3378 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 19\tNet Loss: 94.8250 \tQuestion Loss: 94.8250 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 19\tNet Loss: 91.0290 \tQuestion Loss: 91.0290 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 19\tNet Loss: 95.6509 \tQuestion Loss: 95.6509 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 19\tNet Loss: 90.3526 \tQuestion Loss: 90.3526 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 19\tNet Loss: 92.7447 \tQuestion Loss: 92.7447 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 19\tNet Loss: 92.9798 \tQuestion Loss: 92.9798 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 19\tNet Loss: 95.4509 \tQuestion Loss: 95.4509 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 19\tNet Loss: 90.5230 \tQuestion Loss: 90.5230 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 19\tNet Loss: 92.3985 \tQuestion Loss: 92.3985 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 19\tNet Loss: 90.5683 \tQuestion Loss: 90.5683 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 19\tNet Loss: 86.7604 \tQuestion Loss: 86.7604 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 19\tNet Loss: 96.0558 \tQuestion Loss: 96.0558 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 19\tNet Loss: 88.1454 \tQuestion Loss: 88.1454 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 19\tNet Loss: 92.9443 \tQuestion Loss: 92.9443 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 19\tNet Loss: 95.8350 \tQuestion Loss: 95.8350 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 19\tNet Loss: 89.8169 \tQuestion Loss: 89.8169 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 19\tNet Loss: 93.1390 \tQuestion Loss: 93.1390 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 19\tNet Loss: 98.4097 \tQuestion Loss: 98.4097 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 19\tNet Loss: 91.8817 \tQuestion Loss: 91.8817 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 19\tNet Loss: 86.9539 \tQuestion Loss: 86.9539 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 19\tNet Loss: 89.9136 \tQuestion Loss: 89.9136 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 19\tNet Loss: 91.6920 \tQuestion Loss: 91.6920 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 19\tNet Loss: 90.4177 \tQuestion Loss: 90.4177 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 19\tNet Loss: 88.7820 \tQuestion Loss: 88.7820 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 19\tNet Loss: 93.3165 \tQuestion Loss: 93.3165 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 19\tNet Loss: 87.3395 \tQuestion Loss: 87.3395 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 19\tNet Loss: 90.1389 \tQuestion Loss: 90.1389 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 19\tNet Loss: 93.8945 \tQuestion Loss: 93.8945 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 19\tNet Loss: 95.2231 \tQuestion Loss: 95.2231 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 19\tNet Loss: 91.1863 \tQuestion Loss: 91.1863 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 19\tNet Loss: 93.8427 \tQuestion Loss: 93.8427 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 19\tNet Loss: 91.5055 \tQuestion Loss: 91.5055 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 19\tNet Loss: 90.2303 \tQuestion Loss: 90.2303 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 19\tNet Loss: 92.0236 \tQuestion Loss: 92.0236 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 19\tNet Loss: 93.3320 \tQuestion Loss: 93.3320 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 19\tNet Loss: 90.6400 \tQuestion Loss: 90.6400 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 19\tNet Loss: 93.7803 \tQuestion Loss: 93.7803 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 19\tNet Loss: 92.3198 \tQuestion Loss: 92.3198 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 19\tNet Loss: 97.3244 \tQuestion Loss: 97.3244 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 19\tNet Loss: 86.0574 \tQuestion Loss: 86.0574 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 19\tNet Loss: 94.1331 \tQuestion Loss: 94.1331 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 19\tNet Loss: 92.9817 \tQuestion Loss: 92.9817 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 19\tNet Loss: 88.5232 \tQuestion Loss: 88.5232 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 19\tNet Loss: 89.5261 \tQuestion Loss: 89.5261 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 19\tNet Loss: 89.8016 \tQuestion Loss: 89.8016 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 19\tNet Loss: 96.5631 \tQuestion Loss: 96.5631 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 19\tNet Loss: 93.1881 \tQuestion Loss: 93.1881 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 19\tNet Loss: 94.3939 \tQuestion Loss: 94.3939 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 19\tNet Loss: 96.6343 \tQuestion Loss: 96.6343 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 19\tNet Loss: 87.6126 \tQuestion Loss: 87.6126 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 19\tNet Loss: 92.1308 \tQuestion Loss: 92.1308 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 19\tNet Loss: 89.1122 \tQuestion Loss: 89.1122 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 19\tNet Loss: 95.3048 \tQuestion Loss: 95.3048 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 19\tNet Loss: 88.8457 \tQuestion Loss: 88.8457 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 19\tNet Loss: 93.5369 \tQuestion Loss: 93.5369 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 19\tNet Loss: 92.2568 \tQuestion Loss: 92.2568 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 19\tNet Loss: 90.7778 \tQuestion Loss: 90.7778 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 19\tNet Loss: 92.1551 \tQuestion Loss: 92.1551 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 19\tNet Loss: 91.2959 \tQuestion Loss: 91.2959 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 19\tNet Loss: 90.8038 \tQuestion Loss: 90.8038 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 19\tNet Loss: 95.5927 \tQuestion Loss: 95.5927 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 19\tNet Loss: 93.1880 \tQuestion Loss: 93.1880 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 19\tNet Loss: 92.7505 \tQuestion Loss: 92.7505 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 19\tNet Loss: 95.6205 \tQuestion Loss: 95.6205 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 19\tNet Loss: 93.7376 \tQuestion Loss: 93.7376 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 19\tNet Loss: 94.9697 \tQuestion Loss: 94.9697 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 19\tNet Loss: 93.6042 \tQuestion Loss: 93.6042 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 19\tNet Loss: 86.8698 \tQuestion Loss: 86.8698 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 19\tNet Loss: 91.5978 \tQuestion Loss: 91.5978 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 19\tNet Loss: 96.3020 \tQuestion Loss: 96.3020 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 19\tNet Loss: 92.9516 \tQuestion Loss: 92.9516 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 19\tNet Loss: 94.3457 \tQuestion Loss: 94.3457 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 19\tNet Loss: 92.1932 \tQuestion Loss: 92.1932 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 19\tNet Loss: 89.5695 \tQuestion Loss: 89.5695 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 19\tNet Loss: 88.7279 \tQuestion Loss: 88.7279 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 19\tNet Loss: 92.1293 \tQuestion Loss: 92.1293 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 19\tNet Loss: 90.5289 \tQuestion Loss: 90.5289 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 19\tNet Loss: 92.1148 \tQuestion Loss: 92.1148 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 19\tNet Loss: 91.7097 \tQuestion Loss: 91.7097 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 19\tNet Loss: 94.3995 \tQuestion Loss: 94.3995 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 19\tNet Loss: 90.0666 \tQuestion Loss: 90.0666 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 19\tNet Loss: 95.3582 \tQuestion Loss: 95.3582 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 19\tNet Loss: 93.0415 \tQuestion Loss: 93.0415 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 19\tNet Loss: 91.5089 \tQuestion Loss: 91.5089 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 19\tNet Loss: 92.0912 \tQuestion Loss: 92.0912 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 94 \t Epoch : 19\tNet Loss: 92.7685 \tQuestion Loss: 92.7685 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 19\tNet Loss: 93.5129 \tQuestion Loss: 93.5129 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 19\tNet Loss: 92.0613 \tQuestion Loss: 92.0613 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 19\tNet Loss: 95.8356 \tQuestion Loss: 95.8356 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 19\tNet Loss: 92.7715 \tQuestion Loss: 92.7715 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 19\tNet Loss: 93.1808 \tQuestion Loss: 93.1808 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 19\tNet Loss: 92.4928 \tQuestion Loss: 92.4928 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 19\tNet Loss: 95.5946 \tQuestion Loss: 95.5946 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 19\tNet Loss: 93.0165 \tQuestion Loss: 93.0165 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 19\tNet Loss: 92.5019 \tQuestion Loss: 92.5019 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 19\tNet Loss: 95.2712 \tQuestion Loss: 95.2712 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 19\tNet Loss: 96.4669 \tQuestion Loss: 96.4669 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 19\tNet Loss: 92.2369 \tQuestion Loss: 92.2369 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 19\tNet Loss: 89.2569 \tQuestion Loss: 89.2569 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 19\tNet Loss: 88.3838 \tQuestion Loss: 88.3838 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 19\tNet Loss: 92.9067 \tQuestion Loss: 92.9067 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 19\tNet Loss: 93.3337 \tQuestion Loss: 93.3337 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 19\tNet Loss: 87.8207 \tQuestion Loss: 87.8207 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 19\tNet Loss: 89.7591 \tQuestion Loss: 89.7591 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 19\tNet Loss: 97.1964 \tQuestion Loss: 97.1964 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 19\tNet Loss: 94.1470 \tQuestion Loss: 94.1470 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 19\tNet Loss: 91.0782 \tQuestion Loss: 91.0782 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 19\tNet Loss: 88.3639 \tQuestion Loss: 88.3639 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 19\tNet Loss: 90.5666 \tQuestion Loss: 90.5666 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 19\tNet Loss: 93.8724 \tQuestion Loss: 93.8724 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 19\tNet Loss: 89.6842 \tQuestion Loss: 89.6842 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 19\tNet Loss: 91.5334 \tQuestion Loss: 91.5334 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 19\tNet Loss: 93.8289 \tQuestion Loss: 93.8289 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 19\tNet Loss: 96.8077 \tQuestion Loss: 96.8077 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 19\tNet Loss: 92.7880 \tQuestion Loss: 92.7880 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 19\tNet Loss: 94.7734 \tQuestion Loss: 94.7734 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 19\tNet Loss: 92.8101 \tQuestion Loss: 92.8101 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 19\tNet Loss: 96.3651 \tQuestion Loss: 96.3651 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 19\tNet Loss: 88.0684 \tQuestion Loss: 88.0684 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 19\tNet Loss: 93.3093 \tQuestion Loss: 93.3093 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 19\tNet Loss: 93.2111 \tQuestion Loss: 93.2111 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 19\tNet Loss: 100.1187 \tQuestion Loss: 100.1187 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 19\tNet Loss: 94.7832 \tQuestion Loss: 94.7832 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 19\tNet Loss: 91.1555 \tQuestion Loss: 91.1555 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 19\tNet Loss: 92.8264 \tQuestion Loss: 92.8264 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 19\tNet Loss: 92.8915 \tQuestion Loss: 92.8915 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 19\tNet Loss: 86.0862 \tQuestion Loss: 86.0862 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 19\tNet Loss: 97.3273 \tQuestion Loss: 97.3273 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 19\tNet Loss: 95.2986 \tQuestion Loss: 95.2986 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 19\tNet Loss: 93.3900 \tQuestion Loss: 93.3900 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 19\tNet Loss: 92.1875 \tQuestion Loss: 92.1875 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 19\tNet Loss: 89.3280 \tQuestion Loss: 89.3280 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 19\tNet Loss: 89.8011 \tQuestion Loss: 89.8011 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 19\tNet Loss: 94.5509 \tQuestion Loss: 94.5509 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 19\tNet Loss: 89.5452 \tQuestion Loss: 89.5452 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 19\tNet Loss: 87.0452 \tQuestion Loss: 87.0452 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 19\tNet Loss: 95.3361 \tQuestion Loss: 95.3361 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 19\tNet Loss: 98.7553 \tQuestion Loss: 98.7553 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 19\tNet Loss: 87.1157 \tQuestion Loss: 87.1157 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 19\tNet Loss: 96.5029 \tQuestion Loss: 96.5029 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 19\tNet Loss: 90.1016 \tQuestion Loss: 90.1016 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 19\tNet Loss: 91.1678 \tQuestion Loss: 91.1678 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 19\tNet Loss: 98.0617 \tQuestion Loss: 98.0617 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 19\tNet Loss: 94.0995 \tQuestion Loss: 94.0995 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 19\tNet Loss: 94.5102 \tQuestion Loss: 94.5102 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 19\tNet Loss: 85.2011 \tQuestion Loss: 85.2011 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 19\tNet Loss: 90.5585 \tQuestion Loss: 90.5585 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 19\tNet Loss: 93.5604 \tQuestion Loss: 93.5604 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 19\tNet Loss: 88.3321 \tQuestion Loss: 88.3321 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 19\tNet Loss: 93.6682 \tQuestion Loss: 93.6682 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 19\tNet Loss: 94.8223 \tQuestion Loss: 94.8223 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 19\tNet Loss: 96.0112 \tQuestion Loss: 96.0112 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 19\tNet Loss: 94.3062 \tQuestion Loss: 94.3062 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 19\tNet Loss: 95.9631 \tQuestion Loss: 95.9631 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 19\tNet Loss: 89.6539 \tQuestion Loss: 89.6539 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 19\tNet Loss: 88.5828 \tQuestion Loss: 88.5828 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 19\tNet Loss: 89.7468 \tQuestion Loss: 89.7468 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 19\tNet Loss: 87.5689 \tQuestion Loss: 87.5689 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 19\tNet Loss: 92.9891 \tQuestion Loss: 92.9891 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 19\tNet Loss: 91.4932 \tQuestion Loss: 91.4932 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 19\tNet Loss: 95.8638 \tQuestion Loss: 95.8638 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 19\tNet Loss: 93.6198 \tQuestion Loss: 93.6198 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 19\tNet Loss: 92.6466 \tQuestion Loss: 92.6466 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 19\tNet Loss: 88.9843 \tQuestion Loss: 88.9843 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 19\tNet Loss: 90.6870 \tQuestion Loss: 90.6870 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 19\tNet Loss: 94.9627 \tQuestion Loss: 94.9627 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 19\tNet Loss: 97.7890 \tQuestion Loss: 97.7890 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 19\tNet Loss: 90.1361 \tQuestion Loss: 90.1361 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 19\tNet Loss: 91.6710 \tQuestion Loss: 91.6710 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 19\tNet Loss: 90.5136 \tQuestion Loss: 90.5136 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 19\tNet Loss: 92.0182 \tQuestion Loss: 92.0182 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 19\tNet Loss: 90.1554 \tQuestion Loss: 90.1554 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 19\tNet Loss: 92.4830 \tQuestion Loss: 92.4830 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 19\tNet Loss: 94.3818 \tQuestion Loss: 94.3818 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 19\tNet Loss: 89.6236 \tQuestion Loss: 89.6236 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 19\tNet Loss: 93.4974 \tQuestion Loss: 93.4974 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 19\tNet Loss: 91.6648 \tQuestion Loss: 91.6648 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 186 \t Epoch : 19\tNet Loss: 94.4292 \tQuestion Loss: 94.4292 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 19\tNet Loss: 95.2503 \tQuestion Loss: 95.2503 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 19\tNet Loss: 89.9747 \tQuestion Loss: 89.9747 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 19\tNet Loss: 89.8424 \tQuestion Loss: 89.8424 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 19\tNet Loss: 91.2016 \tQuestion Loss: 91.2016 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 19\tNet Loss: 90.0848 \tQuestion Loss: 90.0848 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 19\tNet Loss: 94.9898 \tQuestion Loss: 94.9898 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 19\tNet Loss: 93.4006 \tQuestion Loss: 93.4006 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 19\tNet Loss: 95.0843 \tQuestion Loss: 95.0843 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 19\tNet Loss: 87.9728 \tQuestion Loss: 87.9728 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 19\tNet Loss: 89.3246 \tQuestion Loss: 89.3246 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 19\tNet Loss: 94.7356 \tQuestion Loss: 94.7356 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 19\tNet Loss: 87.9796 \tQuestion Loss: 87.9796 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 19\tNet Loss: 92.4204 \tQuestion Loss: 92.4204 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 19 : 92.3370 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 20\tNet Loss: 92.7978 \tQuestion Loss: 92.7978 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 20\tNet Loss: 89.1324 \tQuestion Loss: 89.1324 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 20\tNet Loss: 94.3350 \tQuestion Loss: 94.3350 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 20\tNet Loss: 93.5496 \tQuestion Loss: 93.5496 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 20\tNet Loss: 93.8664 \tQuestion Loss: 93.8664 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 20\tNet Loss: 93.7779 \tQuestion Loss: 93.7779 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 20\tNet Loss: 88.8380 \tQuestion Loss: 88.8380 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 20\tNet Loss: 94.9898 \tQuestion Loss: 94.9898 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 20\tNet Loss: 93.6090 \tQuestion Loss: 93.6090 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 20\tNet Loss: 95.2035 \tQuestion Loss: 95.2035 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 20\tNet Loss: 91.2156 \tQuestion Loss: 91.2156 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 20\tNet Loss: 95.6551 \tQuestion Loss: 95.6551 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 20\tNet Loss: 90.4659 \tQuestion Loss: 90.4659 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 20\tNet Loss: 92.7165 \tQuestion Loss: 92.7165 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 20\tNet Loss: 93.0972 \tQuestion Loss: 93.0972 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 20\tNet Loss: 95.5257 \tQuestion Loss: 95.5257 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 20\tNet Loss: 90.4515 \tQuestion Loss: 90.4515 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 20\tNet Loss: 92.5846 \tQuestion Loss: 92.5846 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 20\tNet Loss: 91.0753 \tQuestion Loss: 91.0753 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 20\tNet Loss: 86.8632 \tQuestion Loss: 86.8632 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 20\tNet Loss: 96.1720 \tQuestion Loss: 96.1720 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 20\tNet Loss: 88.2281 \tQuestion Loss: 88.2281 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 20\tNet Loss: 92.8108 \tQuestion Loss: 92.8108 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 20\tNet Loss: 95.9059 \tQuestion Loss: 95.9059 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 20\tNet Loss: 89.7804 \tQuestion Loss: 89.7804 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 20\tNet Loss: 93.3260 \tQuestion Loss: 93.3260 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 20\tNet Loss: 98.4233 \tQuestion Loss: 98.4233 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 20\tNet Loss: 91.9354 \tQuestion Loss: 91.9354 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 20\tNet Loss: 87.0184 \tQuestion Loss: 87.0184 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 20\tNet Loss: 89.8996 \tQuestion Loss: 89.8996 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 20\tNet Loss: 91.5488 \tQuestion Loss: 91.5488 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 20\tNet Loss: 90.4350 \tQuestion Loss: 90.4350 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 20\tNet Loss: 89.1027 \tQuestion Loss: 89.1027 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 20\tNet Loss: 93.3123 \tQuestion Loss: 93.3123 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 20\tNet Loss: 87.4435 \tQuestion Loss: 87.4435 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 20\tNet Loss: 90.1365 \tQuestion Loss: 90.1365 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 20\tNet Loss: 93.9239 \tQuestion Loss: 93.9239 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 20\tNet Loss: 95.1562 \tQuestion Loss: 95.1562 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 20\tNet Loss: 91.0851 \tQuestion Loss: 91.0851 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 20\tNet Loss: 94.2186 \tQuestion Loss: 94.2186 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 20\tNet Loss: 91.5635 \tQuestion Loss: 91.5635 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 20\tNet Loss: 90.3649 \tQuestion Loss: 90.3649 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 20\tNet Loss: 92.1386 \tQuestion Loss: 92.1386 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 20\tNet Loss: 93.5028 \tQuestion Loss: 93.5028 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 20\tNet Loss: 90.7035 \tQuestion Loss: 90.7035 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 20\tNet Loss: 93.6510 \tQuestion Loss: 93.6510 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 20\tNet Loss: 92.0974 \tQuestion Loss: 92.0974 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 20\tNet Loss: 97.2310 \tQuestion Loss: 97.2310 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 20\tNet Loss: 86.8522 \tQuestion Loss: 86.8522 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 20\tNet Loss: 94.2486 \tQuestion Loss: 94.2486 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 20\tNet Loss: 92.9336 \tQuestion Loss: 92.9336 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 20\tNet Loss: 88.5060 \tQuestion Loss: 88.5060 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 20\tNet Loss: 89.5549 \tQuestion Loss: 89.5549 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 20\tNet Loss: 89.9872 \tQuestion Loss: 89.9872 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 20\tNet Loss: 96.3968 \tQuestion Loss: 96.3968 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 20\tNet Loss: 92.9641 \tQuestion Loss: 92.9641 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 20\tNet Loss: 94.4557 \tQuestion Loss: 94.4557 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 20\tNet Loss: 96.7499 \tQuestion Loss: 96.7499 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 20\tNet Loss: 87.5672 \tQuestion Loss: 87.5672 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 20\tNet Loss: 92.1899 \tQuestion Loss: 92.1899 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 20\tNet Loss: 89.0710 \tQuestion Loss: 89.0710 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 20\tNet Loss: 95.2414 \tQuestion Loss: 95.2414 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 20\tNet Loss: 88.8615 \tQuestion Loss: 88.8615 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 20\tNet Loss: 93.4247 \tQuestion Loss: 93.4247 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 20\tNet Loss: 91.9628 \tQuestion Loss: 91.9628 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 20\tNet Loss: 90.3405 \tQuestion Loss: 90.3405 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 20\tNet Loss: 92.5661 \tQuestion Loss: 92.5661 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 20\tNet Loss: 92.1130 \tQuestion Loss: 92.1130 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 20\tNet Loss: 90.7628 \tQuestion Loss: 90.7628 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 20\tNet Loss: 95.9652 \tQuestion Loss: 95.9652 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 20\tNet Loss: 93.3108 \tQuestion Loss: 93.3108 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 20\tNet Loss: 92.8339 \tQuestion Loss: 92.8339 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 20\tNet Loss: 95.4255 \tQuestion Loss: 95.4255 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 20\tNet Loss: 93.5379 \tQuestion Loss: 93.5379 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 20\tNet Loss: 94.8414 \tQuestion Loss: 94.8414 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 20\tNet Loss: 93.5600 \tQuestion Loss: 93.5600 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 20\tNet Loss: 86.7496 \tQuestion Loss: 86.7496 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 20\tNet Loss: 91.3805 \tQuestion Loss: 91.3805 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 78 \t Epoch : 20\tNet Loss: 96.9705 \tQuestion Loss: 96.9705 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 20\tNet Loss: 92.8761 \tQuestion Loss: 92.8761 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 20\tNet Loss: 94.2226 \tQuestion Loss: 94.2226 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 20\tNet Loss: 92.0724 \tQuestion Loss: 92.0724 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 20\tNet Loss: 89.6347 \tQuestion Loss: 89.6347 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 20\tNet Loss: 88.5330 \tQuestion Loss: 88.5330 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 20\tNet Loss: 91.7458 \tQuestion Loss: 91.7458 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 20\tNet Loss: 90.1362 \tQuestion Loss: 90.1362 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 20\tNet Loss: 92.3195 \tQuestion Loss: 92.3195 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 20\tNet Loss: 91.6679 \tQuestion Loss: 91.6679 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 20\tNet Loss: 94.4725 \tQuestion Loss: 94.4725 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 20\tNet Loss: 90.0954 \tQuestion Loss: 90.0954 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 20\tNet Loss: 95.3049 \tQuestion Loss: 95.3049 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 20\tNet Loss: 92.8662 \tQuestion Loss: 92.8662 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 20\tNet Loss: 91.2879 \tQuestion Loss: 91.2879 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 20\tNet Loss: 91.8176 \tQuestion Loss: 91.8176 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 20\tNet Loss: 92.5555 \tQuestion Loss: 92.5555 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 20\tNet Loss: 93.4359 \tQuestion Loss: 93.4359 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 20\tNet Loss: 91.7977 \tQuestion Loss: 91.7977 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 20\tNet Loss: 95.6737 \tQuestion Loss: 95.6737 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 20\tNet Loss: 92.6627 \tQuestion Loss: 92.6627 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 20\tNet Loss: 92.9501 \tQuestion Loss: 92.9501 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 20\tNet Loss: 92.5472 \tQuestion Loss: 92.5472 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 20\tNet Loss: 95.4988 \tQuestion Loss: 95.4988 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 20\tNet Loss: 93.0514 \tQuestion Loss: 93.0514 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 20\tNet Loss: 92.4793 \tQuestion Loss: 92.4793 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 20\tNet Loss: 94.9933 \tQuestion Loss: 94.9933 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 20\tNet Loss: 96.6486 \tQuestion Loss: 96.6486 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 20\tNet Loss: 92.2555 \tQuestion Loss: 92.2555 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 20\tNet Loss: 89.2509 \tQuestion Loss: 89.2509 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 20\tNet Loss: 88.4619 \tQuestion Loss: 88.4619 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 20\tNet Loss: 92.8986 \tQuestion Loss: 92.8986 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 20\tNet Loss: 93.2790 \tQuestion Loss: 93.2790 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 20\tNet Loss: 87.7356 \tQuestion Loss: 87.7356 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 20\tNet Loss: 89.7941 \tQuestion Loss: 89.7941 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 20\tNet Loss: 97.1721 \tQuestion Loss: 97.1721 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 20\tNet Loss: 94.1732 \tQuestion Loss: 94.1732 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 20\tNet Loss: 91.2654 \tQuestion Loss: 91.2654 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 20\tNet Loss: 88.2969 \tQuestion Loss: 88.2969 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 20\tNet Loss: 90.5574 \tQuestion Loss: 90.5574 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 20\tNet Loss: 93.8236 \tQuestion Loss: 93.8236 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 20\tNet Loss: 89.7541 \tQuestion Loss: 89.7541 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 20\tNet Loss: 91.4663 \tQuestion Loss: 91.4663 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 20\tNet Loss: 93.4287 \tQuestion Loss: 93.4287 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 20\tNet Loss: 97.6825 \tQuestion Loss: 97.6825 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 20\tNet Loss: 92.9331 \tQuestion Loss: 92.9331 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 20\tNet Loss: 94.7696 \tQuestion Loss: 94.7696 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 20\tNet Loss: 92.5269 \tQuestion Loss: 92.5269 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 20\tNet Loss: 96.1488 \tQuestion Loss: 96.1488 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 20\tNet Loss: 88.0279 \tQuestion Loss: 88.0279 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 20\tNet Loss: 92.2550 \tQuestion Loss: 92.2550 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 20\tNet Loss: 94.7946 \tQuestion Loss: 94.7946 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 20\tNet Loss: 99.9128 \tQuestion Loss: 99.9128 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 20\tNet Loss: 94.8017 \tQuestion Loss: 94.8017 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 20\tNet Loss: 91.3243 \tQuestion Loss: 91.3243 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 20\tNet Loss: 92.6203 \tQuestion Loss: 92.6203 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 20\tNet Loss: 93.5006 \tQuestion Loss: 93.5006 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 20\tNet Loss: 86.0105 \tQuestion Loss: 86.0105 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 20\tNet Loss: 97.2562 \tQuestion Loss: 97.2562 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 20\tNet Loss: 94.7773 \tQuestion Loss: 94.7773 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 20\tNet Loss: 93.2979 \tQuestion Loss: 93.2979 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 20\tNet Loss: 92.1018 \tQuestion Loss: 92.1018 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 20\tNet Loss: 89.2181 \tQuestion Loss: 89.2181 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 20\tNet Loss: 89.6689 \tQuestion Loss: 89.6689 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 20\tNet Loss: 94.5020 \tQuestion Loss: 94.5020 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 20\tNet Loss: 89.3134 \tQuestion Loss: 89.3134 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 20\tNet Loss: 86.8644 \tQuestion Loss: 86.8644 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 20\tNet Loss: 95.6328 \tQuestion Loss: 95.6328 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 20\tNet Loss: 98.6337 \tQuestion Loss: 98.6337 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 20\tNet Loss: 87.0605 \tQuestion Loss: 87.0605 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 20\tNet Loss: 96.4755 \tQuestion Loss: 96.4755 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 20\tNet Loss: 89.9659 \tQuestion Loss: 89.9659 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 20\tNet Loss: 91.1971 \tQuestion Loss: 91.1971 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 20\tNet Loss: 98.0063 \tQuestion Loss: 98.0063 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 20\tNet Loss: 93.4167 \tQuestion Loss: 93.4167 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 20\tNet Loss: 93.8869 \tQuestion Loss: 93.8869 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 20\tNet Loss: 85.2788 \tQuestion Loss: 85.2788 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 20\tNet Loss: 90.6522 \tQuestion Loss: 90.6522 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 20\tNet Loss: 93.4822 \tQuestion Loss: 93.4822 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 20\tNet Loss: 88.4282 \tQuestion Loss: 88.4282 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 20\tNet Loss: 93.7460 \tQuestion Loss: 93.7460 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 20\tNet Loss: 94.6977 \tQuestion Loss: 94.6977 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 20\tNet Loss: 95.8475 \tQuestion Loss: 95.8475 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 20\tNet Loss: 94.0291 \tQuestion Loss: 94.0291 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 20\tNet Loss: 95.8315 \tQuestion Loss: 95.8315 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 20\tNet Loss: 89.6674 \tQuestion Loss: 89.6674 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 20\tNet Loss: 88.5251 \tQuestion Loss: 88.5251 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 20\tNet Loss: 89.6910 \tQuestion Loss: 89.6910 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 20\tNet Loss: 87.4563 \tQuestion Loss: 87.4563 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 20\tNet Loss: 92.8681 \tQuestion Loss: 92.8681 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 20\tNet Loss: 91.5099 \tQuestion Loss: 91.5099 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 20\tNet Loss: 95.7368 \tQuestion Loss: 95.7368 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 170 \t Epoch : 20\tNet Loss: 93.5202 \tQuestion Loss: 93.5202 \t Time Taken: 1 seconds\n",
      "Batch: 171 \t Epoch : 20\tNet Loss: 92.4853 \tQuestion Loss: 92.4853 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 20\tNet Loss: 89.2470 \tQuestion Loss: 89.2470 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 20\tNet Loss: 90.5732 \tQuestion Loss: 90.5732 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 20\tNet Loss: 94.7727 \tQuestion Loss: 94.7727 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 20\tNet Loss: 97.6171 \tQuestion Loss: 97.6171 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 20\tNet Loss: 90.1282 \tQuestion Loss: 90.1282 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 20\tNet Loss: 91.5303 \tQuestion Loss: 91.5303 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 20\tNet Loss: 90.5199 \tQuestion Loss: 90.5199 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 20\tNet Loss: 91.6546 \tQuestion Loss: 91.6546 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 20\tNet Loss: 90.0960 \tQuestion Loss: 90.0960 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 20\tNet Loss: 92.3510 \tQuestion Loss: 92.3510 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 20\tNet Loss: 94.3538 \tQuestion Loss: 94.3538 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 20\tNet Loss: 89.3750 \tQuestion Loss: 89.3750 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 20\tNet Loss: 93.3093 \tQuestion Loss: 93.3093 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 20\tNet Loss: 91.6191 \tQuestion Loss: 91.6191 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 20\tNet Loss: 93.9001 \tQuestion Loss: 93.9001 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 20\tNet Loss: 95.0286 \tQuestion Loss: 95.0286 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 20\tNet Loss: 89.9618 \tQuestion Loss: 89.9618 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 20\tNet Loss: 89.8145 \tQuestion Loss: 89.8145 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 20\tNet Loss: 91.0860 \tQuestion Loss: 91.0860 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 20\tNet Loss: 90.0328 \tQuestion Loss: 90.0328 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 20\tNet Loss: 94.9052 \tQuestion Loss: 94.9052 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 20\tNet Loss: 93.4473 \tQuestion Loss: 93.4473 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 20\tNet Loss: 95.1837 \tQuestion Loss: 95.1837 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 20\tNet Loss: 87.9426 \tQuestion Loss: 87.9426 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 20\tNet Loss: 89.1859 \tQuestion Loss: 89.1859 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 20\tNet Loss: 94.6074 \tQuestion Loss: 94.6074 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 20\tNet Loss: 87.7436 \tQuestion Loss: 87.7436 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 20\tNet Loss: 92.3901 \tQuestion Loss: 92.3901 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 20 : 92.3178 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 21\tNet Loss: 92.6909 \tQuestion Loss: 92.6909 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 21\tNet Loss: 89.0641 \tQuestion Loss: 89.0641 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 21\tNet Loss: 94.3159 \tQuestion Loss: 94.3159 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 21\tNet Loss: 93.5935 \tQuestion Loss: 93.5935 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 21\tNet Loss: 93.7582 \tQuestion Loss: 93.7582 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 21\tNet Loss: 93.6511 \tQuestion Loss: 93.6511 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 21\tNet Loss: 88.8276 \tQuestion Loss: 88.8276 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 21\tNet Loss: 94.8772 \tQuestion Loss: 94.8772 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 21\tNet Loss: 93.3513 \tQuestion Loss: 93.3513 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 21\tNet Loss: 94.8143 \tQuestion Loss: 94.8143 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 21\tNet Loss: 91.0567 \tQuestion Loss: 91.0567 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 21\tNet Loss: 95.6363 \tQuestion Loss: 95.6363 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 21\tNet Loss: 90.3646 \tQuestion Loss: 90.3646 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 21\tNet Loss: 92.7546 \tQuestion Loss: 92.7546 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 21\tNet Loss: 92.9837 \tQuestion Loss: 92.9837 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 21\tNet Loss: 95.4582 \tQuestion Loss: 95.4582 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 21\tNet Loss: 90.5527 \tQuestion Loss: 90.5527 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 21\tNet Loss: 92.4075 \tQuestion Loss: 92.4075 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 21\tNet Loss: 90.5507 \tQuestion Loss: 90.5507 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 21\tNet Loss: 86.7588 \tQuestion Loss: 86.7588 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 21\tNet Loss: 96.0530 \tQuestion Loss: 96.0530 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 21\tNet Loss: 88.1729 \tQuestion Loss: 88.1729 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 21\tNet Loss: 92.9554 \tQuestion Loss: 92.9554 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 21\tNet Loss: 95.8353 \tQuestion Loss: 95.8353 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 21\tNet Loss: 89.8188 \tQuestion Loss: 89.8188 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 21\tNet Loss: 93.1924 \tQuestion Loss: 93.1924 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 21\tNet Loss: 98.4460 \tQuestion Loss: 98.4460 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 21\tNet Loss: 91.8790 \tQuestion Loss: 91.8790 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 21\tNet Loss: 86.9229 \tQuestion Loss: 86.9229 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 21\tNet Loss: 89.9075 \tQuestion Loss: 89.9075 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 21\tNet Loss: 91.7056 \tQuestion Loss: 91.7056 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 21\tNet Loss: 90.4247 \tQuestion Loss: 90.4247 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 21\tNet Loss: 88.7613 \tQuestion Loss: 88.7613 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 21\tNet Loss: 93.3007 \tQuestion Loss: 93.3007 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 21\tNet Loss: 87.3250 \tQuestion Loss: 87.3250 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 21\tNet Loss: 90.1686 \tQuestion Loss: 90.1686 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 21\tNet Loss: 93.8969 \tQuestion Loss: 93.8969 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 21\tNet Loss: 95.2343 \tQuestion Loss: 95.2343 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 21\tNet Loss: 91.2276 \tQuestion Loss: 91.2276 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 21\tNet Loss: 93.8730 \tQuestion Loss: 93.8730 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 21\tNet Loss: 91.5378 \tQuestion Loss: 91.5378 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 21\tNet Loss: 90.2015 \tQuestion Loss: 90.2015 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 21\tNet Loss: 92.0449 \tQuestion Loss: 92.0449 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 21\tNet Loss: 93.3226 \tQuestion Loss: 93.3226 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 21\tNet Loss: 90.6345 \tQuestion Loss: 90.6345 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 21\tNet Loss: 93.7622 \tQuestion Loss: 93.7622 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 21\tNet Loss: 92.2980 \tQuestion Loss: 92.2980 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 21\tNet Loss: 97.3448 \tQuestion Loss: 97.3448 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 21\tNet Loss: 86.0935 \tQuestion Loss: 86.0935 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 21\tNet Loss: 93.9936 \tQuestion Loss: 93.9936 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 21\tNet Loss: 92.9649 \tQuestion Loss: 92.9649 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 21\tNet Loss: 88.5066 \tQuestion Loss: 88.5066 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 21\tNet Loss: 89.5556 \tQuestion Loss: 89.5556 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 21\tNet Loss: 89.8260 \tQuestion Loss: 89.8260 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 21\tNet Loss: 96.5200 \tQuestion Loss: 96.5200 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 21\tNet Loss: 93.2183 \tQuestion Loss: 93.2183 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 21\tNet Loss: 94.3883 \tQuestion Loss: 94.3883 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 21\tNet Loss: 96.7532 \tQuestion Loss: 96.7532 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 21\tNet Loss: 87.6126 \tQuestion Loss: 87.6126 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 21\tNet Loss: 92.2240 \tQuestion Loss: 92.2240 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 21\tNet Loss: 89.1091 \tQuestion Loss: 89.1091 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 21\tNet Loss: 95.3522 \tQuestion Loss: 95.3522 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 62 \t Epoch : 21\tNet Loss: 88.8524 \tQuestion Loss: 88.8524 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 21\tNet Loss: 93.5175 \tQuestion Loss: 93.5175 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 21\tNet Loss: 92.2372 \tQuestion Loss: 92.2372 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 21\tNet Loss: 90.8516 \tQuestion Loss: 90.8516 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 21\tNet Loss: 92.1505 \tQuestion Loss: 92.1505 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 21\tNet Loss: 91.2829 \tQuestion Loss: 91.2829 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 21\tNet Loss: 90.8892 \tQuestion Loss: 90.8892 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 21\tNet Loss: 95.6060 \tQuestion Loss: 95.6060 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 21\tNet Loss: 93.2232 \tQuestion Loss: 93.2232 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 21\tNet Loss: 92.7249 \tQuestion Loss: 92.7249 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 21\tNet Loss: 95.6689 \tQuestion Loss: 95.6689 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 21\tNet Loss: 93.7710 \tQuestion Loss: 93.7710 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 21\tNet Loss: 94.9632 \tQuestion Loss: 94.9632 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 21\tNet Loss: 93.5861 \tQuestion Loss: 93.5861 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 21\tNet Loss: 86.8304 \tQuestion Loss: 86.8304 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 21\tNet Loss: 91.6589 \tQuestion Loss: 91.6589 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 21\tNet Loss: 96.2899 \tQuestion Loss: 96.2899 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 21\tNet Loss: 92.9620 \tQuestion Loss: 92.9620 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 21\tNet Loss: 94.3221 \tQuestion Loss: 94.3221 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 21\tNet Loss: 92.1790 \tQuestion Loss: 92.1790 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 21\tNet Loss: 89.5986 \tQuestion Loss: 89.5986 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 21\tNet Loss: 88.7419 \tQuestion Loss: 88.7419 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 21\tNet Loss: 92.1645 \tQuestion Loss: 92.1645 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 21\tNet Loss: 90.5121 \tQuestion Loss: 90.5121 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 21\tNet Loss: 92.1255 \tQuestion Loss: 92.1255 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 21\tNet Loss: 91.6994 \tQuestion Loss: 91.6994 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 21\tNet Loss: 94.4380 \tQuestion Loss: 94.4380 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 21\tNet Loss: 90.0360 \tQuestion Loss: 90.0360 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 21\tNet Loss: 95.3459 \tQuestion Loss: 95.3459 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 21\tNet Loss: 92.9288 \tQuestion Loss: 92.9288 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 21\tNet Loss: 91.3898 \tQuestion Loss: 91.3898 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 21\tNet Loss: 92.0906 \tQuestion Loss: 92.0906 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 21\tNet Loss: 92.7695 \tQuestion Loss: 92.7695 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 21\tNet Loss: 93.5047 \tQuestion Loss: 93.5047 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 21\tNet Loss: 91.9704 \tQuestion Loss: 91.9704 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 21\tNet Loss: 95.8471 \tQuestion Loss: 95.8471 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 21\tNet Loss: 92.7883 \tQuestion Loss: 92.7883 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 21\tNet Loss: 93.1890 \tQuestion Loss: 93.1890 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 21\tNet Loss: 92.4555 \tQuestion Loss: 92.4555 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 21\tNet Loss: 95.6033 \tQuestion Loss: 95.6033 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 21\tNet Loss: 93.0206 \tQuestion Loss: 93.0206 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 21\tNet Loss: 92.5134 \tQuestion Loss: 92.5134 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 21\tNet Loss: 95.2432 \tQuestion Loss: 95.2432 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 21\tNet Loss: 96.4703 \tQuestion Loss: 96.4703 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 21\tNet Loss: 92.2701 \tQuestion Loss: 92.2701 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 21\tNet Loss: 89.2588 \tQuestion Loss: 89.2588 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 21\tNet Loss: 88.3755 \tQuestion Loss: 88.3755 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 21\tNet Loss: 92.8923 \tQuestion Loss: 92.8923 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 21\tNet Loss: 93.3940 \tQuestion Loss: 93.3940 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 21\tNet Loss: 87.8350 \tQuestion Loss: 87.8350 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 21\tNet Loss: 89.7360 \tQuestion Loss: 89.7360 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 21\tNet Loss: 97.1513 \tQuestion Loss: 97.1513 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 21\tNet Loss: 94.2122 \tQuestion Loss: 94.2122 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 21\tNet Loss: 91.1245 \tQuestion Loss: 91.1245 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 21\tNet Loss: 88.3797 \tQuestion Loss: 88.3797 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 21\tNet Loss: 90.5533 \tQuestion Loss: 90.5533 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 21\tNet Loss: 93.8429 \tQuestion Loss: 93.8429 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 21\tNet Loss: 89.6634 \tQuestion Loss: 89.6634 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 21\tNet Loss: 91.4682 \tQuestion Loss: 91.4682 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 21\tNet Loss: 93.8286 \tQuestion Loss: 93.8286 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 21\tNet Loss: 96.8041 \tQuestion Loss: 96.8041 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 21\tNet Loss: 92.7778 \tQuestion Loss: 92.7778 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 21\tNet Loss: 94.6943 \tQuestion Loss: 94.6943 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 21\tNet Loss: 92.7513 \tQuestion Loss: 92.7513 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 21\tNet Loss: 96.3622 \tQuestion Loss: 96.3622 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 21\tNet Loss: 88.0942 \tQuestion Loss: 88.0942 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 21\tNet Loss: 93.2847 \tQuestion Loss: 93.2847 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 21\tNet Loss: 93.1515 \tQuestion Loss: 93.1515 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 21\tNet Loss: 100.1141 \tQuestion Loss: 100.1141 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 21\tNet Loss: 94.7810 \tQuestion Loss: 94.7810 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 21\tNet Loss: 91.1504 \tQuestion Loss: 91.1504 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 21\tNet Loss: 92.7697 \tQuestion Loss: 92.7697 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 21\tNet Loss: 92.8546 \tQuestion Loss: 92.8546 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 21\tNet Loss: 86.0798 \tQuestion Loss: 86.0798 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 21\tNet Loss: 97.2718 \tQuestion Loss: 97.2718 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 21\tNet Loss: 95.1925 \tQuestion Loss: 95.1925 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 21\tNet Loss: 93.3543 \tQuestion Loss: 93.3543 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 21\tNet Loss: 92.1252 \tQuestion Loss: 92.1252 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 21\tNet Loss: 89.2995 \tQuestion Loss: 89.2995 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 21\tNet Loss: 89.8082 \tQuestion Loss: 89.8082 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 21\tNet Loss: 94.5342 \tQuestion Loss: 94.5342 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 21\tNet Loss: 89.5226 \tQuestion Loss: 89.5226 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 21\tNet Loss: 86.9975 \tQuestion Loss: 86.9975 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 21\tNet Loss: 95.3358 \tQuestion Loss: 95.3358 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 21\tNet Loss: 98.7751 \tQuestion Loss: 98.7751 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 21\tNet Loss: 87.0850 \tQuestion Loss: 87.0850 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 21\tNet Loss: 96.4736 \tQuestion Loss: 96.4736 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 21\tNet Loss: 90.0392 \tQuestion Loss: 90.0392 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 21\tNet Loss: 91.1689 \tQuestion Loss: 91.1689 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 21\tNet Loss: 98.0062 \tQuestion Loss: 98.0062 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 21\tNet Loss: 94.0355 \tQuestion Loss: 94.0355 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 21\tNet Loss: 94.5489 \tQuestion Loss: 94.5489 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 154 \t Epoch : 21\tNet Loss: 85.2645 \tQuestion Loss: 85.2645 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 21\tNet Loss: 90.5959 \tQuestion Loss: 90.5959 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 21\tNet Loss: 93.5273 \tQuestion Loss: 93.5273 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 21\tNet Loss: 88.3203 \tQuestion Loss: 88.3203 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 21\tNet Loss: 93.7049 \tQuestion Loss: 93.7049 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 21\tNet Loss: 94.8287 \tQuestion Loss: 94.8287 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 21\tNet Loss: 95.9913 \tQuestion Loss: 95.9913 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 21\tNet Loss: 94.2571 \tQuestion Loss: 94.2571 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 21\tNet Loss: 95.9641 \tQuestion Loss: 95.9641 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 21\tNet Loss: 89.7507 \tQuestion Loss: 89.7507 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 21\tNet Loss: 88.5730 \tQuestion Loss: 88.5730 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 21\tNet Loss: 89.7232 \tQuestion Loss: 89.7232 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 21\tNet Loss: 87.5330 \tQuestion Loss: 87.5330 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 21\tNet Loss: 93.0200 \tQuestion Loss: 93.0200 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 21\tNet Loss: 91.5050 \tQuestion Loss: 91.5050 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 21\tNet Loss: 95.8289 \tQuestion Loss: 95.8289 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 21\tNet Loss: 93.6106 \tQuestion Loss: 93.6106 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 21\tNet Loss: 92.6424 \tQuestion Loss: 92.6424 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 21\tNet Loss: 89.0089 \tQuestion Loss: 89.0089 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 21\tNet Loss: 90.6527 \tQuestion Loss: 90.6527 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 21\tNet Loss: 94.9897 \tQuestion Loss: 94.9897 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 21\tNet Loss: 97.7852 \tQuestion Loss: 97.7852 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 21\tNet Loss: 90.1257 \tQuestion Loss: 90.1257 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 21\tNet Loss: 91.6718 \tQuestion Loss: 91.6718 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 21\tNet Loss: 90.5255 \tQuestion Loss: 90.5255 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 21\tNet Loss: 92.0004 \tQuestion Loss: 92.0004 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 21\tNet Loss: 90.1429 \tQuestion Loss: 90.1429 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 21\tNet Loss: 92.4736 \tQuestion Loss: 92.4736 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 21\tNet Loss: 94.3567 \tQuestion Loss: 94.3567 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 21\tNet Loss: 89.6013 \tQuestion Loss: 89.6013 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 21\tNet Loss: 93.4905 \tQuestion Loss: 93.4905 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 21\tNet Loss: 91.6548 \tQuestion Loss: 91.6548 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 21\tNet Loss: 94.4291 \tQuestion Loss: 94.4291 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 21\tNet Loss: 95.3073 \tQuestion Loss: 95.3073 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 21\tNet Loss: 89.9983 \tQuestion Loss: 89.9983 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 21\tNet Loss: 89.8474 \tQuestion Loss: 89.8474 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 21\tNet Loss: 91.1910 \tQuestion Loss: 91.1910 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 21\tNet Loss: 90.0988 \tQuestion Loss: 90.0988 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 21\tNet Loss: 95.0054 \tQuestion Loss: 95.0054 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 21\tNet Loss: 93.3841 \tQuestion Loss: 93.3841 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 21\tNet Loss: 95.0579 \tQuestion Loss: 95.0579 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 21\tNet Loss: 87.9699 \tQuestion Loss: 87.9699 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 21\tNet Loss: 89.3104 \tQuestion Loss: 89.3104 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 21\tNet Loss: 94.7294 \tQuestion Loss: 94.7294 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 21\tNet Loss: 87.9390 \tQuestion Loss: 87.9390 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 21\tNet Loss: 92.4058 \tQuestion Loss: 92.4058 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 21 : 92.3343 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 22\tNet Loss: 92.7794 \tQuestion Loss: 92.7794 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 22\tNet Loss: 89.1586 \tQuestion Loss: 89.1586 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 22\tNet Loss: 94.3346 \tQuestion Loss: 94.3346 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 22\tNet Loss: 93.5460 \tQuestion Loss: 93.5460 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 22\tNet Loss: 93.8536 \tQuestion Loss: 93.8536 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 22\tNet Loss: 93.7359 \tQuestion Loss: 93.7359 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 22\tNet Loss: 88.8701 \tQuestion Loss: 88.8701 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 22\tNet Loss: 94.9491 \tQuestion Loss: 94.9491 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 22\tNet Loss: 93.5830 \tQuestion Loss: 93.5830 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 22\tNet Loss: 95.1949 \tQuestion Loss: 95.1949 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 22\tNet Loss: 91.2211 \tQuestion Loss: 91.2211 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 22\tNet Loss: 95.6504 \tQuestion Loss: 95.6504 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 22\tNet Loss: 90.4744 \tQuestion Loss: 90.4744 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 22\tNet Loss: 92.7057 \tQuestion Loss: 92.7057 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 22\tNet Loss: 93.0922 \tQuestion Loss: 93.0922 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 22\tNet Loss: 95.4717 \tQuestion Loss: 95.4717 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 22\tNet Loss: 90.4532 \tQuestion Loss: 90.4532 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 22\tNet Loss: 92.5731 \tQuestion Loss: 92.5731 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 22\tNet Loss: 91.0441 \tQuestion Loss: 91.0441 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 22\tNet Loss: 86.8567 \tQuestion Loss: 86.8567 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 22\tNet Loss: 96.1507 \tQuestion Loss: 96.1507 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 22\tNet Loss: 88.2424 \tQuestion Loss: 88.2424 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 22\tNet Loss: 92.7685 \tQuestion Loss: 92.7685 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 22\tNet Loss: 95.8888 \tQuestion Loss: 95.8888 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 22\tNet Loss: 89.7895 \tQuestion Loss: 89.7895 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 22\tNet Loss: 93.2984 \tQuestion Loss: 93.2984 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 22\tNet Loss: 98.3898 \tQuestion Loss: 98.3898 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 22\tNet Loss: 91.9045 \tQuestion Loss: 91.9045 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 22\tNet Loss: 87.0173 \tQuestion Loss: 87.0173 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 22\tNet Loss: 89.9013 \tQuestion Loss: 89.9013 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 22\tNet Loss: 91.5319 \tQuestion Loss: 91.5319 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 22\tNet Loss: 90.3840 \tQuestion Loss: 90.3840 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 22\tNet Loss: 89.0900 \tQuestion Loss: 89.0900 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 22\tNet Loss: 93.3051 \tQuestion Loss: 93.3051 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 22\tNet Loss: 87.4637 \tQuestion Loss: 87.4637 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 22\tNet Loss: 90.1225 \tQuestion Loss: 90.1225 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 22\tNet Loss: 93.8577 \tQuestion Loss: 93.8577 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 22\tNet Loss: 95.1234 \tQuestion Loss: 95.1234 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 22\tNet Loss: 91.0608 \tQuestion Loss: 91.0608 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 22\tNet Loss: 94.2160 \tQuestion Loss: 94.2160 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 22\tNet Loss: 91.5276 \tQuestion Loss: 91.5276 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 22\tNet Loss: 90.3654 \tQuestion Loss: 90.3654 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 22\tNet Loss: 92.1315 \tQuestion Loss: 92.1315 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 22\tNet Loss: 93.5486 \tQuestion Loss: 93.5486 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 22\tNet Loss: 90.7132 \tQuestion Loss: 90.7132 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 45 \t Epoch : 22\tNet Loss: 93.6145 \tQuestion Loss: 93.6145 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 22\tNet Loss: 92.0747 \tQuestion Loss: 92.0747 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 22\tNet Loss: 97.2360 \tQuestion Loss: 97.2360 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 22\tNet Loss: 86.8722 \tQuestion Loss: 86.8722 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 22\tNet Loss: 94.2580 \tQuestion Loss: 94.2580 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 22\tNet Loss: 92.9169 \tQuestion Loss: 92.9169 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 22\tNet Loss: 88.5192 \tQuestion Loss: 88.5192 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 22\tNet Loss: 89.5581 \tQuestion Loss: 89.5581 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 22\tNet Loss: 90.0147 \tQuestion Loss: 90.0147 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 22\tNet Loss: 96.3697 \tQuestion Loss: 96.3697 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 22\tNet Loss: 92.9577 \tQuestion Loss: 92.9577 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 22\tNet Loss: 94.4598 \tQuestion Loss: 94.4598 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 22\tNet Loss: 96.6395 \tQuestion Loss: 96.6395 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 22\tNet Loss: 87.5054 \tQuestion Loss: 87.5054 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 22\tNet Loss: 92.0334 \tQuestion Loss: 92.0334 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 22\tNet Loss: 89.1015 \tQuestion Loss: 89.1015 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 22\tNet Loss: 95.2378 \tQuestion Loss: 95.2378 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 22\tNet Loss: 88.8046 \tQuestion Loss: 88.8046 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 22\tNet Loss: 93.4514 \tQuestion Loss: 93.4514 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 22\tNet Loss: 92.0134 \tQuestion Loss: 92.0134 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 22\tNet Loss: 90.4202 \tQuestion Loss: 90.4202 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 22\tNet Loss: 92.5425 \tQuestion Loss: 92.5425 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 22\tNet Loss: 92.1200 \tQuestion Loss: 92.1200 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 22\tNet Loss: 90.7920 \tQuestion Loss: 90.7920 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 22\tNet Loss: 95.9423 \tQuestion Loss: 95.9423 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 22\tNet Loss: 93.2727 \tQuestion Loss: 93.2727 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 22\tNet Loss: 92.7972 \tQuestion Loss: 92.7972 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 22\tNet Loss: 95.3944 \tQuestion Loss: 95.3944 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 22\tNet Loss: 93.5480 \tQuestion Loss: 93.5480 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 22\tNet Loss: 94.8360 \tQuestion Loss: 94.8360 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 22\tNet Loss: 93.5135 \tQuestion Loss: 93.5135 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 22\tNet Loss: 86.7518 \tQuestion Loss: 86.7518 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 22\tNet Loss: 91.3605 \tQuestion Loss: 91.3605 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 22\tNet Loss: 97.0077 \tQuestion Loss: 97.0077 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 22\tNet Loss: 92.8714 \tQuestion Loss: 92.8714 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 22\tNet Loss: 94.2144 \tQuestion Loss: 94.2144 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 22\tNet Loss: 92.0644 \tQuestion Loss: 92.0644 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 22\tNet Loss: 89.6307 \tQuestion Loss: 89.6307 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 22\tNet Loss: 88.5285 \tQuestion Loss: 88.5285 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 22\tNet Loss: 91.6919 \tQuestion Loss: 91.6919 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 22\tNet Loss: 90.0327 \tQuestion Loss: 90.0327 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 22\tNet Loss: 92.2446 \tQuestion Loss: 92.2446 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 22\tNet Loss: 91.6113 \tQuestion Loss: 91.6113 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 22\tNet Loss: 94.3889 \tQuestion Loss: 94.3889 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 22\tNet Loss: 89.9713 \tQuestion Loss: 89.9713 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 22\tNet Loss: 95.3013 \tQuestion Loss: 95.3013 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 22\tNet Loss: 92.8708 \tQuestion Loss: 92.8708 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 22\tNet Loss: 91.2778 \tQuestion Loss: 91.2778 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 22\tNet Loss: 91.8056 \tQuestion Loss: 91.8056 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 22\tNet Loss: 92.5818 \tQuestion Loss: 92.5818 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 22\tNet Loss: 93.4560 \tQuestion Loss: 93.4560 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 22\tNet Loss: 91.8206 \tQuestion Loss: 91.8206 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 22\tNet Loss: 95.6807 \tQuestion Loss: 95.6807 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 22\tNet Loss: 92.6690 \tQuestion Loss: 92.6690 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 22\tNet Loss: 92.9542 \tQuestion Loss: 92.9542 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 22\tNet Loss: 92.5248 \tQuestion Loss: 92.5248 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 22\tNet Loss: 95.5117 \tQuestion Loss: 95.5117 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 22\tNet Loss: 93.0621 \tQuestion Loss: 93.0621 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 22\tNet Loss: 92.5316 \tQuestion Loss: 92.5316 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 22\tNet Loss: 95.0077 \tQuestion Loss: 95.0077 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 22\tNet Loss: 96.6645 \tQuestion Loss: 96.6645 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 22\tNet Loss: 92.2611 \tQuestion Loss: 92.2611 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 22\tNet Loss: 89.2240 \tQuestion Loss: 89.2240 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 22\tNet Loss: 88.3968 \tQuestion Loss: 88.3968 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 22\tNet Loss: 92.9069 \tQuestion Loss: 92.9069 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 22\tNet Loss: 93.2546 \tQuestion Loss: 93.2546 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 22\tNet Loss: 87.7184 \tQuestion Loss: 87.7184 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 22\tNet Loss: 89.7582 \tQuestion Loss: 89.7582 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 22\tNet Loss: 97.2733 \tQuestion Loss: 97.2733 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 22\tNet Loss: 94.1844 \tQuestion Loss: 94.1844 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 22\tNet Loss: 91.2395 \tQuestion Loss: 91.2395 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 22\tNet Loss: 88.1972 \tQuestion Loss: 88.1972 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 22\tNet Loss: 90.5636 \tQuestion Loss: 90.5636 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 22\tNet Loss: 93.9032 \tQuestion Loss: 93.9032 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 22\tNet Loss: 89.7596 \tQuestion Loss: 89.7596 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 22\tNet Loss: 91.4142 \tQuestion Loss: 91.4142 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 22\tNet Loss: 93.5005 \tQuestion Loss: 93.5005 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 22\tNet Loss: 97.6668 \tQuestion Loss: 97.6668 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 22\tNet Loss: 92.9864 \tQuestion Loss: 92.9864 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 22\tNet Loss: 94.7394 \tQuestion Loss: 94.7394 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 22\tNet Loss: 92.5420 \tQuestion Loss: 92.5420 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 22\tNet Loss: 96.1764 \tQuestion Loss: 96.1764 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 22\tNet Loss: 88.0625 \tQuestion Loss: 88.0625 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 22\tNet Loss: 92.1689 \tQuestion Loss: 92.1689 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 22\tNet Loss: 94.8114 \tQuestion Loss: 94.8114 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 22\tNet Loss: 99.9025 \tQuestion Loss: 99.9025 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 22\tNet Loss: 94.8621 \tQuestion Loss: 94.8621 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 22\tNet Loss: 91.3095 \tQuestion Loss: 91.3095 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 22\tNet Loss: 92.5742 \tQuestion Loss: 92.5742 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 22\tNet Loss: 93.5403 \tQuestion Loss: 93.5403 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 22\tNet Loss: 86.0822 \tQuestion Loss: 86.0822 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 22\tNet Loss: 97.2489 \tQuestion Loss: 97.2489 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 137 \t Epoch : 22\tNet Loss: 94.8407 \tQuestion Loss: 94.8407 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 22\tNet Loss: 93.3141 \tQuestion Loss: 93.3141 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 22\tNet Loss: 92.1109 \tQuestion Loss: 92.1109 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 22\tNet Loss: 89.2517 \tQuestion Loss: 89.2517 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 22\tNet Loss: 89.6706 \tQuestion Loss: 89.6706 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 22\tNet Loss: 94.5019 \tQuestion Loss: 94.5019 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 22\tNet Loss: 89.3032 \tQuestion Loss: 89.3032 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 22\tNet Loss: 86.8706 \tQuestion Loss: 86.8706 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 22\tNet Loss: 95.6456 \tQuestion Loss: 95.6456 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 22\tNet Loss: 98.6565 \tQuestion Loss: 98.6565 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 22\tNet Loss: 87.0501 \tQuestion Loss: 87.0501 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 22\tNet Loss: 96.4893 \tQuestion Loss: 96.4893 \t Time Taken: 1 seconds\n",
      "Batch: 149 \t Epoch : 22\tNet Loss: 90.0497 \tQuestion Loss: 90.0497 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 22\tNet Loss: 91.2629 \tQuestion Loss: 91.2629 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 22\tNet Loss: 97.9862 \tQuestion Loss: 97.9862 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 22\tNet Loss: 93.4408 \tQuestion Loss: 93.4408 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 22\tNet Loss: 93.9277 \tQuestion Loss: 93.9277 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 22\tNet Loss: 85.2876 \tQuestion Loss: 85.2876 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 22\tNet Loss: 90.6146 \tQuestion Loss: 90.6146 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 22\tNet Loss: 93.4823 \tQuestion Loss: 93.4823 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 22\tNet Loss: 88.4278 \tQuestion Loss: 88.4278 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 22\tNet Loss: 93.7926 \tQuestion Loss: 93.7926 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 22\tNet Loss: 94.6633 \tQuestion Loss: 94.6633 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 22\tNet Loss: 95.8314 \tQuestion Loss: 95.8314 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 22\tNet Loss: 94.0602 \tQuestion Loss: 94.0602 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 22\tNet Loss: 95.8747 \tQuestion Loss: 95.8747 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 22\tNet Loss: 89.5960 \tQuestion Loss: 89.5960 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 22\tNet Loss: 88.5081 \tQuestion Loss: 88.5081 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 22\tNet Loss: 89.7075 \tQuestion Loss: 89.7075 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 22\tNet Loss: 87.5053 \tQuestion Loss: 87.5053 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 22\tNet Loss: 92.8441 \tQuestion Loss: 92.8441 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 22\tNet Loss: 91.4791 \tQuestion Loss: 91.4791 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 22\tNet Loss: 95.7335 \tQuestion Loss: 95.7335 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 22\tNet Loss: 93.5326 \tQuestion Loss: 93.5326 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 22\tNet Loss: 92.4976 \tQuestion Loss: 92.4976 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 22\tNet Loss: 89.2535 \tQuestion Loss: 89.2535 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 22\tNet Loss: 90.5925 \tQuestion Loss: 90.5925 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 22\tNet Loss: 94.7440 \tQuestion Loss: 94.7440 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 22\tNet Loss: 97.6490 \tQuestion Loss: 97.6490 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 22\tNet Loss: 90.1290 \tQuestion Loss: 90.1290 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 22\tNet Loss: 91.5127 \tQuestion Loss: 91.5127 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 22\tNet Loss: 90.5555 \tQuestion Loss: 90.5555 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 22\tNet Loss: 91.7086 \tQuestion Loss: 91.7086 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 22\tNet Loss: 90.0907 \tQuestion Loss: 90.0907 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 22\tNet Loss: 92.3556 \tQuestion Loss: 92.3556 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 22\tNet Loss: 94.3727 \tQuestion Loss: 94.3727 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 22\tNet Loss: 89.3784 \tQuestion Loss: 89.3784 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 22\tNet Loss: 93.3116 \tQuestion Loss: 93.3116 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 22\tNet Loss: 91.6288 \tQuestion Loss: 91.6288 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 22\tNet Loss: 93.9123 \tQuestion Loss: 93.9123 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 22\tNet Loss: 95.0351 \tQuestion Loss: 95.0351 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 22\tNet Loss: 89.9865 \tQuestion Loss: 89.9865 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 22\tNet Loss: 89.7934 \tQuestion Loss: 89.7934 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 22\tNet Loss: 91.0982 \tQuestion Loss: 91.0982 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 22\tNet Loss: 90.0594 \tQuestion Loss: 90.0594 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 22\tNet Loss: 94.9157 \tQuestion Loss: 94.9157 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 22\tNet Loss: 93.4278 \tQuestion Loss: 93.4278 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 22\tNet Loss: 95.2364 \tQuestion Loss: 95.2364 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 22\tNet Loss: 87.9861 \tQuestion Loss: 87.9861 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 22\tNet Loss: 89.2001 \tQuestion Loss: 89.2001 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 22\tNet Loss: 94.6105 \tQuestion Loss: 94.6105 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 22\tNet Loss: 87.7483 \tQuestion Loss: 87.7483 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 22\tNet Loss: 92.4039 \tQuestion Loss: 92.4039 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 22 : 92.3149 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 23\tNet Loss: 92.7113 \tQuestion Loss: 92.7113 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 23\tNet Loss: 89.0537 \tQuestion Loss: 89.0537 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 23\tNet Loss: 94.3160 \tQuestion Loss: 94.3160 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 23\tNet Loss: 93.6091 \tQuestion Loss: 93.6091 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 23\tNet Loss: 93.7584 \tQuestion Loss: 93.7584 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 23\tNet Loss: 93.6295 \tQuestion Loss: 93.6295 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 23\tNet Loss: 88.8077 \tQuestion Loss: 88.8077 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 23\tNet Loss: 94.8815 \tQuestion Loss: 94.8815 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 23\tNet Loss: 93.3852 \tQuestion Loss: 93.3852 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 23\tNet Loss: 94.8075 \tQuestion Loss: 94.8075 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 23\tNet Loss: 91.0363 \tQuestion Loss: 91.0363 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 23\tNet Loss: 95.6462 \tQuestion Loss: 95.6462 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 23\tNet Loss: 90.3895 \tQuestion Loss: 90.3895 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 23\tNet Loss: 92.7455 \tQuestion Loss: 92.7455 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 23\tNet Loss: 92.9819 \tQuestion Loss: 92.9819 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 23\tNet Loss: 95.4978 \tQuestion Loss: 95.4978 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 23\tNet Loss: 90.5490 \tQuestion Loss: 90.5490 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 23\tNet Loss: 92.4290 \tQuestion Loss: 92.4290 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 23\tNet Loss: 90.5279 \tQuestion Loss: 90.5279 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 23\tNet Loss: 86.7630 \tQuestion Loss: 86.7630 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 23\tNet Loss: 96.0633 \tQuestion Loss: 96.0633 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 23\tNet Loss: 88.1814 \tQuestion Loss: 88.1814 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 23\tNet Loss: 92.9490 \tQuestion Loss: 92.9490 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 23\tNet Loss: 95.8670 \tQuestion Loss: 95.8670 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 23\tNet Loss: 89.8391 \tQuestion Loss: 89.8391 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 23\tNet Loss: 93.2298 \tQuestion Loss: 93.2298 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 23\tNet Loss: 98.4499 \tQuestion Loss: 98.4499 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 23\tNet Loss: 91.8898 \tQuestion Loss: 91.8898 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 28 \t Epoch : 23\tNet Loss: 86.9545 \tQuestion Loss: 86.9545 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 23\tNet Loss: 89.9080 \tQuestion Loss: 89.9080 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 23\tNet Loss: 91.6987 \tQuestion Loss: 91.6987 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 23\tNet Loss: 90.4328 \tQuestion Loss: 90.4328 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 23\tNet Loss: 88.7730 \tQuestion Loss: 88.7730 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 23\tNet Loss: 93.3133 \tQuestion Loss: 93.3133 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 23\tNet Loss: 87.3501 \tQuestion Loss: 87.3501 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 23\tNet Loss: 90.1692 \tQuestion Loss: 90.1692 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 23\tNet Loss: 93.8982 \tQuestion Loss: 93.8982 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 23\tNet Loss: 95.2930 \tQuestion Loss: 95.2930 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 23\tNet Loss: 91.2246 \tQuestion Loss: 91.2246 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 23\tNet Loss: 93.8992 \tQuestion Loss: 93.8992 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 23\tNet Loss: 91.5432 \tQuestion Loss: 91.5432 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 23\tNet Loss: 90.2374 \tQuestion Loss: 90.2374 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 23\tNet Loss: 92.0314 \tQuestion Loss: 92.0314 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 23\tNet Loss: 93.3340 \tQuestion Loss: 93.3340 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 23\tNet Loss: 90.6088 \tQuestion Loss: 90.6088 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 23\tNet Loss: 93.7259 \tQuestion Loss: 93.7259 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 23\tNet Loss: 92.3289 \tQuestion Loss: 92.3289 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 23\tNet Loss: 97.3538 \tQuestion Loss: 97.3538 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 23\tNet Loss: 86.0753 \tQuestion Loss: 86.0753 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 23\tNet Loss: 93.9560 \tQuestion Loss: 93.9560 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 23\tNet Loss: 92.9596 \tQuestion Loss: 92.9596 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 23\tNet Loss: 88.5288 \tQuestion Loss: 88.5288 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 23\tNet Loss: 89.5792 \tQuestion Loss: 89.5792 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 23\tNet Loss: 89.7668 \tQuestion Loss: 89.7668 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 23\tNet Loss: 96.5459 \tQuestion Loss: 96.5459 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 23\tNet Loss: 93.2106 \tQuestion Loss: 93.2106 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 23\tNet Loss: 94.4076 \tQuestion Loss: 94.4076 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 23\tNet Loss: 96.6562 \tQuestion Loss: 96.6562 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 23\tNet Loss: 87.6203 \tQuestion Loss: 87.6203 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 23\tNet Loss: 92.2328 \tQuestion Loss: 92.2328 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 23\tNet Loss: 89.1264 \tQuestion Loss: 89.1264 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 23\tNet Loss: 95.3312 \tQuestion Loss: 95.3312 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 23\tNet Loss: 88.8200 \tQuestion Loss: 88.8200 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 23\tNet Loss: 93.5197 \tQuestion Loss: 93.5197 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 23\tNet Loss: 92.2380 \tQuestion Loss: 92.2380 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 23\tNet Loss: 90.7974 \tQuestion Loss: 90.7974 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 23\tNet Loss: 92.1142 \tQuestion Loss: 92.1142 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 23\tNet Loss: 91.3033 \tQuestion Loss: 91.3033 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 23\tNet Loss: 90.9442 \tQuestion Loss: 90.9442 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 23\tNet Loss: 95.5914 \tQuestion Loss: 95.5914 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 23\tNet Loss: 93.1698 \tQuestion Loss: 93.1698 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 23\tNet Loss: 92.7074 \tQuestion Loss: 92.7074 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 23\tNet Loss: 95.7169 \tQuestion Loss: 95.7169 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 23\tNet Loss: 93.8014 \tQuestion Loss: 93.8014 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 23\tNet Loss: 94.9300 \tQuestion Loss: 94.9300 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 23\tNet Loss: 93.5487 \tQuestion Loss: 93.5487 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 23\tNet Loss: 86.8425 \tQuestion Loss: 86.8425 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 23\tNet Loss: 91.6736 \tQuestion Loss: 91.6736 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 23\tNet Loss: 96.2868 \tQuestion Loss: 96.2868 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 23\tNet Loss: 92.9352 \tQuestion Loss: 92.9352 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 23\tNet Loss: 94.2885 \tQuestion Loss: 94.2885 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 23\tNet Loss: 92.2144 \tQuestion Loss: 92.2144 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 23\tNet Loss: 89.6051 \tQuestion Loss: 89.6051 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 23\tNet Loss: 88.7148 \tQuestion Loss: 88.7148 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 23\tNet Loss: 92.1771 \tQuestion Loss: 92.1771 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 23\tNet Loss: 90.6059 \tQuestion Loss: 90.6059 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 23\tNet Loss: 92.2250 \tQuestion Loss: 92.2250 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 23\tNet Loss: 91.7634 \tQuestion Loss: 91.7634 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 23\tNet Loss: 94.4449 \tQuestion Loss: 94.4449 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 23\tNet Loss: 90.1535 \tQuestion Loss: 90.1535 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 23\tNet Loss: 95.4134 \tQuestion Loss: 95.4134 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 23\tNet Loss: 92.9500 \tQuestion Loss: 92.9500 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 23\tNet Loss: 91.3960 \tQuestion Loss: 91.3960 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 23\tNet Loss: 92.0733 \tQuestion Loss: 92.0733 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 23\tNet Loss: 92.7553 \tQuestion Loss: 92.7553 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 23\tNet Loss: 93.4740 \tQuestion Loss: 93.4740 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 23\tNet Loss: 91.9387 \tQuestion Loss: 91.9387 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 23\tNet Loss: 95.8438 \tQuestion Loss: 95.8438 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 23\tNet Loss: 92.8354 \tQuestion Loss: 92.8354 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 23\tNet Loss: 93.1671 \tQuestion Loss: 93.1671 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 23\tNet Loss: 92.4517 \tQuestion Loss: 92.4517 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 23\tNet Loss: 95.5981 \tQuestion Loss: 95.5981 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 23\tNet Loss: 93.0485 \tQuestion Loss: 93.0485 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 23\tNet Loss: 92.5654 \tQuestion Loss: 92.5654 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 23\tNet Loss: 95.2142 \tQuestion Loss: 95.2142 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 23\tNet Loss: 96.4856 \tQuestion Loss: 96.4856 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 23\tNet Loss: 92.2693 \tQuestion Loss: 92.2693 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 23\tNet Loss: 89.2533 \tQuestion Loss: 89.2533 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 23\tNet Loss: 88.4035 \tQuestion Loss: 88.4035 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 23\tNet Loss: 92.8892 \tQuestion Loss: 92.8892 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 23\tNet Loss: 93.4450 \tQuestion Loss: 93.4450 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 23\tNet Loss: 87.8382 \tQuestion Loss: 87.8382 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 23\tNet Loss: 89.7409 \tQuestion Loss: 89.7409 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 23\tNet Loss: 97.1296 \tQuestion Loss: 97.1296 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 23\tNet Loss: 94.2774 \tQuestion Loss: 94.2774 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 23\tNet Loss: 91.2044 \tQuestion Loss: 91.2044 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 23\tNet Loss: 88.3936 \tQuestion Loss: 88.3936 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 23\tNet Loss: 90.5373 \tQuestion Loss: 90.5373 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 23\tNet Loss: 93.8767 \tQuestion Loss: 93.8767 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 23\tNet Loss: 89.7202 \tQuestion Loss: 89.7202 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 120 \t Epoch : 23\tNet Loss: 91.5223 \tQuestion Loss: 91.5223 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 23\tNet Loss: 93.8380 \tQuestion Loss: 93.8380 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 23\tNet Loss: 96.8437 \tQuestion Loss: 96.8437 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 23\tNet Loss: 92.7894 \tQuestion Loss: 92.7894 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 23\tNet Loss: 94.6932 \tQuestion Loss: 94.6932 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 23\tNet Loss: 92.7695 \tQuestion Loss: 92.7695 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 23\tNet Loss: 96.4091 \tQuestion Loss: 96.4091 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 23\tNet Loss: 88.0872 \tQuestion Loss: 88.0872 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 23\tNet Loss: 93.3162 \tQuestion Loss: 93.3162 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 23\tNet Loss: 93.1638 \tQuestion Loss: 93.1638 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 23\tNet Loss: 100.2198 \tQuestion Loss: 100.2198 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 23\tNet Loss: 94.8300 \tQuestion Loss: 94.8300 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 23\tNet Loss: 91.1213 \tQuestion Loss: 91.1213 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 23\tNet Loss: 92.8120 \tQuestion Loss: 92.8120 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 23\tNet Loss: 92.8583 \tQuestion Loss: 92.8583 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 23\tNet Loss: 86.1141 \tQuestion Loss: 86.1141 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 23\tNet Loss: 97.2718 \tQuestion Loss: 97.2718 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 23\tNet Loss: 95.2096 \tQuestion Loss: 95.2096 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 23\tNet Loss: 93.4368 \tQuestion Loss: 93.4368 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 23\tNet Loss: 92.1339 \tQuestion Loss: 92.1339 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 23\tNet Loss: 89.2909 \tQuestion Loss: 89.2909 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 23\tNet Loss: 89.8050 \tQuestion Loss: 89.8050 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 23\tNet Loss: 94.5740 \tQuestion Loss: 94.5740 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 23\tNet Loss: 89.5466 \tQuestion Loss: 89.5466 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 23\tNet Loss: 87.0011 \tQuestion Loss: 87.0011 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 23\tNet Loss: 95.3877 \tQuestion Loss: 95.3877 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 23\tNet Loss: 98.8129 \tQuestion Loss: 98.8129 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 23\tNet Loss: 87.1046 \tQuestion Loss: 87.1046 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 23\tNet Loss: 96.5037 \tQuestion Loss: 96.5037 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 23\tNet Loss: 90.0421 \tQuestion Loss: 90.0421 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 23\tNet Loss: 91.1923 \tQuestion Loss: 91.1923 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 23\tNet Loss: 98.0283 \tQuestion Loss: 98.0283 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 23\tNet Loss: 94.0132 \tQuestion Loss: 94.0132 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 23\tNet Loss: 94.5402 \tQuestion Loss: 94.5402 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 23\tNet Loss: 85.3408 \tQuestion Loss: 85.3408 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 23\tNet Loss: 90.5559 \tQuestion Loss: 90.5559 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 23\tNet Loss: 93.5316 \tQuestion Loss: 93.5316 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 23\tNet Loss: 88.3279 \tQuestion Loss: 88.3279 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 23\tNet Loss: 93.7455 \tQuestion Loss: 93.7455 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 23\tNet Loss: 94.8637 \tQuestion Loss: 94.8637 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 23\tNet Loss: 95.9908 \tQuestion Loss: 95.9908 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 23\tNet Loss: 94.2346 \tQuestion Loss: 94.2346 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 23\tNet Loss: 96.0103 \tQuestion Loss: 96.0103 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 23\tNet Loss: 89.7925 \tQuestion Loss: 89.7925 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 23\tNet Loss: 88.5533 \tQuestion Loss: 88.5533 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 23\tNet Loss: 89.7307 \tQuestion Loss: 89.7307 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 23\tNet Loss: 87.5468 \tQuestion Loss: 87.5468 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 23\tNet Loss: 93.0755 \tQuestion Loss: 93.0755 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 23\tNet Loss: 91.4741 \tQuestion Loss: 91.4741 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 23\tNet Loss: 95.8410 \tQuestion Loss: 95.8410 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 23\tNet Loss: 93.6294 \tQuestion Loss: 93.6294 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 23\tNet Loss: 92.6571 \tQuestion Loss: 92.6571 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 23\tNet Loss: 88.9683 \tQuestion Loss: 88.9683 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 23\tNet Loss: 90.6439 \tQuestion Loss: 90.6439 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 23\tNet Loss: 95.0291 \tQuestion Loss: 95.0291 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 23\tNet Loss: 97.7898 \tQuestion Loss: 97.7898 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 23\tNet Loss: 90.1155 \tQuestion Loss: 90.1155 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 23\tNet Loss: 91.6757 \tQuestion Loss: 91.6757 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 23\tNet Loss: 90.5265 \tQuestion Loss: 90.5265 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 23\tNet Loss: 92.0029 \tQuestion Loss: 92.0029 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 23\tNet Loss: 90.1471 \tQuestion Loss: 90.1471 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 23\tNet Loss: 92.4708 \tQuestion Loss: 92.4708 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 23\tNet Loss: 94.3557 \tQuestion Loss: 94.3557 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 23\tNet Loss: 89.5871 \tQuestion Loss: 89.5871 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 23\tNet Loss: 93.4783 \tQuestion Loss: 93.4783 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 23\tNet Loss: 91.6330 \tQuestion Loss: 91.6330 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 23\tNet Loss: 94.4241 \tQuestion Loss: 94.4241 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 23\tNet Loss: 95.3302 \tQuestion Loss: 95.3302 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 23\tNet Loss: 90.0021 \tQuestion Loss: 90.0021 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 23\tNet Loss: 89.8650 \tQuestion Loss: 89.8650 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 23\tNet Loss: 91.2016 \tQuestion Loss: 91.2016 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 23\tNet Loss: 90.0836 \tQuestion Loss: 90.0836 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 23\tNet Loss: 95.0055 \tQuestion Loss: 95.0055 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 23\tNet Loss: 93.3757 \tQuestion Loss: 93.3757 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 23\tNet Loss: 95.0564 \tQuestion Loss: 95.0564 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 23\tNet Loss: 87.9960 \tQuestion Loss: 87.9960 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 23\tNet Loss: 89.3180 \tQuestion Loss: 89.3180 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 23\tNet Loss: 94.7179 \tQuestion Loss: 94.7179 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 23\tNet Loss: 87.9202 \tQuestion Loss: 87.9202 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 23\tNet Loss: 92.4044 \tQuestion Loss: 92.4044 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 23 : 92.3435 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 24\tNet Loss: 92.7734 \tQuestion Loss: 92.7734 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 24\tNet Loss: 89.1550 \tQuestion Loss: 89.1550 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 24\tNet Loss: 94.3444 \tQuestion Loss: 94.3444 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 24\tNet Loss: 93.5353 \tQuestion Loss: 93.5353 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 24\tNet Loss: 93.8493 \tQuestion Loss: 93.8493 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 24\tNet Loss: 93.7392 \tQuestion Loss: 93.7392 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 24\tNet Loss: 88.8458 \tQuestion Loss: 88.8458 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 24\tNet Loss: 94.9657 \tQuestion Loss: 94.9657 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 24\tNet Loss: 93.5878 \tQuestion Loss: 93.5878 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 24\tNet Loss: 95.2006 \tQuestion Loss: 95.2006 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 24\tNet Loss: 91.2452 \tQuestion Loss: 91.2452 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 11 \t Epoch : 24\tNet Loss: 95.6682 \tQuestion Loss: 95.6682 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 24\tNet Loss: 90.4685 \tQuestion Loss: 90.4685 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 24\tNet Loss: 92.7094 \tQuestion Loss: 92.7094 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 24\tNet Loss: 93.1012 \tQuestion Loss: 93.1012 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 24\tNet Loss: 95.4478 \tQuestion Loss: 95.4478 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 24\tNet Loss: 90.4633 \tQuestion Loss: 90.4633 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 24\tNet Loss: 92.5494 \tQuestion Loss: 92.5494 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 24\tNet Loss: 91.0614 \tQuestion Loss: 91.0614 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 24\tNet Loss: 86.8642 \tQuestion Loss: 86.8642 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 24\tNet Loss: 96.1621 \tQuestion Loss: 96.1621 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 24\tNet Loss: 88.2478 \tQuestion Loss: 88.2478 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 24\tNet Loss: 92.7685 \tQuestion Loss: 92.7685 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 24\tNet Loss: 95.8831 \tQuestion Loss: 95.8831 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 24\tNet Loss: 89.7974 \tQuestion Loss: 89.7974 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 24\tNet Loss: 93.2814 \tQuestion Loss: 93.2814 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 24\tNet Loss: 98.3920 \tQuestion Loss: 98.3920 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 24\tNet Loss: 91.9125 \tQuestion Loss: 91.9125 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 24\tNet Loss: 87.0301 \tQuestion Loss: 87.0301 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 24\tNet Loss: 89.9100 \tQuestion Loss: 89.9100 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 24\tNet Loss: 91.5383 \tQuestion Loss: 91.5383 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 24\tNet Loss: 90.3827 \tQuestion Loss: 90.3827 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 24\tNet Loss: 89.0883 \tQuestion Loss: 89.0883 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 24\tNet Loss: 93.3027 \tQuestion Loss: 93.3027 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 24\tNet Loss: 87.4453 \tQuestion Loss: 87.4453 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 24\tNet Loss: 90.1451 \tQuestion Loss: 90.1451 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 24\tNet Loss: 93.8720 \tQuestion Loss: 93.8720 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 24\tNet Loss: 95.0586 \tQuestion Loss: 95.0586 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 24\tNet Loss: 91.0551 \tQuestion Loss: 91.0551 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 24\tNet Loss: 94.1888 \tQuestion Loss: 94.1888 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 24\tNet Loss: 91.5251 \tQuestion Loss: 91.5251 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 24\tNet Loss: 90.2769 \tQuestion Loss: 90.2769 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 24\tNet Loss: 92.1281 \tQuestion Loss: 92.1281 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 24\tNet Loss: 93.5709 \tQuestion Loss: 93.5709 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 24\tNet Loss: 90.7218 \tQuestion Loss: 90.7218 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 24\tNet Loss: 93.6338 \tQuestion Loss: 93.6338 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 24\tNet Loss: 92.0884 \tQuestion Loss: 92.0884 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 24\tNet Loss: 97.2243 \tQuestion Loss: 97.2243 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 24\tNet Loss: 86.8931 \tQuestion Loss: 86.8931 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 24\tNet Loss: 94.2996 \tQuestion Loss: 94.2996 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 24\tNet Loss: 92.9274 \tQuestion Loss: 92.9274 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 24\tNet Loss: 88.5393 \tQuestion Loss: 88.5393 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 24\tNet Loss: 89.5406 \tQuestion Loss: 89.5406 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 24\tNet Loss: 90.0096 \tQuestion Loss: 90.0096 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 24\tNet Loss: 96.4010 \tQuestion Loss: 96.4010 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 24\tNet Loss: 92.9629 \tQuestion Loss: 92.9629 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 24\tNet Loss: 94.4380 \tQuestion Loss: 94.4380 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 24\tNet Loss: 96.6320 \tQuestion Loss: 96.6320 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 24\tNet Loss: 87.5176 \tQuestion Loss: 87.5176 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 24\tNet Loss: 92.0500 \tQuestion Loss: 92.0500 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 24\tNet Loss: 89.1157 \tQuestion Loss: 89.1157 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 24\tNet Loss: 95.2213 \tQuestion Loss: 95.2213 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 24\tNet Loss: 88.8024 \tQuestion Loss: 88.8024 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 24\tNet Loss: 93.4900 \tQuestion Loss: 93.4900 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 24\tNet Loss: 92.0431 \tQuestion Loss: 92.0431 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 24\tNet Loss: 90.4033 \tQuestion Loss: 90.4033 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 24\tNet Loss: 92.5281 \tQuestion Loss: 92.5281 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 24\tNet Loss: 92.1257 \tQuestion Loss: 92.1257 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 24\tNet Loss: 90.7496 \tQuestion Loss: 90.7496 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 24\tNet Loss: 95.9345 \tQuestion Loss: 95.9345 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 24\tNet Loss: 93.3173 \tQuestion Loss: 93.3173 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 24\tNet Loss: 92.8391 \tQuestion Loss: 92.8391 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 24\tNet Loss: 95.4020 \tQuestion Loss: 95.4020 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 24\tNet Loss: 93.5077 \tQuestion Loss: 93.5077 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 24\tNet Loss: 94.8039 \tQuestion Loss: 94.8039 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 24\tNet Loss: 93.5059 \tQuestion Loss: 93.5059 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 24\tNet Loss: 86.7605 \tQuestion Loss: 86.7605 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 24\tNet Loss: 91.3423 \tQuestion Loss: 91.3423 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 24\tNet Loss: 96.9999 \tQuestion Loss: 96.9999 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 24\tNet Loss: 92.8658 \tQuestion Loss: 92.8658 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 24\tNet Loss: 94.2383 \tQuestion Loss: 94.2383 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 24\tNet Loss: 92.0494 \tQuestion Loss: 92.0494 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 24\tNet Loss: 89.6269 \tQuestion Loss: 89.6269 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 24\tNet Loss: 88.5644 \tQuestion Loss: 88.5644 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 24\tNet Loss: 91.7358 \tQuestion Loss: 91.7358 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 24\tNet Loss: 90.0196 \tQuestion Loss: 90.0196 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 24\tNet Loss: 92.1916 \tQuestion Loss: 92.1916 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 24\tNet Loss: 91.6511 \tQuestion Loss: 91.6511 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 24\tNet Loss: 94.4901 \tQuestion Loss: 94.4901 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 24\tNet Loss: 90.0606 \tQuestion Loss: 90.0606 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 24\tNet Loss: 95.2987 \tQuestion Loss: 95.2987 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 24\tNet Loss: 92.8821 \tQuestion Loss: 92.8821 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 24\tNet Loss: 91.3206 \tQuestion Loss: 91.3206 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 24\tNet Loss: 91.8187 \tQuestion Loss: 91.8187 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 24\tNet Loss: 92.5382 \tQuestion Loss: 92.5382 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 24\tNet Loss: 93.4600 \tQuestion Loss: 93.4600 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 24\tNet Loss: 91.8433 \tQuestion Loss: 91.8433 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 24\tNet Loss: 95.7129 \tQuestion Loss: 95.7129 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 24\tNet Loss: 92.6195 \tQuestion Loss: 92.6195 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 24\tNet Loss: 92.9382 \tQuestion Loss: 92.9382 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 24\tNet Loss: 92.5491 \tQuestion Loss: 92.5491 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 24\tNet Loss: 95.5371 \tQuestion Loss: 95.5371 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 24\tNet Loss: 93.0526 \tQuestion Loss: 93.0526 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 24\tNet Loss: 92.4970 \tQuestion Loss: 92.4970 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 104 \t Epoch : 24\tNet Loss: 95.0492 \tQuestion Loss: 95.0492 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 24\tNet Loss: 96.6916 \tQuestion Loss: 96.6916 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 24\tNet Loss: 92.2772 \tQuestion Loss: 92.2772 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 24\tNet Loss: 89.2321 \tQuestion Loss: 89.2321 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 24\tNet Loss: 88.4524 \tQuestion Loss: 88.4524 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 24\tNet Loss: 92.9480 \tQuestion Loss: 92.9480 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 24\tNet Loss: 93.2070 \tQuestion Loss: 93.2070 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 24\tNet Loss: 87.6937 \tQuestion Loss: 87.6937 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 24\tNet Loss: 89.7981 \tQuestion Loss: 89.7981 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 24\tNet Loss: 97.3065 \tQuestion Loss: 97.3065 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 24\tNet Loss: 94.1182 \tQuestion Loss: 94.1182 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 24\tNet Loss: 91.1836 \tQuestion Loss: 91.1836 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 24\tNet Loss: 88.2087 \tQuestion Loss: 88.2087 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 24\tNet Loss: 90.5875 \tQuestion Loss: 90.5875 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 24\tNet Loss: 93.9189 \tQuestion Loss: 93.9189 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 24\tNet Loss: 89.7358 \tQuestion Loss: 89.7358 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 24\tNet Loss: 91.4432 \tQuestion Loss: 91.4432 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 24\tNet Loss: 93.5987 \tQuestion Loss: 93.5987 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 24\tNet Loss: 97.7179 \tQuestion Loss: 97.7179 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 24\tNet Loss: 92.9567 \tQuestion Loss: 92.9567 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 24\tNet Loss: 94.7243 \tQuestion Loss: 94.7243 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 24\tNet Loss: 92.5789 \tQuestion Loss: 92.5789 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 24\tNet Loss: 96.2003 \tQuestion Loss: 96.2003 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 24\tNet Loss: 88.0347 \tQuestion Loss: 88.0347 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 24\tNet Loss: 92.1748 \tQuestion Loss: 92.1748 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 24\tNet Loss: 94.7829 \tQuestion Loss: 94.7829 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 24\tNet Loss: 99.9318 \tQuestion Loss: 99.9318 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 24\tNet Loss: 94.8389 \tQuestion Loss: 94.8389 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 24\tNet Loss: 91.2739 \tQuestion Loss: 91.2739 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 24\tNet Loss: 92.6104 \tQuestion Loss: 92.6104 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 24\tNet Loss: 93.6084 \tQuestion Loss: 93.6084 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 24\tNet Loss: 86.0554 \tQuestion Loss: 86.0554 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 24\tNet Loss: 97.2560 \tQuestion Loss: 97.2560 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 24\tNet Loss: 94.8880 \tQuestion Loss: 94.8880 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 24\tNet Loss: 93.3567 \tQuestion Loss: 93.3567 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 24\tNet Loss: 92.0966 \tQuestion Loss: 92.0966 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 24\tNet Loss: 89.2835 \tQuestion Loss: 89.2835 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 24\tNet Loss: 89.6925 \tQuestion Loss: 89.6925 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 24\tNet Loss: 94.5010 \tQuestion Loss: 94.5010 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 24\tNet Loss: 89.2783 \tQuestion Loss: 89.2783 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 24\tNet Loss: 86.8777 \tQuestion Loss: 86.8777 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 24\tNet Loss: 95.6532 \tQuestion Loss: 95.6532 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 24\tNet Loss: 98.6929 \tQuestion Loss: 98.6929 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 24\tNet Loss: 87.0095 \tQuestion Loss: 87.0095 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 24\tNet Loss: 96.5079 \tQuestion Loss: 96.5079 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 24\tNet Loss: 90.0933 \tQuestion Loss: 90.0933 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 24\tNet Loss: 91.2801 \tQuestion Loss: 91.2801 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 24\tNet Loss: 97.9550 \tQuestion Loss: 97.9550 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 24\tNet Loss: 93.4600 \tQuestion Loss: 93.4600 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 24\tNet Loss: 93.9707 \tQuestion Loss: 93.9707 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 24\tNet Loss: 85.3137 \tQuestion Loss: 85.3137 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 24\tNet Loss: 90.6096 \tQuestion Loss: 90.6096 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 24\tNet Loss: 93.4584 \tQuestion Loss: 93.4584 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 24\tNet Loss: 88.4116 \tQuestion Loss: 88.4116 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 24\tNet Loss: 93.8044 \tQuestion Loss: 93.8044 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 24\tNet Loss: 94.6176 \tQuestion Loss: 94.6176 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 24\tNet Loss: 95.8308 \tQuestion Loss: 95.8308 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 24\tNet Loss: 94.1107 \tQuestion Loss: 94.1107 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 24\tNet Loss: 95.8883 \tQuestion Loss: 95.8883 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 24\tNet Loss: 89.5566 \tQuestion Loss: 89.5566 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 24\tNet Loss: 88.5080 \tQuestion Loss: 88.5080 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 24\tNet Loss: 89.7120 \tQuestion Loss: 89.7120 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 24\tNet Loss: 87.5294 \tQuestion Loss: 87.5294 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 24\tNet Loss: 92.7772 \tQuestion Loss: 92.7772 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 24\tNet Loss: 91.4813 \tQuestion Loss: 91.4813 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 24\tNet Loss: 95.7476 \tQuestion Loss: 95.7476 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 24\tNet Loss: 93.5679 \tQuestion Loss: 93.5679 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 24\tNet Loss: 92.5005 \tQuestion Loss: 92.5005 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 24\tNet Loss: 89.2364 \tQuestion Loss: 89.2364 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 24\tNet Loss: 90.6217 \tQuestion Loss: 90.6217 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 24\tNet Loss: 94.7394 \tQuestion Loss: 94.7394 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 24\tNet Loss: 97.6410 \tQuestion Loss: 97.6410 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 24\tNet Loss: 90.1134 \tQuestion Loss: 90.1134 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 24\tNet Loss: 91.5268 \tQuestion Loss: 91.5268 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 24\tNet Loss: 90.5513 \tQuestion Loss: 90.5513 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 24\tNet Loss: 91.7286 \tQuestion Loss: 91.7286 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 24\tNet Loss: 90.0788 \tQuestion Loss: 90.0788 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 24\tNet Loss: 92.3679 \tQuestion Loss: 92.3679 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 24\tNet Loss: 94.3786 \tQuestion Loss: 94.3786 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 24\tNet Loss: 89.4102 \tQuestion Loss: 89.4102 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 24\tNet Loss: 93.2930 \tQuestion Loss: 93.2930 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 24\tNet Loss: 91.6460 \tQuestion Loss: 91.6460 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 24\tNet Loss: 93.9472 \tQuestion Loss: 93.9472 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 24\tNet Loss: 95.0223 \tQuestion Loss: 95.0223 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 24\tNet Loss: 89.9589 \tQuestion Loss: 89.9589 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 24\tNet Loss: 89.7856 \tQuestion Loss: 89.7856 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 24\tNet Loss: 91.1084 \tQuestion Loss: 91.1084 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 24\tNet Loss: 90.0664 \tQuestion Loss: 90.0664 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 24\tNet Loss: 94.9068 \tQuestion Loss: 94.9068 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 24\tNet Loss: 93.4083 \tQuestion Loss: 93.4083 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 24\tNet Loss: 95.2639 \tQuestion Loss: 95.2639 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 24\tNet Loss: 87.9898 \tQuestion Loss: 87.9898 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 196 \t Epoch : 24\tNet Loss: 89.1850 \tQuestion Loss: 89.1850 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 24\tNet Loss: 94.6298 \tQuestion Loss: 94.6298 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 24\tNet Loss: 87.7395 \tQuestion Loss: 87.7395 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 24\tNet Loss: 92.4185 \tQuestion Loss: 92.4185 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 24 : 92.3193 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 25\tNet Loss: 92.7039 \tQuestion Loss: 92.7039 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 25\tNet Loss: 89.0525 \tQuestion Loss: 89.0525 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 25\tNet Loss: 94.3147 \tQuestion Loss: 94.3147 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 25\tNet Loss: 93.6196 \tQuestion Loss: 93.6196 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 25\tNet Loss: 93.7638 \tQuestion Loss: 93.7638 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 25\tNet Loss: 93.6377 \tQuestion Loss: 93.6377 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 25\tNet Loss: 88.8189 \tQuestion Loss: 88.8189 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 25\tNet Loss: 94.9322 \tQuestion Loss: 94.9322 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 25\tNet Loss: 93.3784 \tQuestion Loss: 93.3784 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 25\tNet Loss: 94.7792 \tQuestion Loss: 94.7792 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 25\tNet Loss: 91.0076 \tQuestion Loss: 91.0076 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 25\tNet Loss: 95.6528 \tQuestion Loss: 95.6528 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 25\tNet Loss: 90.3732 \tQuestion Loss: 90.3732 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 25\tNet Loss: 92.7344 \tQuestion Loss: 92.7344 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 25\tNet Loss: 92.9812 \tQuestion Loss: 92.9812 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 25\tNet Loss: 95.5428 \tQuestion Loss: 95.5428 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 25\tNet Loss: 90.5525 \tQuestion Loss: 90.5525 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 25\tNet Loss: 92.4215 \tQuestion Loss: 92.4215 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 25\tNet Loss: 90.5088 \tQuestion Loss: 90.5088 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 25\tNet Loss: 86.7728 \tQuestion Loss: 86.7728 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 25\tNet Loss: 96.0709 \tQuestion Loss: 96.0709 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 25\tNet Loss: 88.1684 \tQuestion Loss: 88.1684 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 25\tNet Loss: 92.9419 \tQuestion Loss: 92.9419 \t Time Taken: 1 seconds\n",
      "Batch: 23 \t Epoch : 25\tNet Loss: 95.8810 \tQuestion Loss: 95.8810 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 25\tNet Loss: 89.8722 \tQuestion Loss: 89.8722 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 25\tNet Loss: 93.2510 \tQuestion Loss: 93.2510 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 25\tNet Loss: 98.4276 \tQuestion Loss: 98.4276 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 25\tNet Loss: 91.8855 \tQuestion Loss: 91.8855 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 25\tNet Loss: 86.9374 \tQuestion Loss: 86.9374 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 25\tNet Loss: 89.9103 \tQuestion Loss: 89.9103 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 25\tNet Loss: 91.6734 \tQuestion Loss: 91.6734 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 25\tNet Loss: 90.4405 \tQuestion Loss: 90.4405 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 25\tNet Loss: 88.7754 \tQuestion Loss: 88.7754 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 25\tNet Loss: 93.2979 \tQuestion Loss: 93.2979 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 25\tNet Loss: 87.3434 \tQuestion Loss: 87.3434 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 25\tNet Loss: 90.1596 \tQuestion Loss: 90.1596 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 25\tNet Loss: 93.8904 \tQuestion Loss: 93.8904 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 25\tNet Loss: 95.3331 \tQuestion Loss: 95.3331 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 25\tNet Loss: 91.2785 \tQuestion Loss: 91.2785 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 25\tNet Loss: 93.8890 \tQuestion Loss: 93.8890 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 25\tNet Loss: 91.5461 \tQuestion Loss: 91.5461 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 25\tNet Loss: 90.2800 \tQuestion Loss: 90.2800 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 25\tNet Loss: 92.0492 \tQuestion Loss: 92.0492 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 25\tNet Loss: 93.3253 \tQuestion Loss: 93.3253 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 25\tNet Loss: 90.6146 \tQuestion Loss: 90.6146 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 25\tNet Loss: 93.6885 \tQuestion Loss: 93.6885 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 25\tNet Loss: 92.3232 \tQuestion Loss: 92.3232 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 25\tNet Loss: 97.3821 \tQuestion Loss: 97.3821 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 25\tNet Loss: 86.0537 \tQuestion Loss: 86.0537 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 25\tNet Loss: 93.9392 \tQuestion Loss: 93.9392 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 25\tNet Loss: 92.9589 \tQuestion Loss: 92.9589 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 25\tNet Loss: 88.5397 \tQuestion Loss: 88.5397 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 25\tNet Loss: 89.5784 \tQuestion Loss: 89.5784 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 25\tNet Loss: 89.7824 \tQuestion Loss: 89.7824 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 25\tNet Loss: 96.5169 \tQuestion Loss: 96.5169 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 25\tNet Loss: 93.2175 \tQuestion Loss: 93.2175 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 25\tNet Loss: 94.3690 \tQuestion Loss: 94.3690 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 25\tNet Loss: 96.6473 \tQuestion Loss: 96.6473 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 25\tNet Loss: 87.6011 \tQuestion Loss: 87.6011 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 25\tNet Loss: 92.2626 \tQuestion Loss: 92.2626 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 25\tNet Loss: 89.1461 \tQuestion Loss: 89.1461 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 25\tNet Loss: 95.2973 \tQuestion Loss: 95.2973 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 25\tNet Loss: 88.7973 \tQuestion Loss: 88.7973 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 25\tNet Loss: 93.5161 \tQuestion Loss: 93.5161 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 25\tNet Loss: 92.2281 \tQuestion Loss: 92.2281 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 25\tNet Loss: 90.8338 \tQuestion Loss: 90.8338 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 25\tNet Loss: 92.1340 \tQuestion Loss: 92.1340 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 25\tNet Loss: 91.3490 \tQuestion Loss: 91.3490 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 25\tNet Loss: 90.8970 \tQuestion Loss: 90.8970 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 25\tNet Loss: 95.6022 \tQuestion Loss: 95.6022 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 25\tNet Loss: 93.2614 \tQuestion Loss: 93.2614 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 25\tNet Loss: 92.7050 \tQuestion Loss: 92.7050 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 25\tNet Loss: 95.7546 \tQuestion Loss: 95.7546 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 25\tNet Loss: 93.8164 \tQuestion Loss: 93.8164 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 25\tNet Loss: 94.9057 \tQuestion Loss: 94.9057 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 25\tNet Loss: 93.5530 \tQuestion Loss: 93.5530 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 25\tNet Loss: 86.8769 \tQuestion Loss: 86.8769 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 25\tNet Loss: 91.9513 \tQuestion Loss: 91.9513 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 25\tNet Loss: 96.3660 \tQuestion Loss: 96.3660 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 25\tNet Loss: 93.0907 \tQuestion Loss: 93.0907 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 25\tNet Loss: 94.4197 \tQuestion Loss: 94.4197 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 25\tNet Loss: 92.3918 \tQuestion Loss: 92.3918 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 25\tNet Loss: 89.7149 \tQuestion Loss: 89.7149 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 25\tNet Loss: 88.7014 \tQuestion Loss: 88.7014 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 25\tNet Loss: 92.3098 \tQuestion Loss: 92.3098 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 25\tNet Loss: 90.8522 \tQuestion Loss: 90.8522 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 25\tNet Loss: 92.4128 \tQuestion Loss: 92.4128 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 25\tNet Loss: 91.6665 \tQuestion Loss: 91.6665 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 88 \t Epoch : 25\tNet Loss: 94.5500 \tQuestion Loss: 94.5500 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 25\tNet Loss: 90.3191 \tQuestion Loss: 90.3191 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 25\tNet Loss: 95.5349 \tQuestion Loss: 95.5349 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 25\tNet Loss: 92.8896 \tQuestion Loss: 92.8896 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 25\tNet Loss: 91.3946 \tQuestion Loss: 91.3946 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 25\tNet Loss: 92.1924 \tQuestion Loss: 92.1924 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 25\tNet Loss: 92.8605 \tQuestion Loss: 92.8605 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 25\tNet Loss: 93.4801 \tQuestion Loss: 93.4801 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 25\tNet Loss: 91.9350 \tQuestion Loss: 91.9350 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 25\tNet Loss: 95.9410 \tQuestion Loss: 95.9410 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 25\tNet Loss: 92.9710 \tQuestion Loss: 92.9710 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 25\tNet Loss: 93.1777 \tQuestion Loss: 93.1777 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 25\tNet Loss: 92.4701 \tQuestion Loss: 92.4701 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 25\tNet Loss: 95.6818 \tQuestion Loss: 95.6818 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 25\tNet Loss: 93.1075 \tQuestion Loss: 93.1075 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 25\tNet Loss: 92.5629 \tQuestion Loss: 92.5629 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 25\tNet Loss: 95.2204 \tQuestion Loss: 95.2204 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 25\tNet Loss: 96.5188 \tQuestion Loss: 96.5188 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 25\tNet Loss: 92.2741 \tQuestion Loss: 92.2741 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 25\tNet Loss: 89.3133 \tQuestion Loss: 89.3133 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 25\tNet Loss: 88.2944 \tQuestion Loss: 88.2944 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 25\tNet Loss: 92.9112 \tQuestion Loss: 92.9112 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 25\tNet Loss: 93.5111 \tQuestion Loss: 93.5111 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 25\tNet Loss: 87.9091 \tQuestion Loss: 87.9091 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 25\tNet Loss: 89.6955 \tQuestion Loss: 89.6955 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 25\tNet Loss: 97.1273 \tQuestion Loss: 97.1273 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 25\tNet Loss: 94.2957 \tQuestion Loss: 94.2957 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 25\tNet Loss: 91.3125 \tQuestion Loss: 91.3125 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 25\tNet Loss: 88.4004 \tQuestion Loss: 88.4004 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 25\tNet Loss: 90.5698 \tQuestion Loss: 90.5698 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 25\tNet Loss: 93.8840 \tQuestion Loss: 93.8840 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 25\tNet Loss: 89.7759 \tQuestion Loss: 89.7759 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 25\tNet Loss: 91.5099 \tQuestion Loss: 91.5099 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 25\tNet Loss: 93.8857 \tQuestion Loss: 93.8857 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 25\tNet Loss: 96.8492 \tQuestion Loss: 96.8492 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 25\tNet Loss: 92.8265 \tQuestion Loss: 92.8265 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 25\tNet Loss: 94.7299 \tQuestion Loss: 94.7299 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 25\tNet Loss: 92.7346 \tQuestion Loss: 92.7346 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 25\tNet Loss: 96.4211 \tQuestion Loss: 96.4211 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 25\tNet Loss: 88.1137 \tQuestion Loss: 88.1137 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 25\tNet Loss: 93.2808 \tQuestion Loss: 93.2808 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 25\tNet Loss: 93.1328 \tQuestion Loss: 93.1328 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 25\tNet Loss: 100.1928 \tQuestion Loss: 100.1928 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 25\tNet Loss: 94.8455 \tQuestion Loss: 94.8455 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 25\tNet Loss: 91.1691 \tQuestion Loss: 91.1691 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 25\tNet Loss: 92.8316 \tQuestion Loss: 92.8316 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 25\tNet Loss: 92.7666 \tQuestion Loss: 92.7666 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 25\tNet Loss: 86.1130 \tQuestion Loss: 86.1130 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 25\tNet Loss: 97.2782 \tQuestion Loss: 97.2782 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 25\tNet Loss: 95.1858 \tQuestion Loss: 95.1858 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 25\tNet Loss: 93.4316 \tQuestion Loss: 93.4316 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 25\tNet Loss: 92.1341 \tQuestion Loss: 92.1341 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 25\tNet Loss: 89.2810 \tQuestion Loss: 89.2810 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 25\tNet Loss: 89.8292 \tQuestion Loss: 89.8292 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 25\tNet Loss: 94.5818 \tQuestion Loss: 94.5818 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 25\tNet Loss: 89.5407 \tQuestion Loss: 89.5407 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 25\tNet Loss: 86.9986 \tQuestion Loss: 86.9986 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 25\tNet Loss: 95.3848 \tQuestion Loss: 95.3848 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 25\tNet Loss: 98.7975 \tQuestion Loss: 98.7975 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 25\tNet Loss: 87.1010 \tQuestion Loss: 87.1010 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 25\tNet Loss: 96.4837 \tQuestion Loss: 96.4837 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 25\tNet Loss: 90.0176 \tQuestion Loss: 90.0176 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 25\tNet Loss: 91.1657 \tQuestion Loss: 91.1657 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 25\tNet Loss: 98.0107 \tQuestion Loss: 98.0107 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 25\tNet Loss: 93.9771 \tQuestion Loss: 93.9771 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 25\tNet Loss: 94.5125 \tQuestion Loss: 94.5125 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 25\tNet Loss: 85.3549 \tQuestion Loss: 85.3549 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 25\tNet Loss: 90.6079 \tQuestion Loss: 90.6079 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 25\tNet Loss: 93.5364 \tQuestion Loss: 93.5364 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 25\tNet Loss: 88.3344 \tQuestion Loss: 88.3344 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 25\tNet Loss: 93.7392 \tQuestion Loss: 93.7392 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 25\tNet Loss: 94.8913 \tQuestion Loss: 94.8913 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 25\tNet Loss: 95.9446 \tQuestion Loss: 95.9446 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 25\tNet Loss: 94.2461 \tQuestion Loss: 94.2461 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 25\tNet Loss: 96.0022 \tQuestion Loss: 96.0022 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 25\tNet Loss: 89.8350 \tQuestion Loss: 89.8350 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 25\tNet Loss: 88.5744 \tQuestion Loss: 88.5744 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 25\tNet Loss: 89.7156 \tQuestion Loss: 89.7156 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 25\tNet Loss: 87.5366 \tQuestion Loss: 87.5366 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 25\tNet Loss: 93.0781 \tQuestion Loss: 93.0781 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 25\tNet Loss: 91.4987 \tQuestion Loss: 91.4987 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 25\tNet Loss: 95.8599 \tQuestion Loss: 95.8599 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 25\tNet Loss: 93.6086 \tQuestion Loss: 93.6086 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 25\tNet Loss: 92.6652 \tQuestion Loss: 92.6652 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 25\tNet Loss: 88.9896 \tQuestion Loss: 88.9896 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 25\tNet Loss: 90.6187 \tQuestion Loss: 90.6187 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 25\tNet Loss: 95.0382 \tQuestion Loss: 95.0382 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 25\tNet Loss: 97.7732 \tQuestion Loss: 97.7732 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 25\tNet Loss: 90.1277 \tQuestion Loss: 90.1277 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 25\tNet Loss: 91.6558 \tQuestion Loss: 91.6558 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 25\tNet Loss: 90.5267 \tQuestion Loss: 90.5267 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 25\tNet Loss: 92.0723 \tQuestion Loss: 92.0723 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 180 \t Epoch : 25\tNet Loss: 90.1463 \tQuestion Loss: 90.1463 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 25\tNet Loss: 92.4425 \tQuestion Loss: 92.4425 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 25\tNet Loss: 94.3184 \tQuestion Loss: 94.3184 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 25\tNet Loss: 89.6509 \tQuestion Loss: 89.6509 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 25\tNet Loss: 93.4779 \tQuestion Loss: 93.4779 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 25\tNet Loss: 91.6063 \tQuestion Loss: 91.6063 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 25\tNet Loss: 94.3832 \tQuestion Loss: 94.3832 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 25\tNet Loss: 95.3297 \tQuestion Loss: 95.3297 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 25\tNet Loss: 90.0028 \tQuestion Loss: 90.0028 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 25\tNet Loss: 89.8499 \tQuestion Loss: 89.8499 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 25\tNet Loss: 91.1828 \tQuestion Loss: 91.1828 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 25\tNet Loss: 90.0894 \tQuestion Loss: 90.0894 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 25\tNet Loss: 94.9921 \tQuestion Loss: 94.9921 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 25\tNet Loss: 93.3970 \tQuestion Loss: 93.3970 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 25\tNet Loss: 95.0193 \tQuestion Loss: 95.0193 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 25\tNet Loss: 87.9660 \tQuestion Loss: 87.9660 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 25\tNet Loss: 89.3137 \tQuestion Loss: 89.3137 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 25\tNet Loss: 94.7079 \tQuestion Loss: 94.7079 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 25\tNet Loss: 87.9566 \tQuestion Loss: 87.9566 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 25\tNet Loss: 92.3866 \tQuestion Loss: 92.3866 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 25 : 92.3572 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 26\tNet Loss: 92.7680 \tQuestion Loss: 92.7680 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 26\tNet Loss: 89.1670 \tQuestion Loss: 89.1670 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 26\tNet Loss: 94.3429 \tQuestion Loss: 94.3429 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 26\tNet Loss: 93.5332 \tQuestion Loss: 93.5332 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 26\tNet Loss: 93.8478 \tQuestion Loss: 93.8478 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 26\tNet Loss: 93.7259 \tQuestion Loss: 93.7259 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 26\tNet Loss: 88.8411 \tQuestion Loss: 88.8411 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 26\tNet Loss: 94.9304 \tQuestion Loss: 94.9304 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 26\tNet Loss: 93.5594 \tQuestion Loss: 93.5594 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 26\tNet Loss: 95.2020 \tQuestion Loss: 95.2020 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 26\tNet Loss: 91.2403 \tQuestion Loss: 91.2403 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 26\tNet Loss: 95.6853 \tQuestion Loss: 95.6853 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 26\tNet Loss: 90.4299 \tQuestion Loss: 90.4299 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 26\tNet Loss: 92.7260 \tQuestion Loss: 92.7260 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 26\tNet Loss: 93.1114 \tQuestion Loss: 93.1114 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 26\tNet Loss: 95.4156 \tQuestion Loss: 95.4156 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 26\tNet Loss: 90.4671 \tQuestion Loss: 90.4671 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 26\tNet Loss: 92.5471 \tQuestion Loss: 92.5471 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 26\tNet Loss: 91.0919 \tQuestion Loss: 91.0919 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 26\tNet Loss: 86.8423 \tQuestion Loss: 86.8423 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 26\tNet Loss: 96.1792 \tQuestion Loss: 96.1792 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 26\tNet Loss: 88.2657 \tQuestion Loss: 88.2657 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 26\tNet Loss: 92.7927 \tQuestion Loss: 92.7927 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 26\tNet Loss: 95.8618 \tQuestion Loss: 95.8618 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 26\tNet Loss: 89.7718 \tQuestion Loss: 89.7718 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 26\tNet Loss: 93.2796 \tQuestion Loss: 93.2796 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 26\tNet Loss: 98.3897 \tQuestion Loss: 98.3897 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 26\tNet Loss: 91.8995 \tQuestion Loss: 91.8995 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 26\tNet Loss: 87.0433 \tQuestion Loss: 87.0433 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 26\tNet Loss: 89.9213 \tQuestion Loss: 89.9213 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 26\tNet Loss: 91.5480 \tQuestion Loss: 91.5480 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 26\tNet Loss: 90.3618 \tQuestion Loss: 90.3618 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 26\tNet Loss: 89.0873 \tQuestion Loss: 89.0873 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 26\tNet Loss: 93.2936 \tQuestion Loss: 93.2936 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 26\tNet Loss: 87.4623 \tQuestion Loss: 87.4623 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 26\tNet Loss: 90.1464 \tQuestion Loss: 90.1464 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 26\tNet Loss: 93.8848 \tQuestion Loss: 93.8848 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 26\tNet Loss: 95.0166 \tQuestion Loss: 95.0166 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 26\tNet Loss: 91.0244 \tQuestion Loss: 91.0244 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 26\tNet Loss: 94.2114 \tQuestion Loss: 94.2114 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 26\tNet Loss: 91.5329 \tQuestion Loss: 91.5329 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 26\tNet Loss: 90.3205 \tQuestion Loss: 90.3205 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 26\tNet Loss: 92.1405 \tQuestion Loss: 92.1405 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 26\tNet Loss: 93.5739 \tQuestion Loss: 93.5739 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 26\tNet Loss: 90.7410 \tQuestion Loss: 90.7410 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 26\tNet Loss: 93.6244 \tQuestion Loss: 93.6244 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 26\tNet Loss: 92.0987 \tQuestion Loss: 92.0987 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 26\tNet Loss: 97.1914 \tQuestion Loss: 97.1914 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 26\tNet Loss: 86.9228 \tQuestion Loss: 86.9228 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 26\tNet Loss: 94.3097 \tQuestion Loss: 94.3097 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 26\tNet Loss: 92.9391 \tQuestion Loss: 92.9391 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 26\tNet Loss: 88.5500 \tQuestion Loss: 88.5500 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 26\tNet Loss: 89.5300 \tQuestion Loss: 89.5300 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 26\tNet Loss: 89.9998 \tQuestion Loss: 89.9998 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 26\tNet Loss: 96.4068 \tQuestion Loss: 96.4068 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 26\tNet Loss: 92.9879 \tQuestion Loss: 92.9879 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 26\tNet Loss: 94.4056 \tQuestion Loss: 94.4056 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 26\tNet Loss: 96.6642 \tQuestion Loss: 96.6642 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 26\tNet Loss: 87.5537 \tQuestion Loss: 87.5537 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 26\tNet Loss: 92.1169 \tQuestion Loss: 92.1169 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 26\tNet Loss: 89.0843 \tQuestion Loss: 89.0843 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 26\tNet Loss: 95.2186 \tQuestion Loss: 95.2186 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 26\tNet Loss: 88.8370 \tQuestion Loss: 88.8370 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 26\tNet Loss: 93.4978 \tQuestion Loss: 93.4978 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 26\tNet Loss: 92.0112 \tQuestion Loss: 92.0112 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 26\tNet Loss: 90.2833 \tQuestion Loss: 90.2833 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 26\tNet Loss: 92.5095 \tQuestion Loss: 92.5095 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 26\tNet Loss: 92.1165 \tQuestion Loss: 92.1165 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 26\tNet Loss: 90.9139 \tQuestion Loss: 90.9139 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 26\tNet Loss: 95.9163 \tQuestion Loss: 95.9163 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 26\tNet Loss: 93.2653 \tQuestion Loss: 93.2653 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 26\tNet Loss: 92.7849 \tQuestion Loss: 92.7849 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 72 \t Epoch : 26\tNet Loss: 95.4732 \tQuestion Loss: 95.4732 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 26\tNet Loss: 93.5526 \tQuestion Loss: 93.5526 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 26\tNet Loss: 94.7502 \tQuestion Loss: 94.7502 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 26\tNet Loss: 93.4233 \tQuestion Loss: 93.4233 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 26\tNet Loss: 86.8296 \tQuestion Loss: 86.8296 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 26\tNet Loss: 91.4191 \tQuestion Loss: 91.4191 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 26\tNet Loss: 97.1142 \tQuestion Loss: 97.1142 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 26\tNet Loss: 92.7298 \tQuestion Loss: 92.7298 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 26\tNet Loss: 94.1339 \tQuestion Loss: 94.1339 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 26\tNet Loss: 92.2582 \tQuestion Loss: 92.2582 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 26\tNet Loss: 89.6499 \tQuestion Loss: 89.6499 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 26\tNet Loss: 88.5296 \tQuestion Loss: 88.5296 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 26\tNet Loss: 91.8547 \tQuestion Loss: 91.8547 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 26\tNet Loss: 89.9797 \tQuestion Loss: 89.9797 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 26\tNet Loss: 92.1827 \tQuestion Loss: 92.1827 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 26\tNet Loss: 91.5421 \tQuestion Loss: 91.5421 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 26\tNet Loss: 94.4151 \tQuestion Loss: 94.4151 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 26\tNet Loss: 90.1699 \tQuestion Loss: 90.1699 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 26\tNet Loss: 95.3349 \tQuestion Loss: 95.3349 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 26\tNet Loss: 92.8288 \tQuestion Loss: 92.8288 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 26\tNet Loss: 91.3117 \tQuestion Loss: 91.3117 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 26\tNet Loss: 91.9646 \tQuestion Loss: 91.9646 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 26\tNet Loss: 93.2092 \tQuestion Loss: 93.2092 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 26\tNet Loss: 93.8909 \tQuestion Loss: 93.8909 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 26\tNet Loss: 91.8636 \tQuestion Loss: 91.8636 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 26\tNet Loss: 96.0824 \tQuestion Loss: 96.0824 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 26\tNet Loss: 93.0694 \tQuestion Loss: 93.0694 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 26\tNet Loss: 93.6714 \tQuestion Loss: 93.6714 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 26\tNet Loss: 92.6975 \tQuestion Loss: 92.6975 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 26\tNet Loss: 95.9301 \tQuestion Loss: 95.9301 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 26\tNet Loss: 93.3983 \tQuestion Loss: 93.3983 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 26\tNet Loss: 93.0871 \tQuestion Loss: 93.0871 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 26\tNet Loss: 95.3936 \tQuestion Loss: 95.3936 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 26\tNet Loss: 96.5704 \tQuestion Loss: 96.5704 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 26\tNet Loss: 92.6338 \tQuestion Loss: 92.6338 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 26\tNet Loss: 89.5740 \tQuestion Loss: 89.5740 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 26\tNet Loss: 88.9285 \tQuestion Loss: 88.9285 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 26\tNet Loss: 92.8286 \tQuestion Loss: 92.8286 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 26\tNet Loss: 93.3919 \tQuestion Loss: 93.3919 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 26\tNet Loss: 87.9287 \tQuestion Loss: 87.9287 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 26\tNet Loss: 90.0752 \tQuestion Loss: 90.0752 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 26\tNet Loss: 97.3910 \tQuestion Loss: 97.3910 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 26\tNet Loss: 94.1556 \tQuestion Loss: 94.1556 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 26\tNet Loss: 91.2879 \tQuestion Loss: 91.2879 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 26\tNet Loss: 88.3022 \tQuestion Loss: 88.3022 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 26\tNet Loss: 90.5802 \tQuestion Loss: 90.5802 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 26\tNet Loss: 93.8496 \tQuestion Loss: 93.8496 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 26\tNet Loss: 89.7713 \tQuestion Loss: 89.7713 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 26\tNet Loss: 91.5466 \tQuestion Loss: 91.5466 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 26\tNet Loss: 93.5565 \tQuestion Loss: 93.5565 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 26\tNet Loss: 97.7084 \tQuestion Loss: 97.7084 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 26\tNet Loss: 92.9974 \tQuestion Loss: 92.9974 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 26\tNet Loss: 94.8443 \tQuestion Loss: 94.8443 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 26\tNet Loss: 92.5492 \tQuestion Loss: 92.5492 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 26\tNet Loss: 96.1614 \tQuestion Loss: 96.1614 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 26\tNet Loss: 88.0267 \tQuestion Loss: 88.0267 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 26\tNet Loss: 92.2389 \tQuestion Loss: 92.2389 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 26\tNet Loss: 94.8179 \tQuestion Loss: 94.8179 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 26\tNet Loss: 99.9196 \tQuestion Loss: 99.9196 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 26\tNet Loss: 94.8741 \tQuestion Loss: 94.8741 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 26\tNet Loss: 91.3170 \tQuestion Loss: 91.3170 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 26\tNet Loss: 92.6290 \tQuestion Loss: 92.6290 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 26\tNet Loss: 93.5671 \tQuestion Loss: 93.5671 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 26\tNet Loss: 86.0389 \tQuestion Loss: 86.0389 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 26\tNet Loss: 97.2689 \tQuestion Loss: 97.2689 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 26\tNet Loss: 94.8600 \tQuestion Loss: 94.8600 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 26\tNet Loss: 93.2325 \tQuestion Loss: 93.2325 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 26\tNet Loss: 92.1352 \tQuestion Loss: 92.1352 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 26\tNet Loss: 89.2991 \tQuestion Loss: 89.2991 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 26\tNet Loss: 89.6490 \tQuestion Loss: 89.6490 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 26\tNet Loss: 94.4692 \tQuestion Loss: 94.4692 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 26\tNet Loss: 89.3299 \tQuestion Loss: 89.3299 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 26\tNet Loss: 86.9197 \tQuestion Loss: 86.9197 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 26\tNet Loss: 95.5824 \tQuestion Loss: 95.5824 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 26\tNet Loss: 98.6502 \tQuestion Loss: 98.6502 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 26\tNet Loss: 87.0603 \tQuestion Loss: 87.0603 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 26\tNet Loss: 96.5377 \tQuestion Loss: 96.5377 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 26\tNet Loss: 90.0236 \tQuestion Loss: 90.0236 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 26\tNet Loss: 91.2349 \tQuestion Loss: 91.2349 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 26\tNet Loss: 97.9660 \tQuestion Loss: 97.9660 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 26\tNet Loss: 93.5589 \tQuestion Loss: 93.5589 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 26\tNet Loss: 93.9674 \tQuestion Loss: 93.9674 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 26\tNet Loss: 85.2061 \tQuestion Loss: 85.2061 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 26\tNet Loss: 90.5937 \tQuestion Loss: 90.5937 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 26\tNet Loss: 93.5393 \tQuestion Loss: 93.5393 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 26\tNet Loss: 88.4466 \tQuestion Loss: 88.4466 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 26\tNet Loss: 93.7793 \tQuestion Loss: 93.7793 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 26\tNet Loss: 94.5915 \tQuestion Loss: 94.5915 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 26\tNet Loss: 95.8274 \tQuestion Loss: 95.8274 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 26\tNet Loss: 94.1687 \tQuestion Loss: 94.1687 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 26\tNet Loss: 95.8415 \tQuestion Loss: 95.8415 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 26\tNet Loss: 89.5701 \tQuestion Loss: 89.5701 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 164 \t Epoch : 26\tNet Loss: 88.5190 \tQuestion Loss: 88.5190 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 26\tNet Loss: 89.7493 \tQuestion Loss: 89.7493 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 26\tNet Loss: 87.5116 \tQuestion Loss: 87.5116 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 26\tNet Loss: 92.7809 \tQuestion Loss: 92.7809 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 26\tNet Loss: 91.4893 \tQuestion Loss: 91.4893 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 26\tNet Loss: 95.7466 \tQuestion Loss: 95.7466 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 26\tNet Loss: 93.5061 \tQuestion Loss: 93.5061 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 26\tNet Loss: 92.4730 \tQuestion Loss: 92.4730 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 26\tNet Loss: 89.2710 \tQuestion Loss: 89.2710 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 26\tNet Loss: 90.6186 \tQuestion Loss: 90.6186 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 26\tNet Loss: 94.6692 \tQuestion Loss: 94.6692 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 26\tNet Loss: 97.6343 \tQuestion Loss: 97.6343 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 26\tNet Loss: 90.1479 \tQuestion Loss: 90.1479 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 26\tNet Loss: 91.5300 \tQuestion Loss: 91.5300 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 26\tNet Loss: 90.5200 \tQuestion Loss: 90.5200 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 26\tNet Loss: 91.6611 \tQuestion Loss: 91.6611 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 26\tNet Loss: 90.0792 \tQuestion Loss: 90.0792 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 26\tNet Loss: 92.3917 \tQuestion Loss: 92.3917 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 26\tNet Loss: 94.3458 \tQuestion Loss: 94.3458 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 26\tNet Loss: 89.3568 \tQuestion Loss: 89.3568 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 26\tNet Loss: 93.3325 \tQuestion Loss: 93.3325 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 26\tNet Loss: 91.6381 \tQuestion Loss: 91.6381 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 26\tNet Loss: 93.9187 \tQuestion Loss: 93.9187 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 26\tNet Loss: 94.9680 \tQuestion Loss: 94.9680 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 26\tNet Loss: 89.9679 \tQuestion Loss: 89.9679 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 26\tNet Loss: 89.8125 \tQuestion Loss: 89.8125 \t Time Taken: 1 seconds\n",
      "Batch: 190 \t Epoch : 26\tNet Loss: 91.0983 \tQuestion Loss: 91.0983 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 26\tNet Loss: 90.0289 \tQuestion Loss: 90.0289 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 26\tNet Loss: 94.9247 \tQuestion Loss: 94.9247 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 26\tNet Loss: 93.4289 \tQuestion Loss: 93.4289 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 26\tNet Loss: 95.2420 \tQuestion Loss: 95.2420 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 26\tNet Loss: 87.9769 \tQuestion Loss: 87.9769 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 26\tNet Loss: 89.1891 \tQuestion Loss: 89.1891 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 26\tNet Loss: 94.6480 \tQuestion Loss: 94.6480 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 26\tNet Loss: 87.7004 \tQuestion Loss: 87.7004 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 26\tNet Loss: 92.4113 \tQuestion Loss: 92.4113 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 26 : 92.3518 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 27\tNet Loss: 92.6975 \tQuestion Loss: 92.6975 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 27\tNet Loss: 89.0577 \tQuestion Loss: 89.0577 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 27\tNet Loss: 94.2897 \tQuestion Loss: 94.2897 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 27\tNet Loss: 93.6358 \tQuestion Loss: 93.6358 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 27\tNet Loss: 93.7585 \tQuestion Loss: 93.7585 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 27\tNet Loss: 93.6210 \tQuestion Loss: 93.6210 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 27\tNet Loss: 88.8136 \tQuestion Loss: 88.8136 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 27\tNet Loss: 94.8996 \tQuestion Loss: 94.8996 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 27\tNet Loss: 93.3874 \tQuestion Loss: 93.3874 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 27\tNet Loss: 94.7839 \tQuestion Loss: 94.7839 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 27\tNet Loss: 91.0422 \tQuestion Loss: 91.0422 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 27\tNet Loss: 95.6219 \tQuestion Loss: 95.6219 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 27\tNet Loss: 90.4157 \tQuestion Loss: 90.4157 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 27\tNet Loss: 92.7628 \tQuestion Loss: 92.7628 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 27\tNet Loss: 92.9615 \tQuestion Loss: 92.9615 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 27\tNet Loss: 95.5119 \tQuestion Loss: 95.5119 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 27\tNet Loss: 90.5375 \tQuestion Loss: 90.5375 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 27\tNet Loss: 92.4563 \tQuestion Loss: 92.4563 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 27\tNet Loss: 90.4736 \tQuestion Loss: 90.4736 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 27\tNet Loss: 86.7273 \tQuestion Loss: 86.7273 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 27\tNet Loss: 96.0582 \tQuestion Loss: 96.0582 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 27\tNet Loss: 88.1695 \tQuestion Loss: 88.1695 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 27\tNet Loss: 92.9531 \tQuestion Loss: 92.9531 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 27\tNet Loss: 95.8732 \tQuestion Loss: 95.8732 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 27\tNet Loss: 89.8395 \tQuestion Loss: 89.8395 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 27\tNet Loss: 93.2488 \tQuestion Loss: 93.2488 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 27\tNet Loss: 98.4429 \tQuestion Loss: 98.4429 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 27\tNet Loss: 91.9086 \tQuestion Loss: 91.9086 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 27\tNet Loss: 86.9222 \tQuestion Loss: 86.9222 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 27\tNet Loss: 89.8808 \tQuestion Loss: 89.8808 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 27\tNet Loss: 91.6822 \tQuestion Loss: 91.6822 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 27\tNet Loss: 90.4779 \tQuestion Loss: 90.4779 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 27\tNet Loss: 88.8000 \tQuestion Loss: 88.8000 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 27\tNet Loss: 93.2820 \tQuestion Loss: 93.2820 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 27\tNet Loss: 87.3144 \tQuestion Loss: 87.3144 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 27\tNet Loss: 90.1324 \tQuestion Loss: 90.1324 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 27\tNet Loss: 93.8919 \tQuestion Loss: 93.8919 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 27\tNet Loss: 95.3434 \tQuestion Loss: 95.3434 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 27\tNet Loss: 91.2454 \tQuestion Loss: 91.2454 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 27\tNet Loss: 93.8931 \tQuestion Loss: 93.8931 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 27\tNet Loss: 91.5736 \tQuestion Loss: 91.5736 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 27\tNet Loss: 90.1923 \tQuestion Loss: 90.1923 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 27\tNet Loss: 92.0013 \tQuestion Loss: 92.0013 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 27\tNet Loss: 93.2892 \tQuestion Loss: 93.2892 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 27\tNet Loss: 90.6115 \tQuestion Loss: 90.6115 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 27\tNet Loss: 93.7492 \tQuestion Loss: 93.7492 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 27\tNet Loss: 92.3130 \tQuestion Loss: 92.3130 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 27\tNet Loss: 97.3913 \tQuestion Loss: 97.3913 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 27\tNet Loss: 85.9888 \tQuestion Loss: 85.9888 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 27\tNet Loss: 93.9455 \tQuestion Loss: 93.9455 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 27\tNet Loss: 92.9249 \tQuestion Loss: 92.9249 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 27\tNet Loss: 88.4755 \tQuestion Loss: 88.4755 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 27\tNet Loss: 89.6054 \tQuestion Loss: 89.6054 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 27\tNet Loss: 89.8105 \tQuestion Loss: 89.8105 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 27\tNet Loss: 96.5171 \tQuestion Loss: 96.5171 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 27\tNet Loss: 93.6587 \tQuestion Loss: 93.6587 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 56 \t Epoch : 27\tNet Loss: 94.4281 \tQuestion Loss: 94.4281 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 27\tNet Loss: 96.7608 \tQuestion Loss: 96.7608 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 27\tNet Loss: 87.6213 \tQuestion Loss: 87.6213 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 27\tNet Loss: 92.1405 \tQuestion Loss: 92.1405 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 27\tNet Loss: 89.1615 \tQuestion Loss: 89.1615 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 27\tNet Loss: 95.3731 \tQuestion Loss: 95.3731 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 27\tNet Loss: 88.8442 \tQuestion Loss: 88.8442 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 27\tNet Loss: 93.4760 \tQuestion Loss: 93.4760 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 27\tNet Loss: 92.1999 \tQuestion Loss: 92.1999 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 27\tNet Loss: 90.8925 \tQuestion Loss: 90.8925 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 27\tNet Loss: 92.1620 \tQuestion Loss: 92.1620 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 27\tNet Loss: 91.3266 \tQuestion Loss: 91.3266 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 27\tNet Loss: 90.7470 \tQuestion Loss: 90.7470 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 27\tNet Loss: 95.6178 \tQuestion Loss: 95.6178 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 27\tNet Loss: 93.3136 \tQuestion Loss: 93.3136 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 27\tNet Loss: 92.7022 \tQuestion Loss: 92.7022 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 27\tNet Loss: 95.6086 \tQuestion Loss: 95.6086 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 27\tNet Loss: 93.8858 \tQuestion Loss: 93.8858 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 27\tNet Loss: 95.0871 \tQuestion Loss: 95.0871 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 27\tNet Loss: 93.6486 \tQuestion Loss: 93.6486 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 27\tNet Loss: 86.7682 \tQuestion Loss: 86.7682 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 27\tNet Loss: 91.6701 \tQuestion Loss: 91.6701 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 27\tNet Loss: 96.1934 \tQuestion Loss: 96.1934 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 27\tNet Loss: 93.1309 \tQuestion Loss: 93.1309 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 27\tNet Loss: 94.2439 \tQuestion Loss: 94.2439 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 27\tNet Loss: 92.0591 \tQuestion Loss: 92.0591 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 27\tNet Loss: 89.5743 \tQuestion Loss: 89.5743 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 27\tNet Loss: 88.7781 \tQuestion Loss: 88.7781 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 27\tNet Loss: 92.0469 \tQuestion Loss: 92.0469 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 27\tNet Loss: 90.5326 \tQuestion Loss: 90.5326 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 27\tNet Loss: 92.3688 \tQuestion Loss: 92.3688 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 27\tNet Loss: 91.9654 \tQuestion Loss: 91.9654 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 27\tNet Loss: 94.2961 \tQuestion Loss: 94.2961 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 27\tNet Loss: 89.9038 \tQuestion Loss: 89.9038 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 27\tNet Loss: 95.5085 \tQuestion Loss: 95.5085 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 27\tNet Loss: 93.1335 \tQuestion Loss: 93.1335 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 27\tNet Loss: 91.3432 \tQuestion Loss: 91.3432 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 27\tNet Loss: 91.7938 \tQuestion Loss: 91.7938 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 27\tNet Loss: 92.7940 \tQuestion Loss: 92.7940 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 27\tNet Loss: 93.8064 \tQuestion Loss: 93.8064 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 27\tNet Loss: 91.9287 \tQuestion Loss: 91.9287 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 27\tNet Loss: 95.5685 \tQuestion Loss: 95.5685 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 27\tNet Loss: 92.6579 \tQuestion Loss: 92.6579 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 27\tNet Loss: 93.3786 \tQuestion Loss: 93.3786 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 27\tNet Loss: 92.7005 \tQuestion Loss: 92.7005 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 27\tNet Loss: 95.4098 \tQuestion Loss: 95.4098 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 27\tNet Loss: 92.7813 \tQuestion Loss: 92.7813 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 27\tNet Loss: 92.6641 \tQuestion Loss: 92.6641 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 27\tNet Loss: 95.4694 \tQuestion Loss: 95.4694 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 27\tNet Loss: 96.4875 \tQuestion Loss: 96.4875 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 27\tNet Loss: 92.0535 \tQuestion Loss: 92.0535 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 27\tNet Loss: 89.2327 \tQuestion Loss: 89.2327 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 27\tNet Loss: 88.5598 \tQuestion Loss: 88.5598 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 27\tNet Loss: 92.9192 \tQuestion Loss: 92.9192 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 27\tNet Loss: 93.3704 \tQuestion Loss: 93.3704 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 27\tNet Loss: 87.7478 \tQuestion Loss: 87.7478 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 27\tNet Loss: 89.7598 \tQuestion Loss: 89.7598 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 27\tNet Loss: 97.1537 \tQuestion Loss: 97.1537 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 27\tNet Loss: 94.2074 \tQuestion Loss: 94.2074 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 27\tNet Loss: 91.2106 \tQuestion Loss: 91.2106 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 27\tNet Loss: 88.4525 \tQuestion Loss: 88.4525 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 27\tNet Loss: 90.6099 \tQuestion Loss: 90.6099 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 27\tNet Loss: 93.8800 \tQuestion Loss: 93.8800 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 27\tNet Loss: 89.6496 \tQuestion Loss: 89.6496 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 27\tNet Loss: 91.5119 \tQuestion Loss: 91.5119 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 27\tNet Loss: 93.8730 \tQuestion Loss: 93.8730 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 27\tNet Loss: 96.8888 \tQuestion Loss: 96.8888 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 27\tNet Loss: 92.7626 \tQuestion Loss: 92.7626 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 27\tNet Loss: 94.7173 \tQuestion Loss: 94.7173 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 27\tNet Loss: 92.7277 \tQuestion Loss: 92.7277 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 27\tNet Loss: 96.3949 \tQuestion Loss: 96.3949 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 27\tNet Loss: 88.0625 \tQuestion Loss: 88.0625 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 27\tNet Loss: 93.3258 \tQuestion Loss: 93.3258 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 27\tNet Loss: 93.1106 \tQuestion Loss: 93.1106 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 27\tNet Loss: 100.1925 \tQuestion Loss: 100.1925 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 27\tNet Loss: 94.8288 \tQuestion Loss: 94.8288 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 27\tNet Loss: 91.1649 \tQuestion Loss: 91.1649 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 27\tNet Loss: 92.8853 \tQuestion Loss: 92.8853 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 27\tNet Loss: 92.8107 \tQuestion Loss: 92.8107 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 27\tNet Loss: 86.1206 \tQuestion Loss: 86.1206 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 27\tNet Loss: 97.2823 \tQuestion Loss: 97.2823 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 27\tNet Loss: 95.1401 \tQuestion Loss: 95.1401 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 27\tNet Loss: 93.3766 \tQuestion Loss: 93.3766 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 27\tNet Loss: 92.1624 \tQuestion Loss: 92.1624 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 27\tNet Loss: 89.2849 \tQuestion Loss: 89.2849 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 27\tNet Loss: 89.7903 \tQuestion Loss: 89.7903 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 27\tNet Loss: 94.5645 \tQuestion Loss: 94.5645 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 27\tNet Loss: 89.5925 \tQuestion Loss: 89.5925 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 27\tNet Loss: 86.9913 \tQuestion Loss: 86.9913 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 27\tNet Loss: 95.4039 \tQuestion Loss: 95.4039 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 27\tNet Loss: 98.7850 \tQuestion Loss: 98.7850 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 27\tNet Loss: 87.1610 \tQuestion Loss: 87.1610 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 148 \t Epoch : 27\tNet Loss: 96.5043 \tQuestion Loss: 96.5043 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 27\tNet Loss: 90.0143 \tQuestion Loss: 90.0143 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 27\tNet Loss: 91.1316 \tQuestion Loss: 91.1316 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 27\tNet Loss: 98.0174 \tQuestion Loss: 98.0174 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 27\tNet Loss: 93.9803 \tQuestion Loss: 93.9803 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 27\tNet Loss: 94.4790 \tQuestion Loss: 94.4790 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 27\tNet Loss: 85.3425 \tQuestion Loss: 85.3425 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 27\tNet Loss: 90.5571 \tQuestion Loss: 90.5571 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 27\tNet Loss: 93.5313 \tQuestion Loss: 93.5313 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 27\tNet Loss: 88.3185 \tQuestion Loss: 88.3185 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 27\tNet Loss: 93.7262 \tQuestion Loss: 93.7262 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 27\tNet Loss: 94.8705 \tQuestion Loss: 94.8705 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 27\tNet Loss: 95.9694 \tQuestion Loss: 95.9694 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 27\tNet Loss: 94.2359 \tQuestion Loss: 94.2359 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 27\tNet Loss: 96.0019 \tQuestion Loss: 96.0019 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 27\tNet Loss: 89.7115 \tQuestion Loss: 89.7115 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 27\tNet Loss: 88.5470 \tQuestion Loss: 88.5470 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 27\tNet Loss: 89.7322 \tQuestion Loss: 89.7322 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 27\tNet Loss: 87.5264 \tQuestion Loss: 87.5264 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 27\tNet Loss: 93.0884 \tQuestion Loss: 93.0884 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 27\tNet Loss: 91.4848 \tQuestion Loss: 91.4848 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 27\tNet Loss: 95.8753 \tQuestion Loss: 95.8753 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 27\tNet Loss: 93.6038 \tQuestion Loss: 93.6038 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 27\tNet Loss: 92.6361 \tQuestion Loss: 92.6361 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 27\tNet Loss: 88.9650 \tQuestion Loss: 88.9650 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 27\tNet Loss: 90.6337 \tQuestion Loss: 90.6337 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 27\tNet Loss: 95.0413 \tQuestion Loss: 95.0413 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 27\tNet Loss: 97.7556 \tQuestion Loss: 97.7556 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 27\tNet Loss: 90.1178 \tQuestion Loss: 90.1178 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 27\tNet Loss: 91.6742 \tQuestion Loss: 91.6742 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 27\tNet Loss: 90.5020 \tQuestion Loss: 90.5020 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 27\tNet Loss: 91.9929 \tQuestion Loss: 91.9929 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 27\tNet Loss: 90.1548 \tQuestion Loss: 90.1548 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 27\tNet Loss: 92.4677 \tQuestion Loss: 92.4677 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 27\tNet Loss: 94.3639 \tQuestion Loss: 94.3639 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 27\tNet Loss: 89.6286 \tQuestion Loss: 89.6286 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 27\tNet Loss: 93.4821 \tQuestion Loss: 93.4821 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 27\tNet Loss: 91.6047 \tQuestion Loss: 91.6047 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 27\tNet Loss: 94.3943 \tQuestion Loss: 94.3943 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 27\tNet Loss: 95.3006 \tQuestion Loss: 95.3006 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 27\tNet Loss: 89.9692 \tQuestion Loss: 89.9692 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 27\tNet Loss: 89.8721 \tQuestion Loss: 89.8721 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 27\tNet Loss: 91.1948 \tQuestion Loss: 91.1948 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 27\tNet Loss: 90.0773 \tQuestion Loss: 90.0773 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 27\tNet Loss: 94.9624 \tQuestion Loss: 94.9624 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 27\tNet Loss: 93.3934 \tQuestion Loss: 93.3934 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 27\tNet Loss: 95.0571 \tQuestion Loss: 95.0571 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 27\tNet Loss: 87.9616 \tQuestion Loss: 87.9616 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 27\tNet Loss: 89.3096 \tQuestion Loss: 89.3096 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 27\tNet Loss: 94.7024 \tQuestion Loss: 94.7024 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 27\tNet Loss: 87.9740 \tQuestion Loss: 87.9740 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 27\tNet Loss: 92.3810 \tQuestion Loss: 92.3810 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 27 : 92.3418 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 28\tNet Loss: 92.7753 \tQuestion Loss: 92.7753 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 28\tNet Loss: 89.1488 \tQuestion Loss: 89.1488 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 28\tNet Loss: 94.3519 \tQuestion Loss: 94.3519 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 28\tNet Loss: 93.5249 \tQuestion Loss: 93.5249 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 28\tNet Loss: 93.8419 \tQuestion Loss: 93.8419 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 28\tNet Loss: 93.7455 \tQuestion Loss: 93.7455 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 28\tNet Loss: 88.8522 \tQuestion Loss: 88.8522 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 28\tNet Loss: 94.9403 \tQuestion Loss: 94.9403 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 28\tNet Loss: 93.5764 \tQuestion Loss: 93.5764 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 28\tNet Loss: 95.2436 \tQuestion Loss: 95.2436 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 28\tNet Loss: 91.2503 \tQuestion Loss: 91.2503 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 28\tNet Loss: 95.6659 \tQuestion Loss: 95.6659 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 28\tNet Loss: 90.4103 \tQuestion Loss: 90.4103 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 28\tNet Loss: 92.7389 \tQuestion Loss: 92.7389 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 28\tNet Loss: 93.1319 \tQuestion Loss: 93.1319 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 28\tNet Loss: 95.4124 \tQuestion Loss: 95.4124 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 28\tNet Loss: 90.4281 \tQuestion Loss: 90.4281 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 28\tNet Loss: 92.5455 \tQuestion Loss: 92.5455 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 28\tNet Loss: 91.1506 \tQuestion Loss: 91.1506 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 28\tNet Loss: 86.8489 \tQuestion Loss: 86.8489 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 28\tNet Loss: 96.1665 \tQuestion Loss: 96.1665 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 28\tNet Loss: 88.2472 \tQuestion Loss: 88.2472 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 28\tNet Loss: 92.8375 \tQuestion Loss: 92.8375 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 28\tNet Loss: 95.8719 \tQuestion Loss: 95.8719 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 28\tNet Loss: 89.7621 \tQuestion Loss: 89.7621 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 28\tNet Loss: 93.2576 \tQuestion Loss: 93.2576 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 28\tNet Loss: 98.3719 \tQuestion Loss: 98.3719 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 28\tNet Loss: 91.9013 \tQuestion Loss: 91.9013 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 28\tNet Loss: 87.0785 \tQuestion Loss: 87.0785 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 28\tNet Loss: 89.9244 \tQuestion Loss: 89.9244 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 28\tNet Loss: 91.5339 \tQuestion Loss: 91.5339 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 28\tNet Loss: 90.3795 \tQuestion Loss: 90.3795 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 28\tNet Loss: 89.1081 \tQuestion Loss: 89.1081 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 28\tNet Loss: 93.3061 \tQuestion Loss: 93.3061 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 28\tNet Loss: 87.4167 \tQuestion Loss: 87.4167 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 28\tNet Loss: 90.1486 \tQuestion Loss: 90.1486 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 28\tNet Loss: 93.9431 \tQuestion Loss: 93.9431 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 28\tNet Loss: 95.0044 \tQuestion Loss: 95.0044 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 28\tNet Loss: 91.0261 \tQuestion Loss: 91.0261 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 39 \t Epoch : 28\tNet Loss: 94.1885 \tQuestion Loss: 94.1885 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 28\tNet Loss: 91.5366 \tQuestion Loss: 91.5366 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 28\tNet Loss: 90.3783 \tQuestion Loss: 90.3783 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 28\tNet Loss: 92.1500 \tQuestion Loss: 92.1500 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 28\tNet Loss: 93.5296 \tQuestion Loss: 93.5296 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 28\tNet Loss: 90.7401 \tQuestion Loss: 90.7401 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 28\tNet Loss: 93.6966 \tQuestion Loss: 93.6966 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 28\tNet Loss: 92.0989 \tQuestion Loss: 92.0989 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 28\tNet Loss: 97.2009 \tQuestion Loss: 97.2009 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 28\tNet Loss: 86.8828 \tQuestion Loss: 86.8828 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 28\tNet Loss: 94.3781 \tQuestion Loss: 94.3781 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 28\tNet Loss: 92.9508 \tQuestion Loss: 92.9508 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 28\tNet Loss: 88.5049 \tQuestion Loss: 88.5049 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 28\tNet Loss: 89.4902 \tQuestion Loss: 89.4902 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 28\tNet Loss: 90.0691 \tQuestion Loss: 90.0691 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 28\tNet Loss: 96.4379 \tQuestion Loss: 96.4379 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 28\tNet Loss: 93.0610 \tQuestion Loss: 93.0610 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 28\tNet Loss: 94.3897 \tQuestion Loss: 94.3897 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 28\tNet Loss: 96.6592 \tQuestion Loss: 96.6592 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 28\tNet Loss: 87.9863 \tQuestion Loss: 87.9863 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 28\tNet Loss: 92.0361 \tQuestion Loss: 92.0361 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 28\tNet Loss: 88.8996 \tQuestion Loss: 88.8996 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 28\tNet Loss: 94.8950 \tQuestion Loss: 94.8950 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 28\tNet Loss: 88.7604 \tQuestion Loss: 88.7604 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 28\tNet Loss: 93.3262 \tQuestion Loss: 93.3262 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 28\tNet Loss: 91.7990 \tQuestion Loss: 91.7990 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 28\tNet Loss: 90.5371 \tQuestion Loss: 90.5371 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 28\tNet Loss: 93.5245 \tQuestion Loss: 93.5245 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 28\tNet Loss: 93.2229 \tQuestion Loss: 93.2229 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 28\tNet Loss: 90.7529 \tQuestion Loss: 90.7529 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 28\tNet Loss: 96.3345 \tQuestion Loss: 96.3345 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 28\tNet Loss: 93.4974 \tQuestion Loss: 93.4974 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 28\tNet Loss: 93.4553 \tQuestion Loss: 93.4553 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 28\tNet Loss: 95.8153 \tQuestion Loss: 95.8153 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 28\tNet Loss: 93.0924 \tQuestion Loss: 93.0924 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 28\tNet Loss: 94.9901 \tQuestion Loss: 94.9901 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 28\tNet Loss: 93.4853 \tQuestion Loss: 93.4853 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 28\tNet Loss: 87.2361 \tQuestion Loss: 87.2361 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 28\tNet Loss: 91.1374 \tQuestion Loss: 91.1374 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 28\tNet Loss: 96.8656 \tQuestion Loss: 96.8656 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 28\tNet Loss: 92.6505 \tQuestion Loss: 92.6505 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 28\tNet Loss: 94.3460 \tQuestion Loss: 94.3460 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 28\tNet Loss: 92.1907 \tQuestion Loss: 92.1907 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 28\tNet Loss: 89.3631 \tQuestion Loss: 89.3631 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 28\tNet Loss: 88.3951 \tQuestion Loss: 88.3951 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 28\tNet Loss: 91.7796 \tQuestion Loss: 91.7796 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 28\tNet Loss: 90.0393 \tQuestion Loss: 90.0393 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 28\tNet Loss: 91.8918 \tQuestion Loss: 91.8918 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 28\tNet Loss: 91.1355 \tQuestion Loss: 91.1355 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 28\tNet Loss: 94.3074 \tQuestion Loss: 94.3074 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 28\tNet Loss: 90.1415 \tQuestion Loss: 90.1415 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 28\tNet Loss: 94.9782 \tQuestion Loss: 94.9782 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 28\tNet Loss: 92.5248 \tQuestion Loss: 92.5248 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 28\tNet Loss: 91.1632 \tQuestion Loss: 91.1632 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 28\tNet Loss: 91.8878 \tQuestion Loss: 91.8878 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 28\tNet Loss: 92.3302 \tQuestion Loss: 92.3302 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 28\tNet Loss: 93.0349 \tQuestion Loss: 93.0349 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 28\tNet Loss: 91.7539 \tQuestion Loss: 91.7539 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 28\tNet Loss: 95.8128 \tQuestion Loss: 95.8128 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 28\tNet Loss: 92.6206 \tQuestion Loss: 92.6206 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 28\tNet Loss: 92.5973 \tQuestion Loss: 92.5973 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 28\tNet Loss: 92.2867 \tQuestion Loss: 92.2867 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 28\tNet Loss: 95.5466 \tQuestion Loss: 95.5466 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 28\tNet Loss: 93.1281 \tQuestion Loss: 93.1281 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 28\tNet Loss: 92.1893 \tQuestion Loss: 92.1893 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 28\tNet Loss: 94.5749 \tQuestion Loss: 94.5749 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 28\tNet Loss: 96.5309 \tQuestion Loss: 96.5309 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 28\tNet Loss: 92.2508 \tQuestion Loss: 92.2508 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 28\tNet Loss: 89.0946 \tQuestion Loss: 89.0946 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 28\tNet Loss: 88.2471 \tQuestion Loss: 88.2471 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 28\tNet Loss: 92.7084 \tQuestion Loss: 92.7084 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 28\tNet Loss: 93.0539 \tQuestion Loss: 93.0539 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 28\tNet Loss: 87.6245 \tQuestion Loss: 87.6245 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 28\tNet Loss: 89.6133 \tQuestion Loss: 89.6133 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 28\tNet Loss: 97.0510 \tQuestion Loss: 97.0510 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 28\tNet Loss: 93.8839 \tQuestion Loss: 93.8839 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 28\tNet Loss: 91.0839 \tQuestion Loss: 91.0839 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 28\tNet Loss: 87.9370 \tQuestion Loss: 87.9370 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 28\tNet Loss: 90.2836 \tQuestion Loss: 90.2836 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 28\tNet Loss: 93.6604 \tQuestion Loss: 93.6604 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 28\tNet Loss: 89.6506 \tQuestion Loss: 89.6506 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 28\tNet Loss: 91.3968 \tQuestion Loss: 91.3968 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 28\tNet Loss: 93.3575 \tQuestion Loss: 93.3575 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 28\tNet Loss: 97.5103 \tQuestion Loss: 97.5103 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 28\tNet Loss: 92.7718 \tQuestion Loss: 92.7718 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 28\tNet Loss: 94.7378 \tQuestion Loss: 94.7378 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 28\tNet Loss: 92.3755 \tQuestion Loss: 92.3755 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 28\tNet Loss: 95.9853 \tQuestion Loss: 95.9853 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 28\tNet Loss: 87.9403 \tQuestion Loss: 87.9403 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 28\tNet Loss: 92.1264 \tQuestion Loss: 92.1264 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 28\tNet Loss: 94.7118 \tQuestion Loss: 94.7118 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 28\tNet Loss: 99.7949 \tQuestion Loss: 99.7949 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 131 \t Epoch : 28\tNet Loss: 94.6161 \tQuestion Loss: 94.6161 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 28\tNet Loss: 91.2366 \tQuestion Loss: 91.2366 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 28\tNet Loss: 92.5462 \tQuestion Loss: 92.5462 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 28\tNet Loss: 93.4958 \tQuestion Loss: 93.4958 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 28\tNet Loss: 85.8591 \tQuestion Loss: 85.8591 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 28\tNet Loss: 97.1112 \tQuestion Loss: 97.1112 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 28\tNet Loss: 94.8326 \tQuestion Loss: 94.8326 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 28\tNet Loss: 93.2478 \tQuestion Loss: 93.2478 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 28\tNet Loss: 91.9518 \tQuestion Loss: 91.9518 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 28\tNet Loss: 89.1467 \tQuestion Loss: 89.1467 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 28\tNet Loss: 89.6304 \tQuestion Loss: 89.6304 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 28\tNet Loss: 94.5150 \tQuestion Loss: 94.5150 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 28\tNet Loss: 89.2357 \tQuestion Loss: 89.2357 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 28\tNet Loss: 86.8200 \tQuestion Loss: 86.8200 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 28\tNet Loss: 95.4199 \tQuestion Loss: 95.4199 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 28\tNet Loss: 98.5840 \tQuestion Loss: 98.5840 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 28\tNet Loss: 86.9064 \tQuestion Loss: 86.9064 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 28\tNet Loss: 96.3934 \tQuestion Loss: 96.3934 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 28\tNet Loss: 89.9119 \tQuestion Loss: 89.9119 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 28\tNet Loss: 91.1364 \tQuestion Loss: 91.1364 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 28\tNet Loss: 97.9314 \tQuestion Loss: 97.9314 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 28\tNet Loss: 93.5129 \tQuestion Loss: 93.5129 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 28\tNet Loss: 93.9554 \tQuestion Loss: 93.9554 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 28\tNet Loss: 85.0939 \tQuestion Loss: 85.0939 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 28\tNet Loss: 90.5288 \tQuestion Loss: 90.5288 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 28\tNet Loss: 93.3087 \tQuestion Loss: 93.3087 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 28\tNet Loss: 88.3255 \tQuestion Loss: 88.3255 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 28\tNet Loss: 93.5099 \tQuestion Loss: 93.5099 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 28\tNet Loss: 94.5000 \tQuestion Loss: 94.5000 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 28\tNet Loss: 95.7643 \tQuestion Loss: 95.7643 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 28\tNet Loss: 94.0938 \tQuestion Loss: 94.0938 \t Time Taken: 1 seconds\n",
      "Batch: 162 \t Epoch : 28\tNet Loss: 95.7565 \tQuestion Loss: 95.7565 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 28\tNet Loss: 89.5784 \tQuestion Loss: 89.5784 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 28\tNet Loss: 88.4473 \tQuestion Loss: 88.4473 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 28\tNet Loss: 89.6761 \tQuestion Loss: 89.6761 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 28\tNet Loss: 87.3722 \tQuestion Loss: 87.3722 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 28\tNet Loss: 92.7116 \tQuestion Loss: 92.7116 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 28\tNet Loss: 91.3881 \tQuestion Loss: 91.3881 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 28\tNet Loss: 95.6450 \tQuestion Loss: 95.6450 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 28\tNet Loss: 93.4737 \tQuestion Loss: 93.4737 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 28\tNet Loss: 92.4042 \tQuestion Loss: 92.4042 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 28\tNet Loss: 89.1724 \tQuestion Loss: 89.1724 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 28\tNet Loss: 90.5409 \tQuestion Loss: 90.5409 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 28\tNet Loss: 94.5993 \tQuestion Loss: 94.5993 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 28\tNet Loss: 97.4742 \tQuestion Loss: 97.4742 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 28\tNet Loss: 90.1165 \tQuestion Loss: 90.1165 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 28\tNet Loss: 91.4357 \tQuestion Loss: 91.4357 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 28\tNet Loss: 90.4292 \tQuestion Loss: 90.4292 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 28\tNet Loss: 91.5782 \tQuestion Loss: 91.5782 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 28\tNet Loss: 90.0120 \tQuestion Loss: 90.0120 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 28\tNet Loss: 92.3229 \tQuestion Loss: 92.3229 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 28\tNet Loss: 94.2749 \tQuestion Loss: 94.2749 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 28\tNet Loss: 89.2708 \tQuestion Loss: 89.2708 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 28\tNet Loss: 93.2339 \tQuestion Loss: 93.2339 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 28\tNet Loss: 91.4393 \tQuestion Loss: 91.4393 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 28\tNet Loss: 93.7430 \tQuestion Loss: 93.7430 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 28\tNet Loss: 94.7143 \tQuestion Loss: 94.7143 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 28\tNet Loss: 89.7819 \tQuestion Loss: 89.7819 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 28\tNet Loss: 89.5624 \tQuestion Loss: 89.5624 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 28\tNet Loss: 91.0029 \tQuestion Loss: 91.0029 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 28\tNet Loss: 89.9706 \tQuestion Loss: 89.9706 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 28\tNet Loss: 94.7884 \tQuestion Loss: 94.7884 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 28\tNet Loss: 93.3018 \tQuestion Loss: 93.3018 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 28\tNet Loss: 95.0635 \tQuestion Loss: 95.0635 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 28\tNet Loss: 87.8230 \tQuestion Loss: 87.8230 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 28\tNet Loss: 89.0293 \tQuestion Loss: 89.0293 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 28\tNet Loss: 94.4572 \tQuestion Loss: 94.4572 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 28\tNet Loss: 87.5573 \tQuestion Loss: 87.5573 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 28\tNet Loss: 92.3081 \tQuestion Loss: 92.3081 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 28 : 92.2575 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 29\tNet Loss: 92.5655 \tQuestion Loss: 92.5655 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 29\tNet Loss: 88.8439 \tQuestion Loss: 88.8439 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 29\tNet Loss: 93.9857 \tQuestion Loss: 93.9857 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 29\tNet Loss: 93.6499 \tQuestion Loss: 93.6499 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 29\tNet Loss: 93.5789 \tQuestion Loss: 93.5789 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 29\tNet Loss: 93.4166 \tQuestion Loss: 93.4166 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 29\tNet Loss: 88.7158 \tQuestion Loss: 88.7158 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 29\tNet Loss: 94.7824 \tQuestion Loss: 94.7824 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 29\tNet Loss: 93.3228 \tQuestion Loss: 93.3228 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 29\tNet Loss: 94.7444 \tQuestion Loss: 94.7444 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 29\tNet Loss: 90.8789 \tQuestion Loss: 90.8789 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 29\tNet Loss: 95.4911 \tQuestion Loss: 95.4911 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 29\tNet Loss: 90.2986 \tQuestion Loss: 90.2986 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 29\tNet Loss: 92.6538 \tQuestion Loss: 92.6538 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 29\tNet Loss: 92.8801 \tQuestion Loss: 92.8801 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 29\tNet Loss: 95.4171 \tQuestion Loss: 95.4171 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 29\tNet Loss: 90.4356 \tQuestion Loss: 90.4356 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 29\tNet Loss: 92.3993 \tQuestion Loss: 92.3993 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 29\tNet Loss: 90.4088 \tQuestion Loss: 90.4088 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 29\tNet Loss: 86.6355 \tQuestion Loss: 86.6355 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 29\tNet Loss: 95.9538 \tQuestion Loss: 95.9538 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 29\tNet Loss: 88.0763 \tQuestion Loss: 88.0763 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 22 \t Epoch : 29\tNet Loss: 92.8644 \tQuestion Loss: 92.8644 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 29\tNet Loss: 95.7618 \tQuestion Loss: 95.7618 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 29\tNet Loss: 89.7392 \tQuestion Loss: 89.7392 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 29\tNet Loss: 93.2284 \tQuestion Loss: 93.2284 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 29\tNet Loss: 98.3962 \tQuestion Loss: 98.3962 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 29\tNet Loss: 91.8311 \tQuestion Loss: 91.8311 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 29\tNet Loss: 86.8315 \tQuestion Loss: 86.8315 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 29\tNet Loss: 89.7676 \tQuestion Loss: 89.7676 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 29\tNet Loss: 91.6175 \tQuestion Loss: 91.6175 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 29\tNet Loss: 90.4079 \tQuestion Loss: 90.4079 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 29\tNet Loss: 88.7585 \tQuestion Loss: 88.7585 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 29\tNet Loss: 93.1920 \tQuestion Loss: 93.1920 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 29\tNet Loss: 87.2425 \tQuestion Loss: 87.2425 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 29\tNet Loss: 90.0383 \tQuestion Loss: 90.0383 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 29\tNet Loss: 93.7602 \tQuestion Loss: 93.7602 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 29\tNet Loss: 95.3102 \tQuestion Loss: 95.3102 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 29\tNet Loss: 91.2285 \tQuestion Loss: 91.2285 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 29\tNet Loss: 93.8529 \tQuestion Loss: 93.8529 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 29\tNet Loss: 91.4937 \tQuestion Loss: 91.4937 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 29\tNet Loss: 90.1943 \tQuestion Loss: 90.1943 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 29\tNet Loss: 91.9378 \tQuestion Loss: 91.9378 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 29\tNet Loss: 93.1964 \tQuestion Loss: 93.1964 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 29\tNet Loss: 90.5990 \tQuestion Loss: 90.5990 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 29\tNet Loss: 93.6516 \tQuestion Loss: 93.6516 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 29\tNet Loss: 92.2651 \tQuestion Loss: 92.2651 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 29\tNet Loss: 97.2949 \tQuestion Loss: 97.2949 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 29\tNet Loss: 86.0803 \tQuestion Loss: 86.0803 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 29\tNet Loss: 94.1013 \tQuestion Loss: 94.1013 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 29\tNet Loss: 93.1618 \tQuestion Loss: 93.1618 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 29\tNet Loss: 88.6269 \tQuestion Loss: 88.6269 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 29\tNet Loss: 89.5179 \tQuestion Loss: 89.5179 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 29\tNet Loss: 89.7708 \tQuestion Loss: 89.7708 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 29\tNet Loss: 96.5517 \tQuestion Loss: 96.5517 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 29\tNet Loss: 93.2075 \tQuestion Loss: 93.2075 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 29\tNet Loss: 94.3290 \tQuestion Loss: 94.3290 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 29\tNet Loss: 96.5158 \tQuestion Loss: 96.5158 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 29\tNet Loss: 87.6595 \tQuestion Loss: 87.6595 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 29\tNet Loss: 92.4262 \tQuestion Loss: 92.4262 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 29\tNet Loss: 89.1497 \tQuestion Loss: 89.1497 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 29\tNet Loss: 95.1092 \tQuestion Loss: 95.1092 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 29\tNet Loss: 88.9746 \tQuestion Loss: 88.9746 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 29\tNet Loss: 93.5688 \tQuestion Loss: 93.5688 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 29\tNet Loss: 92.1676 \tQuestion Loss: 92.1676 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 29\tNet Loss: 90.4942 \tQuestion Loss: 90.4942 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 29\tNet Loss: 92.0930 \tQuestion Loss: 92.0930 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 29\tNet Loss: 91.2190 \tQuestion Loss: 91.2190 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 29\tNet Loss: 91.1122 \tQuestion Loss: 91.1122 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 29\tNet Loss: 95.5903 \tQuestion Loss: 95.5903 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 29\tNet Loss: 92.9789 \tQuestion Loss: 92.9789 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 29\tNet Loss: 92.6825 \tQuestion Loss: 92.6825 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 29\tNet Loss: 95.8288 \tQuestion Loss: 95.8288 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 29\tNet Loss: 94.0444 \tQuestion Loss: 94.0444 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 29\tNet Loss: 94.8543 \tQuestion Loss: 94.8543 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 29\tNet Loss: 93.5472 \tQuestion Loss: 93.5472 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 29\tNet Loss: 86.9595 \tQuestion Loss: 86.9595 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 29\tNet Loss: 91.7678 \tQuestion Loss: 91.7678 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 29\tNet Loss: 96.0432 \tQuestion Loss: 96.0432 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 29\tNet Loss: 92.8606 \tQuestion Loss: 92.8606 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 29\tNet Loss: 94.1770 \tQuestion Loss: 94.1770 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 29\tNet Loss: 92.2078 \tQuestion Loss: 92.2078 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 29\tNet Loss: 89.7054 \tQuestion Loss: 89.7054 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 29\tNet Loss: 88.7326 \tQuestion Loss: 88.7326 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 29\tNet Loss: 92.0594 \tQuestion Loss: 92.0594 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 29\tNet Loss: 90.5816 \tQuestion Loss: 90.5816 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 29\tNet Loss: 92.5788 \tQuestion Loss: 92.5788 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 29\tNet Loss: 92.0702 \tQuestion Loss: 92.0702 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 29\tNet Loss: 94.3563 \tQuestion Loss: 94.3563 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 29\tNet Loss: 89.9844 \tQuestion Loss: 89.9844 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 29\tNet Loss: 95.4912 \tQuestion Loss: 95.4912 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 29\tNet Loss: 93.1338 \tQuestion Loss: 93.1338 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 29\tNet Loss: 91.3720 \tQuestion Loss: 91.3720 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 29\tNet Loss: 91.8650 \tQuestion Loss: 91.8650 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 29\tNet Loss: 92.7682 \tQuestion Loss: 92.7682 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 29\tNet Loss: 93.6529 \tQuestion Loss: 93.6529 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 29\tNet Loss: 91.9298 \tQuestion Loss: 91.9298 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 29\tNet Loss: 95.6660 \tQuestion Loss: 95.6660 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 29\tNet Loss: 92.6650 \tQuestion Loss: 92.6650 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 29\tNet Loss: 93.3309 \tQuestion Loss: 93.3309 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 29\tNet Loss: 92.5366 \tQuestion Loss: 92.5366 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 29\tNet Loss: 95.4894 \tQuestion Loss: 95.4894 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 29\tNet Loss: 92.8184 \tQuestion Loss: 92.8184 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 29\tNet Loss: 92.5347 \tQuestion Loss: 92.5347 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 29\tNet Loss: 95.3601 \tQuestion Loss: 95.3601 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 29\tNet Loss: 96.5274 \tQuestion Loss: 96.5274 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 29\tNet Loss: 92.0741 \tQuestion Loss: 92.0741 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 29\tNet Loss: 89.2715 \tQuestion Loss: 89.2715 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 29\tNet Loss: 88.4896 \tQuestion Loss: 88.4896 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 29\tNet Loss: 92.9358 \tQuestion Loss: 92.9358 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 29\tNet Loss: 93.4671 \tQuestion Loss: 93.4671 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 29\tNet Loss: 87.8884 \tQuestion Loss: 87.8884 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 29\tNet Loss: 89.7967 \tQuestion Loss: 89.7967 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 29\tNet Loss: 97.1236 \tQuestion Loss: 97.1236 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 114 \t Epoch : 29\tNet Loss: 94.2089 \tQuestion Loss: 94.2089 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 29\tNet Loss: 91.3123 \tQuestion Loss: 91.3123 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 29\tNet Loss: 88.5516 \tQuestion Loss: 88.5516 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 29\tNet Loss: 90.6916 \tQuestion Loss: 90.6916 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 29\tNet Loss: 93.8661 \tQuestion Loss: 93.8661 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 29\tNet Loss: 89.6709 \tQuestion Loss: 89.6709 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 29\tNet Loss: 91.5434 \tQuestion Loss: 91.5434 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 29\tNet Loss: 93.8639 \tQuestion Loss: 93.8639 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 29\tNet Loss: 96.9547 \tQuestion Loss: 96.9547 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 29\tNet Loss: 92.8126 \tQuestion Loss: 92.8126 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 29\tNet Loss: 94.7370 \tQuestion Loss: 94.7370 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 29\tNet Loss: 92.8175 \tQuestion Loss: 92.8175 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 29\tNet Loss: 96.4356 \tQuestion Loss: 96.4356 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 29\tNet Loss: 88.0560 \tQuestion Loss: 88.0560 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 29\tNet Loss: 93.2693 \tQuestion Loss: 93.2693 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 29\tNet Loss: 93.1932 \tQuestion Loss: 93.1932 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 29\tNet Loss: 100.2142 \tQuestion Loss: 100.2142 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 29\tNet Loss: 94.8158 \tQuestion Loss: 94.8158 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 29\tNet Loss: 91.1825 \tQuestion Loss: 91.1825 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 29\tNet Loss: 92.8121 \tQuestion Loss: 92.8121 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 29\tNet Loss: 92.9076 \tQuestion Loss: 92.9076 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 29\tNet Loss: 86.1978 \tQuestion Loss: 86.1978 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 29\tNet Loss: 97.3752 \tQuestion Loss: 97.3752 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 29\tNet Loss: 95.0994 \tQuestion Loss: 95.0994 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 29\tNet Loss: 93.4473 \tQuestion Loss: 93.4473 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 29\tNet Loss: 92.2209 \tQuestion Loss: 92.2209 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 29\tNet Loss: 89.3272 \tQuestion Loss: 89.3272 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 29\tNet Loss: 89.8035 \tQuestion Loss: 89.8035 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 29\tNet Loss: 94.5370 \tQuestion Loss: 94.5370 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 29\tNet Loss: 89.6472 \tQuestion Loss: 89.6472 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 29\tNet Loss: 87.0799 \tQuestion Loss: 87.0799 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 29\tNet Loss: 95.4730 \tQuestion Loss: 95.4730 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 29\tNet Loss: 98.7791 \tQuestion Loss: 98.7791 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 29\tNet Loss: 87.1968 \tQuestion Loss: 87.1968 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 29\tNet Loss: 96.5627 \tQuestion Loss: 96.5627 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 29\tNet Loss: 90.0423 \tQuestion Loss: 90.0423 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 29\tNet Loss: 91.1672 \tQuestion Loss: 91.1672 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 29\tNet Loss: 98.0435 \tQuestion Loss: 98.0435 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 29\tNet Loss: 93.9753 \tQuestion Loss: 93.9753 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 29\tNet Loss: 94.4551 \tQuestion Loss: 94.4551 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 29\tNet Loss: 85.3924 \tQuestion Loss: 85.3924 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 29\tNet Loss: 90.5707 \tQuestion Loss: 90.5707 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 29\tNet Loss: 93.5856 \tQuestion Loss: 93.5856 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 29\tNet Loss: 88.3234 \tQuestion Loss: 88.3234 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 29\tNet Loss: 93.7982 \tQuestion Loss: 93.7982 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 29\tNet Loss: 94.9313 \tQuestion Loss: 94.9313 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 29\tNet Loss: 95.9720 \tQuestion Loss: 95.9720 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 29\tNet Loss: 94.2757 \tQuestion Loss: 94.2757 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 29\tNet Loss: 96.0434 \tQuestion Loss: 96.0434 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 29\tNet Loss: 89.7863 \tQuestion Loss: 89.7863 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 29\tNet Loss: 88.6140 \tQuestion Loss: 88.6140 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 29\tNet Loss: 89.7602 \tQuestion Loss: 89.7602 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 29\tNet Loss: 87.5682 \tQuestion Loss: 87.5682 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 29\tNet Loss: 93.1294 \tQuestion Loss: 93.1294 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 29\tNet Loss: 91.5106 \tQuestion Loss: 91.5106 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 29\tNet Loss: 95.9006 \tQuestion Loss: 95.9006 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 29\tNet Loss: 93.6210 \tQuestion Loss: 93.6210 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 29\tNet Loss: 92.6278 \tQuestion Loss: 92.6278 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 29\tNet Loss: 89.0066 \tQuestion Loss: 89.0066 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 29\tNet Loss: 90.6645 \tQuestion Loss: 90.6645 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 29\tNet Loss: 95.1145 \tQuestion Loss: 95.1145 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 29\tNet Loss: 97.9524 \tQuestion Loss: 97.9524 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 29\tNet Loss: 90.3029 \tQuestion Loss: 90.3029 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 29\tNet Loss: 91.8454 \tQuestion Loss: 91.8454 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 29\tNet Loss: 90.5530 \tQuestion Loss: 90.5530 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 29\tNet Loss: 92.0864 \tQuestion Loss: 92.0864 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 29\tNet Loss: 90.1737 \tQuestion Loss: 90.1737 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 29\tNet Loss: 92.4687 \tQuestion Loss: 92.4687 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 29\tNet Loss: 94.3301 \tQuestion Loss: 94.3301 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 29\tNet Loss: 89.6902 \tQuestion Loss: 89.6902 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 29\tNet Loss: 93.5163 \tQuestion Loss: 93.5163 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 29\tNet Loss: 91.6727 \tQuestion Loss: 91.6727 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 29\tNet Loss: 94.4781 \tQuestion Loss: 94.4781 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 29\tNet Loss: 95.4593 \tQuestion Loss: 95.4593 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 29\tNet Loss: 90.0819 \tQuestion Loss: 90.0819 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 29\tNet Loss: 89.9607 \tQuestion Loss: 89.9607 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 29\tNet Loss: 91.2186 \tQuestion Loss: 91.2186 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 29\tNet Loss: 90.1258 \tQuestion Loss: 90.1258 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 29\tNet Loss: 95.0773 \tQuestion Loss: 95.0773 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 29\tNet Loss: 93.4144 \tQuestion Loss: 93.4144 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 29\tNet Loss: 95.0608 \tQuestion Loss: 95.0608 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 29\tNet Loss: 87.9802 \tQuestion Loss: 87.9802 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 29\tNet Loss: 89.4044 \tQuestion Loss: 89.4044 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 29\tNet Loss: 94.8060 \tQuestion Loss: 94.8060 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 29\tNet Loss: 87.9881 \tQuestion Loss: 87.9881 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 29\tNet Loss: 92.4164 \tQuestion Loss: 92.4164 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 29 : 92.3392 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 30\tNet Loss: 92.8367 \tQuestion Loss: 92.8367 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 30\tNet Loss: 89.2797 \tQuestion Loss: 89.2797 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 30\tNet Loss: 94.5226 \tQuestion Loss: 94.5226 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 30\tNet Loss: 93.4732 \tQuestion Loss: 93.4732 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 30\tNet Loss: 93.9336 \tQuestion Loss: 93.9336 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 5 \t Epoch : 30\tNet Loss: 93.8120 \tQuestion Loss: 93.8120 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 30\tNet Loss: 88.8948 \tQuestion Loss: 88.8948 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 30\tNet Loss: 94.9830 \tQuestion Loss: 94.9830 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 30\tNet Loss: 93.5511 \tQuestion Loss: 93.5511 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 30\tNet Loss: 95.3083 \tQuestion Loss: 95.3083 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 30\tNet Loss: 91.3753 \tQuestion Loss: 91.3753 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 30\tNet Loss: 95.6910 \tQuestion Loss: 95.6910 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 30\tNet Loss: 90.4493 \tQuestion Loss: 90.4493 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 30\tNet Loss: 92.7373 \tQuestion Loss: 92.7373 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 30\tNet Loss: 93.2092 \tQuestion Loss: 93.2092 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 30\tNet Loss: 95.5026 \tQuestion Loss: 95.5026 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 30\tNet Loss: 90.4281 \tQuestion Loss: 90.4281 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 30\tNet Loss: 92.5417 \tQuestion Loss: 92.5417 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 30\tNet Loss: 91.2071 \tQuestion Loss: 91.2071 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 30\tNet Loss: 86.9433 \tQuestion Loss: 86.9433 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 30\tNet Loss: 96.2589 \tQuestion Loss: 96.2589 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 30\tNet Loss: 88.2437 \tQuestion Loss: 88.2437 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 30\tNet Loss: 92.8610 \tQuestion Loss: 92.8610 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 30\tNet Loss: 95.9360 \tQuestion Loss: 95.9360 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 30\tNet Loss: 89.8409 \tQuestion Loss: 89.8409 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 30\tNet Loss: 93.3044 \tQuestion Loss: 93.3044 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 30\tNet Loss: 98.3481 \tQuestion Loss: 98.3481 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 30\tNet Loss: 91.9318 \tQuestion Loss: 91.9318 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 30\tNet Loss: 87.1277 \tQuestion Loss: 87.1277 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 30\tNet Loss: 89.9654 \tQuestion Loss: 89.9654 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 30\tNet Loss: 91.5944 \tQuestion Loss: 91.5944 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 30\tNet Loss: 90.3776 \tQuestion Loss: 90.3776 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 30\tNet Loss: 89.1798 \tQuestion Loss: 89.1798 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 30\tNet Loss: 93.3813 \tQuestion Loss: 93.3813 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 30\tNet Loss: 87.4383 \tQuestion Loss: 87.4383 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 30\tNet Loss: 90.1352 \tQuestion Loss: 90.1352 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 30\tNet Loss: 94.0319 \tQuestion Loss: 94.0319 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 30\tNet Loss: 95.1151 \tQuestion Loss: 95.1151 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 30\tNet Loss: 91.0636 \tQuestion Loss: 91.0636 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 30\tNet Loss: 94.2650 \tQuestion Loss: 94.2650 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 30\tNet Loss: 91.5270 \tQuestion Loss: 91.5270 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 30\tNet Loss: 90.3412 \tQuestion Loss: 90.3412 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 30\tNet Loss: 92.2623 \tQuestion Loss: 92.2623 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 30\tNet Loss: 93.5594 \tQuestion Loss: 93.5594 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 30\tNet Loss: 90.7310 \tQuestion Loss: 90.7310 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 30\tNet Loss: 93.6895 \tQuestion Loss: 93.6895 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 30\tNet Loss: 92.1556 \tQuestion Loss: 92.1556 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 30\tNet Loss: 97.2375 \tQuestion Loss: 97.2375 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 30\tNet Loss: 86.7709 \tQuestion Loss: 86.7709 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 30\tNet Loss: 94.1306 \tQuestion Loss: 94.1306 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 30\tNet Loss: 92.7359 \tQuestion Loss: 92.7359 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 30\tNet Loss: 88.3464 \tQuestion Loss: 88.3464 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 30\tNet Loss: 89.5341 \tQuestion Loss: 89.5341 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 30\tNet Loss: 90.0436 \tQuestion Loss: 90.0436 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 30\tNet Loss: 96.3066 \tQuestion Loss: 96.3066 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 30\tNet Loss: 92.9622 \tQuestion Loss: 92.9622 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 30\tNet Loss: 94.4657 \tQuestion Loss: 94.4657 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 30\tNet Loss: 96.8311 \tQuestion Loss: 96.8311 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 30\tNet Loss: 87.3913 \tQuestion Loss: 87.3913 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 30\tNet Loss: 91.7802 \tQuestion Loss: 91.7802 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 30\tNet Loss: 89.0127 \tQuestion Loss: 89.0127 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 30\tNet Loss: 95.3834 \tQuestion Loss: 95.3834 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 30\tNet Loss: 88.7297 \tQuestion Loss: 88.7297 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 30\tNet Loss: 93.2532 \tQuestion Loss: 93.2532 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 30\tNet Loss: 91.9815 \tQuestion Loss: 91.9815 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 30\tNet Loss: 90.6929 \tQuestion Loss: 90.6929 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 30\tNet Loss: 92.5206 \tQuestion Loss: 92.5206 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 30\tNet Loss: 92.0773 \tQuestion Loss: 92.0773 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 30\tNet Loss: 90.5736 \tQuestion Loss: 90.5736 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 30\tNet Loss: 95.8881 \tQuestion Loss: 95.8881 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 30\tNet Loss: 93.3439 \tQuestion Loss: 93.3439 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 30\tNet Loss: 92.8321 \tQuestion Loss: 92.8321 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 30\tNet Loss: 95.1757 \tQuestion Loss: 95.1757 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 30\tNet Loss: 93.2300 \tQuestion Loss: 93.2300 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 30\tNet Loss: 94.8642 \tQuestion Loss: 94.8642 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 30\tNet Loss: 93.5202 \tQuestion Loss: 93.5202 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 30\tNet Loss: 86.6441 \tQuestion Loss: 86.6441 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 30\tNet Loss: 91.2278 \tQuestion Loss: 91.2278 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 30\tNet Loss: 97.2115 \tQuestion Loss: 97.2115 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 30\tNet Loss: 92.9505 \tQuestion Loss: 92.9505 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 30\tNet Loss: 94.2919 \tQuestion Loss: 94.2919 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 30\tNet Loss: 91.9750 \tQuestion Loss: 91.9750 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 30\tNet Loss: 89.4957 \tQuestion Loss: 89.4957 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 30\tNet Loss: 88.5035 \tQuestion Loss: 88.5035 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 30\tNet Loss: 91.9207 \tQuestion Loss: 91.9207 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 30\tNet Loss: 90.0193 \tQuestion Loss: 90.0193 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 30\tNet Loss: 91.8040 \tQuestion Loss: 91.8040 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 30\tNet Loss: 91.3323 \tQuestion Loss: 91.3323 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 30\tNet Loss: 94.5051 \tQuestion Loss: 94.5051 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 30\tNet Loss: 90.0781 \tQuestion Loss: 90.0781 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 30\tNet Loss: 95.2065 \tQuestion Loss: 95.2065 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 30\tNet Loss: 92.7310 \tQuestion Loss: 92.7310 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 30\tNet Loss: 91.3498 \tQuestion Loss: 91.3498 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 30\tNet Loss: 92.0278 \tQuestion Loss: 92.0278 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 30\tNet Loss: 92.4726 \tQuestion Loss: 92.4726 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 30\tNet Loss: 93.2715 \tQuestion Loss: 93.2715 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 30\tNet Loss: 91.8874 \tQuestion Loss: 91.8874 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 30\tNet Loss: 95.8525 \tQuestion Loss: 95.8525 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 98 \t Epoch : 30\tNet Loss: 92.7262 \tQuestion Loss: 92.7262 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 30\tNet Loss: 92.7934 \tQuestion Loss: 92.7934 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 30\tNet Loss: 92.4376 \tQuestion Loss: 92.4376 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 30\tNet Loss: 95.5553 \tQuestion Loss: 95.5553 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 30\tNet Loss: 93.1258 \tQuestion Loss: 93.1258 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 30\tNet Loss: 92.4680 \tQuestion Loss: 92.4680 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 30\tNet Loss: 94.8950 \tQuestion Loss: 94.8950 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 30\tNet Loss: 96.5630 \tQuestion Loss: 96.5630 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 30\tNet Loss: 92.3080 \tQuestion Loss: 92.3080 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 30\tNet Loss: 89.2331 \tQuestion Loss: 89.2331 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 30\tNet Loss: 88.3613 \tQuestion Loss: 88.3613 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 30\tNet Loss: 92.8614 \tQuestion Loss: 92.8614 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 30\tNet Loss: 93.1822 \tQuestion Loss: 93.1822 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 30\tNet Loss: 87.6630 \tQuestion Loss: 87.6630 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 30\tNet Loss: 89.7522 \tQuestion Loss: 89.7522 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 30\tNet Loss: 97.2667 \tQuestion Loss: 97.2667 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 30\tNet Loss: 94.1250 \tQuestion Loss: 94.1250 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 30\tNet Loss: 91.0902 \tQuestion Loss: 91.0902 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 30\tNet Loss: 88.0272 \tQuestion Loss: 88.0272 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 30\tNet Loss: 90.4396 \tQuestion Loss: 90.4396 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 30\tNet Loss: 93.8367 \tQuestion Loss: 93.8367 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 30\tNet Loss: 89.7269 \tQuestion Loss: 89.7269 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 30\tNet Loss: 91.4411 \tQuestion Loss: 91.4411 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 30\tNet Loss: 93.5212 \tQuestion Loss: 93.5212 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 30\tNet Loss: 97.6222 \tQuestion Loss: 97.6222 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 30\tNet Loss: 92.9159 \tQuestion Loss: 92.9159 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 30\tNet Loss: 94.7123 \tQuestion Loss: 94.7123 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 30\tNet Loss: 92.4939 \tQuestion Loss: 92.4939 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 30\tNet Loss: 96.1195 \tQuestion Loss: 96.1195 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 30\tNet Loss: 88.0249 \tQuestion Loss: 88.0249 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 30\tNet Loss: 92.2572 \tQuestion Loss: 92.2572 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 30\tNet Loss: 94.7470 \tQuestion Loss: 94.7470 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 30\tNet Loss: 99.9191 \tQuestion Loss: 99.9191 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 30\tNet Loss: 94.8079 \tQuestion Loss: 94.8079 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 30\tNet Loss: 91.2709 \tQuestion Loss: 91.2709 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 30\tNet Loss: 92.6616 \tQuestion Loss: 92.6616 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 30\tNet Loss: 93.5137 \tQuestion Loss: 93.5137 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 30\tNet Loss: 85.8863 \tQuestion Loss: 85.8863 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 30\tNet Loss: 97.2475 \tQuestion Loss: 97.2475 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 30\tNet Loss: 94.9363 \tQuestion Loss: 94.9363 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 30\tNet Loss: 93.3535 \tQuestion Loss: 93.3535 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 30\tNet Loss: 92.1084 \tQuestion Loss: 92.1084 \t Time Taken: 1 seconds\n",
      "Batch: 140 \t Epoch : 30\tNet Loss: 89.1917 \tQuestion Loss: 89.1917 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 30\tNet Loss: 89.6716 \tQuestion Loss: 89.6716 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 30\tNet Loss: 94.5520 \tQuestion Loss: 94.5520 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 30\tNet Loss: 89.2691 \tQuestion Loss: 89.2691 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 30\tNet Loss: 86.8247 \tQuestion Loss: 86.8247 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 30\tNet Loss: 95.5279 \tQuestion Loss: 95.5279 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 30\tNet Loss: 98.6433 \tQuestion Loss: 98.6433 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 30\tNet Loss: 87.0409 \tQuestion Loss: 87.0409 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 30\tNet Loss: 96.4677 \tQuestion Loss: 96.4677 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 30\tNet Loss: 89.9981 \tQuestion Loss: 89.9981 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 30\tNet Loss: 91.2163 \tQuestion Loss: 91.2163 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 30\tNet Loss: 97.9843 \tQuestion Loss: 97.9843 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 30\tNet Loss: 93.6227 \tQuestion Loss: 93.6227 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 30\tNet Loss: 94.0043 \tQuestion Loss: 94.0043 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 30\tNet Loss: 85.0896 \tQuestion Loss: 85.0896 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 30\tNet Loss: 90.5749 \tQuestion Loss: 90.5749 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 30\tNet Loss: 93.4963 \tQuestion Loss: 93.4963 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 30\tNet Loss: 88.4657 \tQuestion Loss: 88.4657 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 30\tNet Loss: 93.7213 \tQuestion Loss: 93.7213 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 30\tNet Loss: 94.5292 \tQuestion Loss: 94.5292 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 30\tNet Loss: 95.8322 \tQuestion Loss: 95.8322 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 30\tNet Loss: 94.1529 \tQuestion Loss: 94.1529 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 30\tNet Loss: 95.8281 \tQuestion Loss: 95.8281 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 30\tNet Loss: 89.5250 \tQuestion Loss: 89.5250 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 30\tNet Loss: 88.4778 \tQuestion Loss: 88.4778 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 30\tNet Loss: 89.7248 \tQuestion Loss: 89.7248 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 30\tNet Loss: 87.5142 \tQuestion Loss: 87.5142 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 30\tNet Loss: 92.7185 \tQuestion Loss: 92.7185 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 30\tNet Loss: 91.4582 \tQuestion Loss: 91.4582 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 30\tNet Loss: 95.7140 \tQuestion Loss: 95.7140 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 30\tNet Loss: 93.5264 \tQuestion Loss: 93.5264 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 30\tNet Loss: 92.4773 \tQuestion Loss: 92.4773 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 30\tNet Loss: 89.2473 \tQuestion Loss: 89.2473 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 30\tNet Loss: 90.5962 \tQuestion Loss: 90.5962 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 30\tNet Loss: 94.6187 \tQuestion Loss: 94.6187 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 30\tNet Loss: 97.4099 \tQuestion Loss: 97.4099 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 30\tNet Loss: 89.9690 \tQuestion Loss: 89.9690 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 30\tNet Loss: 91.4185 \tQuestion Loss: 91.4185 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 30\tNet Loss: 90.5005 \tQuestion Loss: 90.5005 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 30\tNet Loss: 91.6085 \tQuestion Loss: 91.6085 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 30\tNet Loss: 90.1012 \tQuestion Loss: 90.1012 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 30\tNet Loss: 92.4147 \tQuestion Loss: 92.4147 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 30\tNet Loss: 94.3691 \tQuestion Loss: 94.3691 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 30\tNet Loss: 89.3111 \tQuestion Loss: 89.3111 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 30\tNet Loss: 93.2755 \tQuestion Loss: 93.2755 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 30\tNet Loss: 91.5786 \tQuestion Loss: 91.5786 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 30\tNet Loss: 93.8208 \tQuestion Loss: 93.8208 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 30\tNet Loss: 94.8283 \tQuestion Loss: 94.8283 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 30\tNet Loss: 89.8714 \tQuestion Loss: 89.8714 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 30\tNet Loss: 89.6582 \tQuestion Loss: 89.6582 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 190 \t Epoch : 30\tNet Loss: 91.0574 \tQuestion Loss: 91.0574 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 30\tNet Loss: 90.0240 \tQuestion Loss: 90.0240 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 30\tNet Loss: 94.8470 \tQuestion Loss: 94.8470 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 30\tNet Loss: 93.4420 \tQuestion Loss: 93.4420 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 30\tNet Loss: 95.2510 \tQuestion Loss: 95.2510 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 30\tNet Loss: 87.9332 \tQuestion Loss: 87.9332 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 30\tNet Loss: 89.1139 \tQuestion Loss: 89.1139 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 30\tNet Loss: 94.5582 \tQuestion Loss: 94.5582 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 30\tNet Loss: 87.7089 \tQuestion Loss: 87.7089 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 30\tNet Loss: 92.4184 \tQuestion Loss: 92.4184 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 30 : 92.2976 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 31\tNet Loss: 92.6610 \tQuestion Loss: 92.6610 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 31\tNet Loss: 88.9454 \tQuestion Loss: 88.9454 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 31\tNet Loss: 94.1875 \tQuestion Loss: 94.1875 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 31\tNet Loss: 93.6943 \tQuestion Loss: 93.6943 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 31\tNet Loss: 93.7121 \tQuestion Loss: 93.7121 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 31\tNet Loss: 93.5728 \tQuestion Loss: 93.5728 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 31\tNet Loss: 88.8037 \tQuestion Loss: 88.8037 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 31\tNet Loss: 94.8913 \tQuestion Loss: 94.8913 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 31\tNet Loss: 93.4407 \tQuestion Loss: 93.4407 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 31\tNet Loss: 94.7133 \tQuestion Loss: 94.7133 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 31\tNet Loss: 90.9515 \tQuestion Loss: 90.9515 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 31\tNet Loss: 95.6581 \tQuestion Loss: 95.6581 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 31\tNet Loss: 90.3856 \tQuestion Loss: 90.3856 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 31\tNet Loss: 92.7690 \tQuestion Loss: 92.7690 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 31\tNet Loss: 92.9201 \tQuestion Loss: 92.9201 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 31\tNet Loss: 95.4654 \tQuestion Loss: 95.4654 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 31\tNet Loss: 90.5700 \tQuestion Loss: 90.5700 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 31\tNet Loss: 92.4653 \tQuestion Loss: 92.4653 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 31\tNet Loss: 90.4100 \tQuestion Loss: 90.4100 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 31\tNet Loss: 86.6756 \tQuestion Loss: 86.6756 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 31\tNet Loss: 96.0426 \tQuestion Loss: 96.0426 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 31\tNet Loss: 88.1924 \tQuestion Loss: 88.1924 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 31\tNet Loss: 92.9618 \tQuestion Loss: 92.9618 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 31\tNet Loss: 95.8415 \tQuestion Loss: 95.8415 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 31\tNet Loss: 89.8039 \tQuestion Loss: 89.8039 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 31\tNet Loss: 93.2396 \tQuestion Loss: 93.2396 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 31\tNet Loss: 98.4602 \tQuestion Loss: 98.4602 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 31\tNet Loss: 91.9072 \tQuestion Loss: 91.9072 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 31\tNet Loss: 86.9529 \tQuestion Loss: 86.9529 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 31\tNet Loss: 89.8870 \tQuestion Loss: 89.8870 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 31\tNet Loss: 91.6766 \tQuestion Loss: 91.6766 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 31\tNet Loss: 90.4922 \tQuestion Loss: 90.4922 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 31\tNet Loss: 88.7884 \tQuestion Loss: 88.7884 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 31\tNet Loss: 93.2475 \tQuestion Loss: 93.2475 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 31\tNet Loss: 87.3342 \tQuestion Loss: 87.3342 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 31\tNet Loss: 90.1448 \tQuestion Loss: 90.1448 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 31\tNet Loss: 93.8934 \tQuestion Loss: 93.8934 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 31\tNet Loss: 95.3410 \tQuestion Loss: 95.3410 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 31\tNet Loss: 91.2272 \tQuestion Loss: 91.2272 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 31\tNet Loss: 93.8758 \tQuestion Loss: 93.8758 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 31\tNet Loss: 91.5893 \tQuestion Loss: 91.5893 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 31\tNet Loss: 90.2931 \tQuestion Loss: 90.2931 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 31\tNet Loss: 91.9363 \tQuestion Loss: 91.9363 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 31\tNet Loss: 93.3378 \tQuestion Loss: 93.3378 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 31\tNet Loss: 90.6002 \tQuestion Loss: 90.6002 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 31\tNet Loss: 93.7501 \tQuestion Loss: 93.7501 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 31\tNet Loss: 92.3249 \tQuestion Loss: 92.3249 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 31\tNet Loss: 97.3044 \tQuestion Loss: 97.3044 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 31\tNet Loss: 86.0996 \tQuestion Loss: 86.0996 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 31\tNet Loss: 94.0399 \tQuestion Loss: 94.0399 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 31\tNet Loss: 93.0961 \tQuestion Loss: 93.0961 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 31\tNet Loss: 88.5827 \tQuestion Loss: 88.5827 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 31\tNet Loss: 89.5707 \tQuestion Loss: 89.5707 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 31\tNet Loss: 89.8327 \tQuestion Loss: 89.8327 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 31\tNet Loss: 96.6508 \tQuestion Loss: 96.6508 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 31\tNet Loss: 93.3915 \tQuestion Loss: 93.3915 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 31\tNet Loss: 94.4169 \tQuestion Loss: 94.4169 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 31\tNet Loss: 96.5356 \tQuestion Loss: 96.5356 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 31\tNet Loss: 87.7150 \tQuestion Loss: 87.7150 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 31\tNet Loss: 92.3872 \tQuestion Loss: 92.3872 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 31\tNet Loss: 89.1740 \tQuestion Loss: 89.1740 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 31\tNet Loss: 95.1947 \tQuestion Loss: 95.1947 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 31\tNet Loss: 88.9471 \tQuestion Loss: 88.9471 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 31\tNet Loss: 93.6244 \tQuestion Loss: 93.6244 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 31\tNet Loss: 92.2459 \tQuestion Loss: 92.2459 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 31\tNet Loss: 90.5999 \tQuestion Loss: 90.5999 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 31\tNet Loss: 92.1702 \tQuestion Loss: 92.1702 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 31\tNet Loss: 91.3144 \tQuestion Loss: 91.3144 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 31\tNet Loss: 91.1342 \tQuestion Loss: 91.1342 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 31\tNet Loss: 95.6157 \tQuestion Loss: 95.6157 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 31\tNet Loss: 93.1271 \tQuestion Loss: 93.1271 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 31\tNet Loss: 92.7312 \tQuestion Loss: 92.7312 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 31\tNet Loss: 95.8569 \tQuestion Loss: 95.8569 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 31\tNet Loss: 94.1266 \tQuestion Loss: 94.1266 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 31\tNet Loss: 94.9359 \tQuestion Loss: 94.9359 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 31\tNet Loss: 93.5666 \tQuestion Loss: 93.5666 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 31\tNet Loss: 86.9216 \tQuestion Loss: 86.9216 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 31\tNet Loss: 91.7517 \tQuestion Loss: 91.7517 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 31\tNet Loss: 96.0792 \tQuestion Loss: 96.0792 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 31\tNet Loss: 92.9091 \tQuestion Loss: 92.9091 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 31\tNet Loss: 94.2221 \tQuestion Loss: 94.2221 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 31\tNet Loss: 92.2150 \tQuestion Loss: 92.2150 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 82 \t Epoch : 31\tNet Loss: 89.6904 \tQuestion Loss: 89.6904 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 31\tNet Loss: 88.7594 \tQuestion Loss: 88.7594 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 31\tNet Loss: 92.0551 \tQuestion Loss: 92.0551 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 31\tNet Loss: 90.5674 \tQuestion Loss: 90.5674 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 31\tNet Loss: 92.5830 \tQuestion Loss: 92.5830 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 31\tNet Loss: 92.0417 \tQuestion Loss: 92.0417 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 31\tNet Loss: 94.3955 \tQuestion Loss: 94.3955 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 31\tNet Loss: 90.0399 \tQuestion Loss: 90.0399 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 31\tNet Loss: 95.4280 \tQuestion Loss: 95.4280 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 31\tNet Loss: 93.0082 \tQuestion Loss: 93.0082 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 31\tNet Loss: 91.3890 \tQuestion Loss: 91.3890 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 31\tNet Loss: 91.9237 \tQuestion Loss: 91.9237 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 31\tNet Loss: 92.7593 \tQuestion Loss: 92.7593 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 31\tNet Loss: 93.5713 \tQuestion Loss: 93.5713 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 31\tNet Loss: 91.9560 \tQuestion Loss: 91.9560 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 31\tNet Loss: 95.7415 \tQuestion Loss: 95.7415 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 31\tNet Loss: 92.7430 \tQuestion Loss: 92.7430 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 31\tNet Loss: 93.3025 \tQuestion Loss: 93.3025 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 31\tNet Loss: 92.5441 \tQuestion Loss: 92.5441 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 31\tNet Loss: 95.6003 \tQuestion Loss: 95.6003 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 31\tNet Loss: 92.9462 \tQuestion Loss: 92.9462 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 31\tNet Loss: 92.5298 \tQuestion Loss: 92.5298 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 31\tNet Loss: 95.3077 \tQuestion Loss: 95.3077 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 31\tNet Loss: 96.5950 \tQuestion Loss: 96.5950 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 31\tNet Loss: 92.1515 \tQuestion Loss: 92.1515 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 31\tNet Loss: 89.2334 \tQuestion Loss: 89.2334 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 31\tNet Loss: 88.4098 \tQuestion Loss: 88.4098 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 31\tNet Loss: 92.8861 \tQuestion Loss: 92.8861 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 31\tNet Loss: 93.4374 \tQuestion Loss: 93.4374 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 31\tNet Loss: 87.8823 \tQuestion Loss: 87.8823 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 31\tNet Loss: 89.7111 \tQuestion Loss: 89.7111 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 31\tNet Loss: 97.0970 \tQuestion Loss: 97.0970 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 31\tNet Loss: 94.2354 \tQuestion Loss: 94.2354 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 31\tNet Loss: 91.3443 \tQuestion Loss: 91.3443 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 31\tNet Loss: 88.6185 \tQuestion Loss: 88.6185 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 31\tNet Loss: 90.6115 \tQuestion Loss: 90.6115 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 31\tNet Loss: 93.8798 \tQuestion Loss: 93.8798 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 31\tNet Loss: 89.7084 \tQuestion Loss: 89.7084 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 31\tNet Loss: 91.5269 \tQuestion Loss: 91.5269 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 31\tNet Loss: 93.8528 \tQuestion Loss: 93.8528 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 31\tNet Loss: 96.9382 \tQuestion Loss: 96.9382 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 31\tNet Loss: 92.7789 \tQuestion Loss: 92.7789 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 31\tNet Loss: 94.7336 \tQuestion Loss: 94.7336 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 31\tNet Loss: 92.8029 \tQuestion Loss: 92.8029 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 31\tNet Loss: 96.4313 \tQuestion Loss: 96.4313 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 31\tNet Loss: 88.0549 \tQuestion Loss: 88.0549 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 31\tNet Loss: 93.2655 \tQuestion Loss: 93.2655 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 31\tNet Loss: 93.1725 \tQuestion Loss: 93.1725 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 31\tNet Loss: 100.2188 \tQuestion Loss: 100.2188 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 31\tNet Loss: 94.7857 \tQuestion Loss: 94.7857 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 31\tNet Loss: 91.1561 \tQuestion Loss: 91.1561 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 31\tNet Loss: 92.8487 \tQuestion Loss: 92.8487 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 31\tNet Loss: 92.8126 \tQuestion Loss: 92.8126 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 31\tNet Loss: 86.1957 \tQuestion Loss: 86.1957 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 31\tNet Loss: 97.3233 \tQuestion Loss: 97.3233 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 31\tNet Loss: 95.1714 \tQuestion Loss: 95.1714 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 31\tNet Loss: 93.4701 \tQuestion Loss: 93.4701 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 31\tNet Loss: 92.1430 \tQuestion Loss: 92.1430 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 31\tNet Loss: 89.3275 \tQuestion Loss: 89.3275 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 31\tNet Loss: 89.8125 \tQuestion Loss: 89.8125 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 31\tNet Loss: 94.5493 \tQuestion Loss: 94.5493 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 31\tNet Loss: 89.5877 \tQuestion Loss: 89.5877 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 31\tNet Loss: 87.0335 \tQuestion Loss: 87.0335 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 31\tNet Loss: 95.5048 \tQuestion Loss: 95.5048 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 31\tNet Loss: 98.7948 \tQuestion Loss: 98.7948 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 31\tNet Loss: 87.1596 \tQuestion Loss: 87.1596 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 31\tNet Loss: 96.5296 \tQuestion Loss: 96.5296 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 31\tNet Loss: 90.0599 \tQuestion Loss: 90.0599 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 31\tNet Loss: 91.1715 \tQuestion Loss: 91.1715 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 31\tNet Loss: 98.0070 \tQuestion Loss: 98.0070 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 31\tNet Loss: 93.9265 \tQuestion Loss: 93.9265 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 31\tNet Loss: 94.4689 \tQuestion Loss: 94.4689 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 31\tNet Loss: 85.4469 \tQuestion Loss: 85.4469 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 31\tNet Loss: 90.5970 \tQuestion Loss: 90.5970 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 31\tNet Loss: 93.5391 \tQuestion Loss: 93.5391 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 31\tNet Loss: 88.2986 \tQuestion Loss: 88.2986 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 31\tNet Loss: 93.7328 \tQuestion Loss: 93.7328 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 31\tNet Loss: 94.9020 \tQuestion Loss: 94.9020 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 31\tNet Loss: 96.0024 \tQuestion Loss: 96.0024 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 31\tNet Loss: 94.2332 \tQuestion Loss: 94.2332 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 31\tNet Loss: 95.9906 \tQuestion Loss: 95.9906 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 31\tNet Loss: 89.7412 \tQuestion Loss: 89.7412 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 31\tNet Loss: 88.6141 \tQuestion Loss: 88.6141 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 31\tNet Loss: 89.7435 \tQuestion Loss: 89.7435 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 31\tNet Loss: 87.5363 \tQuestion Loss: 87.5363 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 31\tNet Loss: 93.1071 \tQuestion Loss: 93.1071 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 31\tNet Loss: 91.5068 \tQuestion Loss: 91.5068 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 31\tNet Loss: 95.9053 \tQuestion Loss: 95.9053 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 31\tNet Loss: 93.5911 \tQuestion Loss: 93.5911 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 31\tNet Loss: 92.6264 \tQuestion Loss: 92.6264 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 31\tNet Loss: 88.9661 \tQuestion Loss: 88.9661 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 31\tNet Loss: 90.6413 \tQuestion Loss: 90.6413 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 174 \t Epoch : 31\tNet Loss: 95.0455 \tQuestion Loss: 95.0455 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 31\tNet Loss: 97.8565 \tQuestion Loss: 97.8565 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 31\tNet Loss: 90.2167 \tQuestion Loss: 90.2167 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 31\tNet Loss: 91.6901 \tQuestion Loss: 91.6901 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 31\tNet Loss: 90.5204 \tQuestion Loss: 90.5204 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 31\tNet Loss: 92.0615 \tQuestion Loss: 92.0615 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 31\tNet Loss: 90.1331 \tQuestion Loss: 90.1331 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 31\tNet Loss: 92.4457 \tQuestion Loss: 92.4457 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 31\tNet Loss: 94.3123 \tQuestion Loss: 94.3123 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 31\tNet Loss: 89.6061 \tQuestion Loss: 89.6061 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 31\tNet Loss: 93.4593 \tQuestion Loss: 93.4593 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 31\tNet Loss: 91.6201 \tQuestion Loss: 91.6201 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 31\tNet Loss: 94.4614 \tQuestion Loss: 94.4614 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 31\tNet Loss: 95.3722 \tQuestion Loss: 95.3722 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 31\tNet Loss: 90.0090 \tQuestion Loss: 90.0090 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 31\tNet Loss: 89.9677 \tQuestion Loss: 89.9677 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 31\tNet Loss: 91.1859 \tQuestion Loss: 91.1859 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 31\tNet Loss: 90.0912 \tQuestion Loss: 90.0912 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 31\tNet Loss: 95.0020 \tQuestion Loss: 95.0020 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 31\tNet Loss: 93.3752 \tQuestion Loss: 93.3752 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 31\tNet Loss: 95.0058 \tQuestion Loss: 95.0058 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 31\tNet Loss: 87.9499 \tQuestion Loss: 87.9499 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 31\tNet Loss: 89.3259 \tQuestion Loss: 89.3259 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 31\tNet Loss: 94.7465 \tQuestion Loss: 94.7465 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 31\tNet Loss: 87.9504 \tQuestion Loss: 87.9504 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 31\tNet Loss: 92.3712 \tQuestion Loss: 92.3712 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 31 : 92.3546 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 32\tNet Loss: 92.7847 \tQuestion Loss: 92.7847 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 32\tNet Loss: 89.2213 \tQuestion Loss: 89.2213 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 32\tNet Loss: 94.4142 \tQuestion Loss: 94.4142 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 32\tNet Loss: 93.4564 \tQuestion Loss: 93.4564 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 32\tNet Loss: 93.8607 \tQuestion Loss: 93.8607 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 32\tNet Loss: 93.7199 \tQuestion Loss: 93.7199 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 32\tNet Loss: 88.8556 \tQuestion Loss: 88.8556 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 32\tNet Loss: 94.9169 \tQuestion Loss: 94.9169 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 32\tNet Loss: 93.4816 \tQuestion Loss: 93.4816 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 32\tNet Loss: 95.2948 \tQuestion Loss: 95.2948 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 32\tNet Loss: 91.2962 \tQuestion Loss: 91.2962 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 32\tNet Loss: 95.6289 \tQuestion Loss: 95.6289 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 32\tNet Loss: 90.4105 \tQuestion Loss: 90.4105 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 32\tNet Loss: 92.6916 \tQuestion Loss: 92.6916 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 32\tNet Loss: 93.1662 \tQuestion Loss: 93.1662 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 32\tNet Loss: 95.4528 \tQuestion Loss: 95.4528 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 32\tNet Loss: 90.4196 \tQuestion Loss: 90.4196 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 32\tNet Loss: 92.4949 \tQuestion Loss: 92.4949 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 32\tNet Loss: 91.1591 \tQuestion Loss: 91.1591 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 32\tNet Loss: 86.8931 \tQuestion Loss: 86.8931 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 32\tNet Loss: 96.1846 \tQuestion Loss: 96.1846 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 32\tNet Loss: 88.2248 \tQuestion Loss: 88.2248 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 32\tNet Loss: 92.8028 \tQuestion Loss: 92.8028 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 32\tNet Loss: 95.8715 \tQuestion Loss: 95.8715 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 32\tNet Loss: 89.7913 \tQuestion Loss: 89.7913 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 32\tNet Loss: 93.2670 \tQuestion Loss: 93.2670 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 32\tNet Loss: 98.3496 \tQuestion Loss: 98.3496 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 32\tNet Loss: 91.8905 \tQuestion Loss: 91.8905 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 32\tNet Loss: 87.0279 \tQuestion Loss: 87.0279 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 32\tNet Loss: 89.9197 \tQuestion Loss: 89.9197 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 32\tNet Loss: 91.5761 \tQuestion Loss: 91.5761 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 32\tNet Loss: 90.3536 \tQuestion Loss: 90.3536 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 32\tNet Loss: 89.0909 \tQuestion Loss: 89.0909 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 32\tNet Loss: 93.3106 \tQuestion Loss: 93.3106 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 32\tNet Loss: 87.4173 \tQuestion Loss: 87.4173 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 32\tNet Loss: 90.1422 \tQuestion Loss: 90.1422 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 32\tNet Loss: 93.9122 \tQuestion Loss: 93.9122 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 32\tNet Loss: 94.9717 \tQuestion Loss: 94.9717 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 32\tNet Loss: 91.0510 \tQuestion Loss: 91.0510 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 32\tNet Loss: 94.1868 \tQuestion Loss: 94.1868 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 32\tNet Loss: 91.5174 \tQuestion Loss: 91.5174 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 32\tNet Loss: 90.3032 \tQuestion Loss: 90.3032 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 32\tNet Loss: 92.1571 \tQuestion Loss: 92.1571 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 32\tNet Loss: 93.5139 \tQuestion Loss: 93.5139 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 32\tNet Loss: 90.7567 \tQuestion Loss: 90.7567 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 32\tNet Loss: 93.6567 \tQuestion Loss: 93.6567 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 32\tNet Loss: 92.0727 \tQuestion Loss: 92.0727 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 32\tNet Loss: 97.2483 \tQuestion Loss: 97.2483 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 32\tNet Loss: 86.8047 \tQuestion Loss: 86.8047 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 32\tNet Loss: 94.2259 \tQuestion Loss: 94.2259 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 32\tNet Loss: 92.8313 \tQuestion Loss: 92.8313 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 32\tNet Loss: 88.4401 \tQuestion Loss: 88.4401 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 32\tNet Loss: 89.5221 \tQuestion Loss: 89.5221 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 32\tNet Loss: 90.0365 \tQuestion Loss: 90.0365 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 32\tNet Loss: 96.3168 \tQuestion Loss: 96.3168 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 32\tNet Loss: 92.9642 \tQuestion Loss: 92.9642 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 32\tNet Loss: 94.4331 \tQuestion Loss: 94.4331 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 32\tNet Loss: 96.7557 \tQuestion Loss: 96.7557 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 32\tNet Loss: 87.4615 \tQuestion Loss: 87.4615 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 32\tNet Loss: 91.8261 \tQuestion Loss: 91.8261 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 32\tNet Loss: 89.0133 \tQuestion Loss: 89.0133 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 32\tNet Loss: 95.3369 \tQuestion Loss: 95.3369 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 32\tNet Loss: 88.8010 \tQuestion Loss: 88.8010 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 32\tNet Loss: 93.3295 \tQuestion Loss: 93.3295 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 32\tNet Loss: 91.9595 \tQuestion Loss: 91.9595 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 32\tNet Loss: 90.5491 \tQuestion Loss: 90.5491 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 66 \t Epoch : 32\tNet Loss: 92.5165 \tQuestion Loss: 92.5165 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 32\tNet Loss: 92.0789 \tQuestion Loss: 92.0789 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 32\tNet Loss: 90.6135 \tQuestion Loss: 90.6135 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 32\tNet Loss: 95.9056 \tQuestion Loss: 95.9056 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 32\tNet Loss: 93.3170 \tQuestion Loss: 93.3170 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 32\tNet Loss: 92.8639 \tQuestion Loss: 92.8639 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 32\tNet Loss: 95.1730 \tQuestion Loss: 95.1730 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 32\tNet Loss: 93.2274 \tQuestion Loss: 93.2274 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 32\tNet Loss: 94.8255 \tQuestion Loss: 94.8255 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 32\tNet Loss: 93.5608 \tQuestion Loss: 93.5608 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 32\tNet Loss: 86.6531 \tQuestion Loss: 86.6531 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 32\tNet Loss: 91.2632 \tQuestion Loss: 91.2632 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 32\tNet Loss: 97.1942 \tQuestion Loss: 97.1942 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 32\tNet Loss: 92.9744 \tQuestion Loss: 92.9744 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 32\tNet Loss: 94.3146 \tQuestion Loss: 94.3146 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 32\tNet Loss: 91.9966 \tQuestion Loss: 91.9966 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 32\tNet Loss: 89.5464 \tQuestion Loss: 89.5464 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 32\tNet Loss: 88.5587 \tQuestion Loss: 88.5587 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 32\tNet Loss: 91.9511 \tQuestion Loss: 91.9511 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 32\tNet Loss: 90.0656 \tQuestion Loss: 90.0656 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 32\tNet Loss: 91.7402 \tQuestion Loss: 91.7402 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 32\tNet Loss: 91.3840 \tQuestion Loss: 91.3840 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 32\tNet Loss: 94.4935 \tQuestion Loss: 94.4935 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 32\tNet Loss: 90.0878 \tQuestion Loss: 90.0878 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 32\tNet Loss: 95.2577 \tQuestion Loss: 95.2577 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 32\tNet Loss: 92.8188 \tQuestion Loss: 92.8188 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 32\tNet Loss: 91.3942 \tQuestion Loss: 91.3942 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 32\tNet Loss: 92.0301 \tQuestion Loss: 92.0301 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 32\tNet Loss: 92.4609 \tQuestion Loss: 92.4609 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 32\tNet Loss: 93.3112 \tQuestion Loss: 93.3112 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 32\tNet Loss: 91.8420 \tQuestion Loss: 91.8420 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 32\tNet Loss: 95.8252 \tQuestion Loss: 95.8252 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 32\tNet Loss: 92.7120 \tQuestion Loss: 92.7120 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 32\tNet Loss: 92.8454 \tQuestion Loss: 92.8454 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 32\tNet Loss: 92.4864 \tQuestion Loss: 92.4864 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 32\tNet Loss: 95.5004 \tQuestion Loss: 95.5004 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 32\tNet Loss: 93.0791 \tQuestion Loss: 93.0791 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 32\tNet Loss: 92.5234 \tQuestion Loss: 92.5234 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 32\tNet Loss: 94.9321 \tQuestion Loss: 94.9321 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 32\tNet Loss: 96.5330 \tQuestion Loss: 96.5330 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 32\tNet Loss: 92.2908 \tQuestion Loss: 92.2908 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 32\tNet Loss: 89.2769 \tQuestion Loss: 89.2769 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 32\tNet Loss: 88.4027 \tQuestion Loss: 88.4027 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 32\tNet Loss: 92.8996 \tQuestion Loss: 92.8996 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 32\tNet Loss: 93.2053 \tQuestion Loss: 93.2053 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 32\tNet Loss: 87.6937 \tQuestion Loss: 87.6937 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 32\tNet Loss: 89.8002 \tQuestion Loss: 89.8002 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 32\tNet Loss: 97.2824 \tQuestion Loss: 97.2824 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 32\tNet Loss: 94.1136 \tQuestion Loss: 94.1136 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 32\tNet Loss: 91.0718 \tQuestion Loss: 91.0718 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 32\tNet Loss: 87.9645 \tQuestion Loss: 87.9645 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 32\tNet Loss: 90.4838 \tQuestion Loss: 90.4838 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 32\tNet Loss: 93.8577 \tQuestion Loss: 93.8577 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 32\tNet Loss: 89.7403 \tQuestion Loss: 89.7403 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 32\tNet Loss: 91.4594 \tQuestion Loss: 91.4594 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 32\tNet Loss: 93.5487 \tQuestion Loss: 93.5487 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 32\tNet Loss: 97.6448 \tQuestion Loss: 97.6448 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 32\tNet Loss: 92.9537 \tQuestion Loss: 92.9537 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 32\tNet Loss: 94.7144 \tQuestion Loss: 94.7144 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 32\tNet Loss: 92.5359 \tQuestion Loss: 92.5359 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 32\tNet Loss: 96.1315 \tQuestion Loss: 96.1315 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 32\tNet Loss: 88.0448 \tQuestion Loss: 88.0448 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 32\tNet Loss: 92.2790 \tQuestion Loss: 92.2790 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 32\tNet Loss: 94.8010 \tQuestion Loss: 94.8010 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 32\tNet Loss: 99.9070 \tQuestion Loss: 99.9070 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 32\tNet Loss: 94.7935 \tQuestion Loss: 94.7935 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 32\tNet Loss: 91.3112 \tQuestion Loss: 91.3112 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 32\tNet Loss: 92.6813 \tQuestion Loss: 92.6813 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 32\tNet Loss: 93.5595 \tQuestion Loss: 93.5595 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 32\tNet Loss: 85.8775 \tQuestion Loss: 85.8775 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 32\tNet Loss: 97.2705 \tQuestion Loss: 97.2705 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 32\tNet Loss: 94.9569 \tQuestion Loss: 94.9569 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 32\tNet Loss: 93.3056 \tQuestion Loss: 93.3056 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 32\tNet Loss: 92.1227 \tQuestion Loss: 92.1227 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 32\tNet Loss: 89.2305 \tQuestion Loss: 89.2305 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 32\tNet Loss: 89.7272 \tQuestion Loss: 89.7272 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 32\tNet Loss: 94.5303 \tQuestion Loss: 94.5303 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 32\tNet Loss: 89.2305 \tQuestion Loss: 89.2305 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 32\tNet Loss: 86.8624 \tQuestion Loss: 86.8624 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 32\tNet Loss: 95.5319 \tQuestion Loss: 95.5319 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 32\tNet Loss: 98.6688 \tQuestion Loss: 98.6688 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 32\tNet Loss: 87.0088 \tQuestion Loss: 87.0088 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 32\tNet Loss: 96.4626 \tQuestion Loss: 96.4626 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 32\tNet Loss: 90.0442 \tQuestion Loss: 90.0442 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 32\tNet Loss: 91.2226 \tQuestion Loss: 91.2226 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 32\tNet Loss: 97.9895 \tQuestion Loss: 97.9895 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 32\tNet Loss: 93.5950 \tQuestion Loss: 93.5950 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 32\tNet Loss: 94.0351 \tQuestion Loss: 94.0351 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 32\tNet Loss: 85.1001 \tQuestion Loss: 85.1001 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 32\tNet Loss: 90.6018 \tQuestion Loss: 90.6018 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 32\tNet Loss: 93.5000 \tQuestion Loss: 93.5000 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 32\tNet Loss: 88.4431 \tQuestion Loss: 88.4431 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 158 \t Epoch : 32\tNet Loss: 93.7254 \tQuestion Loss: 93.7254 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 32\tNet Loss: 94.6245 \tQuestion Loss: 94.6245 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 32\tNet Loss: 95.8733 \tQuestion Loss: 95.8733 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 32\tNet Loss: 94.1548 \tQuestion Loss: 94.1548 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 32\tNet Loss: 95.8042 \tQuestion Loss: 95.8042 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 32\tNet Loss: 89.6113 \tQuestion Loss: 89.6113 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 32\tNet Loss: 88.5045 \tQuestion Loss: 88.5045 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 32\tNet Loss: 89.7213 \tQuestion Loss: 89.7213 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 32\tNet Loss: 87.4742 \tQuestion Loss: 87.4742 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 32\tNet Loss: 92.7749 \tQuestion Loss: 92.7749 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 32\tNet Loss: 91.5056 \tQuestion Loss: 91.5056 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 32\tNet Loss: 95.7291 \tQuestion Loss: 95.7291 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 32\tNet Loss: 93.5104 \tQuestion Loss: 93.5104 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 32\tNet Loss: 92.5011 \tQuestion Loss: 92.5011 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 32\tNet Loss: 89.3112 \tQuestion Loss: 89.3112 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 32\tNet Loss: 90.6101 \tQuestion Loss: 90.6101 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 32\tNet Loss: 94.6683 \tQuestion Loss: 94.6683 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 32\tNet Loss: 97.5296 \tQuestion Loss: 97.5296 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 32\tNet Loss: 90.0643 \tQuestion Loss: 90.0643 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 32\tNet Loss: 91.5112 \tQuestion Loss: 91.5112 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 32\tNet Loss: 90.5152 \tQuestion Loss: 90.5152 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 32\tNet Loss: 91.6172 \tQuestion Loss: 91.6172 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 32\tNet Loss: 90.1215 \tQuestion Loss: 90.1215 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 32\tNet Loss: 92.4021 \tQuestion Loss: 92.4021 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 32\tNet Loss: 94.3789 \tQuestion Loss: 94.3789 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 32\tNet Loss: 89.3617 \tQuestion Loss: 89.3617 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 32\tNet Loss: 93.3311 \tQuestion Loss: 93.3311 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 32\tNet Loss: 91.6413 \tQuestion Loss: 91.6413 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 32\tNet Loss: 93.8789 \tQuestion Loss: 93.8789 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 32\tNet Loss: 94.9153 \tQuestion Loss: 94.9153 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 32\tNet Loss: 89.9471 \tQuestion Loss: 89.9471 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 32\tNet Loss: 89.6463 \tQuestion Loss: 89.6463 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 32\tNet Loss: 91.0828 \tQuestion Loss: 91.0828 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 32\tNet Loss: 90.0480 \tQuestion Loss: 90.0480 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 32\tNet Loss: 94.8936 \tQuestion Loss: 94.8936 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 32\tNet Loss: 93.4447 \tQuestion Loss: 93.4447 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 32\tNet Loss: 95.2732 \tQuestion Loss: 95.2732 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 32\tNet Loss: 87.9674 \tQuestion Loss: 87.9674 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 32\tNet Loss: 89.2034 \tQuestion Loss: 89.2034 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 32\tNet Loss: 94.6161 \tQuestion Loss: 94.6161 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 32\tNet Loss: 87.7449 \tQuestion Loss: 87.7449 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 32\tNet Loss: 92.4308 \tQuestion Loss: 92.4308 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 32 : 92.3005 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 33\tNet Loss: 92.7274 \tQuestion Loss: 92.7274 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 33\tNet Loss: 89.0100 \tQuestion Loss: 89.0100 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 33\tNet Loss: 94.2608 \tQuestion Loss: 94.2608 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 33\tNet Loss: 93.6662 \tQuestion Loss: 93.6662 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 33\tNet Loss: 93.7767 \tQuestion Loss: 93.7767 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 33\tNet Loss: 93.6586 \tQuestion Loss: 93.6586 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 33\tNet Loss: 88.8205 \tQuestion Loss: 88.8205 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 33\tNet Loss: 94.9180 \tQuestion Loss: 94.9180 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 33\tNet Loss: 93.4997 \tQuestion Loss: 93.4997 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 33\tNet Loss: 94.7238 \tQuestion Loss: 94.7238 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 33\tNet Loss: 90.9772 \tQuestion Loss: 90.9772 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 33\tNet Loss: 95.6452 \tQuestion Loss: 95.6452 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 33\tNet Loss: 90.4059 \tQuestion Loss: 90.4059 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 33\tNet Loss: 92.7938 \tQuestion Loss: 92.7938 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 33\tNet Loss: 92.9249 \tQuestion Loss: 92.9249 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 33\tNet Loss: 95.5042 \tQuestion Loss: 95.5042 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 33\tNet Loss: 90.5826 \tQuestion Loss: 90.5826 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 33\tNet Loss: 92.4916 \tQuestion Loss: 92.4916 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 33\tNet Loss: 90.4459 \tQuestion Loss: 90.4459 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 33\tNet Loss: 86.6911 \tQuestion Loss: 86.6911 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 33\tNet Loss: 96.0648 \tQuestion Loss: 96.0648 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 33\tNet Loss: 88.2158 \tQuestion Loss: 88.2158 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 33\tNet Loss: 92.9646 \tQuestion Loss: 92.9646 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 33\tNet Loss: 95.8845 \tQuestion Loss: 95.8845 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 33\tNet Loss: 89.8321 \tQuestion Loss: 89.8321 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 33\tNet Loss: 93.2702 \tQuestion Loss: 93.2702 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 33\tNet Loss: 98.4754 \tQuestion Loss: 98.4754 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 33\tNet Loss: 91.9303 \tQuestion Loss: 91.9303 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 33\tNet Loss: 86.9344 \tQuestion Loss: 86.9344 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 33\tNet Loss: 89.9051 \tQuestion Loss: 89.9051 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 33\tNet Loss: 91.6785 \tQuestion Loss: 91.6785 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 33\tNet Loss: 90.5022 \tQuestion Loss: 90.5022 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 33\tNet Loss: 88.7830 \tQuestion Loss: 88.7830 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 33\tNet Loss: 93.2646 \tQuestion Loss: 93.2646 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 33\tNet Loss: 87.3360 \tQuestion Loss: 87.3360 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 33\tNet Loss: 90.1706 \tQuestion Loss: 90.1706 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 33\tNet Loss: 93.8810 \tQuestion Loss: 93.8810 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 33\tNet Loss: 95.4053 \tQuestion Loss: 95.4053 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 33\tNet Loss: 91.2743 \tQuestion Loss: 91.2743 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 33\tNet Loss: 93.9220 \tQuestion Loss: 93.9220 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 33\tNet Loss: 91.5835 \tQuestion Loss: 91.5835 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 33\tNet Loss: 90.2213 \tQuestion Loss: 90.2213 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 33\tNet Loss: 92.0137 \tQuestion Loss: 92.0137 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 33\tNet Loss: 93.3671 \tQuestion Loss: 93.3671 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 33\tNet Loss: 90.6210 \tQuestion Loss: 90.6210 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 33\tNet Loss: 93.6801 \tQuestion Loss: 93.6801 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 33\tNet Loss: 92.3213 \tQuestion Loss: 92.3213 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 33\tNet Loss: 97.3420 \tQuestion Loss: 97.3420 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 33\tNet Loss: 86.1311 \tQuestion Loss: 86.1311 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 49 \t Epoch : 33\tNet Loss: 94.1281 \tQuestion Loss: 94.1281 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 33\tNet Loss: 93.0224 \tQuestion Loss: 93.0224 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 33\tNet Loss: 88.5571 \tQuestion Loss: 88.5571 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 33\tNet Loss: 89.5667 \tQuestion Loss: 89.5667 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 33\tNet Loss: 89.8029 \tQuestion Loss: 89.8029 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 33\tNet Loss: 96.6173 \tQuestion Loss: 96.6173 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 33\tNet Loss: 93.3468 \tQuestion Loss: 93.3468 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 33\tNet Loss: 94.4186 \tQuestion Loss: 94.4186 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 33\tNet Loss: 96.6366 \tQuestion Loss: 96.6366 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 33\tNet Loss: 87.6818 \tQuestion Loss: 87.6818 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 33\tNet Loss: 92.3907 \tQuestion Loss: 92.3907 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 33\tNet Loss: 89.1557 \tQuestion Loss: 89.1557 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 33\tNet Loss: 95.2690 \tQuestion Loss: 95.2690 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 33\tNet Loss: 88.8973 \tQuestion Loss: 88.8973 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 33\tNet Loss: 93.5722 \tQuestion Loss: 93.5722 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 33\tNet Loss: 92.2522 \tQuestion Loss: 92.2522 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 33\tNet Loss: 90.7375 \tQuestion Loss: 90.7375 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 33\tNet Loss: 92.1439 \tQuestion Loss: 92.1439 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 33\tNet Loss: 91.3136 \tQuestion Loss: 91.3136 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 33\tNet Loss: 91.0742 \tQuestion Loss: 91.0742 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 33\tNet Loss: 95.6339 \tQuestion Loss: 95.6339 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 33\tNet Loss: 93.1636 \tQuestion Loss: 93.1636 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 33\tNet Loss: 92.6628 \tQuestion Loss: 92.6628 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 33\tNet Loss: 95.8530 \tQuestion Loss: 95.8530 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 33\tNet Loss: 94.1415 \tQuestion Loss: 94.1415 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 33\tNet Loss: 94.9551 \tQuestion Loss: 94.9551 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 33\tNet Loss: 93.5100 \tQuestion Loss: 93.5100 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 33\tNet Loss: 86.8579 \tQuestion Loss: 86.8579 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 33\tNet Loss: 91.7678 \tQuestion Loss: 91.7678 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 33\tNet Loss: 96.0955 \tQuestion Loss: 96.0955 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 33\tNet Loss: 92.9140 \tQuestion Loss: 92.9140 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 33\tNet Loss: 94.1890 \tQuestion Loss: 94.1890 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 33\tNet Loss: 92.1740 \tQuestion Loss: 92.1740 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 33\tNet Loss: 89.6742 \tQuestion Loss: 89.6742 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 33\tNet Loss: 88.7537 \tQuestion Loss: 88.7537 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 33\tNet Loss: 92.0114 \tQuestion Loss: 92.0114 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 33\tNet Loss: 90.5145 \tQuestion Loss: 90.5145 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 33\tNet Loss: 92.6030 \tQuestion Loss: 92.6030 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 33\tNet Loss: 92.0119 \tQuestion Loss: 92.0119 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 33\tNet Loss: 94.4235 \tQuestion Loss: 94.4235 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 33\tNet Loss: 90.0170 \tQuestion Loss: 90.0170 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 33\tNet Loss: 95.3817 \tQuestion Loss: 95.3817 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 33\tNet Loss: 93.0028 \tQuestion Loss: 93.0028 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 33\tNet Loss: 91.3951 \tQuestion Loss: 91.3951 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 33\tNet Loss: 91.8892 \tQuestion Loss: 91.8892 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 33\tNet Loss: 92.7153 \tQuestion Loss: 92.7153 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 33\tNet Loss: 93.5666 \tQuestion Loss: 93.5666 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 33\tNet Loss: 91.9794 \tQuestion Loss: 91.9794 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 33\tNet Loss: 95.7323 \tQuestion Loss: 95.7323 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 33\tNet Loss: 92.7112 \tQuestion Loss: 92.7112 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 33\tNet Loss: 93.2548 \tQuestion Loss: 93.2548 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 33\tNet Loss: 92.5103 \tQuestion Loss: 92.5103 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 33\tNet Loss: 95.6116 \tQuestion Loss: 95.6116 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 33\tNet Loss: 92.9759 \tQuestion Loss: 92.9759 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 33\tNet Loss: 92.5383 \tQuestion Loss: 92.5383 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 33\tNet Loss: 95.2921 \tQuestion Loss: 95.2921 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 33\tNet Loss: 96.6216 \tQuestion Loss: 96.6216 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 33\tNet Loss: 92.1805 \tQuestion Loss: 92.1805 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 33\tNet Loss: 89.2142 \tQuestion Loss: 89.2142 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 33\tNet Loss: 88.3802 \tQuestion Loss: 88.3802 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 33\tNet Loss: 92.8836 \tQuestion Loss: 92.8836 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 33\tNet Loss: 93.4501 \tQuestion Loss: 93.4501 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 33\tNet Loss: 87.8755 \tQuestion Loss: 87.8755 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 33\tNet Loss: 89.6905 \tQuestion Loss: 89.6905 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 33\tNet Loss: 97.0724 \tQuestion Loss: 97.0724 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 33\tNet Loss: 94.2621 \tQuestion Loss: 94.2621 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 33\tNet Loss: 91.3613 \tQuestion Loss: 91.3613 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 33\tNet Loss: 88.6339 \tQuestion Loss: 88.6339 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 33\tNet Loss: 90.5696 \tQuestion Loss: 90.5696 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 33\tNet Loss: 93.8923 \tQuestion Loss: 93.8923 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 33\tNet Loss: 89.7041 \tQuestion Loss: 89.7041 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 33\tNet Loss: 91.4990 \tQuestion Loss: 91.4990 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 33\tNet Loss: 93.8357 \tQuestion Loss: 93.8357 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 33\tNet Loss: 96.9315 \tQuestion Loss: 96.9315 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 33\tNet Loss: 92.7761 \tQuestion Loss: 92.7761 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 33\tNet Loss: 94.6759 \tQuestion Loss: 94.6759 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 33\tNet Loss: 92.7972 \tQuestion Loss: 92.7972 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 33\tNet Loss: 96.3986 \tQuestion Loss: 96.3986 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 33\tNet Loss: 88.0953 \tQuestion Loss: 88.0953 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 33\tNet Loss: 93.2608 \tQuestion Loss: 93.2608 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 33\tNet Loss: 93.0959 \tQuestion Loss: 93.0959 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 33\tNet Loss: 100.1769 \tQuestion Loss: 100.1769 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 33\tNet Loss: 94.8333 \tQuestion Loss: 94.8333 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 33\tNet Loss: 91.1539 \tQuestion Loss: 91.1539 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 33\tNet Loss: 92.7919 \tQuestion Loss: 92.7919 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 33\tNet Loss: 92.7582 \tQuestion Loss: 92.7582 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 33\tNet Loss: 86.2331 \tQuestion Loss: 86.2331 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 33\tNet Loss: 97.2839 \tQuestion Loss: 97.2839 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 33\tNet Loss: 95.1233 \tQuestion Loss: 95.1233 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 33\tNet Loss: 93.4420 \tQuestion Loss: 93.4420 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 33\tNet Loss: 92.1558 \tQuestion Loss: 92.1558 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 33\tNet Loss: 89.3254 \tQuestion Loss: 89.3254 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 141 \t Epoch : 33\tNet Loss: 89.8037 \tQuestion Loss: 89.8037 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 33\tNet Loss: 94.5069 \tQuestion Loss: 94.5069 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 33\tNet Loss: 89.5683 \tQuestion Loss: 89.5683 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 33\tNet Loss: 87.0109 \tQuestion Loss: 87.0109 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 33\tNet Loss: 95.5128 \tQuestion Loss: 95.5128 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 33\tNet Loss: 98.7889 \tQuestion Loss: 98.7889 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 33\tNet Loss: 87.1267 \tQuestion Loss: 87.1267 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 33\tNet Loss: 96.4993 \tQuestion Loss: 96.4993 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 33\tNet Loss: 90.0352 \tQuestion Loss: 90.0352 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 33\tNet Loss: 91.1880 \tQuestion Loss: 91.1880 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 33\tNet Loss: 98.0084 \tQuestion Loss: 98.0084 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 33\tNet Loss: 93.8899 \tQuestion Loss: 93.8899 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 33\tNet Loss: 94.4583 \tQuestion Loss: 94.4583 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 33\tNet Loss: 85.4880 \tQuestion Loss: 85.4880 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 33\tNet Loss: 90.5989 \tQuestion Loss: 90.5989 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 33\tNet Loss: 93.5196 \tQuestion Loss: 93.5196 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 33\tNet Loss: 88.3042 \tQuestion Loss: 88.3042 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 33\tNet Loss: 93.7270 \tQuestion Loss: 93.7270 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 33\tNet Loss: 94.9012 \tQuestion Loss: 94.9012 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 33\tNet Loss: 95.9568 \tQuestion Loss: 95.9568 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 33\tNet Loss: 94.2258 \tQuestion Loss: 94.2258 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 33\tNet Loss: 95.9843 \tQuestion Loss: 95.9843 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 33\tNet Loss: 89.7465 \tQuestion Loss: 89.7465 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 33\tNet Loss: 88.5872 \tQuestion Loss: 88.5872 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 33\tNet Loss: 89.6980 \tQuestion Loss: 89.6980 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 33\tNet Loss: 87.5203 \tQuestion Loss: 87.5203 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 33\tNet Loss: 93.1053 \tQuestion Loss: 93.1053 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 33\tNet Loss: 91.5083 \tQuestion Loss: 91.5083 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 33\tNet Loss: 95.8474 \tQuestion Loss: 95.8474 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 33\tNet Loss: 93.5870 \tQuestion Loss: 93.5870 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 33\tNet Loss: 92.6685 \tQuestion Loss: 92.6685 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 33\tNet Loss: 88.9612 \tQuestion Loss: 88.9612 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 33\tNet Loss: 90.6015 \tQuestion Loss: 90.6015 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 33\tNet Loss: 95.0308 \tQuestion Loss: 95.0308 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 33\tNet Loss: 97.8451 \tQuestion Loss: 97.8451 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 33\tNet Loss: 90.1740 \tQuestion Loss: 90.1740 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 33\tNet Loss: 91.6513 \tQuestion Loss: 91.6513 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 33\tNet Loss: 90.5199 \tQuestion Loss: 90.5199 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 33\tNet Loss: 92.1070 \tQuestion Loss: 92.1070 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 33\tNet Loss: 90.1210 \tQuestion Loss: 90.1210 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 33\tNet Loss: 92.4348 \tQuestion Loss: 92.4348 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 33\tNet Loss: 94.3169 \tQuestion Loss: 94.3169 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 33\tNet Loss: 89.6555 \tQuestion Loss: 89.6555 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 33\tNet Loss: 93.4420 \tQuestion Loss: 93.4420 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 33\tNet Loss: 91.5760 \tQuestion Loss: 91.5760 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 33\tNet Loss: 94.4351 \tQuestion Loss: 94.4351 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 33\tNet Loss: 95.3464 \tQuestion Loss: 95.3464 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 33\tNet Loss: 89.9948 \tQuestion Loss: 89.9948 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 33\tNet Loss: 89.9924 \tQuestion Loss: 89.9924 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 33\tNet Loss: 91.1872 \tQuestion Loss: 91.1872 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 33\tNet Loss: 90.0863 \tQuestion Loss: 90.0863 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 33\tNet Loss: 94.9966 \tQuestion Loss: 94.9966 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 33\tNet Loss: 93.3654 \tQuestion Loss: 93.3654 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 33\tNet Loss: 95.0072 \tQuestion Loss: 95.0072 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 33\tNet Loss: 87.9527 \tQuestion Loss: 87.9527 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 33\tNet Loss: 89.2843 \tQuestion Loss: 89.2843 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 33\tNet Loss: 94.7112 \tQuestion Loss: 94.7112 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 33\tNet Loss: 87.9515 \tQuestion Loss: 87.9515 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 33\tNet Loss: 92.3569 \tQuestion Loss: 92.3569 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 33 : 92.3524 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 34\tNet Loss: 92.7540 \tQuestion Loss: 92.7540 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 34\tNet Loss: 89.1944 \tQuestion Loss: 89.1944 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 34\tNet Loss: 94.3699 \tQuestion Loss: 94.3699 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 34\tNet Loss: 93.4389 \tQuestion Loss: 93.4389 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 34\tNet Loss: 93.8408 \tQuestion Loss: 93.8408 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 34\tNet Loss: 93.6995 \tQuestion Loss: 93.6995 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 34\tNet Loss: 88.8557 \tQuestion Loss: 88.8557 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 34\tNet Loss: 94.8899 \tQuestion Loss: 94.8899 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 34\tNet Loss: 93.4653 \tQuestion Loss: 93.4653 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 34\tNet Loss: 95.2928 \tQuestion Loss: 95.2928 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 34\tNet Loss: 91.2976 \tQuestion Loss: 91.2976 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 34\tNet Loss: 95.6575 \tQuestion Loss: 95.6575 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 34\tNet Loss: 90.4270 \tQuestion Loss: 90.4270 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 34\tNet Loss: 92.6828 \tQuestion Loss: 92.6828 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 34\tNet Loss: 93.1417 \tQuestion Loss: 93.1417 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 34\tNet Loss: 95.4459 \tQuestion Loss: 95.4459 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 34\tNet Loss: 90.4334 \tQuestion Loss: 90.4334 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 34\tNet Loss: 92.4679 \tQuestion Loss: 92.4679 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 34\tNet Loss: 91.1326 \tQuestion Loss: 91.1326 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 34\tNet Loss: 86.8961 \tQuestion Loss: 86.8961 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 34\tNet Loss: 96.1744 \tQuestion Loss: 96.1744 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 34\tNet Loss: 88.2247 \tQuestion Loss: 88.2247 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 34\tNet Loss: 92.7611 \tQuestion Loss: 92.7611 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 34\tNet Loss: 95.8587 \tQuestion Loss: 95.8587 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 34\tNet Loss: 89.8052 \tQuestion Loss: 89.8052 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 34\tNet Loss: 93.2473 \tQuestion Loss: 93.2473 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 34\tNet Loss: 98.3394 \tQuestion Loss: 98.3394 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 34\tNet Loss: 91.8656 \tQuestion Loss: 91.8656 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 34\tNet Loss: 87.0543 \tQuestion Loss: 87.0543 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 34\tNet Loss: 89.9182 \tQuestion Loss: 89.9182 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 34\tNet Loss: 91.5493 \tQuestion Loss: 91.5493 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 34\tNet Loss: 90.3069 \tQuestion Loss: 90.3069 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 32 \t Epoch : 34\tNet Loss: 89.0687 \tQuestion Loss: 89.0687 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 34\tNet Loss: 93.3026 \tQuestion Loss: 93.3026 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 34\tNet Loss: 87.4679 \tQuestion Loss: 87.4679 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 34\tNet Loss: 90.1383 \tQuestion Loss: 90.1383 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 34\tNet Loss: 93.8809 \tQuestion Loss: 93.8809 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 34\tNet Loss: 94.9666 \tQuestion Loss: 94.9666 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 34\tNet Loss: 91.0441 \tQuestion Loss: 91.0441 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 34\tNet Loss: 94.1625 \tQuestion Loss: 94.1625 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 34\tNet Loss: 91.5002 \tQuestion Loss: 91.5002 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 34\tNet Loss: 90.3368 \tQuestion Loss: 90.3368 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 34\tNet Loss: 92.1884 \tQuestion Loss: 92.1884 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 34\tNet Loss: 93.5281 \tQuestion Loss: 93.5281 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 34\tNet Loss: 90.7433 \tQuestion Loss: 90.7433 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 34\tNet Loss: 93.6472 \tQuestion Loss: 93.6472 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 34\tNet Loss: 92.0825 \tQuestion Loss: 92.0825 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 34\tNet Loss: 97.2544 \tQuestion Loss: 97.2544 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 34\tNet Loss: 86.8325 \tQuestion Loss: 86.8325 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 34\tNet Loss: 94.1981 \tQuestion Loss: 94.1981 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 34\tNet Loss: 92.8930 \tQuestion Loss: 92.8930 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 34\tNet Loss: 88.5086 \tQuestion Loss: 88.5086 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 34\tNet Loss: 89.5452 \tQuestion Loss: 89.5452 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 34\tNet Loss: 90.0448 \tQuestion Loss: 90.0448 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 34\tNet Loss: 96.3331 \tQuestion Loss: 96.3331 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 34\tNet Loss: 93.0111 \tQuestion Loss: 93.0111 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 34\tNet Loss: 94.4392 \tQuestion Loss: 94.4392 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 34\tNet Loss: 96.7052 \tQuestion Loss: 96.7052 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 34\tNet Loss: 87.4446 \tQuestion Loss: 87.4446 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 34\tNet Loss: 91.9038 \tQuestion Loss: 91.9038 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 34\tNet Loss: 89.0637 \tQuestion Loss: 89.0637 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 34\tNet Loss: 95.2937 \tQuestion Loss: 95.2937 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 34\tNet Loss: 88.7726 \tQuestion Loss: 88.7726 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 34\tNet Loss: 93.4175 \tQuestion Loss: 93.4175 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 34\tNet Loss: 92.0246 \tQuestion Loss: 92.0246 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 34\tNet Loss: 90.5664 \tQuestion Loss: 90.5664 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 34\tNet Loss: 92.5201 \tQuestion Loss: 92.5201 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 34\tNet Loss: 92.1192 \tQuestion Loss: 92.1192 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 34\tNet Loss: 90.6249 \tQuestion Loss: 90.6249 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 34\tNet Loss: 95.9257 \tQuestion Loss: 95.9257 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 34\tNet Loss: 93.3262 \tQuestion Loss: 93.3262 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 34\tNet Loss: 92.8709 \tQuestion Loss: 92.8709 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 34\tNet Loss: 95.2435 \tQuestion Loss: 95.2435 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 34\tNet Loss: 93.2533 \tQuestion Loss: 93.2533 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 34\tNet Loss: 94.7957 \tQuestion Loss: 94.7957 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 34\tNet Loss: 93.5169 \tQuestion Loss: 93.5169 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 34\tNet Loss: 86.7041 \tQuestion Loss: 86.7041 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 34\tNet Loss: 91.3173 \tQuestion Loss: 91.3173 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 34\tNet Loss: 97.2164 \tQuestion Loss: 97.2164 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 34\tNet Loss: 92.9334 \tQuestion Loss: 92.9334 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 34\tNet Loss: 94.3279 \tQuestion Loss: 94.3279 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 34\tNet Loss: 92.0440 \tQuestion Loss: 92.0440 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 34\tNet Loss: 89.5931 \tQuestion Loss: 89.5931 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 34\tNet Loss: 88.5532 \tQuestion Loss: 88.5532 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 34\tNet Loss: 91.9818 \tQuestion Loss: 91.9818 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 34\tNet Loss: 90.1054 \tQuestion Loss: 90.1054 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 34\tNet Loss: 91.7353 \tQuestion Loss: 91.7353 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 34\tNet Loss: 91.3856 \tQuestion Loss: 91.3856 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 34\tNet Loss: 94.5120 \tQuestion Loss: 94.5120 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 34\tNet Loss: 90.1213 \tQuestion Loss: 90.1213 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 34\tNet Loss: 95.2975 \tQuestion Loss: 95.2975 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 34\tNet Loss: 92.8160 \tQuestion Loss: 92.8160 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 34\tNet Loss: 91.3816 \tQuestion Loss: 91.3816 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 34\tNet Loss: 92.0484 \tQuestion Loss: 92.0484 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 34\tNet Loss: 92.5101 \tQuestion Loss: 92.5101 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 34\tNet Loss: 93.3584 \tQuestion Loss: 93.3584 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 34\tNet Loss: 91.8274 \tQuestion Loss: 91.8274 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 34\tNet Loss: 95.8122 \tQuestion Loss: 95.8122 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 34\tNet Loss: 92.7652 \tQuestion Loss: 92.7652 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 34\tNet Loss: 92.8944 \tQuestion Loss: 92.8944 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 34\tNet Loss: 92.4884 \tQuestion Loss: 92.4884 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 34\tNet Loss: 95.5133 \tQuestion Loss: 95.5133 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 34\tNet Loss: 93.1210 \tQuestion Loss: 93.1210 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 34\tNet Loss: 92.5018 \tQuestion Loss: 92.5018 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 34\tNet Loss: 94.9752 \tQuestion Loss: 94.9752 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 34\tNet Loss: 96.5322 \tQuestion Loss: 96.5322 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 34\tNet Loss: 92.3210 \tQuestion Loss: 92.3210 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 34\tNet Loss: 89.3072 \tQuestion Loss: 89.3072 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 34\tNet Loss: 88.4126 \tQuestion Loss: 88.4126 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 34\tNet Loss: 92.9464 \tQuestion Loss: 92.9464 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 34\tNet Loss: 93.2178 \tQuestion Loss: 93.2178 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 34\tNet Loss: 87.7149 \tQuestion Loss: 87.7149 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 34\tNet Loss: 89.7953 \tQuestion Loss: 89.7953 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 34\tNet Loss: 97.3542 \tQuestion Loss: 97.3542 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 34\tNet Loss: 94.1630 \tQuestion Loss: 94.1630 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 34\tNet Loss: 91.0454 \tQuestion Loss: 91.0454 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 34\tNet Loss: 87.9557 \tQuestion Loss: 87.9557 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 34\tNet Loss: 90.5725 \tQuestion Loss: 90.5725 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 34\tNet Loss: 93.9003 \tQuestion Loss: 93.9003 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 34\tNet Loss: 89.7470 \tQuestion Loss: 89.7470 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 34\tNet Loss: 91.4623 \tQuestion Loss: 91.4623 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 34\tNet Loss: 93.6074 \tQuestion Loss: 93.6074 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 34\tNet Loss: 97.7005 \tQuestion Loss: 97.7005 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 34\tNet Loss: 92.9694 \tQuestion Loss: 92.9694 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 124 \t Epoch : 34\tNet Loss: 94.6716 \tQuestion Loss: 94.6716 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 34\tNet Loss: 92.5667 \tQuestion Loss: 92.5667 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 34\tNet Loss: 96.1891 \tQuestion Loss: 96.1891 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 34\tNet Loss: 88.0394 \tQuestion Loss: 88.0394 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 34\tNet Loss: 92.2174 \tQuestion Loss: 92.2174 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 34\tNet Loss: 94.8501 \tQuestion Loss: 94.8501 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 34\tNet Loss: 99.9443 \tQuestion Loss: 99.9443 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 34\tNet Loss: 94.8322 \tQuestion Loss: 94.8322 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 34\tNet Loss: 91.2808 \tQuestion Loss: 91.2808 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 34\tNet Loss: 92.6836 \tQuestion Loss: 92.6836 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 34\tNet Loss: 93.6236 \tQuestion Loss: 93.6236 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 34\tNet Loss: 85.9271 \tQuestion Loss: 85.9271 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 34\tNet Loss: 97.2725 \tQuestion Loss: 97.2725 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 34\tNet Loss: 94.9469 \tQuestion Loss: 94.9469 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 34\tNet Loss: 93.3175 \tQuestion Loss: 93.3175 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 34\tNet Loss: 92.1105 \tQuestion Loss: 92.1105 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 34\tNet Loss: 89.2606 \tQuestion Loss: 89.2606 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 34\tNet Loss: 89.7151 \tQuestion Loss: 89.7151 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 34\tNet Loss: 94.5351 \tQuestion Loss: 94.5351 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 34\tNet Loss: 89.2485 \tQuestion Loss: 89.2485 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 34\tNet Loss: 86.8804 \tQuestion Loss: 86.8804 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 34\tNet Loss: 95.5288 \tQuestion Loss: 95.5288 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 34\tNet Loss: 98.6989 \tQuestion Loss: 98.6989 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 34\tNet Loss: 87.0204 \tQuestion Loss: 87.0204 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 34\tNet Loss: 96.5004 \tQuestion Loss: 96.5004 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 34\tNet Loss: 90.0660 \tQuestion Loss: 90.0660 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 34\tNet Loss: 91.2768 \tQuestion Loss: 91.2768 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 34\tNet Loss: 97.9885 \tQuestion Loss: 97.9885 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 34\tNet Loss: 93.6079 \tQuestion Loss: 93.6079 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 34\tNet Loss: 94.0657 \tQuestion Loss: 94.0657 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 34\tNet Loss: 85.1344 \tQuestion Loss: 85.1344 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 34\tNet Loss: 90.6062 \tQuestion Loss: 90.6062 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 34\tNet Loss: 93.5018 \tQuestion Loss: 93.5018 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 34\tNet Loss: 88.4823 \tQuestion Loss: 88.4823 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 34\tNet Loss: 93.7937 \tQuestion Loss: 93.7937 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 34\tNet Loss: 94.6006 \tQuestion Loss: 94.6006 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 34\tNet Loss: 95.8569 \tQuestion Loss: 95.8569 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 34\tNet Loss: 94.1734 \tQuestion Loss: 94.1734 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 34\tNet Loss: 95.8706 \tQuestion Loss: 95.8706 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 34\tNet Loss: 89.6534 \tQuestion Loss: 89.6534 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 34\tNet Loss: 88.5031 \tQuestion Loss: 88.5031 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 34\tNet Loss: 89.7325 \tQuestion Loss: 89.7325 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 34\tNet Loss: 87.5559 \tQuestion Loss: 87.5559 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 34\tNet Loss: 92.7605 \tQuestion Loss: 92.7605 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 34\tNet Loss: 91.4911 \tQuestion Loss: 91.4911 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 34\tNet Loss: 95.7260 \tQuestion Loss: 95.7260 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 34\tNet Loss: 93.5510 \tQuestion Loss: 93.5510 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 34\tNet Loss: 92.5215 \tQuestion Loss: 92.5215 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 34\tNet Loss: 89.2804 \tQuestion Loss: 89.2804 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 34\tNet Loss: 90.6285 \tQuestion Loss: 90.6285 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 34\tNet Loss: 94.7252 \tQuestion Loss: 94.7252 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 34\tNet Loss: 97.6067 \tQuestion Loss: 97.6067 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 34\tNet Loss: 90.0661 \tQuestion Loss: 90.0661 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 34\tNet Loss: 91.5316 \tQuestion Loss: 91.5316 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 34\tNet Loss: 90.5489 \tQuestion Loss: 90.5489 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 34\tNet Loss: 91.6065 \tQuestion Loss: 91.6065 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 34\tNet Loss: 90.0945 \tQuestion Loss: 90.0945 \t Time Taken: 1 seconds\n",
      "Batch: 181 \t Epoch : 34\tNet Loss: 92.4051 \tQuestion Loss: 92.4051 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 34\tNet Loss: 94.3842 \tQuestion Loss: 94.3842 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 34\tNet Loss: 89.3908 \tQuestion Loss: 89.3908 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 34\tNet Loss: 93.3268 \tQuestion Loss: 93.3268 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 34\tNet Loss: 91.6667 \tQuestion Loss: 91.6667 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 34\tNet Loss: 93.9180 \tQuestion Loss: 93.9180 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 34\tNet Loss: 94.9782 \tQuestion Loss: 94.9782 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 34\tNet Loss: 89.9772 \tQuestion Loss: 89.9772 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 34\tNet Loss: 89.6086 \tQuestion Loss: 89.6086 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 34\tNet Loss: 91.1117 \tQuestion Loss: 91.1117 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 34\tNet Loss: 90.0682 \tQuestion Loss: 90.0682 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 34\tNet Loss: 94.9255 \tQuestion Loss: 94.9255 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 34\tNet Loss: 93.4189 \tQuestion Loss: 93.4189 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 34\tNet Loss: 95.2800 \tQuestion Loss: 95.2800 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 34\tNet Loss: 88.0104 \tQuestion Loss: 88.0104 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 34\tNet Loss: 89.2364 \tQuestion Loss: 89.2364 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 34\tNet Loss: 94.6044 \tQuestion Loss: 94.6044 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 34\tNet Loss: 87.7424 \tQuestion Loss: 87.7424 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 34\tNet Loss: 92.4474 \tQuestion Loss: 92.4474 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 34 : 92.3127 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 35\tNet Loss: 92.7343 \tQuestion Loss: 92.7343 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 35\tNet Loss: 89.0058 \tQuestion Loss: 89.0058 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 35\tNet Loss: 94.2792 \tQuestion Loss: 94.2792 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 35\tNet Loss: 93.7085 \tQuestion Loss: 93.7085 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 35\tNet Loss: 93.8021 \tQuestion Loss: 93.8021 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 35\tNet Loss: 93.6669 \tQuestion Loss: 93.6669 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 35\tNet Loss: 88.8105 \tQuestion Loss: 88.8105 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 35\tNet Loss: 94.9825 \tQuestion Loss: 94.9825 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 35\tNet Loss: 93.5426 \tQuestion Loss: 93.5426 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 35\tNet Loss: 94.6933 \tQuestion Loss: 94.6933 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 35\tNet Loss: 90.9885 \tQuestion Loss: 90.9885 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 35\tNet Loss: 95.6478 \tQuestion Loss: 95.6478 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 35\tNet Loss: 90.4177 \tQuestion Loss: 90.4177 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 35\tNet Loss: 92.7702 \tQuestion Loss: 92.7702 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 35\tNet Loss: 92.9491 \tQuestion Loss: 92.9491 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 15 \t Epoch : 35\tNet Loss: 95.5518 \tQuestion Loss: 95.5518 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 35\tNet Loss: 90.5958 \tQuestion Loss: 90.5958 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 35\tNet Loss: 92.4906 \tQuestion Loss: 92.4906 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 35\tNet Loss: 90.4340 \tQuestion Loss: 90.4340 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 35\tNet Loss: 86.7168 \tQuestion Loss: 86.7168 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 35\tNet Loss: 96.0639 \tQuestion Loss: 96.0639 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 35\tNet Loss: 88.2033 \tQuestion Loss: 88.2033 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 35\tNet Loss: 92.9630 \tQuestion Loss: 92.9630 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 35\tNet Loss: 95.9181 \tQuestion Loss: 95.9181 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 35\tNet Loss: 89.8534 \tQuestion Loss: 89.8534 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 35\tNet Loss: 93.2814 \tQuestion Loss: 93.2814 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 35\tNet Loss: 98.4764 \tQuestion Loss: 98.4764 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 35\tNet Loss: 91.9279 \tQuestion Loss: 91.9279 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 35\tNet Loss: 86.9393 \tQuestion Loss: 86.9393 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 35\tNet Loss: 89.9006 \tQuestion Loss: 89.9006 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 35\tNet Loss: 91.6833 \tQuestion Loss: 91.6833 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 35\tNet Loss: 90.5044 \tQuestion Loss: 90.5044 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 35\tNet Loss: 88.7910 \tQuestion Loss: 88.7910 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 35\tNet Loss: 93.3001 \tQuestion Loss: 93.3001 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 35\tNet Loss: 87.3366 \tQuestion Loss: 87.3366 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 35\tNet Loss: 90.1659 \tQuestion Loss: 90.1659 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 35\tNet Loss: 93.8757 \tQuestion Loss: 93.8757 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 35\tNet Loss: 95.4136 \tQuestion Loss: 95.4136 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 35\tNet Loss: 91.2653 \tQuestion Loss: 91.2653 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 35\tNet Loss: 93.9230 \tQuestion Loss: 93.9230 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 35\tNet Loss: 91.5680 \tQuestion Loss: 91.5680 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 35\tNet Loss: 90.2485 \tQuestion Loss: 90.2485 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 35\tNet Loss: 91.9871 \tQuestion Loss: 91.9871 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 35\tNet Loss: 93.3712 \tQuestion Loss: 93.3712 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 35\tNet Loss: 90.6037 \tQuestion Loss: 90.6037 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 35\tNet Loss: 93.6731 \tQuestion Loss: 93.6731 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 35\tNet Loss: 92.3197 \tQuestion Loss: 92.3197 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 35\tNet Loss: 97.3423 \tQuestion Loss: 97.3423 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 35\tNet Loss: 86.1075 \tQuestion Loss: 86.1075 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 35\tNet Loss: 94.0253 \tQuestion Loss: 94.0253 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 35\tNet Loss: 92.9765 \tQuestion Loss: 92.9765 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 35\tNet Loss: 88.5527 \tQuestion Loss: 88.5527 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 35\tNet Loss: 89.5728 \tQuestion Loss: 89.5728 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 35\tNet Loss: 89.7300 \tQuestion Loss: 89.7300 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 35\tNet Loss: 96.5743 \tQuestion Loss: 96.5743 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 35\tNet Loss: 93.3307 \tQuestion Loss: 93.3307 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 35\tNet Loss: 94.4146 \tQuestion Loss: 94.4146 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 35\tNet Loss: 96.7042 \tQuestion Loss: 96.7042 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 35\tNet Loss: 87.6710 \tQuestion Loss: 87.6710 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 35\tNet Loss: 92.4128 \tQuestion Loss: 92.4128 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 35\tNet Loss: 89.1601 \tQuestion Loss: 89.1601 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 35\tNet Loss: 95.2628 \tQuestion Loss: 95.2628 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 35\tNet Loss: 88.8590 \tQuestion Loss: 88.8590 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 35\tNet Loss: 93.5537 \tQuestion Loss: 93.5537 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 35\tNet Loss: 92.2365 \tQuestion Loss: 92.2365 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 35\tNet Loss: 90.6804 \tQuestion Loss: 90.6804 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 35\tNet Loss: 92.0972 \tQuestion Loss: 92.0972 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 35\tNet Loss: 91.3190 \tQuestion Loss: 91.3190 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 35\tNet Loss: 91.1143 \tQuestion Loss: 91.1143 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 35\tNet Loss: 95.5974 \tQuestion Loss: 95.5974 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 35\tNet Loss: 93.1676 \tQuestion Loss: 93.1676 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 35\tNet Loss: 92.6525 \tQuestion Loss: 92.6525 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 35\tNet Loss: 95.8423 \tQuestion Loss: 95.8423 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 35\tNet Loss: 94.1142 \tQuestion Loss: 94.1142 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 35\tNet Loss: 94.9236 \tQuestion Loss: 94.9236 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 35\tNet Loss: 93.4793 \tQuestion Loss: 93.4793 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 35\tNet Loss: 86.8594 \tQuestion Loss: 86.8594 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 35\tNet Loss: 91.7324 \tQuestion Loss: 91.7324 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 35\tNet Loss: 96.0921 \tQuestion Loss: 96.0921 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 35\tNet Loss: 92.8818 \tQuestion Loss: 92.8818 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 35\tNet Loss: 94.1821 \tQuestion Loss: 94.1821 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 35\tNet Loss: 92.1600 \tQuestion Loss: 92.1600 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 35\tNet Loss: 89.6382 \tQuestion Loss: 89.6382 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 35\tNet Loss: 88.6933 \tQuestion Loss: 88.6933 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 35\tNet Loss: 91.9973 \tQuestion Loss: 91.9973 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 35\tNet Loss: 90.5469 \tQuestion Loss: 90.5469 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 35\tNet Loss: 92.6350 \tQuestion Loss: 92.6350 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 35\tNet Loss: 91.9633 \tQuestion Loss: 91.9633 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 35\tNet Loss: 94.3932 \tQuestion Loss: 94.3932 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 35\tNet Loss: 90.0495 \tQuestion Loss: 90.0495 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 35\tNet Loss: 95.3817 \tQuestion Loss: 95.3817 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 35\tNet Loss: 92.9796 \tQuestion Loss: 92.9796 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 35\tNet Loss: 91.3870 \tQuestion Loss: 91.3870 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 35\tNet Loss: 91.8951 \tQuestion Loss: 91.8951 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 35\tNet Loss: 92.7076 \tQuestion Loss: 92.7076 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 35\tNet Loss: 93.5379 \tQuestion Loss: 93.5379 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 35\tNet Loss: 91.9586 \tQuestion Loss: 91.9586 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 35\tNet Loss: 95.7526 \tQuestion Loss: 95.7526 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 35\tNet Loss: 92.7309 \tQuestion Loss: 92.7309 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 35\tNet Loss: 93.2215 \tQuestion Loss: 93.2215 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 35\tNet Loss: 92.4742 \tQuestion Loss: 92.4742 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 35\tNet Loss: 95.6229 \tQuestion Loss: 95.6229 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 35\tNet Loss: 92.9949 \tQuestion Loss: 92.9949 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 35\tNet Loss: 92.5300 \tQuestion Loss: 92.5300 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 35\tNet Loss: 95.2379 \tQuestion Loss: 95.2379 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 35\tNet Loss: 96.6442 \tQuestion Loss: 96.6442 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 35\tNet Loss: 92.1915 \tQuestion Loss: 92.1915 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 107 \t Epoch : 35\tNet Loss: 89.1767 \tQuestion Loss: 89.1767 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 35\tNet Loss: 88.3904 \tQuestion Loss: 88.3904 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 35\tNet Loss: 92.8755 \tQuestion Loss: 92.8755 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 35\tNet Loss: 93.4667 \tQuestion Loss: 93.4667 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 35\tNet Loss: 87.8208 \tQuestion Loss: 87.8208 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 35\tNet Loss: 89.6886 \tQuestion Loss: 89.6886 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 35\tNet Loss: 97.0905 \tQuestion Loss: 97.0905 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 35\tNet Loss: 94.2986 \tQuestion Loss: 94.2986 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 35\tNet Loss: 91.3677 \tQuestion Loss: 91.3677 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 35\tNet Loss: 88.6499 \tQuestion Loss: 88.6499 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 35\tNet Loss: 90.5552 \tQuestion Loss: 90.5552 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 35\tNet Loss: 93.9166 \tQuestion Loss: 93.9166 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 35\tNet Loss: 89.6847 \tQuestion Loss: 89.6847 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 35\tNet Loss: 91.4982 \tQuestion Loss: 91.4982 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 35\tNet Loss: 93.8364 \tQuestion Loss: 93.8364 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 35\tNet Loss: 96.9082 \tQuestion Loss: 96.9082 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 35\tNet Loss: 92.7425 \tQuestion Loss: 92.7425 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 35\tNet Loss: 94.6538 \tQuestion Loss: 94.6538 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 35\tNet Loss: 92.7882 \tQuestion Loss: 92.7882 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 35\tNet Loss: 96.4243 \tQuestion Loss: 96.4243 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 35\tNet Loss: 88.0807 \tQuestion Loss: 88.0807 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 35\tNet Loss: 93.2682 \tQuestion Loss: 93.2682 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 35\tNet Loss: 93.0668 \tQuestion Loss: 93.0668 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 35\tNet Loss: 100.1957 \tQuestion Loss: 100.1957 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 35\tNet Loss: 94.8296 \tQuestion Loss: 94.8296 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 35\tNet Loss: 91.1359 \tQuestion Loss: 91.1359 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 35\tNet Loss: 92.7955 \tQuestion Loss: 92.7955 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 35\tNet Loss: 92.7584 \tQuestion Loss: 92.7584 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 35\tNet Loss: 86.2358 \tQuestion Loss: 86.2358 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 35\tNet Loss: 97.2675 \tQuestion Loss: 97.2675 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 35\tNet Loss: 95.1320 \tQuestion Loss: 95.1320 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 35\tNet Loss: 93.4948 \tQuestion Loss: 93.4948 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 35\tNet Loss: 92.1180 \tQuestion Loss: 92.1180 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 35\tNet Loss: 89.3097 \tQuestion Loss: 89.3097 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 35\tNet Loss: 89.8173 \tQuestion Loss: 89.8173 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 35\tNet Loss: 94.5328 \tQuestion Loss: 94.5328 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 35\tNet Loss: 89.5424 \tQuestion Loss: 89.5424 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 35\tNet Loss: 86.9971 \tQuestion Loss: 86.9971 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 35\tNet Loss: 95.5420 \tQuestion Loss: 95.5420 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 35\tNet Loss: 98.8222 \tQuestion Loss: 98.8222 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 35\tNet Loss: 87.1008 \tQuestion Loss: 87.1008 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 35\tNet Loss: 96.5270 \tQuestion Loss: 96.5270 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 35\tNet Loss: 90.0437 \tQuestion Loss: 90.0437 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 35\tNet Loss: 91.2106 \tQuestion Loss: 91.2106 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 35\tNet Loss: 97.9761 \tQuestion Loss: 97.9761 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 35\tNet Loss: 93.8532 \tQuestion Loss: 93.8532 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 35\tNet Loss: 94.4439 \tQuestion Loss: 94.4439 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 35\tNet Loss: 85.5317 \tQuestion Loss: 85.5317 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 35\tNet Loss: 90.5894 \tQuestion Loss: 90.5894 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 35\tNet Loss: 93.5071 \tQuestion Loss: 93.5071 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 35\tNet Loss: 88.3061 \tQuestion Loss: 88.3061 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 35\tNet Loss: 93.7675 \tQuestion Loss: 93.7675 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 35\tNet Loss: 94.8923 \tQuestion Loss: 94.8923 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 35\tNet Loss: 95.9820 \tQuestion Loss: 95.9820 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 35\tNet Loss: 94.1979 \tQuestion Loss: 94.1979 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 35\tNet Loss: 95.9920 \tQuestion Loss: 95.9920 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 35\tNet Loss: 89.7936 \tQuestion Loss: 89.7936 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 35\tNet Loss: 88.5855 \tQuestion Loss: 88.5855 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 35\tNet Loss: 89.7174 \tQuestion Loss: 89.7174 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 35\tNet Loss: 87.5258 \tQuestion Loss: 87.5258 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 35\tNet Loss: 93.1260 \tQuestion Loss: 93.1260 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 35\tNet Loss: 91.4843 \tQuestion Loss: 91.4843 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 35\tNet Loss: 95.8407 \tQuestion Loss: 95.8407 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 35\tNet Loss: 93.6193 \tQuestion Loss: 93.6193 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 35\tNet Loss: 92.6658 \tQuestion Loss: 92.6658 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 35\tNet Loss: 88.9454 \tQuestion Loss: 88.9454 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 35\tNet Loss: 90.6212 \tQuestion Loss: 90.6212 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 35\tNet Loss: 95.0320 \tQuestion Loss: 95.0320 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 35\tNet Loss: 97.8091 \tQuestion Loss: 97.8091 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 35\tNet Loss: 90.1273 \tQuestion Loss: 90.1273 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 35\tNet Loss: 91.6482 \tQuestion Loss: 91.6482 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 35\tNet Loss: 90.5243 \tQuestion Loss: 90.5243 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 35\tNet Loss: 92.1414 \tQuestion Loss: 92.1414 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 35\tNet Loss: 90.1179 \tQuestion Loss: 90.1179 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 35\tNet Loss: 92.4266 \tQuestion Loss: 92.4266 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 35\tNet Loss: 94.3432 \tQuestion Loss: 94.3432 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 35\tNet Loss: 89.6067 \tQuestion Loss: 89.6067 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 35\tNet Loss: 93.4489 \tQuestion Loss: 93.4489 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 35\tNet Loss: 91.5887 \tQuestion Loss: 91.5887 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 35\tNet Loss: 94.4346 \tQuestion Loss: 94.4346 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 35\tNet Loss: 95.3494 \tQuestion Loss: 95.3494 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 35\tNet Loss: 89.9706 \tQuestion Loss: 89.9706 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 35\tNet Loss: 90.0193 \tQuestion Loss: 90.0193 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 35\tNet Loss: 91.1850 \tQuestion Loss: 91.1850 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 35\tNet Loss: 90.0819 \tQuestion Loss: 90.0819 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 35\tNet Loss: 94.9929 \tQuestion Loss: 94.9929 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 35\tNet Loss: 93.3668 \tQuestion Loss: 93.3668 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 35\tNet Loss: 95.0150 \tQuestion Loss: 95.0150 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 35\tNet Loss: 87.9848 \tQuestion Loss: 87.9848 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 35\tNet Loss: 89.2623 \tQuestion Loss: 89.2623 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 35\tNet Loss: 94.7108 \tQuestion Loss: 94.7108 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 35\tNet Loss: 87.9624 \tQuestion Loss: 87.9624 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 199 \t Epoch : 35\tNet Loss: 92.3638 \tQuestion Loss: 92.3638 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 35 : 92.3501 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 36\tNet Loss: 92.7475 \tQuestion Loss: 92.7475 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 36\tNet Loss: 89.1924 \tQuestion Loss: 89.1924 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 36\tNet Loss: 94.3629 \tQuestion Loss: 94.3629 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 36\tNet Loss: 93.4330 \tQuestion Loss: 93.4330 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 36\tNet Loss: 93.8168 \tQuestion Loss: 93.8168 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 36\tNet Loss: 93.7738 \tQuestion Loss: 93.7738 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 36\tNet Loss: 88.8464 \tQuestion Loss: 88.8464 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 36\tNet Loss: 94.9951 \tQuestion Loss: 94.9951 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 36\tNet Loss: 93.4181 \tQuestion Loss: 93.4181 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 36\tNet Loss: 95.3028 \tQuestion Loss: 95.3028 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 36\tNet Loss: 91.2636 \tQuestion Loss: 91.2636 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 36\tNet Loss: 95.6469 \tQuestion Loss: 95.6469 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 36\tNet Loss: 90.4102 \tQuestion Loss: 90.4102 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 36\tNet Loss: 92.7270 \tQuestion Loss: 92.7270 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 36\tNet Loss: 93.1687 \tQuestion Loss: 93.1687 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 36\tNet Loss: 95.4260 \tQuestion Loss: 95.4260 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 36\tNet Loss: 90.4220 \tQuestion Loss: 90.4220 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 36\tNet Loss: 92.4543 \tQuestion Loss: 92.4543 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 36\tNet Loss: 91.1569 \tQuestion Loss: 91.1569 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 36\tNet Loss: 86.9028 \tQuestion Loss: 86.9028 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 36\tNet Loss: 96.1660 \tQuestion Loss: 96.1660 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 36\tNet Loss: 88.2187 \tQuestion Loss: 88.2187 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 36\tNet Loss: 92.7635 \tQuestion Loss: 92.7635 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 36\tNet Loss: 95.8464 \tQuestion Loss: 95.8464 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 36\tNet Loss: 89.8205 \tQuestion Loss: 89.8205 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 36\tNet Loss: 93.2368 \tQuestion Loss: 93.2368 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 36\tNet Loss: 98.3537 \tQuestion Loss: 98.3537 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 36\tNet Loss: 91.8874 \tQuestion Loss: 91.8874 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 36\tNet Loss: 87.0742 \tQuestion Loss: 87.0742 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 36\tNet Loss: 89.9311 \tQuestion Loss: 89.9311 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 36\tNet Loss: 91.5553 \tQuestion Loss: 91.5553 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 36\tNet Loss: 90.3069 \tQuestion Loss: 90.3069 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 36\tNet Loss: 89.0648 \tQuestion Loss: 89.0648 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 36\tNet Loss: 93.2918 \tQuestion Loss: 93.2918 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 36\tNet Loss: 87.4678 \tQuestion Loss: 87.4678 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 36\tNet Loss: 90.1580 \tQuestion Loss: 90.1580 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 36\tNet Loss: 93.8918 \tQuestion Loss: 93.8918 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 36\tNet Loss: 94.9852 \tQuestion Loss: 94.9852 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 36\tNet Loss: 91.0414 \tQuestion Loss: 91.0414 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 36\tNet Loss: 94.1716 \tQuestion Loss: 94.1716 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 36\tNet Loss: 91.5135 \tQuestion Loss: 91.5135 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 36\tNet Loss: 90.2579 \tQuestion Loss: 90.2579 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 36\tNet Loss: 92.1836 \tQuestion Loss: 92.1836 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 36\tNet Loss: 93.5283 \tQuestion Loss: 93.5283 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 36\tNet Loss: 90.7442 \tQuestion Loss: 90.7442 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 36\tNet Loss: 93.6531 \tQuestion Loss: 93.6531 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 36\tNet Loss: 92.1017 \tQuestion Loss: 92.1017 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 36\tNet Loss: 97.2389 \tQuestion Loss: 97.2389 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 36\tNet Loss: 86.8648 \tQuestion Loss: 86.8648 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 36\tNet Loss: 94.2456 \tQuestion Loss: 94.2456 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 36\tNet Loss: 92.9286 \tQuestion Loss: 92.9286 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 36\tNet Loss: 88.5364 \tQuestion Loss: 88.5364 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 36\tNet Loss: 89.5366 \tQuestion Loss: 89.5366 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 36\tNet Loss: 90.0458 \tQuestion Loss: 90.0458 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 36\tNet Loss: 96.3838 \tQuestion Loss: 96.3838 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 36\tNet Loss: 93.0564 \tQuestion Loss: 93.0564 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 36\tNet Loss: 94.4336 \tQuestion Loss: 94.4336 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 36\tNet Loss: 96.6921 \tQuestion Loss: 96.6921 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 36\tNet Loss: 87.4819 \tQuestion Loss: 87.4819 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 36\tNet Loss: 92.0267 \tQuestion Loss: 92.0267 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 36\tNet Loss: 89.0855 \tQuestion Loss: 89.0855 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 36\tNet Loss: 95.2734 \tQuestion Loss: 95.2734 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 36\tNet Loss: 88.7267 \tQuestion Loss: 88.7267 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 36\tNet Loss: 93.3787 \tQuestion Loss: 93.3787 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 36\tNet Loss: 92.0201 \tQuestion Loss: 92.0201 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 36\tNet Loss: 90.6744 \tQuestion Loss: 90.6744 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 36\tNet Loss: 92.7340 \tQuestion Loss: 92.7340 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 36\tNet Loss: 91.9717 \tQuestion Loss: 91.9717 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 36\tNet Loss: 90.7725 \tQuestion Loss: 90.7725 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 36\tNet Loss: 95.9101 \tQuestion Loss: 95.9101 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 36\tNet Loss: 93.5300 \tQuestion Loss: 93.5300 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 36\tNet Loss: 92.7351 \tQuestion Loss: 92.7351 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 36\tNet Loss: 95.4404 \tQuestion Loss: 95.4404 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 36\tNet Loss: 93.2249 \tQuestion Loss: 93.2249 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 36\tNet Loss: 94.9498 \tQuestion Loss: 94.9498 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 36\tNet Loss: 93.4849 \tQuestion Loss: 93.4849 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 36\tNet Loss: 86.5926 \tQuestion Loss: 86.5926 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 36\tNet Loss: 91.3697 \tQuestion Loss: 91.3697 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 36\tNet Loss: 97.3690 \tQuestion Loss: 97.3690 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 36\tNet Loss: 93.0324 \tQuestion Loss: 93.0324 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 36\tNet Loss: 94.2252 \tQuestion Loss: 94.2252 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 36\tNet Loss: 92.0742 \tQuestion Loss: 92.0742 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 36\tNet Loss: 89.6084 \tQuestion Loss: 89.6084 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 36\tNet Loss: 88.6482 \tQuestion Loss: 88.6482 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 36\tNet Loss: 91.9284 \tQuestion Loss: 91.9284 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 36\tNet Loss: 90.0659 \tQuestion Loss: 90.0659 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 36\tNet Loss: 91.6804 \tQuestion Loss: 91.6804 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 36\tNet Loss: 91.4528 \tQuestion Loss: 91.4528 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 36\tNet Loss: 94.4183 \tQuestion Loss: 94.4183 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 36\tNet Loss: 89.9807 \tQuestion Loss: 89.9807 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 36\tNet Loss: 95.1388 \tQuestion Loss: 95.1388 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 91 \t Epoch : 36\tNet Loss: 92.9250 \tQuestion Loss: 92.9250 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 36\tNet Loss: 91.6813 \tQuestion Loss: 91.6813 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 36\tNet Loss: 92.3582 \tQuestion Loss: 92.3582 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 36\tNet Loss: 92.2512 \tQuestion Loss: 92.2512 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 36\tNet Loss: 93.5116 \tQuestion Loss: 93.5116 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 36\tNet Loss: 91.7996 \tQuestion Loss: 91.7996 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 36\tNet Loss: 96.0879 \tQuestion Loss: 96.0879 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 36\tNet Loss: 92.7254 \tQuestion Loss: 92.7254 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 36\tNet Loss: 92.7642 \tQuestion Loss: 92.7642 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 36\tNet Loss: 92.4680 \tQuestion Loss: 92.4680 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 36\tNet Loss: 95.4713 \tQuestion Loss: 95.4713 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 36\tNet Loss: 93.0942 \tQuestion Loss: 93.0942 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 36\tNet Loss: 92.3159 \tQuestion Loss: 92.3159 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 36\tNet Loss: 94.9548 \tQuestion Loss: 94.9548 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 36\tNet Loss: 96.5143 \tQuestion Loss: 96.5143 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 36\tNet Loss: 92.2182 \tQuestion Loss: 92.2182 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 36\tNet Loss: 89.2660 \tQuestion Loss: 89.2660 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 36\tNet Loss: 88.2393 \tQuestion Loss: 88.2393 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 36\tNet Loss: 92.9596 \tQuestion Loss: 92.9596 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 36\tNet Loss: 93.1160 \tQuestion Loss: 93.1160 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 36\tNet Loss: 87.6977 \tQuestion Loss: 87.6977 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 36\tNet Loss: 89.7293 \tQuestion Loss: 89.7293 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 36\tNet Loss: 97.1754 \tQuestion Loss: 97.1754 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 36\tNet Loss: 93.9669 \tQuestion Loss: 93.9669 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 36\tNet Loss: 91.0033 \tQuestion Loss: 91.0033 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 36\tNet Loss: 87.8919 \tQuestion Loss: 87.8919 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 36\tNet Loss: 90.4501 \tQuestion Loss: 90.4501 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 36\tNet Loss: 94.3205 \tQuestion Loss: 94.3205 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 36\tNet Loss: 90.1757 \tQuestion Loss: 90.1757 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 36\tNet Loss: 92.5815 \tQuestion Loss: 92.5815 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 36\tNet Loss: 94.0902 \tQuestion Loss: 94.0902 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 36\tNet Loss: 97.4099 \tQuestion Loss: 97.4099 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 36\tNet Loss: 93.6952 \tQuestion Loss: 93.6952 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 36\tNet Loss: 95.0536 \tQuestion Loss: 95.0536 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 36\tNet Loss: 93.5427 \tQuestion Loss: 93.5427 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 36\tNet Loss: 96.7372 \tQuestion Loss: 96.7372 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 36\tNet Loss: 87.8250 \tQuestion Loss: 87.8250 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 36\tNet Loss: 93.0260 \tQuestion Loss: 93.0260 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 36\tNet Loss: 94.8250 \tQuestion Loss: 94.8250 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 36\tNet Loss: 100.4276 \tQuestion Loss: 100.4276 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 36\tNet Loss: 95.1886 \tQuestion Loss: 95.1886 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 36\tNet Loss: 91.3416 \tQuestion Loss: 91.3416 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 36\tNet Loss: 92.9496 \tQuestion Loss: 92.9496 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 36\tNet Loss: 93.7708 \tQuestion Loss: 93.7708 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 36\tNet Loss: 85.9073 \tQuestion Loss: 85.9073 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 36\tNet Loss: 97.4407 \tQuestion Loss: 97.4407 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 36\tNet Loss: 95.0290 \tQuestion Loss: 95.0290 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 36\tNet Loss: 93.2426 \tQuestion Loss: 93.2426 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 36\tNet Loss: 92.1300 \tQuestion Loss: 92.1300 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 36\tNet Loss: 89.6616 \tQuestion Loss: 89.6616 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 36\tNet Loss: 90.4349 \tQuestion Loss: 90.4349 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 36\tNet Loss: 95.6685 \tQuestion Loss: 95.6685 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 36\tNet Loss: 90.1410 \tQuestion Loss: 90.1410 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 36\tNet Loss: 87.0225 \tQuestion Loss: 87.0225 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 36\tNet Loss: 96.1077 \tQuestion Loss: 96.1077 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 36\tNet Loss: 98.7408 \tQuestion Loss: 98.7408 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 36\tNet Loss: 87.1489 \tQuestion Loss: 87.1489 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 36\tNet Loss: 97.0289 \tQuestion Loss: 97.0289 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 36\tNet Loss: 90.2797 \tQuestion Loss: 90.2797 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 36\tNet Loss: 91.1808 \tQuestion Loss: 91.1808 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 36\tNet Loss: 98.3129 \tQuestion Loss: 98.3129 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 36\tNet Loss: 93.5499 \tQuestion Loss: 93.5499 \t Time Taken: 1 seconds\n",
      "Batch: 153 \t Epoch : 36\tNet Loss: 94.4182 \tQuestion Loss: 94.4182 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 36\tNet Loss: 85.3573 \tQuestion Loss: 85.3573 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 36\tNet Loss: 90.5712 \tQuestion Loss: 90.5712 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 36\tNet Loss: 93.5456 \tQuestion Loss: 93.5456 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 36\tNet Loss: 88.5527 \tQuestion Loss: 88.5527 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 36\tNet Loss: 93.6818 \tQuestion Loss: 93.6818 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 36\tNet Loss: 94.7890 \tQuestion Loss: 94.7890 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 36\tNet Loss: 95.8796 \tQuestion Loss: 95.8796 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 36\tNet Loss: 94.1286 \tQuestion Loss: 94.1286 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 36\tNet Loss: 95.9013 \tQuestion Loss: 95.9013 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 36\tNet Loss: 89.4780 \tQuestion Loss: 89.4780 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 36\tNet Loss: 88.4257 \tQuestion Loss: 88.4257 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 36\tNet Loss: 89.8157 \tQuestion Loss: 89.8157 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 36\tNet Loss: 87.4156 \tQuestion Loss: 87.4156 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 36\tNet Loss: 92.7068 \tQuestion Loss: 92.7068 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 36\tNet Loss: 91.4423 \tQuestion Loss: 91.4423 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 36\tNet Loss: 95.6767 \tQuestion Loss: 95.6767 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 36\tNet Loss: 93.5911 \tQuestion Loss: 93.5911 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 36\tNet Loss: 92.4356 \tQuestion Loss: 92.4356 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 36\tNet Loss: 89.1981 \tQuestion Loss: 89.1981 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 36\tNet Loss: 90.6293 \tQuestion Loss: 90.6293 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 36\tNet Loss: 94.6904 \tQuestion Loss: 94.6904 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 36\tNet Loss: 97.5266 \tQuestion Loss: 97.5266 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 36\tNet Loss: 90.0505 \tQuestion Loss: 90.0505 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 36\tNet Loss: 91.4458 \tQuestion Loss: 91.4458 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 36\tNet Loss: 90.4856 \tQuestion Loss: 90.4856 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 36\tNet Loss: 91.5605 \tQuestion Loss: 91.5605 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 36\tNet Loss: 90.0610 \tQuestion Loss: 90.0610 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 36\tNet Loss: 92.3893 \tQuestion Loss: 92.3893 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 36\tNet Loss: 94.3597 \tQuestion Loss: 94.3597 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 183 \t Epoch : 36\tNet Loss: 89.3207 \tQuestion Loss: 89.3207 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 36\tNet Loss: 93.2615 \tQuestion Loss: 93.2615 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 36\tNet Loss: 91.5674 \tQuestion Loss: 91.5674 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 36\tNet Loss: 93.8467 \tQuestion Loss: 93.8467 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 36\tNet Loss: 94.9042 \tQuestion Loss: 94.9042 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 36\tNet Loss: 89.8690 \tQuestion Loss: 89.8690 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 36\tNet Loss: 89.5750 \tQuestion Loss: 89.5750 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 36\tNet Loss: 91.0744 \tQuestion Loss: 91.0744 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 36\tNet Loss: 90.0563 \tQuestion Loss: 90.0563 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 36\tNet Loss: 94.8263 \tQuestion Loss: 94.8263 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 36\tNet Loss: 93.3373 \tQuestion Loss: 93.3373 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 36\tNet Loss: 95.2559 \tQuestion Loss: 95.2559 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 36\tNet Loss: 87.9604 \tQuestion Loss: 87.9604 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 36\tNet Loss: 89.1667 \tQuestion Loss: 89.1667 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 36\tNet Loss: 94.5047 \tQuestion Loss: 94.5047 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 36\tNet Loss: 87.6471 \tQuestion Loss: 87.6471 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 36\tNet Loss: 92.3978 \tQuestion Loss: 92.3978 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 36 : 92.3653 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 37\tNet Loss: 92.6279 \tQuestion Loss: 92.6279 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 37\tNet Loss: 88.8705 \tQuestion Loss: 88.8705 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 37\tNet Loss: 94.0323 \tQuestion Loss: 94.0323 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 37\tNet Loss: 93.7830 \tQuestion Loss: 93.7830 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 37\tNet Loss: 93.6702 \tQuestion Loss: 93.6702 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 37\tNet Loss: 93.5453 \tQuestion Loss: 93.5453 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 37\tNet Loss: 88.7493 \tQuestion Loss: 88.7493 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 37\tNet Loss: 94.8096 \tQuestion Loss: 94.8096 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 37\tNet Loss: 93.4989 \tQuestion Loss: 93.4989 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 37\tNet Loss: 94.6393 \tQuestion Loss: 94.6393 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 37\tNet Loss: 90.9249 \tQuestion Loss: 90.9249 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 37\tNet Loss: 95.5531 \tQuestion Loss: 95.5531 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 37\tNet Loss: 90.3007 \tQuestion Loss: 90.3007 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 37\tNet Loss: 92.6984 \tQuestion Loss: 92.6984 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 37\tNet Loss: 92.8798 \tQuestion Loss: 92.8798 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 37\tNet Loss: 95.4754 \tQuestion Loss: 95.4754 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 37\tNet Loss: 90.5546 \tQuestion Loss: 90.5546 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 37\tNet Loss: 92.4375 \tQuestion Loss: 92.4375 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 37\tNet Loss: 90.4360 \tQuestion Loss: 90.4360 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 37\tNet Loss: 86.6354 \tQuestion Loss: 86.6354 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 37\tNet Loss: 95.9891 \tQuestion Loss: 95.9891 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 37\tNet Loss: 88.1010 \tQuestion Loss: 88.1010 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 37\tNet Loss: 92.8968 \tQuestion Loss: 92.8968 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 37\tNet Loss: 95.8787 \tQuestion Loss: 95.8787 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 37\tNet Loss: 89.7870 \tQuestion Loss: 89.7870 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 37\tNet Loss: 93.2509 \tQuestion Loss: 93.2509 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 37\tNet Loss: 98.3646 \tQuestion Loss: 98.3646 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 37\tNet Loss: 91.8510 \tQuestion Loss: 91.8510 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 37\tNet Loss: 86.8922 \tQuestion Loss: 86.8922 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 37\tNet Loss: 89.8599 \tQuestion Loss: 89.8599 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 37\tNet Loss: 91.6027 \tQuestion Loss: 91.6027 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 37\tNet Loss: 90.4248 \tQuestion Loss: 90.4248 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 37\tNet Loss: 88.7344 \tQuestion Loss: 88.7344 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 37\tNet Loss: 93.2688 \tQuestion Loss: 93.2688 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 37\tNet Loss: 87.2358 \tQuestion Loss: 87.2358 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 37\tNet Loss: 90.0783 \tQuestion Loss: 90.0783 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 37\tNet Loss: 93.7637 \tQuestion Loss: 93.7637 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 37\tNet Loss: 95.2943 \tQuestion Loss: 95.2943 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 37\tNet Loss: 91.2881 \tQuestion Loss: 91.2881 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 37\tNet Loss: 93.8511 \tQuestion Loss: 93.8511 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 37\tNet Loss: 91.5459 \tQuestion Loss: 91.5459 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 37\tNet Loss: 90.2001 \tQuestion Loss: 90.2001 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 37\tNet Loss: 91.8940 \tQuestion Loss: 91.8940 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 37\tNet Loss: 93.2411 \tQuestion Loss: 93.2411 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 37\tNet Loss: 90.5576 \tQuestion Loss: 90.5576 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 37\tNet Loss: 93.5583 \tQuestion Loss: 93.5583 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 37\tNet Loss: 92.3277 \tQuestion Loss: 92.3277 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 37\tNet Loss: 97.3007 \tQuestion Loss: 97.3007 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 37\tNet Loss: 86.0036 \tQuestion Loss: 86.0036 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 37\tNet Loss: 93.9827 \tQuestion Loss: 93.9827 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 37\tNet Loss: 92.8965 \tQuestion Loss: 92.8965 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 37\tNet Loss: 88.4608 \tQuestion Loss: 88.4608 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 37\tNet Loss: 89.5289 \tQuestion Loss: 89.5289 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 37\tNet Loss: 89.7912 \tQuestion Loss: 89.7912 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 37\tNet Loss: 96.5617 \tQuestion Loss: 96.5617 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 37\tNet Loss: 93.2912 \tQuestion Loss: 93.2912 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 37\tNet Loss: 94.3643 \tQuestion Loss: 94.3643 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 37\tNet Loss: 96.6280 \tQuestion Loss: 96.6280 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 37\tNet Loss: 87.7048 \tQuestion Loss: 87.7048 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 37\tNet Loss: 92.0590 \tQuestion Loss: 92.0590 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 37\tNet Loss: 88.9084 \tQuestion Loss: 88.9084 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 37\tNet Loss: 95.2303 \tQuestion Loss: 95.2303 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 37\tNet Loss: 88.8333 \tQuestion Loss: 88.8333 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 37\tNet Loss: 93.4939 \tQuestion Loss: 93.4939 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 37\tNet Loss: 92.0941 \tQuestion Loss: 92.0941 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 37\tNet Loss: 90.8868 \tQuestion Loss: 90.8868 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 37\tNet Loss: 92.1637 \tQuestion Loss: 92.1637 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 37\tNet Loss: 91.4316 \tQuestion Loss: 91.4316 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 37\tNet Loss: 90.9503 \tQuestion Loss: 90.9503 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 37\tNet Loss: 95.5935 \tQuestion Loss: 95.5935 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 37\tNet Loss: 93.3283 \tQuestion Loss: 93.3283 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 37\tNet Loss: 92.7066 \tQuestion Loss: 92.7066 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 37\tNet Loss: 95.7017 \tQuestion Loss: 95.7017 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 37\tNet Loss: 93.9908 \tQuestion Loss: 93.9908 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 37\tNet Loss: 95.0306 \tQuestion Loss: 95.0306 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 75 \t Epoch : 37\tNet Loss: 93.5450 \tQuestion Loss: 93.5450 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 37\tNet Loss: 86.8220 \tQuestion Loss: 86.8220 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 37\tNet Loss: 91.6029 \tQuestion Loss: 91.6029 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 37\tNet Loss: 96.0071 \tQuestion Loss: 96.0071 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 37\tNet Loss: 92.9813 \tQuestion Loss: 92.9813 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 37\tNet Loss: 94.1820 \tQuestion Loss: 94.1820 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 37\tNet Loss: 92.0670 \tQuestion Loss: 92.0670 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 37\tNet Loss: 89.7192 \tQuestion Loss: 89.7192 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 37\tNet Loss: 88.8150 \tQuestion Loss: 88.8150 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 37\tNet Loss: 92.1411 \tQuestion Loss: 92.1411 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 37\tNet Loss: 90.6327 \tQuestion Loss: 90.6327 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 37\tNet Loss: 92.7513 \tQuestion Loss: 92.7513 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 37\tNet Loss: 91.8055 \tQuestion Loss: 91.8055 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 37\tNet Loss: 94.5388 \tQuestion Loss: 94.5388 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 37\tNet Loss: 90.2052 \tQuestion Loss: 90.2052 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 37\tNet Loss: 95.3452 \tQuestion Loss: 95.3452 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 37\tNet Loss: 92.8867 \tQuestion Loss: 92.8867 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 37\tNet Loss: 91.4279 \tQuestion Loss: 91.4279 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 37\tNet Loss: 92.1309 \tQuestion Loss: 92.1309 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 37\tNet Loss: 92.7942 \tQuestion Loss: 92.7942 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 37\tNet Loss: 93.4064 \tQuestion Loss: 93.4064 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 37\tNet Loss: 91.8736 \tQuestion Loss: 91.8736 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 37\tNet Loss: 95.9147 \tQuestion Loss: 95.9147 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 37\tNet Loss: 92.9183 \tQuestion Loss: 92.9183 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 37\tNet Loss: 93.2284 \tQuestion Loss: 93.2284 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 37\tNet Loss: 92.4042 \tQuestion Loss: 92.4042 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 37\tNet Loss: 95.6100 \tQuestion Loss: 95.6100 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 37\tNet Loss: 93.0861 \tQuestion Loss: 93.0861 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 37\tNet Loss: 92.6076 \tQuestion Loss: 92.6076 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 37\tNet Loss: 95.2591 \tQuestion Loss: 95.2591 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 37\tNet Loss: 96.5741 \tQuestion Loss: 96.5741 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 37\tNet Loss: 92.1673 \tQuestion Loss: 92.1673 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 37\tNet Loss: 89.3266 \tQuestion Loss: 89.3266 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 37\tNet Loss: 88.4333 \tQuestion Loss: 88.4333 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 37\tNet Loss: 92.7650 \tQuestion Loss: 92.7650 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 37\tNet Loss: 93.3376 \tQuestion Loss: 93.3376 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 37\tNet Loss: 88.0636 \tQuestion Loss: 88.0636 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 37\tNet Loss: 89.8845 \tQuestion Loss: 89.8845 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 37\tNet Loss: 97.0184 \tQuestion Loss: 97.0184 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 37\tNet Loss: 94.1078 \tQuestion Loss: 94.1078 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 37\tNet Loss: 91.1628 \tQuestion Loss: 91.1628 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 37\tNet Loss: 88.5798 \tQuestion Loss: 88.5798 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 37\tNet Loss: 90.9375 \tQuestion Loss: 90.9375 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 37\tNet Loss: 94.0347 \tQuestion Loss: 94.0347 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 37\tNet Loss: 89.4201 \tQuestion Loss: 89.4201 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 37\tNet Loss: 91.4525 \tQuestion Loss: 91.4525 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 37\tNet Loss: 94.0704 \tQuestion Loss: 94.0704 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 37\tNet Loss: 97.0259 \tQuestion Loss: 97.0259 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 37\tNet Loss: 92.7824 \tQuestion Loss: 92.7824 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 37\tNet Loss: 94.6481 \tQuestion Loss: 94.6481 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 37\tNet Loss: 92.7314 \tQuestion Loss: 92.7314 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 37\tNet Loss: 96.4684 \tQuestion Loss: 96.4684 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 37\tNet Loss: 88.1034 \tQuestion Loss: 88.1034 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 37\tNet Loss: 93.3140 \tQuestion Loss: 93.3140 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 37\tNet Loss: 92.9710 \tQuestion Loss: 92.9710 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 37\tNet Loss: 100.1773 \tQuestion Loss: 100.1773 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 37\tNet Loss: 94.7121 \tQuestion Loss: 94.7121 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 37\tNet Loss: 91.0649 \tQuestion Loss: 91.0649 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 37\tNet Loss: 92.9938 \tQuestion Loss: 92.9938 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 37\tNet Loss: 92.8764 \tQuestion Loss: 92.8764 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 37\tNet Loss: 86.4765 \tQuestion Loss: 86.4765 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 37\tNet Loss: 97.3499 \tQuestion Loss: 97.3499 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 37\tNet Loss: 94.9593 \tQuestion Loss: 94.9593 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 37\tNet Loss: 93.4973 \tQuestion Loss: 93.4973 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 37\tNet Loss: 92.1872 \tQuestion Loss: 92.1872 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 37\tNet Loss: 89.3730 \tQuestion Loss: 89.3730 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 37\tNet Loss: 89.7793 \tQuestion Loss: 89.7793 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 37\tNet Loss: 94.4173 \tQuestion Loss: 94.4173 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 37\tNet Loss: 89.4819 \tQuestion Loss: 89.4819 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 37\tNet Loss: 87.1565 \tQuestion Loss: 87.1565 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 37\tNet Loss: 95.5957 \tQuestion Loss: 95.5957 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 37\tNet Loss: 98.7796 \tQuestion Loss: 98.7796 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 37\tNet Loss: 87.1139 \tQuestion Loss: 87.1139 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 37\tNet Loss: 96.5248 \tQuestion Loss: 96.5248 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 37\tNet Loss: 90.1516 \tQuestion Loss: 90.1516 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 37\tNet Loss: 91.3461 \tQuestion Loss: 91.3461 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 37\tNet Loss: 98.0678 \tQuestion Loss: 98.0678 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 37\tNet Loss: 93.9446 \tQuestion Loss: 93.9446 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 37\tNet Loss: 94.4108 \tQuestion Loss: 94.4108 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 37\tNet Loss: 85.5365 \tQuestion Loss: 85.5365 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 37\tNet Loss: 90.6175 \tQuestion Loss: 90.6175 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 37\tNet Loss: 93.6762 \tQuestion Loss: 93.6762 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 37\tNet Loss: 88.3662 \tQuestion Loss: 88.3662 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 37\tNet Loss: 93.7496 \tQuestion Loss: 93.7496 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 37\tNet Loss: 94.8502 \tQuestion Loss: 94.8502 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 37\tNet Loss: 95.9911 \tQuestion Loss: 95.9911 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 37\tNet Loss: 94.3156 \tQuestion Loss: 94.3156 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 37\tNet Loss: 95.9913 \tQuestion Loss: 95.9913 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 37\tNet Loss: 89.7985 \tQuestion Loss: 89.7985 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 37\tNet Loss: 88.6353 \tQuestion Loss: 88.6353 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 37\tNet Loss: 89.7488 \tQuestion Loss: 89.7488 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 37\tNet Loss: 87.5696 \tQuestion Loss: 87.5696 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 167 \t Epoch : 37\tNet Loss: 93.1577 \tQuestion Loss: 93.1577 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 37\tNet Loss: 91.5525 \tQuestion Loss: 91.5525 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 37\tNet Loss: 95.9703 \tQuestion Loss: 95.9703 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 37\tNet Loss: 93.5847 \tQuestion Loss: 93.5847 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 37\tNet Loss: 92.6605 \tQuestion Loss: 92.6605 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 37\tNet Loss: 89.0093 \tQuestion Loss: 89.0093 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 37\tNet Loss: 90.6681 \tQuestion Loss: 90.6681 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 37\tNet Loss: 95.0906 \tQuestion Loss: 95.0906 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 37\tNet Loss: 97.8073 \tQuestion Loss: 97.8073 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 37\tNet Loss: 90.1247 \tQuestion Loss: 90.1247 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 37\tNet Loss: 91.7027 \tQuestion Loss: 91.7027 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 37\tNet Loss: 90.5304 \tQuestion Loss: 90.5304 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 37\tNet Loss: 92.0717 \tQuestion Loss: 92.0717 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 37\tNet Loss: 90.1578 \tQuestion Loss: 90.1578 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 37\tNet Loss: 92.4958 \tQuestion Loss: 92.4958 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 37\tNet Loss: 94.3646 \tQuestion Loss: 94.3646 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 37\tNet Loss: 89.6651 \tQuestion Loss: 89.6651 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 37\tNet Loss: 93.4726 \tQuestion Loss: 93.4726 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 37\tNet Loss: 91.6504 \tQuestion Loss: 91.6504 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 37\tNet Loss: 94.4593 \tQuestion Loss: 94.4593 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 37\tNet Loss: 95.3763 \tQuestion Loss: 95.3763 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 37\tNet Loss: 90.0174 \tQuestion Loss: 90.0174 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 37\tNet Loss: 90.1103 \tQuestion Loss: 90.1103 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 37\tNet Loss: 91.1975 \tQuestion Loss: 91.1975 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 37\tNet Loss: 90.0893 \tQuestion Loss: 90.0893 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 37\tNet Loss: 94.9556 \tQuestion Loss: 94.9556 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 37\tNet Loss: 93.3977 \tQuestion Loss: 93.3977 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 37\tNet Loss: 95.0392 \tQuestion Loss: 95.0392 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 37\tNet Loss: 87.9554 \tQuestion Loss: 87.9554 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 37\tNet Loss: 89.2814 \tQuestion Loss: 89.2814 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 37\tNet Loss: 94.7059 \tQuestion Loss: 94.7059 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 37\tNet Loss: 88.0137 \tQuestion Loss: 88.0137 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 37\tNet Loss: 92.3868 \tQuestion Loss: 92.3868 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 37 : 92.3444 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 38\tNet Loss: 92.7829 \tQuestion Loss: 92.7829 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 38\tNet Loss: 89.2938 \tQuestion Loss: 89.2938 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 38\tNet Loss: 94.5170 \tQuestion Loss: 94.5170 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 38\tNet Loss: 93.3293 \tQuestion Loss: 93.3293 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 38\tNet Loss: 93.8574 \tQuestion Loss: 93.8574 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 38\tNet Loss: 93.7472 \tQuestion Loss: 93.7472 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 38\tNet Loss: 88.8820 \tQuestion Loss: 88.8820 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 38\tNet Loss: 94.9135 \tQuestion Loss: 94.9135 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 38\tNet Loss: 93.4045 \tQuestion Loss: 93.4045 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 38\tNet Loss: 95.3518 \tQuestion Loss: 95.3518 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 38\tNet Loss: 91.3805 \tQuestion Loss: 91.3805 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 38\tNet Loss: 95.6793 \tQuestion Loss: 95.6793 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 38\tNet Loss: 90.4053 \tQuestion Loss: 90.4053 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 38\tNet Loss: 92.6835 \tQuestion Loss: 92.6835 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 38\tNet Loss: 93.2112 \tQuestion Loss: 93.2112 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 38\tNet Loss: 95.4917 \tQuestion Loss: 95.4917 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 38\tNet Loss: 90.4287 \tQuestion Loss: 90.4287 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 38\tNet Loss: 92.4465 \tQuestion Loss: 92.4465 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 38\tNet Loss: 91.2030 \tQuestion Loss: 91.2030 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 38\tNet Loss: 86.9238 \tQuestion Loss: 86.9238 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 38\tNet Loss: 96.2216 \tQuestion Loss: 96.2216 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 38\tNet Loss: 88.2266 \tQuestion Loss: 88.2266 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 38\tNet Loss: 92.8045 \tQuestion Loss: 92.8045 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 38\tNet Loss: 95.8587 \tQuestion Loss: 95.8587 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 38\tNet Loss: 89.8680 \tQuestion Loss: 89.8680 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 38\tNet Loss: 93.2669 \tQuestion Loss: 93.2669 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 38\tNet Loss: 98.3883 \tQuestion Loss: 98.3883 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 38\tNet Loss: 91.9147 \tQuestion Loss: 91.9147 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 38\tNet Loss: 87.0964 \tQuestion Loss: 87.0964 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 38\tNet Loss: 89.9147 \tQuestion Loss: 89.9147 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 38\tNet Loss: 91.5674 \tQuestion Loss: 91.5674 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 38\tNet Loss: 90.3069 \tQuestion Loss: 90.3069 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 38\tNet Loss: 89.0955 \tQuestion Loss: 89.0955 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 38\tNet Loss: 93.3396 \tQuestion Loss: 93.3396 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 38\tNet Loss: 87.4149 \tQuestion Loss: 87.4149 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 38\tNet Loss: 90.1343 \tQuestion Loss: 90.1343 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 38\tNet Loss: 93.9296 \tQuestion Loss: 93.9296 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 38\tNet Loss: 95.0241 \tQuestion Loss: 95.0241 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 38\tNet Loss: 91.0635 \tQuestion Loss: 91.0635 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 38\tNet Loss: 94.2286 \tQuestion Loss: 94.2286 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 38\tNet Loss: 91.5103 \tQuestion Loss: 91.5103 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 38\tNet Loss: 90.3210 \tQuestion Loss: 90.3210 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 38\tNet Loss: 92.1732 \tQuestion Loss: 92.1732 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 38\tNet Loss: 93.5159 \tQuestion Loss: 93.5159 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 38\tNet Loss: 90.7297 \tQuestion Loss: 90.7297 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 38\tNet Loss: 93.6859 \tQuestion Loss: 93.6859 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 38\tNet Loss: 92.1292 \tQuestion Loss: 92.1292 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 38\tNet Loss: 97.1977 \tQuestion Loss: 97.1977 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 38\tNet Loss: 86.8256 \tQuestion Loss: 86.8256 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 38\tNet Loss: 94.3427 \tQuestion Loss: 94.3427 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 38\tNet Loss: 92.9655 \tQuestion Loss: 92.9655 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 38\tNet Loss: 88.5306 \tQuestion Loss: 88.5306 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 38\tNet Loss: 89.5355 \tQuestion Loss: 89.5355 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 38\tNet Loss: 90.0069 \tQuestion Loss: 90.0069 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 38\tNet Loss: 96.4215 \tQuestion Loss: 96.4215 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 38\tNet Loss: 93.1534 \tQuestion Loss: 93.1534 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 38\tNet Loss: 94.3549 \tQuestion Loss: 94.3549 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 38\tNet Loss: 96.5152 \tQuestion Loss: 96.5152 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 38\tNet Loss: 87.5047 \tQuestion Loss: 87.5047 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 59 \t Epoch : 38\tNet Loss: 92.2071 \tQuestion Loss: 92.2071 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 38\tNet Loss: 89.2627 \tQuestion Loss: 89.2627 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 38\tNet Loss: 95.2683 \tQuestion Loss: 95.2683 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 38\tNet Loss: 88.8062 \tQuestion Loss: 88.8062 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 38\tNet Loss: 93.5852 \tQuestion Loss: 93.5852 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 38\tNet Loss: 92.1540 \tQuestion Loss: 92.1540 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 38\tNet Loss: 90.1845 \tQuestion Loss: 90.1845 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 38\tNet Loss: 92.4991 \tQuestion Loss: 92.4991 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 38\tNet Loss: 92.0542 \tQuestion Loss: 92.0542 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 38\tNet Loss: 90.7622 \tQuestion Loss: 90.7622 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 38\tNet Loss: 95.9136 \tQuestion Loss: 95.9136 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 38\tNet Loss: 93.1470 \tQuestion Loss: 93.1470 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 38\tNet Loss: 92.8815 \tQuestion Loss: 92.8815 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 38\tNet Loss: 95.5272 \tQuestion Loss: 95.5272 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 38\tNet Loss: 93.3029 \tQuestion Loss: 93.3029 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 38\tNet Loss: 94.5991 \tQuestion Loss: 94.5991 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 38\tNet Loss: 93.4796 \tQuestion Loss: 93.4796 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 38\tNet Loss: 86.7791 \tQuestion Loss: 86.7791 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 38\tNet Loss: 91.4127 \tQuestion Loss: 91.4127 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 38\tNet Loss: 97.0875 \tQuestion Loss: 97.0875 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 38\tNet Loss: 92.8122 \tQuestion Loss: 92.8122 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 38\tNet Loss: 94.3274 \tQuestion Loss: 94.3274 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 38\tNet Loss: 92.0980 \tQuestion Loss: 92.0980 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 38\tNet Loss: 89.5322 \tQuestion Loss: 89.5322 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 38\tNet Loss: 88.4409 \tQuestion Loss: 88.4409 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 38\tNet Loss: 91.7905 \tQuestion Loss: 91.7905 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 38\tNet Loss: 89.9925 \tQuestion Loss: 89.9925 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 38\tNet Loss: 91.6863 \tQuestion Loss: 91.6863 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 38\tNet Loss: 91.5547 \tQuestion Loss: 91.5547 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 38\tNet Loss: 94.4576 \tQuestion Loss: 94.4576 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 38\tNet Loss: 89.9048 \tQuestion Loss: 89.9048 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 38\tNet Loss: 95.2692 \tQuestion Loss: 95.2692 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 38\tNet Loss: 93.0000 \tQuestion Loss: 93.0000 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 38\tNet Loss: 91.4538 \tQuestion Loss: 91.4538 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 38\tNet Loss: 91.7140 \tQuestion Loss: 91.7140 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 38\tNet Loss: 92.3952 \tQuestion Loss: 92.3952 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 38\tNet Loss: 93.4492 \tQuestion Loss: 93.4492 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 38\tNet Loss: 91.8529 \tQuestion Loss: 91.8529 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 38\tNet Loss: 95.6220 \tQuestion Loss: 95.6220 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 38\tNet Loss: 92.4710 \tQuestion Loss: 92.4710 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 38\tNet Loss: 92.9023 \tQuestion Loss: 92.9023 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 38\tNet Loss: 92.6526 \tQuestion Loss: 92.6526 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 38\tNet Loss: 95.5171 \tQuestion Loss: 95.5171 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 38\tNet Loss: 92.8928 \tQuestion Loss: 92.8928 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 38\tNet Loss: 92.4595 \tQuestion Loss: 92.4595 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 38\tNet Loss: 95.0472 \tQuestion Loss: 95.0472 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 38\tNet Loss: 96.5655 \tQuestion Loss: 96.5655 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 38\tNet Loss: 92.2479 \tQuestion Loss: 92.2479 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 38\tNet Loss: 89.1774 \tQuestion Loss: 89.1774 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 38\tNet Loss: 88.4237 \tQuestion Loss: 88.4237 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 38\tNet Loss: 93.0108 \tQuestion Loss: 93.0108 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 38\tNet Loss: 93.2494 \tQuestion Loss: 93.2494 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 38\tNet Loss: 87.4474 \tQuestion Loss: 87.4474 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 38\tNet Loss: 89.6900 \tQuestion Loss: 89.6900 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 38\tNet Loss: 97.2642 \tQuestion Loss: 97.2642 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 38\tNet Loss: 94.2742 \tQuestion Loss: 94.2742 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 38\tNet Loss: 91.2111 \tQuestion Loss: 91.2111 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 38\tNet Loss: 88.0424 \tQuestion Loss: 88.0424 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 38\tNet Loss: 90.3207 \tQuestion Loss: 90.3207 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 38\tNet Loss: 93.7536 \tQuestion Loss: 93.7536 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 38\tNet Loss: 89.7236 \tQuestion Loss: 89.7236 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 38\tNet Loss: 91.5916 \tQuestion Loss: 91.5916 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 38\tNet Loss: 93.2454 \tQuestion Loss: 93.2454 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 38\tNet Loss: 97.5430 \tQuestion Loss: 97.5430 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 38\tNet Loss: 92.8766 \tQuestion Loss: 92.8766 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 38\tNet Loss: 94.8630 \tQuestion Loss: 94.8630 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 38\tNet Loss: 92.5773 \tQuestion Loss: 92.5773 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 38\tNet Loss: 96.0476 \tQuestion Loss: 96.0476 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 38\tNet Loss: 87.9356 \tQuestion Loss: 87.9356 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 38\tNet Loss: 92.2185 \tQuestion Loss: 92.2185 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 38\tNet Loss: 94.9097 \tQuestion Loss: 94.9097 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 38\tNet Loss: 99.9583 \tQuestion Loss: 99.9583 \t Time Taken: 1 seconds\n",
      "Batch: 131 \t Epoch : 38\tNet Loss: 94.8883 \tQuestion Loss: 94.8883 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 38\tNet Loss: 91.3367 \tQuestion Loss: 91.3367 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 38\tNet Loss: 92.4722 \tQuestion Loss: 92.4722 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 38\tNet Loss: 93.4853 \tQuestion Loss: 93.4853 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 38\tNet Loss: 85.6898 \tQuestion Loss: 85.6898 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 38\tNet Loss: 97.3435 \tQuestion Loss: 97.3435 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 38\tNet Loss: 95.1682 \tQuestion Loss: 95.1682 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 38\tNet Loss: 93.2150 \tQuestion Loss: 93.2150 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 38\tNet Loss: 92.0606 \tQuestion Loss: 92.0606 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 38\tNet Loss: 89.2467 \tQuestion Loss: 89.2467 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 38\tNet Loss: 89.7439 \tQuestion Loss: 89.7439 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 38\tNet Loss: 94.5757 \tQuestion Loss: 94.5757 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 38\tNet Loss: 89.2764 \tQuestion Loss: 89.2764 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 38\tNet Loss: 86.8658 \tQuestion Loss: 86.8658 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 38\tNet Loss: 95.3672 \tQuestion Loss: 95.3672 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 38\tNet Loss: 98.5776 \tQuestion Loss: 98.5776 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 38\tNet Loss: 86.8200 \tQuestion Loss: 86.8200 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 38\tNet Loss: 96.5724 \tQuestion Loss: 96.5724 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 38\tNet Loss: 90.0549 \tQuestion Loss: 90.0549 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 38\tNet Loss: 91.0728 \tQuestion Loss: 91.0728 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 151 \t Epoch : 38\tNet Loss: 97.8728 \tQuestion Loss: 97.8728 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 38\tNet Loss: 93.6458 \tQuestion Loss: 93.6458 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 38\tNet Loss: 94.1217 \tQuestion Loss: 94.1217 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 38\tNet Loss: 85.0594 \tQuestion Loss: 85.0594 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 38\tNet Loss: 90.4779 \tQuestion Loss: 90.4779 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 38\tNet Loss: 93.4734 \tQuestion Loss: 93.4734 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 38\tNet Loss: 88.5029 \tQuestion Loss: 88.5029 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 38\tNet Loss: 93.7378 \tQuestion Loss: 93.7378 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 38\tNet Loss: 94.5816 \tQuestion Loss: 94.5816 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 38\tNet Loss: 95.8558 \tQuestion Loss: 95.8558 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 38\tNet Loss: 94.1656 \tQuestion Loss: 94.1656 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 38\tNet Loss: 95.8099 \tQuestion Loss: 95.8099 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 38\tNet Loss: 89.5155 \tQuestion Loss: 89.5155 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 38\tNet Loss: 88.5037 \tQuestion Loss: 88.5037 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 38\tNet Loss: 89.7443 \tQuestion Loss: 89.7443 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 38\tNet Loss: 87.4753 \tQuestion Loss: 87.4753 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 38\tNet Loss: 92.6787 \tQuestion Loss: 92.6787 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 38\tNet Loss: 91.5026 \tQuestion Loss: 91.5026 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 38\tNet Loss: 95.7757 \tQuestion Loss: 95.7757 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 38\tNet Loss: 93.5239 \tQuestion Loss: 93.5239 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 38\tNet Loss: 92.4455 \tQuestion Loss: 92.4455 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 38\tNet Loss: 89.2562 \tQuestion Loss: 89.2562 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 38\tNet Loss: 90.6399 \tQuestion Loss: 90.6399 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 38\tNet Loss: 94.6934 \tQuestion Loss: 94.6934 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 38\tNet Loss: 97.5348 \tQuestion Loss: 97.5348 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 38\tNet Loss: 90.0786 \tQuestion Loss: 90.0786 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 38\tNet Loss: 91.5242 \tQuestion Loss: 91.5242 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 38\tNet Loss: 90.5138 \tQuestion Loss: 90.5138 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 38\tNet Loss: 91.5801 \tQuestion Loss: 91.5801 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 38\tNet Loss: 90.0998 \tQuestion Loss: 90.0998 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 38\tNet Loss: 92.3811 \tQuestion Loss: 92.3811 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 38\tNet Loss: 94.3577 \tQuestion Loss: 94.3577 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 38\tNet Loss: 89.3382 \tQuestion Loss: 89.3382 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 38\tNet Loss: 93.3292 \tQuestion Loss: 93.3292 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 38\tNet Loss: 91.6646 \tQuestion Loss: 91.6646 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 38\tNet Loss: 93.9133 \tQuestion Loss: 93.9133 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 38\tNet Loss: 94.9227 \tQuestion Loss: 94.9227 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 38\tNet Loss: 89.9198 \tQuestion Loss: 89.9198 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 38\tNet Loss: 89.5376 \tQuestion Loss: 89.5376 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 38\tNet Loss: 91.0956 \tQuestion Loss: 91.0956 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 38\tNet Loss: 90.0514 \tQuestion Loss: 90.0514 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 38\tNet Loss: 94.8838 \tQuestion Loss: 94.8838 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 38\tNet Loss: 93.4310 \tQuestion Loss: 93.4310 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 38\tNet Loss: 95.2342 \tQuestion Loss: 95.2342 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 38\tNet Loss: 88.0033 \tQuestion Loss: 88.0033 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 38\tNet Loss: 89.2200 \tQuestion Loss: 89.2200 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 38\tNet Loss: 94.6016 \tQuestion Loss: 94.6016 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 38\tNet Loss: 87.6828 \tQuestion Loss: 87.6828 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 38\tNet Loss: 92.4200 \tQuestion Loss: 92.4200 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 38 : 92.2980 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 39\tNet Loss: 92.7115 \tQuestion Loss: 92.7115 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 39\tNet Loss: 88.9343 \tQuestion Loss: 88.9343 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 39\tNet Loss: 94.1998 \tQuestion Loss: 94.1998 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 39\tNet Loss: 93.7801 \tQuestion Loss: 93.7801 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 39\tNet Loss: 93.7513 \tQuestion Loss: 93.7513 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 39\tNet Loss: 93.6102 \tQuestion Loss: 93.6102 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 39\tNet Loss: 88.7951 \tQuestion Loss: 88.7951 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 39\tNet Loss: 94.8820 \tQuestion Loss: 94.8820 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 39\tNet Loss: 93.5519 \tQuestion Loss: 93.5519 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 39\tNet Loss: 94.6696 \tQuestion Loss: 94.6696 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 39\tNet Loss: 90.9994 \tQuestion Loss: 90.9994 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 39\tNet Loss: 95.6366 \tQuestion Loss: 95.6366 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 39\tNet Loss: 90.3811 \tQuestion Loss: 90.3811 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 39\tNet Loss: 92.7696 \tQuestion Loss: 92.7696 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 39\tNet Loss: 92.9393 \tQuestion Loss: 92.9393 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 39\tNet Loss: 95.5036 \tQuestion Loss: 95.5036 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 39\tNet Loss: 90.5741 \tQuestion Loss: 90.5741 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 39\tNet Loss: 92.4785 \tQuestion Loss: 92.4785 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 39\tNet Loss: 90.4446 \tQuestion Loss: 90.4446 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 39\tNet Loss: 86.7128 \tQuestion Loss: 86.7128 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 39\tNet Loss: 96.0609 \tQuestion Loss: 96.0609 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 39\tNet Loss: 88.1532 \tQuestion Loss: 88.1532 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 39\tNet Loss: 92.9382 \tQuestion Loss: 92.9382 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 39\tNet Loss: 95.9118 \tQuestion Loss: 95.9118 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 39\tNet Loss: 89.8425 \tQuestion Loss: 89.8425 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 39\tNet Loss: 93.2555 \tQuestion Loss: 93.2555 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 39\tNet Loss: 98.4096 \tQuestion Loss: 98.4096 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 39\tNet Loss: 91.8912 \tQuestion Loss: 91.8912 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 39\tNet Loss: 86.9815 \tQuestion Loss: 86.9815 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 39\tNet Loss: 89.9042 \tQuestion Loss: 89.9042 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 39\tNet Loss: 91.6540 \tQuestion Loss: 91.6540 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 39\tNet Loss: 90.4611 \tQuestion Loss: 90.4611 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 39\tNet Loss: 88.7994 \tQuestion Loss: 88.7994 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 39\tNet Loss: 93.2963 \tQuestion Loss: 93.2963 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 39\tNet Loss: 87.3245 \tQuestion Loss: 87.3245 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 39\tNet Loss: 90.1185 \tQuestion Loss: 90.1185 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 39\tNet Loss: 93.8287 \tQuestion Loss: 93.8287 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 39\tNet Loss: 95.4015 \tQuestion Loss: 95.4015 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 39\tNet Loss: 91.2431 \tQuestion Loss: 91.2431 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 39\tNet Loss: 93.8751 \tQuestion Loss: 93.8751 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 39\tNet Loss: 91.5553 \tQuestion Loss: 91.5553 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 39\tNet Loss: 90.2318 \tQuestion Loss: 90.2318 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 42 \t Epoch : 39\tNet Loss: 91.9869 \tQuestion Loss: 91.9869 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 39\tNet Loss: 93.3262 \tQuestion Loss: 93.3262 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 39\tNet Loss: 90.5797 \tQuestion Loss: 90.5797 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 39\tNet Loss: 93.6692 \tQuestion Loss: 93.6692 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 39\tNet Loss: 92.3554 \tQuestion Loss: 92.3554 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 39\tNet Loss: 97.3414 \tQuestion Loss: 97.3414 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 39\tNet Loss: 86.0568 \tQuestion Loss: 86.0568 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 39\tNet Loss: 94.1255 \tQuestion Loss: 94.1255 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 39\tNet Loss: 92.9613 \tQuestion Loss: 92.9613 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 39\tNet Loss: 88.5200 \tQuestion Loss: 88.5200 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 39\tNet Loss: 89.5517 \tQuestion Loss: 89.5517 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 39\tNet Loss: 89.7949 \tQuestion Loss: 89.7949 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 39\tNet Loss: 96.5171 \tQuestion Loss: 96.5171 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 39\tNet Loss: 93.3190 \tQuestion Loss: 93.3190 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 39\tNet Loss: 94.4563 \tQuestion Loss: 94.4563 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 39\tNet Loss: 96.6454 \tQuestion Loss: 96.6454 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 39\tNet Loss: 87.6969 \tQuestion Loss: 87.6969 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 39\tNet Loss: 92.1064 \tQuestion Loss: 92.1064 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 39\tNet Loss: 89.0484 \tQuestion Loss: 89.0484 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 39\tNet Loss: 95.2752 \tQuestion Loss: 95.2752 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 39\tNet Loss: 88.9073 \tQuestion Loss: 88.9073 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 39\tNet Loss: 93.5452 \tQuestion Loss: 93.5452 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 39\tNet Loss: 92.1174 \tQuestion Loss: 92.1174 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 39\tNet Loss: 90.8583 \tQuestion Loss: 90.8583 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 39\tNet Loss: 92.1403 \tQuestion Loss: 92.1403 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 39\tNet Loss: 91.4313 \tQuestion Loss: 91.4313 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 39\tNet Loss: 90.9943 \tQuestion Loss: 90.9943 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 39\tNet Loss: 95.5810 \tQuestion Loss: 95.5810 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 39\tNet Loss: 93.2269 \tQuestion Loss: 93.2269 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 39\tNet Loss: 92.7116 \tQuestion Loss: 92.7116 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 39\tNet Loss: 95.7269 \tQuestion Loss: 95.7269 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 39\tNet Loss: 93.9973 \tQuestion Loss: 93.9973 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 39\tNet Loss: 94.9975 \tQuestion Loss: 94.9975 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 39\tNet Loss: 93.5613 \tQuestion Loss: 93.5613 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 39\tNet Loss: 86.8386 \tQuestion Loss: 86.8386 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 39\tNet Loss: 91.6416 \tQuestion Loss: 91.6416 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 39\tNet Loss: 96.1356 \tQuestion Loss: 96.1356 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 39\tNet Loss: 93.0187 \tQuestion Loss: 93.0187 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 39\tNet Loss: 94.2232 \tQuestion Loss: 94.2232 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 39\tNet Loss: 92.1695 \tQuestion Loss: 92.1695 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 39\tNet Loss: 89.6792 \tQuestion Loss: 89.6792 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 39\tNet Loss: 88.7435 \tQuestion Loss: 88.7435 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 39\tNet Loss: 92.1006 \tQuestion Loss: 92.1006 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 39\tNet Loss: 90.6291 \tQuestion Loss: 90.6291 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 39\tNet Loss: 92.7703 \tQuestion Loss: 92.7703 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 39\tNet Loss: 91.7731 \tQuestion Loss: 91.7731 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 39\tNet Loss: 94.4365 \tQuestion Loss: 94.4365 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 39\tNet Loss: 90.1562 \tQuestion Loss: 90.1562 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 39\tNet Loss: 95.3700 \tQuestion Loss: 95.3700 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 39\tNet Loss: 92.9636 \tQuestion Loss: 92.9636 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 39\tNet Loss: 91.4724 \tQuestion Loss: 91.4724 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 39\tNet Loss: 92.1052 \tQuestion Loss: 92.1052 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 39\tNet Loss: 92.7669 \tQuestion Loss: 92.7669 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 39\tNet Loss: 93.4641 \tQuestion Loss: 93.4641 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 39\tNet Loss: 91.9643 \tQuestion Loss: 91.9643 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 39\tNet Loss: 95.8978 \tQuestion Loss: 95.8978 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 39\tNet Loss: 92.9068 \tQuestion Loss: 92.9068 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 39\tNet Loss: 93.1979 \tQuestion Loss: 93.1979 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 39\tNet Loss: 92.4467 \tQuestion Loss: 92.4467 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 39\tNet Loss: 95.5986 \tQuestion Loss: 95.5986 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 39\tNet Loss: 93.0913 \tQuestion Loss: 93.0913 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 39\tNet Loss: 92.5627 \tQuestion Loss: 92.5627 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 39\tNet Loss: 95.2277 \tQuestion Loss: 95.2277 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 39\tNet Loss: 96.5768 \tQuestion Loss: 96.5768 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 39\tNet Loss: 92.1959 \tQuestion Loss: 92.1959 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 39\tNet Loss: 89.2800 \tQuestion Loss: 89.2800 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 39\tNet Loss: 88.4123 \tQuestion Loss: 88.4123 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 39\tNet Loss: 92.7783 \tQuestion Loss: 92.7783 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 39\tNet Loss: 93.3279 \tQuestion Loss: 93.3279 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 39\tNet Loss: 87.9915 \tQuestion Loss: 87.9915 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 39\tNet Loss: 89.7749 \tQuestion Loss: 89.7749 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 39\tNet Loss: 97.0703 \tQuestion Loss: 97.0703 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 39\tNet Loss: 94.0579 \tQuestion Loss: 94.0579 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 39\tNet Loss: 91.1318 \tQuestion Loss: 91.1318 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 39\tNet Loss: 88.5158 \tQuestion Loss: 88.5158 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 39\tNet Loss: 90.7937 \tQuestion Loss: 90.7937 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 39\tNet Loss: 93.9699 \tQuestion Loss: 93.9699 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 39\tNet Loss: 89.6012 \tQuestion Loss: 89.6012 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 39\tNet Loss: 91.4721 \tQuestion Loss: 91.4721 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 39\tNet Loss: 93.9882 \tQuestion Loss: 93.9882 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 39\tNet Loss: 96.9102 \tQuestion Loss: 96.9102 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 39\tNet Loss: 92.7148 \tQuestion Loss: 92.7148 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 39\tNet Loss: 94.6653 \tQuestion Loss: 94.6653 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 39\tNet Loss: 92.8106 \tQuestion Loss: 92.8106 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 39\tNet Loss: 96.4023 \tQuestion Loss: 96.4023 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 39\tNet Loss: 88.0541 \tQuestion Loss: 88.0541 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 39\tNet Loss: 93.3183 \tQuestion Loss: 93.3183 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 39\tNet Loss: 93.1300 \tQuestion Loss: 93.1300 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 39\tNet Loss: 100.1969 \tQuestion Loss: 100.1969 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 39\tNet Loss: 94.7665 \tQuestion Loss: 94.7665 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 39\tNet Loss: 91.1106 \tQuestion Loss: 91.1106 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 39\tNet Loss: 92.8941 \tQuestion Loss: 92.8941 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 134 \t Epoch : 39\tNet Loss: 92.8769 \tQuestion Loss: 92.8769 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 39\tNet Loss: 86.4465 \tQuestion Loss: 86.4465 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 39\tNet Loss: 97.3935 \tQuestion Loss: 97.3935 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 39\tNet Loss: 95.0043 \tQuestion Loss: 95.0043 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 39\tNet Loss: 93.4009 \tQuestion Loss: 93.4009 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 39\tNet Loss: 92.1360 \tQuestion Loss: 92.1360 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 39\tNet Loss: 89.3330 \tQuestion Loss: 89.3330 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 39\tNet Loss: 89.8085 \tQuestion Loss: 89.8085 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 39\tNet Loss: 94.4138 \tQuestion Loss: 94.4138 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 39\tNet Loss: 89.4586 \tQuestion Loss: 89.4586 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 39\tNet Loss: 87.1414 \tQuestion Loss: 87.1414 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 39\tNet Loss: 95.6125 \tQuestion Loss: 95.6125 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 39\tNet Loss: 98.7603 \tQuestion Loss: 98.7603 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 39\tNet Loss: 87.0731 \tQuestion Loss: 87.0731 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 39\tNet Loss: 96.5135 \tQuestion Loss: 96.5135 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 39\tNet Loss: 90.1163 \tQuestion Loss: 90.1163 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 39\tNet Loss: 91.2592 \tQuestion Loss: 91.2592 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 39\tNet Loss: 98.0146 \tQuestion Loss: 98.0146 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 39\tNet Loss: 93.9018 \tQuestion Loss: 93.9018 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 39\tNet Loss: 94.4320 \tQuestion Loss: 94.4320 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 39\tNet Loss: 85.5298 \tQuestion Loss: 85.5298 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 39\tNet Loss: 90.5429 \tQuestion Loss: 90.5429 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 39\tNet Loss: 93.6051 \tQuestion Loss: 93.6051 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 39\tNet Loss: 88.3580 \tQuestion Loss: 88.3580 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 39\tNet Loss: 93.7096 \tQuestion Loss: 93.7096 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 39\tNet Loss: 94.8253 \tQuestion Loss: 94.8253 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 39\tNet Loss: 95.9693 \tQuestion Loss: 95.9693 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 39\tNet Loss: 94.3004 \tQuestion Loss: 94.3004 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 39\tNet Loss: 95.9819 \tQuestion Loss: 95.9819 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 39\tNet Loss: 89.7174 \tQuestion Loss: 89.7174 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 39\tNet Loss: 88.6055 \tQuestion Loss: 88.6055 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 39\tNet Loss: 89.7443 \tQuestion Loss: 89.7443 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 39\tNet Loss: 87.5078 \tQuestion Loss: 87.5078 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 39\tNet Loss: 93.1601 \tQuestion Loss: 93.1601 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 39\tNet Loss: 91.5161 \tQuestion Loss: 91.5161 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 39\tNet Loss: 95.9110 \tQuestion Loss: 95.9110 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 39\tNet Loss: 93.5886 \tQuestion Loss: 93.5886 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 39\tNet Loss: 92.6520 \tQuestion Loss: 92.6520 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 39\tNet Loss: 88.9860 \tQuestion Loss: 88.9860 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 39\tNet Loss: 90.6536 \tQuestion Loss: 90.6536 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 39\tNet Loss: 95.0658 \tQuestion Loss: 95.0658 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 39\tNet Loss: 97.7571 \tQuestion Loss: 97.7571 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 39\tNet Loss: 90.1275 \tQuestion Loss: 90.1275 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 39\tNet Loss: 91.6601 \tQuestion Loss: 91.6601 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 39\tNet Loss: 90.4937 \tQuestion Loss: 90.4937 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 39\tNet Loss: 92.0509 \tQuestion Loss: 92.0509 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 39\tNet Loss: 90.1536 \tQuestion Loss: 90.1536 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 39\tNet Loss: 92.4724 \tQuestion Loss: 92.4724 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 39\tNet Loss: 94.3597 \tQuestion Loss: 94.3597 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 39\tNet Loss: 89.6334 \tQuestion Loss: 89.6334 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 39\tNet Loss: 93.4681 \tQuestion Loss: 93.4681 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 39\tNet Loss: 91.6024 \tQuestion Loss: 91.6024 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 39\tNet Loss: 94.4252 \tQuestion Loss: 94.4252 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 39\tNet Loss: 95.3341 \tQuestion Loss: 95.3341 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 39\tNet Loss: 89.9677 \tQuestion Loss: 89.9677 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 39\tNet Loss: 90.1169 \tQuestion Loss: 90.1169 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 39\tNet Loss: 91.1655 \tQuestion Loss: 91.1655 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 39\tNet Loss: 90.0773 \tQuestion Loss: 90.0773 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 39\tNet Loss: 94.9609 \tQuestion Loss: 94.9609 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 39\tNet Loss: 93.3762 \tQuestion Loss: 93.3762 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 39\tNet Loss: 95.0160 \tQuestion Loss: 95.0160 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 39\tNet Loss: 87.9585 \tQuestion Loss: 87.9585 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 39\tNet Loss: 89.2547 \tQuestion Loss: 89.2547 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 39\tNet Loss: 94.6886 \tQuestion Loss: 94.6886 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 39\tNet Loss: 87.9648 \tQuestion Loss: 87.9648 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 39\tNet Loss: 92.3935 \tQuestion Loss: 92.3935 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 39 : 92.3518 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 40\tNet Loss: 92.7756 \tQuestion Loss: 92.7756 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 40\tNet Loss: 89.2375 \tQuestion Loss: 89.2375 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 40\tNet Loss: 94.4331 \tQuestion Loss: 94.4331 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 40\tNet Loss: 93.3483 \tQuestion Loss: 93.3483 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 40\tNet Loss: 93.8454 \tQuestion Loss: 93.8454 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 40\tNet Loss: 93.7502 \tQuestion Loss: 93.7502 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 40\tNet Loss: 88.8457 \tQuestion Loss: 88.8457 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 40\tNet Loss: 94.8930 \tQuestion Loss: 94.8930 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 40\tNet Loss: 93.3889 \tQuestion Loss: 93.3889 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 40\tNet Loss: 95.3354 \tQuestion Loss: 95.3354 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 40\tNet Loss: 91.3163 \tQuestion Loss: 91.3163 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 40\tNet Loss: 95.6278 \tQuestion Loss: 95.6278 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 40\tNet Loss: 90.3875 \tQuestion Loss: 90.3875 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 40\tNet Loss: 92.6879 \tQuestion Loss: 92.6879 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 40\tNet Loss: 93.1726 \tQuestion Loss: 93.1726 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 40\tNet Loss: 95.4633 \tQuestion Loss: 95.4633 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 40\tNet Loss: 90.4143 \tQuestion Loss: 90.4143 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 40\tNet Loss: 92.4481 \tQuestion Loss: 92.4481 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 40\tNet Loss: 91.1649 \tQuestion Loss: 91.1649 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 40\tNet Loss: 86.8853 \tQuestion Loss: 86.8853 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 40\tNet Loss: 96.1855 \tQuestion Loss: 96.1855 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 40\tNet Loss: 88.2034 \tQuestion Loss: 88.2034 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 40\tNet Loss: 92.7942 \tQuestion Loss: 92.7942 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 40\tNet Loss: 95.8482 \tQuestion Loss: 95.8482 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 40\tNet Loss: 89.8205 \tQuestion Loss: 89.8205 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 25 \t Epoch : 40\tNet Loss: 93.2419 \tQuestion Loss: 93.2419 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 40\tNet Loss: 98.3580 \tQuestion Loss: 98.3580 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 40\tNet Loss: 91.8849 \tQuestion Loss: 91.8849 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 40\tNet Loss: 87.0692 \tQuestion Loss: 87.0692 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 40\tNet Loss: 89.8978 \tQuestion Loss: 89.8978 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 40\tNet Loss: 91.5684 \tQuestion Loss: 91.5684 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 40\tNet Loss: 90.3089 \tQuestion Loss: 90.3089 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 40\tNet Loss: 89.0990 \tQuestion Loss: 89.0990 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 40\tNet Loss: 93.3133 \tQuestion Loss: 93.3133 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 40\tNet Loss: 87.4058 \tQuestion Loss: 87.4058 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 40\tNet Loss: 90.1386 \tQuestion Loss: 90.1386 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 40\tNet Loss: 93.9164 \tQuestion Loss: 93.9164 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 40\tNet Loss: 95.0080 \tQuestion Loss: 95.0080 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 40\tNet Loss: 91.0607 \tQuestion Loss: 91.0607 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 40\tNet Loss: 94.1991 \tQuestion Loss: 94.1991 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 40\tNet Loss: 91.4934 \tQuestion Loss: 91.4934 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 40\tNet Loss: 90.3267 \tQuestion Loss: 90.3267 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 40\tNet Loss: 92.1751 \tQuestion Loss: 92.1751 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 40\tNet Loss: 93.5076 \tQuestion Loss: 93.5076 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 40\tNet Loss: 90.7368 \tQuestion Loss: 90.7368 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 40\tNet Loss: 93.6671 \tQuestion Loss: 93.6671 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 40\tNet Loss: 92.1024 \tQuestion Loss: 92.1024 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 40\tNet Loss: 97.2024 \tQuestion Loss: 97.2024 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 40\tNet Loss: 86.7935 \tQuestion Loss: 86.7935 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 40\tNet Loss: 94.3306 \tQuestion Loss: 94.3306 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 40\tNet Loss: 92.9501 \tQuestion Loss: 92.9501 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 40\tNet Loss: 88.5175 \tQuestion Loss: 88.5175 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 40\tNet Loss: 89.5325 \tQuestion Loss: 89.5325 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 40\tNet Loss: 89.9719 \tQuestion Loss: 89.9719 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 40\tNet Loss: 96.4258 \tQuestion Loss: 96.4258 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 40\tNet Loss: 93.1233 \tQuestion Loss: 93.1233 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 40\tNet Loss: 94.3670 \tQuestion Loss: 94.3670 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 40\tNet Loss: 96.6139 \tQuestion Loss: 96.6139 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 40\tNet Loss: 87.4968 \tQuestion Loss: 87.4968 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 40\tNet Loss: 92.1876 \tQuestion Loss: 92.1876 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 40\tNet Loss: 89.1495 \tQuestion Loss: 89.1495 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 40\tNet Loss: 95.2285 \tQuestion Loss: 95.2285 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 40\tNet Loss: 88.8338 \tQuestion Loss: 88.8338 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 40\tNet Loss: 93.4939 \tQuestion Loss: 93.4939 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 40\tNet Loss: 92.0754 \tQuestion Loss: 92.0754 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 40\tNet Loss: 90.2005 \tQuestion Loss: 90.2005 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 40\tNet Loss: 92.5042 \tQuestion Loss: 92.5042 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 40\tNet Loss: 92.0438 \tQuestion Loss: 92.0438 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 40\tNet Loss: 90.7773 \tQuestion Loss: 90.7773 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 40\tNet Loss: 95.9253 \tQuestion Loss: 95.9253 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 40\tNet Loss: 93.2412 \tQuestion Loss: 93.2412 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 40\tNet Loss: 92.8851 \tQuestion Loss: 92.8851 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 40\tNet Loss: 95.5402 \tQuestion Loss: 95.5402 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 40\tNet Loss: 93.2782 \tQuestion Loss: 93.2782 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 40\tNet Loss: 94.6878 \tQuestion Loss: 94.6878 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 40\tNet Loss: 93.5084 \tQuestion Loss: 93.5084 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 40\tNet Loss: 86.7962 \tQuestion Loss: 86.7962 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 40\tNet Loss: 91.4183 \tQuestion Loss: 91.4183 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 40\tNet Loss: 97.0678 \tQuestion Loss: 97.0678 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 40\tNet Loss: 92.8000 \tQuestion Loss: 92.8000 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 40\tNet Loss: 94.3004 \tQuestion Loss: 94.3004 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 40\tNet Loss: 92.0488 \tQuestion Loss: 92.0488 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 40\tNet Loss: 89.6128 \tQuestion Loss: 89.6128 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 40\tNet Loss: 88.5294 \tQuestion Loss: 88.5294 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 40\tNet Loss: 91.8590 \tQuestion Loss: 91.8590 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 40\tNet Loss: 90.0241 \tQuestion Loss: 90.0241 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 40\tNet Loss: 91.6838 \tQuestion Loss: 91.6838 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 40\tNet Loss: 91.5948 \tQuestion Loss: 91.5948 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 40\tNet Loss: 94.4908 \tQuestion Loss: 94.4908 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 40\tNet Loss: 90.0594 \tQuestion Loss: 90.0594 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 40\tNet Loss: 95.2517 \tQuestion Loss: 95.2517 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 40\tNet Loss: 92.9983 \tQuestion Loss: 92.9983 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 40\tNet Loss: 91.4713 \tQuestion Loss: 91.4713 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 40\tNet Loss: 91.7430 \tQuestion Loss: 91.7430 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 40\tNet Loss: 92.4134 \tQuestion Loss: 92.4134 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 40\tNet Loss: 93.4400 \tQuestion Loss: 93.4400 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 40\tNet Loss: 91.8830 \tQuestion Loss: 91.8830 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 40\tNet Loss: 95.6786 \tQuestion Loss: 95.6786 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 40\tNet Loss: 92.4851 \tQuestion Loss: 92.4851 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 40\tNet Loss: 92.9270 \tQuestion Loss: 92.9270 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 40\tNet Loss: 92.6499 \tQuestion Loss: 92.6499 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 40\tNet Loss: 95.5240 \tQuestion Loss: 95.5240 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 40\tNet Loss: 92.9525 \tQuestion Loss: 92.9525 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 40\tNet Loss: 92.5081 \tQuestion Loss: 92.5081 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 40\tNet Loss: 95.0772 \tQuestion Loss: 95.0772 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 40\tNet Loss: 96.5486 \tQuestion Loss: 96.5486 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 40\tNet Loss: 92.2378 \tQuestion Loss: 92.2378 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 40\tNet Loss: 89.2384 \tQuestion Loss: 89.2384 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 40\tNet Loss: 88.4598 \tQuestion Loss: 88.4598 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 40\tNet Loss: 92.9815 \tQuestion Loss: 92.9815 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 40\tNet Loss: 93.2934 \tQuestion Loss: 93.2934 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 40\tNet Loss: 87.5579 \tQuestion Loss: 87.5579 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 40\tNet Loss: 89.7790 \tQuestion Loss: 89.7790 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 40\tNet Loss: 97.2509 \tQuestion Loss: 97.2509 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 40\tNet Loss: 94.2815 \tQuestion Loss: 94.2815 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 40\tNet Loss: 91.2609 \tQuestion Loss: 91.2609 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 40\tNet Loss: 88.1087 \tQuestion Loss: 88.1087 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 117 \t Epoch : 40\tNet Loss: 90.4328 \tQuestion Loss: 90.4328 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 40\tNet Loss: 93.7652 \tQuestion Loss: 93.7652 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 40\tNet Loss: 89.7144 \tQuestion Loss: 89.7144 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 40\tNet Loss: 91.5875 \tQuestion Loss: 91.5875 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 40\tNet Loss: 93.3585 \tQuestion Loss: 93.3585 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 40\tNet Loss: 97.6228 \tQuestion Loss: 97.6228 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 40\tNet Loss: 92.8947 \tQuestion Loss: 92.8947 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 40\tNet Loss: 94.8327 \tQuestion Loss: 94.8327 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 40\tNet Loss: 92.5921 \tQuestion Loss: 92.5921 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 40\tNet Loss: 96.1038 \tQuestion Loss: 96.1038 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 40\tNet Loss: 87.9716 \tQuestion Loss: 87.9716 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 40\tNet Loss: 92.2571 \tQuestion Loss: 92.2571 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 40\tNet Loss: 94.8921 \tQuestion Loss: 94.8921 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 40\tNet Loss: 99.9166 \tQuestion Loss: 99.9166 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 40\tNet Loss: 94.8779 \tQuestion Loss: 94.8779 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 40\tNet Loss: 91.3336 \tQuestion Loss: 91.3336 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 40\tNet Loss: 92.6232 \tQuestion Loss: 92.6232 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 40\tNet Loss: 93.5270 \tQuestion Loss: 93.5270 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 40\tNet Loss: 85.6816 \tQuestion Loss: 85.6816 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 40\tNet Loss: 97.3348 \tQuestion Loss: 97.3348 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 40\tNet Loss: 95.1168 \tQuestion Loss: 95.1168 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 40\tNet Loss: 93.2370 \tQuestion Loss: 93.2370 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 40\tNet Loss: 92.1442 \tQuestion Loss: 92.1442 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 40\tNet Loss: 89.2480 \tQuestion Loss: 89.2480 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 40\tNet Loss: 89.7138 \tQuestion Loss: 89.7138 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 40\tNet Loss: 94.5321 \tQuestion Loss: 94.5321 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 40\tNet Loss: 89.3762 \tQuestion Loss: 89.3762 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 40\tNet Loss: 86.9061 \tQuestion Loss: 86.9061 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 40\tNet Loss: 95.3349 \tQuestion Loss: 95.3349 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 40\tNet Loss: 98.6040 \tQuestion Loss: 98.6040 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 40\tNet Loss: 86.9485 \tQuestion Loss: 86.9485 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 40\tNet Loss: 96.5730 \tQuestion Loss: 96.5730 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 40\tNet Loss: 90.0462 \tQuestion Loss: 90.0462 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 40\tNet Loss: 91.0860 \tQuestion Loss: 91.0860 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 40\tNet Loss: 97.9605 \tQuestion Loss: 97.9605 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 40\tNet Loss: 93.7030 \tQuestion Loss: 93.7030 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 40\tNet Loss: 94.0775 \tQuestion Loss: 94.0775 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 40\tNet Loss: 84.9767 \tQuestion Loss: 84.9767 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 40\tNet Loss: 90.5851 \tQuestion Loss: 90.5851 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 40\tNet Loss: 93.5364 \tQuestion Loss: 93.5364 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 40\tNet Loss: 88.4851 \tQuestion Loss: 88.4851 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 40\tNet Loss: 93.7515 \tQuestion Loss: 93.7515 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 40\tNet Loss: 94.6663 \tQuestion Loss: 94.6663 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 40\tNet Loss: 95.8956 \tQuestion Loss: 95.8956 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 40\tNet Loss: 94.1626 \tQuestion Loss: 94.1626 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 40\tNet Loss: 95.7986 \tQuestion Loss: 95.7986 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 40\tNet Loss: 89.6074 \tQuestion Loss: 89.6074 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 40\tNet Loss: 88.5398 \tQuestion Loss: 88.5398 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 40\tNet Loss: 89.7538 \tQuestion Loss: 89.7538 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 40\tNet Loss: 87.4810 \tQuestion Loss: 87.4810 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 40\tNet Loss: 92.7224 \tQuestion Loss: 92.7224 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 40\tNet Loss: 91.5398 \tQuestion Loss: 91.5398 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 40\tNet Loss: 95.7662 \tQuestion Loss: 95.7662 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 40\tNet Loss: 93.5180 \tQuestion Loss: 93.5180 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 40\tNet Loss: 92.4696 \tQuestion Loss: 92.4696 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 40\tNet Loss: 89.2889 \tQuestion Loss: 89.2889 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 40\tNet Loss: 90.6458 \tQuestion Loss: 90.6458 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 40\tNet Loss: 94.6999 \tQuestion Loss: 94.6999 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 40\tNet Loss: 97.5941 \tQuestion Loss: 97.5941 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 40\tNet Loss: 90.1079 \tQuestion Loss: 90.1079 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 40\tNet Loss: 91.5271 \tQuestion Loss: 91.5271 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 40\tNet Loss: 90.5176 \tQuestion Loss: 90.5176 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 40\tNet Loss: 91.6011 \tQuestion Loss: 91.6011 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 40\tNet Loss: 90.1013 \tQuestion Loss: 90.1013 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 40\tNet Loss: 92.3724 \tQuestion Loss: 92.3724 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 40\tNet Loss: 94.3482 \tQuestion Loss: 94.3482 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 40\tNet Loss: 89.3917 \tQuestion Loss: 89.3917 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 40\tNet Loss: 93.3343 \tQuestion Loss: 93.3343 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 40\tNet Loss: 91.6727 \tQuestion Loss: 91.6727 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 40\tNet Loss: 93.9122 \tQuestion Loss: 93.9122 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 40\tNet Loss: 94.9590 \tQuestion Loss: 94.9590 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 40\tNet Loss: 89.9634 \tQuestion Loss: 89.9634 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 40\tNet Loss: 89.5284 \tQuestion Loss: 89.5284 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 40\tNet Loss: 91.1132 \tQuestion Loss: 91.1132 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 40\tNet Loss: 90.0584 \tQuestion Loss: 90.0584 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 40\tNet Loss: 94.8778 \tQuestion Loss: 94.8778 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 40\tNet Loss: 93.4368 \tQuestion Loss: 93.4368 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 40\tNet Loss: 95.2492 \tQuestion Loss: 95.2492 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 40\tNet Loss: 87.9879 \tQuestion Loss: 87.9879 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 40\tNet Loss: 89.2417 \tQuestion Loss: 89.2417 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 40\tNet Loss: 94.6368 \tQuestion Loss: 94.6368 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 40\tNet Loss: 87.7649 \tQuestion Loss: 87.7649 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 40\tNet Loss: 92.4503 \tQuestion Loss: 92.4503 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 40 : 92.3075 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 41\tNet Loss: 92.7291 \tQuestion Loss: 92.7291 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 41\tNet Loss: 88.9614 \tQuestion Loss: 88.9614 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 41\tNet Loss: 94.2633 \tQuestion Loss: 94.2633 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 41\tNet Loss: 93.7716 \tQuestion Loss: 93.7716 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 41\tNet Loss: 93.7971 \tQuestion Loss: 93.7971 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 41\tNet Loss: 93.6353 \tQuestion Loss: 93.6353 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 41\tNet Loss: 88.8175 \tQuestion Loss: 88.8175 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 41\tNet Loss: 94.9055 \tQuestion Loss: 94.9055 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 8 \t Epoch : 41\tNet Loss: 93.5837 \tQuestion Loss: 93.5837 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 41\tNet Loss: 94.6904 \tQuestion Loss: 94.6904 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 41\tNet Loss: 91.0168 \tQuestion Loss: 91.0168 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 41\tNet Loss: 95.6341 \tQuestion Loss: 95.6341 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 41\tNet Loss: 90.4091 \tQuestion Loss: 90.4091 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 41\tNet Loss: 92.7934 \tQuestion Loss: 92.7934 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 41\tNet Loss: 92.9483 \tQuestion Loss: 92.9483 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 41\tNet Loss: 95.4967 \tQuestion Loss: 95.4967 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 41\tNet Loss: 90.5755 \tQuestion Loss: 90.5755 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 41\tNet Loss: 92.4896 \tQuestion Loss: 92.4896 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 41\tNet Loss: 90.5180 \tQuestion Loss: 90.5180 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 41\tNet Loss: 86.7323 \tQuestion Loss: 86.7323 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 41\tNet Loss: 96.0753 \tQuestion Loss: 96.0753 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 41\tNet Loss: 88.1544 \tQuestion Loss: 88.1544 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 41\tNet Loss: 92.9461 \tQuestion Loss: 92.9461 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 41\tNet Loss: 95.9389 \tQuestion Loss: 95.9389 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 41\tNet Loss: 89.8647 \tQuestion Loss: 89.8647 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 41\tNet Loss: 93.2592 \tQuestion Loss: 93.2592 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 41\tNet Loss: 98.4202 \tQuestion Loss: 98.4202 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 41\tNet Loss: 91.9156 \tQuestion Loss: 91.9156 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 41\tNet Loss: 86.9806 \tQuestion Loss: 86.9806 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 41\tNet Loss: 89.9032 \tQuestion Loss: 89.9032 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 41\tNet Loss: 91.6764 \tQuestion Loss: 91.6764 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 41\tNet Loss: 90.4762 \tQuestion Loss: 90.4762 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 41\tNet Loss: 88.8647 \tQuestion Loss: 88.8647 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 41\tNet Loss: 93.3158 \tQuestion Loss: 93.3158 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 41\tNet Loss: 87.3162 \tQuestion Loss: 87.3162 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 41\tNet Loss: 90.1229 \tQuestion Loss: 90.1229 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 41\tNet Loss: 93.8514 \tQuestion Loss: 93.8514 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 41\tNet Loss: 95.4400 \tQuestion Loss: 95.4400 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 41\tNet Loss: 91.2525 \tQuestion Loss: 91.2525 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 41\tNet Loss: 93.8622 \tQuestion Loss: 93.8622 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 41\tNet Loss: 91.5520 \tQuestion Loss: 91.5520 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 41\tNet Loss: 90.2513 \tQuestion Loss: 90.2513 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 41\tNet Loss: 92.0613 \tQuestion Loss: 92.0613 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 41\tNet Loss: 93.3369 \tQuestion Loss: 93.3369 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 41\tNet Loss: 90.5746 \tQuestion Loss: 90.5746 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 41\tNet Loss: 93.6740 \tQuestion Loss: 93.6740 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 41\tNet Loss: 92.3626 \tQuestion Loss: 92.3626 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 41\tNet Loss: 97.3474 \tQuestion Loss: 97.3474 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 41\tNet Loss: 86.0617 \tQuestion Loss: 86.0617 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 41\tNet Loss: 94.1227 \tQuestion Loss: 94.1227 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 41\tNet Loss: 92.9751 \tQuestion Loss: 92.9751 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 41\tNet Loss: 88.5265 \tQuestion Loss: 88.5265 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 41\tNet Loss: 89.5365 \tQuestion Loss: 89.5365 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 41\tNet Loss: 89.8235 \tQuestion Loss: 89.8235 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 41\tNet Loss: 96.5136 \tQuestion Loss: 96.5136 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 41\tNet Loss: 93.3169 \tQuestion Loss: 93.3169 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 41\tNet Loss: 94.4311 \tQuestion Loss: 94.4311 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 41\tNet Loss: 96.6212 \tQuestion Loss: 96.6212 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 41\tNet Loss: 87.6815 \tQuestion Loss: 87.6815 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 41\tNet Loss: 92.1418 \tQuestion Loss: 92.1418 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 41\tNet Loss: 89.0725 \tQuestion Loss: 89.0725 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 41\tNet Loss: 95.2785 \tQuestion Loss: 95.2785 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 41\tNet Loss: 88.8873 \tQuestion Loss: 88.8873 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 41\tNet Loss: 93.5276 \tQuestion Loss: 93.5276 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 41\tNet Loss: 92.1459 \tQuestion Loss: 92.1459 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 41\tNet Loss: 90.8611 \tQuestion Loss: 90.8611 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 41\tNet Loss: 92.1357 \tQuestion Loss: 92.1357 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 41\tNet Loss: 91.4066 \tQuestion Loss: 91.4066 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 41\tNet Loss: 90.9223 \tQuestion Loss: 90.9223 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 41\tNet Loss: 95.6295 \tQuestion Loss: 95.6295 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 41\tNet Loss: 93.2641 \tQuestion Loss: 93.2641 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 41\tNet Loss: 92.7313 \tQuestion Loss: 92.7313 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 41\tNet Loss: 95.7436 \tQuestion Loss: 95.7436 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 41\tNet Loss: 93.9886 \tQuestion Loss: 93.9886 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 41\tNet Loss: 94.9927 \tQuestion Loss: 94.9927 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 41\tNet Loss: 93.5443 \tQuestion Loss: 93.5443 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 41\tNet Loss: 86.8715 \tQuestion Loss: 86.8715 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 41\tNet Loss: 91.6745 \tQuestion Loss: 91.6745 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 41\tNet Loss: 96.1181 \tQuestion Loss: 96.1181 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 41\tNet Loss: 92.9746 \tQuestion Loss: 92.9746 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 41\tNet Loss: 94.2164 \tQuestion Loss: 94.2164 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 41\tNet Loss: 92.1942 \tQuestion Loss: 92.1942 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 41\tNet Loss: 89.6296 \tQuestion Loss: 89.6296 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 41\tNet Loss: 88.6926 \tQuestion Loss: 88.6926 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 41\tNet Loss: 92.0807 \tQuestion Loss: 92.0807 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 41\tNet Loss: 90.6235 \tQuestion Loss: 90.6235 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 41\tNet Loss: 92.7318 \tQuestion Loss: 92.7318 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 41\tNet Loss: 91.7142 \tQuestion Loss: 91.7142 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 41\tNet Loss: 94.4370 \tQuestion Loss: 94.4370 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 41\tNet Loss: 90.0955 \tQuestion Loss: 90.0955 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 41\tNet Loss: 95.3616 \tQuestion Loss: 95.3616 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 41\tNet Loss: 92.9969 \tQuestion Loss: 92.9969 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 41\tNet Loss: 91.4415 \tQuestion Loss: 91.4415 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 41\tNet Loss: 92.0920 \tQuestion Loss: 92.0920 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 41\tNet Loss: 92.7454 \tQuestion Loss: 92.7454 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 41\tNet Loss: 93.4992 \tQuestion Loss: 93.4992 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 41\tNet Loss: 91.9878 \tQuestion Loss: 91.9878 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 41\tNet Loss: 95.8664 \tQuestion Loss: 95.8664 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 41\tNet Loss: 92.8810 \tQuestion Loss: 92.8810 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 41\tNet Loss: 93.1853 \tQuestion Loss: 93.1853 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 41\tNet Loss: 92.4204 \tQuestion Loss: 92.4204 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 101 \t Epoch : 41\tNet Loss: 95.5833 \tQuestion Loss: 95.5833 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 41\tNet Loss: 93.1168 \tQuestion Loss: 93.1168 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 41\tNet Loss: 92.5542 \tQuestion Loss: 92.5542 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 41\tNet Loss: 95.1948 \tQuestion Loss: 95.1948 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 41\tNet Loss: 96.6094 \tQuestion Loss: 96.6094 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 41\tNet Loss: 92.2250 \tQuestion Loss: 92.2250 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 41\tNet Loss: 89.2484 \tQuestion Loss: 89.2484 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 41\tNet Loss: 88.4421 \tQuestion Loss: 88.4421 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 41\tNet Loss: 92.7882 \tQuestion Loss: 92.7882 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 41\tNet Loss: 93.3401 \tQuestion Loss: 93.3401 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 41\tNet Loss: 87.9220 \tQuestion Loss: 87.9220 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 41\tNet Loss: 89.7612 \tQuestion Loss: 89.7612 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 41\tNet Loss: 97.0555 \tQuestion Loss: 97.0555 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 41\tNet Loss: 94.0429 \tQuestion Loss: 94.0429 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 41\tNet Loss: 91.1151 \tQuestion Loss: 91.1151 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 41\tNet Loss: 88.4862 \tQuestion Loss: 88.4862 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 41\tNet Loss: 90.6903 \tQuestion Loss: 90.6903 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 41\tNet Loss: 93.9156 \tQuestion Loss: 93.9156 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 41\tNet Loss: 89.6196 \tQuestion Loss: 89.6196 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 41\tNet Loss: 91.4840 \tQuestion Loss: 91.4840 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 41\tNet Loss: 93.9769 \tQuestion Loss: 93.9769 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 41\tNet Loss: 96.8535 \tQuestion Loss: 96.8535 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 41\tNet Loss: 92.7199 \tQuestion Loss: 92.7199 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 41\tNet Loss: 94.6981 \tQuestion Loss: 94.6981 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 41\tNet Loss: 92.7904 \tQuestion Loss: 92.7904 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 41\tNet Loss: 96.4014 \tQuestion Loss: 96.4014 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 41\tNet Loss: 88.0468 \tQuestion Loss: 88.0468 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 41\tNet Loss: 93.3025 \tQuestion Loss: 93.3025 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 41\tNet Loss: 93.1207 \tQuestion Loss: 93.1207 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 41\tNet Loss: 100.1463 \tQuestion Loss: 100.1463 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 41\tNet Loss: 94.7897 \tQuestion Loss: 94.7897 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 41\tNet Loss: 91.1434 \tQuestion Loss: 91.1434 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 41\tNet Loss: 92.8354 \tQuestion Loss: 92.8354 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 41\tNet Loss: 92.8751 \tQuestion Loss: 92.8751 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 41\tNet Loss: 86.4109 \tQuestion Loss: 86.4109 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 41\tNet Loss: 97.4063 \tQuestion Loss: 97.4063 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 41\tNet Loss: 95.0058 \tQuestion Loss: 95.0058 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 41\tNet Loss: 93.4076 \tQuestion Loss: 93.4076 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 41\tNet Loss: 92.1641 \tQuestion Loss: 92.1641 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 41\tNet Loss: 89.3293 \tQuestion Loss: 89.3293 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 41\tNet Loss: 89.8155 \tQuestion Loss: 89.8155 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 41\tNet Loss: 94.4165 \tQuestion Loss: 94.4165 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 41\tNet Loss: 89.4936 \tQuestion Loss: 89.4936 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 41\tNet Loss: 87.0747 \tQuestion Loss: 87.0747 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 41\tNet Loss: 95.6714 \tQuestion Loss: 95.6714 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 41\tNet Loss: 98.7404 \tQuestion Loss: 98.7404 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 41\tNet Loss: 87.1131 \tQuestion Loss: 87.1131 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 41\tNet Loss: 96.4916 \tQuestion Loss: 96.4916 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 41\tNet Loss: 90.0625 \tQuestion Loss: 90.0625 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 41\tNet Loss: 91.2054 \tQuestion Loss: 91.2054 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 41\tNet Loss: 98.0048 \tQuestion Loss: 98.0048 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 41\tNet Loss: 93.9174 \tQuestion Loss: 93.9174 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 41\tNet Loss: 94.4087 \tQuestion Loss: 94.4087 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 41\tNet Loss: 85.5121 \tQuestion Loss: 85.5121 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 41\tNet Loss: 90.5958 \tQuestion Loss: 90.5958 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 41\tNet Loss: 93.5653 \tQuestion Loss: 93.5653 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 41\tNet Loss: 88.3267 \tQuestion Loss: 88.3267 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 41\tNet Loss: 93.6873 \tQuestion Loss: 93.6873 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 41\tNet Loss: 94.8250 \tQuestion Loss: 94.8250 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 41\tNet Loss: 95.9570 \tQuestion Loss: 95.9570 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 41\tNet Loss: 94.2513 \tQuestion Loss: 94.2513 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 41\tNet Loss: 95.9517 \tQuestion Loss: 95.9517 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 41\tNet Loss: 89.7286 \tQuestion Loss: 89.7286 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 41\tNet Loss: 88.6055 \tQuestion Loss: 88.6055 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 41\tNet Loss: 89.7283 \tQuestion Loss: 89.7283 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 41\tNet Loss: 87.4985 \tQuestion Loss: 87.4985 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 41\tNet Loss: 93.1383 \tQuestion Loss: 93.1383 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 41\tNet Loss: 91.4978 \tQuestion Loss: 91.4978 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 41\tNet Loss: 95.8607 \tQuestion Loss: 95.8607 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 41\tNet Loss: 93.5960 \tQuestion Loss: 93.5960 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 41\tNet Loss: 92.6487 \tQuestion Loss: 92.6487 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 41\tNet Loss: 88.9945 \tQuestion Loss: 88.9945 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 41\tNet Loss: 90.6288 \tQuestion Loss: 90.6288 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 41\tNet Loss: 95.0569 \tQuestion Loss: 95.0569 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 41\tNet Loss: 97.7558 \tQuestion Loss: 97.7558 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 41\tNet Loss: 90.1155 \tQuestion Loss: 90.1155 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 41\tNet Loss: 91.6475 \tQuestion Loss: 91.6475 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 41\tNet Loss: 90.5018 \tQuestion Loss: 90.5018 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 41\tNet Loss: 92.0694 \tQuestion Loss: 92.0694 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 41\tNet Loss: 90.1291 \tQuestion Loss: 90.1291 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 41\tNet Loss: 92.4548 \tQuestion Loss: 92.4548 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 41\tNet Loss: 94.3539 \tQuestion Loss: 94.3539 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 41\tNet Loss: 89.6228 \tQuestion Loss: 89.6228 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 41\tNet Loss: 93.4570 \tQuestion Loss: 93.4570 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 41\tNet Loss: 91.5808 \tQuestion Loss: 91.5808 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 41\tNet Loss: 94.4028 \tQuestion Loss: 94.4028 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 41\tNet Loss: 95.2945 \tQuestion Loss: 95.2945 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 41\tNet Loss: 89.9295 \tQuestion Loss: 89.9295 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 41\tNet Loss: 90.1224 \tQuestion Loss: 90.1224 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 41\tNet Loss: 91.1667 \tQuestion Loss: 91.1667 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 41\tNet Loss: 90.0880 \tQuestion Loss: 90.0880 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 41\tNet Loss: 94.9668 \tQuestion Loss: 94.9668 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 193 \t Epoch : 41\tNet Loss: 93.3882 \tQuestion Loss: 93.3882 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 41\tNet Loss: 95.0156 \tQuestion Loss: 95.0156 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 41\tNet Loss: 87.9349 \tQuestion Loss: 87.9349 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 41\tNet Loss: 89.2329 \tQuestion Loss: 89.2329 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 41\tNet Loss: 94.6645 \tQuestion Loss: 94.6645 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 41\tNet Loss: 87.9611 \tQuestion Loss: 87.9611 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 41\tNet Loss: 92.3708 \tQuestion Loss: 92.3708 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 41 : 92.3495 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 42\tNet Loss: 92.7635 \tQuestion Loss: 92.7635 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 42\tNet Loss: 89.2208 \tQuestion Loss: 89.2208 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 42\tNet Loss: 94.4149 \tQuestion Loss: 94.4149 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 42\tNet Loss: 93.3262 \tQuestion Loss: 93.3262 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 42\tNet Loss: 93.8421 \tQuestion Loss: 93.8421 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 42\tNet Loss: 93.7462 \tQuestion Loss: 93.7462 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 42\tNet Loss: 88.8535 \tQuestion Loss: 88.8535 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 42\tNet Loss: 94.9071 \tQuestion Loss: 94.9071 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 42\tNet Loss: 93.3757 \tQuestion Loss: 93.3757 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 42\tNet Loss: 95.3423 \tQuestion Loss: 95.3423 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 42\tNet Loss: 91.2914 \tQuestion Loss: 91.2914 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 42\tNet Loss: 95.6371 \tQuestion Loss: 95.6371 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 42\tNet Loss: 90.4079 \tQuestion Loss: 90.4079 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 42\tNet Loss: 92.7058 \tQuestion Loss: 92.7058 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 42\tNet Loss: 93.1366 \tQuestion Loss: 93.1366 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 42\tNet Loss: 95.4517 \tQuestion Loss: 95.4517 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 42\tNet Loss: 90.4202 \tQuestion Loss: 90.4202 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 42\tNet Loss: 92.4783 \tQuestion Loss: 92.4783 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 42\tNet Loss: 91.1173 \tQuestion Loss: 91.1173 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 42\tNet Loss: 86.8715 \tQuestion Loss: 86.8715 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 42\tNet Loss: 96.1842 \tQuestion Loss: 96.1842 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 42\tNet Loss: 88.2218 \tQuestion Loss: 88.2218 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 42\tNet Loss: 92.8249 \tQuestion Loss: 92.8249 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 42\tNet Loss: 95.8125 \tQuestion Loss: 95.8125 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 42\tNet Loss: 89.7741 \tQuestion Loss: 89.7741 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 42\tNet Loss: 93.2394 \tQuestion Loss: 93.2394 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 42\tNet Loss: 98.3670 \tQuestion Loss: 98.3670 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 42\tNet Loss: 91.9177 \tQuestion Loss: 91.9177 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 42\tNet Loss: 87.0420 \tQuestion Loss: 87.0420 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 42\tNet Loss: 89.9134 \tQuestion Loss: 89.9134 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 42\tNet Loss: 91.5701 \tQuestion Loss: 91.5701 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 42\tNet Loss: 90.3409 \tQuestion Loss: 90.3409 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 42\tNet Loss: 89.0761 \tQuestion Loss: 89.0761 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 42\tNet Loss: 93.3045 \tQuestion Loss: 93.3045 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 42\tNet Loss: 87.4530 \tQuestion Loss: 87.4530 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 42\tNet Loss: 90.1524 \tQuestion Loss: 90.1524 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 42\tNet Loss: 93.9685 \tQuestion Loss: 93.9685 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 42\tNet Loss: 94.9943 \tQuestion Loss: 94.9943 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 42\tNet Loss: 91.0187 \tQuestion Loss: 91.0187 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 42\tNet Loss: 94.2186 \tQuestion Loss: 94.2186 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 42\tNet Loss: 91.5014 \tQuestion Loss: 91.5014 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 42\tNet Loss: 90.3914 \tQuestion Loss: 90.3914 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 42\tNet Loss: 92.1646 \tQuestion Loss: 92.1646 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 42\tNet Loss: 93.5389 \tQuestion Loss: 93.5389 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 42\tNet Loss: 90.7465 \tQuestion Loss: 90.7465 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 42\tNet Loss: 93.6988 \tQuestion Loss: 93.6988 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 42\tNet Loss: 92.1031 \tQuestion Loss: 92.1031 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 42\tNet Loss: 97.1889 \tQuestion Loss: 97.1889 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 42\tNet Loss: 86.8134 \tQuestion Loss: 86.8134 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 42\tNet Loss: 94.3020 \tQuestion Loss: 94.3020 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 42\tNet Loss: 92.9546 \tQuestion Loss: 92.9546 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 42\tNet Loss: 88.5172 \tQuestion Loss: 88.5172 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 42\tNet Loss: 89.5424 \tQuestion Loss: 89.5424 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 42\tNet Loss: 89.9848 \tQuestion Loss: 89.9848 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 42\tNet Loss: 96.4411 \tQuestion Loss: 96.4411 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 42\tNet Loss: 93.0963 \tQuestion Loss: 93.0963 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 42\tNet Loss: 94.3708 \tQuestion Loss: 94.3708 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 42\tNet Loss: 96.6930 \tQuestion Loss: 96.6930 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 42\tNet Loss: 87.4901 \tQuestion Loss: 87.4901 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 42\tNet Loss: 92.1504 \tQuestion Loss: 92.1504 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 42\tNet Loss: 89.1119 \tQuestion Loss: 89.1119 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 42\tNet Loss: 95.2524 \tQuestion Loss: 95.2524 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 42\tNet Loss: 88.8449 \tQuestion Loss: 88.8449 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 42\tNet Loss: 93.4553 \tQuestion Loss: 93.4553 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 42\tNet Loss: 92.0542 \tQuestion Loss: 92.0542 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 42\tNet Loss: 90.3223 \tQuestion Loss: 90.3223 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 42\tNet Loss: 92.5063 \tQuestion Loss: 92.5063 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 42\tNet Loss: 92.0181 \tQuestion Loss: 92.0181 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 42\tNet Loss: 90.7499 \tQuestion Loss: 90.7499 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 42\tNet Loss: 95.9084 \tQuestion Loss: 95.9084 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 42\tNet Loss: 93.2107 \tQuestion Loss: 93.2107 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 42\tNet Loss: 92.9181 \tQuestion Loss: 92.9181 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 42\tNet Loss: 95.4746 \tQuestion Loss: 95.4746 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 42\tNet Loss: 93.2621 \tQuestion Loss: 93.2621 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 42\tNet Loss: 94.7168 \tQuestion Loss: 94.7168 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 42\tNet Loss: 93.5407 \tQuestion Loss: 93.5407 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 42\tNet Loss: 86.8163 \tQuestion Loss: 86.8163 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 42\tNet Loss: 91.3652 \tQuestion Loss: 91.3652 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 42\tNet Loss: 97.1086 \tQuestion Loss: 97.1086 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 42\tNet Loss: 92.8298 \tQuestion Loss: 92.8298 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 42\tNet Loss: 94.3393 \tQuestion Loss: 94.3393 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 42\tNet Loss: 92.0682 \tQuestion Loss: 92.0682 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 42\tNet Loss: 89.5739 \tQuestion Loss: 89.5739 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 42\tNet Loss: 88.5848 \tQuestion Loss: 88.5848 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 42\tNet Loss: 91.8965 \tQuestion Loss: 91.8965 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 85 \t Epoch : 42\tNet Loss: 90.0214 \tQuestion Loss: 90.0214 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 42\tNet Loss: 91.6609 \tQuestion Loss: 91.6609 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 42\tNet Loss: 91.6070 \tQuestion Loss: 91.6070 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 42\tNet Loss: 94.5120 \tQuestion Loss: 94.5120 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 42\tNet Loss: 90.0566 \tQuestion Loss: 90.0566 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 42\tNet Loss: 95.2893 \tQuestion Loss: 95.2893 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 42\tNet Loss: 92.9471 \tQuestion Loss: 92.9471 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 42\tNet Loss: 91.4378 \tQuestion Loss: 91.4378 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 42\tNet Loss: 91.7794 \tQuestion Loss: 91.7794 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 42\tNet Loss: 92.4398 \tQuestion Loss: 92.4398 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 42\tNet Loss: 93.4728 \tQuestion Loss: 93.4728 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 42\tNet Loss: 91.8914 \tQuestion Loss: 91.8914 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 42\tNet Loss: 95.6581 \tQuestion Loss: 95.6581 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 42\tNet Loss: 92.5275 \tQuestion Loss: 92.5275 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 42\tNet Loss: 92.9637 \tQuestion Loss: 92.9637 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 42\tNet Loss: 92.6194 \tQuestion Loss: 92.6194 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 42\tNet Loss: 95.5297 \tQuestion Loss: 95.5297 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 42\tNet Loss: 92.9711 \tQuestion Loss: 92.9711 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 42\tNet Loss: 92.5010 \tQuestion Loss: 92.5010 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 42\tNet Loss: 95.0800 \tQuestion Loss: 95.0800 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 42\tNet Loss: 96.5319 \tQuestion Loss: 96.5319 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 42\tNet Loss: 92.2200 \tQuestion Loss: 92.2200 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 42\tNet Loss: 89.2302 \tQuestion Loss: 89.2302 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 42\tNet Loss: 88.4268 \tQuestion Loss: 88.4268 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 42\tNet Loss: 92.9444 \tQuestion Loss: 92.9444 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 42\tNet Loss: 93.1762 \tQuestion Loss: 93.1762 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 42\tNet Loss: 87.5742 \tQuestion Loss: 87.5742 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 42\tNet Loss: 89.7687 \tQuestion Loss: 89.7687 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 42\tNet Loss: 97.2167 \tQuestion Loss: 97.2167 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 42\tNet Loss: 94.1927 \tQuestion Loss: 94.1927 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 42\tNet Loss: 91.2344 \tQuestion Loss: 91.2344 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 42\tNet Loss: 88.1098 \tQuestion Loss: 88.1098 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 42\tNet Loss: 90.4229 \tQuestion Loss: 90.4229 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 42\tNet Loss: 93.7676 \tQuestion Loss: 93.7676 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 42\tNet Loss: 89.6840 \tQuestion Loss: 89.6840 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 42\tNet Loss: 91.5182 \tQuestion Loss: 91.5182 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 42\tNet Loss: 93.3629 \tQuestion Loss: 93.3629 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 42\tNet Loss: 97.5890 \tQuestion Loss: 97.5890 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 42\tNet Loss: 92.8783 \tQuestion Loss: 92.8783 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 42\tNet Loss: 94.7695 \tQuestion Loss: 94.7695 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 42\tNet Loss: 92.5550 \tQuestion Loss: 92.5550 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 42\tNet Loss: 96.0522 \tQuestion Loss: 96.0522 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 42\tNet Loss: 87.9859 \tQuestion Loss: 87.9859 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 42\tNet Loss: 92.2885 \tQuestion Loss: 92.2885 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 42\tNet Loss: 94.8512 \tQuestion Loss: 94.8512 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 42\tNet Loss: 99.8639 \tQuestion Loss: 99.8639 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 42\tNet Loss: 94.8436 \tQuestion Loss: 94.8436 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 42\tNet Loss: 91.2810 \tQuestion Loss: 91.2810 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 42\tNet Loss: 92.6342 \tQuestion Loss: 92.6342 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 42\tNet Loss: 93.5150 \tQuestion Loss: 93.5150 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 42\tNet Loss: 85.6563 \tQuestion Loss: 85.6563 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 42\tNet Loss: 97.2679 \tQuestion Loss: 97.2679 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 42\tNet Loss: 95.0378 \tQuestion Loss: 95.0378 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 42\tNet Loss: 93.1997 \tQuestion Loss: 93.1997 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 42\tNet Loss: 92.1284 \tQuestion Loss: 92.1284 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 42\tNet Loss: 89.2038 \tQuestion Loss: 89.2038 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 42\tNet Loss: 89.6806 \tQuestion Loss: 89.6806 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 42\tNet Loss: 94.5358 \tQuestion Loss: 94.5358 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 42\tNet Loss: 89.3602 \tQuestion Loss: 89.3602 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 42\tNet Loss: 86.8711 \tQuestion Loss: 86.8711 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 42\tNet Loss: 95.2854 \tQuestion Loss: 95.2854 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 42\tNet Loss: 98.5975 \tQuestion Loss: 98.5975 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 42\tNet Loss: 87.0662 \tQuestion Loss: 87.0662 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 42\tNet Loss: 96.5312 \tQuestion Loss: 96.5312 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 42\tNet Loss: 90.0066 \tQuestion Loss: 90.0066 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 42\tNet Loss: 91.1090 \tQuestion Loss: 91.1090 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 42\tNet Loss: 98.0409 \tQuestion Loss: 98.0409 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 42\tNet Loss: 93.7246 \tQuestion Loss: 93.7246 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 42\tNet Loss: 94.0170 \tQuestion Loss: 94.0170 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 42\tNet Loss: 84.9225 \tQuestion Loss: 84.9225 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 42\tNet Loss: 90.5274 \tQuestion Loss: 90.5274 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 42\tNet Loss: 93.4776 \tQuestion Loss: 93.4776 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 42\tNet Loss: 88.4190 \tQuestion Loss: 88.4190 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 42\tNet Loss: 93.6742 \tQuestion Loss: 93.6742 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 42\tNet Loss: 94.6182 \tQuestion Loss: 94.6182 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 42\tNet Loss: 95.8513 \tQuestion Loss: 95.8513 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 42\tNet Loss: 94.0121 \tQuestion Loss: 94.0121 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 42\tNet Loss: 95.7062 \tQuestion Loss: 95.7062 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 42\tNet Loss: 89.5446 \tQuestion Loss: 89.5446 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 42\tNet Loss: 88.4798 \tQuestion Loss: 88.4798 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 42\tNet Loss: 89.6076 \tQuestion Loss: 89.6076 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 42\tNet Loss: 87.4127 \tQuestion Loss: 87.4127 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 42\tNet Loss: 92.6550 \tQuestion Loss: 92.6550 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 42\tNet Loss: 91.4539 \tQuestion Loss: 91.4539 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 42\tNet Loss: 95.6830 \tQuestion Loss: 95.6830 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 42\tNet Loss: 93.4884 \tQuestion Loss: 93.4884 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 42\tNet Loss: 92.3781 \tQuestion Loss: 92.3781 \t Time Taken: 1 seconds\n",
      "Batch: 172 \t Epoch : 42\tNet Loss: 89.1186 \tQuestion Loss: 89.1186 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 42\tNet Loss: 90.5425 \tQuestion Loss: 90.5425 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 42\tNet Loss: 94.6121 \tQuestion Loss: 94.6121 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 42\tNet Loss: 97.5580 \tQuestion Loss: 97.5580 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 42\tNet Loss: 90.0678 \tQuestion Loss: 90.0678 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 177 \t Epoch : 42\tNet Loss: 91.4265 \tQuestion Loss: 91.4265 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 42\tNet Loss: 90.4130 \tQuestion Loss: 90.4130 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 42\tNet Loss: 91.5066 \tQuestion Loss: 91.5066 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 42\tNet Loss: 89.9803 \tQuestion Loss: 89.9803 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 42\tNet Loss: 92.3052 \tQuestion Loss: 92.3052 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 42\tNet Loss: 94.3665 \tQuestion Loss: 94.3665 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 42\tNet Loss: 89.3298 \tQuestion Loss: 89.3298 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 42\tNet Loss: 93.2445 \tQuestion Loss: 93.2445 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 42\tNet Loss: 91.6016 \tQuestion Loss: 91.6016 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 42\tNet Loss: 93.8235 \tQuestion Loss: 93.8235 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 42\tNet Loss: 94.9128 \tQuestion Loss: 94.9128 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 42\tNet Loss: 89.9262 \tQuestion Loss: 89.9262 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 42\tNet Loss: 89.4224 \tQuestion Loss: 89.4224 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 42\tNet Loss: 91.0407 \tQuestion Loss: 91.0407 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 42\tNet Loss: 89.9503 \tQuestion Loss: 89.9503 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 42\tNet Loss: 94.8224 \tQuestion Loss: 94.8224 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 42\tNet Loss: 93.3624 \tQuestion Loss: 93.3624 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 42\tNet Loss: 95.1487 \tQuestion Loss: 95.1487 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 42\tNet Loss: 87.9231 \tQuestion Loss: 87.9231 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 42\tNet Loss: 89.1505 \tQuestion Loss: 89.1505 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 42\tNet Loss: 94.5591 \tQuestion Loss: 94.5591 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 42\tNet Loss: 87.7643 \tQuestion Loss: 87.7643 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 42\tNet Loss: 92.4020 \tQuestion Loss: 92.4020 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 42 : 92.2865 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 43\tNet Loss: 92.6342 \tQuestion Loss: 92.6342 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 43\tNet Loss: 88.8161 \tQuestion Loss: 88.8161 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 43\tNet Loss: 93.9675 \tQuestion Loss: 93.9675 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 43\tNet Loss: 93.8287 \tQuestion Loss: 93.8287 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 43\tNet Loss: 93.5549 \tQuestion Loss: 93.5549 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 43\tNet Loss: 93.3698 \tQuestion Loss: 93.3698 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 43\tNet Loss: 88.6524 \tQuestion Loss: 88.6524 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 43\tNet Loss: 94.7802 \tQuestion Loss: 94.7802 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 43\tNet Loss: 93.5534 \tQuestion Loss: 93.5534 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 43\tNet Loss: 94.6788 \tQuestion Loss: 94.6788 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 43\tNet Loss: 90.8974 \tQuestion Loss: 90.8974 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 43\tNet Loss: 95.6003 \tQuestion Loss: 95.6003 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 43\tNet Loss: 90.4175 \tQuestion Loss: 90.4175 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 43\tNet Loss: 92.7283 \tQuestion Loss: 92.7283 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 43\tNet Loss: 92.9040 \tQuestion Loss: 92.9040 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 43\tNet Loss: 95.4677 \tQuestion Loss: 95.4677 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 43\tNet Loss: 90.4922 \tQuestion Loss: 90.4922 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 43\tNet Loss: 92.3631 \tQuestion Loss: 92.3631 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 43\tNet Loss: 90.4606 \tQuestion Loss: 90.4606 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 43\tNet Loss: 86.6645 \tQuestion Loss: 86.6645 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 43\tNet Loss: 95.9875 \tQuestion Loss: 95.9875 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 43\tNet Loss: 88.1326 \tQuestion Loss: 88.1326 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 43\tNet Loss: 92.9390 \tQuestion Loss: 92.9390 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 43\tNet Loss: 95.8761 \tQuestion Loss: 95.8761 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 43\tNet Loss: 89.7616 \tQuestion Loss: 89.7616 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 43\tNet Loss: 93.2071 \tQuestion Loss: 93.2071 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 43\tNet Loss: 98.3244 \tQuestion Loss: 98.3244 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 43\tNet Loss: 91.7848 \tQuestion Loss: 91.7848 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 43\tNet Loss: 86.7974 \tQuestion Loss: 86.7974 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 43\tNet Loss: 89.7653 \tQuestion Loss: 89.7653 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 43\tNet Loss: 91.5253 \tQuestion Loss: 91.5253 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 43\tNet Loss: 90.4354 \tQuestion Loss: 90.4354 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 43\tNet Loss: 88.6846 \tQuestion Loss: 88.6846 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 43\tNet Loss: 93.1716 \tQuestion Loss: 93.1716 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 43\tNet Loss: 87.2042 \tQuestion Loss: 87.2042 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 43\tNet Loss: 90.0634 \tQuestion Loss: 90.0634 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 43\tNet Loss: 93.8146 \tQuestion Loss: 93.8146 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 43\tNet Loss: 95.3465 \tQuestion Loss: 95.3465 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 43\tNet Loss: 91.1686 \tQuestion Loss: 91.1686 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 43\tNet Loss: 93.7217 \tQuestion Loss: 93.7217 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 43\tNet Loss: 91.4803 \tQuestion Loss: 91.4803 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 43\tNet Loss: 90.1617 \tQuestion Loss: 90.1617 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 43\tNet Loss: 91.8862 \tQuestion Loss: 91.8862 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 43\tNet Loss: 93.2815 \tQuestion Loss: 93.2815 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 43\tNet Loss: 90.5160 \tQuestion Loss: 90.5160 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 43\tNet Loss: 93.6171 \tQuestion Loss: 93.6171 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 43\tNet Loss: 92.2329 \tQuestion Loss: 92.2329 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 43\tNet Loss: 97.2322 \tQuestion Loss: 97.2322 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 43\tNet Loss: 86.0695 \tQuestion Loss: 86.0695 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 43\tNet Loss: 93.9569 \tQuestion Loss: 93.9569 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 43\tNet Loss: 92.9066 \tQuestion Loss: 92.9066 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 43\tNet Loss: 88.4003 \tQuestion Loss: 88.4003 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 43\tNet Loss: 89.4381 \tQuestion Loss: 89.4381 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 43\tNet Loss: 89.7409 \tQuestion Loss: 89.7409 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 43\tNet Loss: 96.4252 \tQuestion Loss: 96.4252 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 43\tNet Loss: 93.1911 \tQuestion Loss: 93.1911 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 43\tNet Loss: 94.3100 \tQuestion Loss: 94.3100 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 43\tNet Loss: 96.5382 \tQuestion Loss: 96.5382 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 43\tNet Loss: 87.6880 \tQuestion Loss: 87.6880 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 43\tNet Loss: 92.1336 \tQuestion Loss: 92.1336 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 43\tNet Loss: 89.0664 \tQuestion Loss: 89.0664 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 43\tNet Loss: 95.2064 \tQuestion Loss: 95.2064 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 43\tNet Loss: 88.8450 \tQuestion Loss: 88.8450 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 43\tNet Loss: 93.4910 \tQuestion Loss: 93.4910 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 43\tNet Loss: 92.1158 \tQuestion Loss: 92.1158 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 43\tNet Loss: 90.7042 \tQuestion Loss: 90.7042 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 43\tNet Loss: 92.0868 \tQuestion Loss: 92.0868 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 43\tNet Loss: 91.3606 \tQuestion Loss: 91.3606 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 43\tNet Loss: 90.8760 \tQuestion Loss: 90.8760 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 69 \t Epoch : 43\tNet Loss: 95.5819 \tQuestion Loss: 95.5819 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 43\tNet Loss: 93.2202 \tQuestion Loss: 93.2202 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 43\tNet Loss: 92.6871 \tQuestion Loss: 92.6871 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 43\tNet Loss: 95.6722 \tQuestion Loss: 95.6722 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 43\tNet Loss: 93.9639 \tQuestion Loss: 93.9639 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 43\tNet Loss: 94.8382 \tQuestion Loss: 94.8382 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 43\tNet Loss: 93.5163 \tQuestion Loss: 93.5163 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 43\tNet Loss: 86.7920 \tQuestion Loss: 86.7920 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 43\tNet Loss: 91.6126 \tQuestion Loss: 91.6126 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 43\tNet Loss: 96.1117 \tQuestion Loss: 96.1117 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 43\tNet Loss: 92.9124 \tQuestion Loss: 92.9124 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 43\tNet Loss: 94.1942 \tQuestion Loss: 94.1942 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 43\tNet Loss: 92.1634 \tQuestion Loss: 92.1634 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 43\tNet Loss: 89.5604 \tQuestion Loss: 89.5604 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 43\tNet Loss: 88.6388 \tQuestion Loss: 88.6388 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 43\tNet Loss: 92.0311 \tQuestion Loss: 92.0311 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 43\tNet Loss: 90.5852 \tQuestion Loss: 90.5852 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 43\tNet Loss: 92.6463 \tQuestion Loss: 92.6463 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 43\tNet Loss: 91.6373 \tQuestion Loss: 91.6373 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 43\tNet Loss: 94.3832 \tQuestion Loss: 94.3832 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 43\tNet Loss: 89.9918 \tQuestion Loss: 89.9918 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 43\tNet Loss: 95.2605 \tQuestion Loss: 95.2605 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 43\tNet Loss: 92.8518 \tQuestion Loss: 92.8518 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 43\tNet Loss: 91.3419 \tQuestion Loss: 91.3419 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 43\tNet Loss: 92.0441 \tQuestion Loss: 92.0441 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 43\tNet Loss: 92.7128 \tQuestion Loss: 92.7128 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 43\tNet Loss: 93.4713 \tQuestion Loss: 93.4713 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 43\tNet Loss: 91.8423 \tQuestion Loss: 91.8423 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 43\tNet Loss: 95.7895 \tQuestion Loss: 95.7895 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 43\tNet Loss: 92.7339 \tQuestion Loss: 92.7339 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 43\tNet Loss: 93.1501 \tQuestion Loss: 93.1501 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 43\tNet Loss: 92.4340 \tQuestion Loss: 92.4340 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 43\tNet Loss: 95.5383 \tQuestion Loss: 95.5383 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 43\tNet Loss: 93.0063 \tQuestion Loss: 93.0063 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 43\tNet Loss: 92.5268 \tQuestion Loss: 92.5268 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 43\tNet Loss: 95.1981 \tQuestion Loss: 95.1981 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 43\tNet Loss: 96.5245 \tQuestion Loss: 96.5245 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 43\tNet Loss: 92.1453 \tQuestion Loss: 92.1453 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 43\tNet Loss: 89.2013 \tQuestion Loss: 89.2013 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 43\tNet Loss: 88.3509 \tQuestion Loss: 88.3509 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 43\tNet Loss: 92.8083 \tQuestion Loss: 92.8083 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 43\tNet Loss: 93.3808 \tQuestion Loss: 93.3808 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 43\tNet Loss: 87.8671 \tQuestion Loss: 87.8671 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 43\tNet Loss: 89.7710 \tQuestion Loss: 89.7710 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 43\tNet Loss: 97.0642 \tQuestion Loss: 97.0642 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 43\tNet Loss: 94.0171 \tQuestion Loss: 94.0171 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 43\tNet Loss: 91.0697 \tQuestion Loss: 91.0697 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 43\tNet Loss: 88.4293 \tQuestion Loss: 88.4293 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 43\tNet Loss: 90.6472 \tQuestion Loss: 90.6472 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 43\tNet Loss: 93.8230 \tQuestion Loss: 93.8230 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 43\tNet Loss: 89.6476 \tQuestion Loss: 89.6476 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 43\tNet Loss: 91.4572 \tQuestion Loss: 91.4572 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 43\tNet Loss: 93.9138 \tQuestion Loss: 93.9138 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 43\tNet Loss: 96.8202 \tQuestion Loss: 96.8202 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 43\tNet Loss: 92.7092 \tQuestion Loss: 92.7092 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 43\tNet Loss: 94.7211 \tQuestion Loss: 94.7211 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 43\tNet Loss: 92.7123 \tQuestion Loss: 92.7123 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 43\tNet Loss: 96.3401 \tQuestion Loss: 96.3401 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 43\tNet Loss: 88.0417 \tQuestion Loss: 88.0417 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 43\tNet Loss: 93.2583 \tQuestion Loss: 93.2583 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 43\tNet Loss: 93.0639 \tQuestion Loss: 93.0639 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 43\tNet Loss: 100.1106 \tQuestion Loss: 100.1106 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 43\tNet Loss: 94.7964 \tQuestion Loss: 94.7964 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 43\tNet Loss: 91.1523 \tQuestion Loss: 91.1523 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 43\tNet Loss: 92.7774 \tQuestion Loss: 92.7774 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 43\tNet Loss: 92.8036 \tQuestion Loss: 92.8036 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 43\tNet Loss: 86.4088 \tQuestion Loss: 86.4088 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 43\tNet Loss: 97.3345 \tQuestion Loss: 97.3345 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 43\tNet Loss: 95.0428 \tQuestion Loss: 95.0428 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 43\tNet Loss: 93.3594 \tQuestion Loss: 93.3594 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 43\tNet Loss: 92.1325 \tQuestion Loss: 92.1325 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 43\tNet Loss: 89.3059 \tQuestion Loss: 89.3059 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 43\tNet Loss: 89.8010 \tQuestion Loss: 89.8010 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 43\tNet Loss: 94.4261 \tQuestion Loss: 94.4261 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 43\tNet Loss: 89.4302 \tQuestion Loss: 89.4302 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 43\tNet Loss: 87.0289 \tQuestion Loss: 87.0289 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 43\tNet Loss: 95.6987 \tQuestion Loss: 95.6987 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 43\tNet Loss: 98.6896 \tQuestion Loss: 98.6896 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 43\tNet Loss: 87.1215 \tQuestion Loss: 87.1215 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 43\tNet Loss: 96.4241 \tQuestion Loss: 96.4241 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 43\tNet Loss: 90.0150 \tQuestion Loss: 90.0150 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 43\tNet Loss: 91.1571 \tQuestion Loss: 91.1571 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 43\tNet Loss: 97.9672 \tQuestion Loss: 97.9672 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 43\tNet Loss: 93.7314 \tQuestion Loss: 93.7314 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 43\tNet Loss: 94.3488 \tQuestion Loss: 94.3488 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 43\tNet Loss: 85.4537 \tQuestion Loss: 85.4537 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 43\tNet Loss: 90.5604 \tQuestion Loss: 90.5604 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 43\tNet Loss: 93.5836 \tQuestion Loss: 93.5836 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 43\tNet Loss: 88.2651 \tQuestion Loss: 88.2651 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 43\tNet Loss: 93.6447 \tQuestion Loss: 93.6447 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 43\tNet Loss: 94.8078 \tQuestion Loss: 94.8078 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 43\tNet Loss: 95.9668 \tQuestion Loss: 95.9668 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 161 \t Epoch : 43\tNet Loss: 94.2268 \tQuestion Loss: 94.2268 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 43\tNet Loss: 95.9450 \tQuestion Loss: 95.9450 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 43\tNet Loss: 89.7335 \tQuestion Loss: 89.7335 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 43\tNet Loss: 88.5077 \tQuestion Loss: 88.5077 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 43\tNet Loss: 89.6757 \tQuestion Loss: 89.6757 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 43\tNet Loss: 87.5197 \tQuestion Loss: 87.5197 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 43\tNet Loss: 93.1152 \tQuestion Loss: 93.1152 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 43\tNet Loss: 91.4814 \tQuestion Loss: 91.4814 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 43\tNet Loss: 95.8335 \tQuestion Loss: 95.8335 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 43\tNet Loss: 93.5737 \tQuestion Loss: 93.5737 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 43\tNet Loss: 92.5586 \tQuestion Loss: 92.5586 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 43\tNet Loss: 88.9613 \tQuestion Loss: 88.9613 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 43\tNet Loss: 90.5999 \tQuestion Loss: 90.5999 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 43\tNet Loss: 95.0623 \tQuestion Loss: 95.0623 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 43\tNet Loss: 97.7687 \tQuestion Loss: 97.7687 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 43\tNet Loss: 90.0972 \tQuestion Loss: 90.0972 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 43\tNet Loss: 91.6943 \tQuestion Loss: 91.6943 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 43\tNet Loss: 90.5185 \tQuestion Loss: 90.5185 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 43\tNet Loss: 92.0877 \tQuestion Loss: 92.0877 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 43\tNet Loss: 90.1448 \tQuestion Loss: 90.1448 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 43\tNet Loss: 92.4609 \tQuestion Loss: 92.4609 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 43\tNet Loss: 94.3601 \tQuestion Loss: 94.3601 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 43\tNet Loss: 89.5996 \tQuestion Loss: 89.5996 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 43\tNet Loss: 93.4886 \tQuestion Loss: 93.4886 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 43\tNet Loss: 91.6110 \tQuestion Loss: 91.6110 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 43\tNet Loss: 94.4189 \tQuestion Loss: 94.4189 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 43\tNet Loss: 95.2846 \tQuestion Loss: 95.2846 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 43\tNet Loss: 89.9333 \tQuestion Loss: 89.9333 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 43\tNet Loss: 90.1181 \tQuestion Loss: 90.1181 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 43\tNet Loss: 91.1561 \tQuestion Loss: 91.1561 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 43\tNet Loss: 90.0676 \tQuestion Loss: 90.0676 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 43\tNet Loss: 94.9715 \tQuestion Loss: 94.9715 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 43\tNet Loss: 93.4005 \tQuestion Loss: 93.4005 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 43\tNet Loss: 95.0406 \tQuestion Loss: 95.0406 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 43\tNet Loss: 87.9631 \tQuestion Loss: 87.9631 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 43\tNet Loss: 89.2742 \tQuestion Loss: 89.2742 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 43\tNet Loss: 94.6387 \tQuestion Loss: 94.6387 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 43\tNet Loss: 87.9408 \tQuestion Loss: 87.9408 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 43\tNet Loss: 92.3541 \tQuestion Loss: 92.3541 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 43 : 92.2973 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 44\tNet Loss: 92.7721 \tQuestion Loss: 92.7721 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 44\tNet Loss: 89.2238 \tQuestion Loss: 89.2238 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 44\tNet Loss: 94.3596 \tQuestion Loss: 94.3596 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 44\tNet Loss: 93.3552 \tQuestion Loss: 93.3552 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 44\tNet Loss: 93.9360 \tQuestion Loss: 93.9360 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 44\tNet Loss: 93.8032 \tQuestion Loss: 93.8032 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 44\tNet Loss: 88.8804 \tQuestion Loss: 88.8804 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 44\tNet Loss: 94.9498 \tQuestion Loss: 94.9498 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 44\tNet Loss: 93.4229 \tQuestion Loss: 93.4229 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 44\tNet Loss: 95.3186 \tQuestion Loss: 95.3186 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 44\tNet Loss: 91.3021 \tQuestion Loss: 91.3021 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 44\tNet Loss: 95.6323 \tQuestion Loss: 95.6323 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 44\tNet Loss: 90.4492 \tQuestion Loss: 90.4492 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 44\tNet Loss: 92.8242 \tQuestion Loss: 92.8242 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 44\tNet Loss: 93.1932 \tQuestion Loss: 93.1932 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 44\tNet Loss: 95.4184 \tQuestion Loss: 95.4184 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 44\tNet Loss: 90.4473 \tQuestion Loss: 90.4473 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 44\tNet Loss: 92.5982 \tQuestion Loss: 92.5982 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 44\tNet Loss: 91.1943 \tQuestion Loss: 91.1943 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 44\tNet Loss: 86.9090 \tQuestion Loss: 86.9090 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 44\tNet Loss: 96.1107 \tQuestion Loss: 96.1107 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 44\tNet Loss: 88.2582 \tQuestion Loss: 88.2582 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 44\tNet Loss: 92.9129 \tQuestion Loss: 92.9129 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 44\tNet Loss: 95.8426 \tQuestion Loss: 95.8426 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 44\tNet Loss: 89.7580 \tQuestion Loss: 89.7580 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 44\tNet Loss: 93.2324 \tQuestion Loss: 93.2324 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 44\tNet Loss: 98.4305 \tQuestion Loss: 98.4305 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 44\tNet Loss: 91.9529 \tQuestion Loss: 91.9529 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 44\tNet Loss: 87.0541 \tQuestion Loss: 87.0541 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 44\tNet Loss: 89.9713 \tQuestion Loss: 89.9713 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 44\tNet Loss: 91.6240 \tQuestion Loss: 91.6240 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 44\tNet Loss: 90.3882 \tQuestion Loss: 90.3882 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 44\tNet Loss: 89.0685 \tQuestion Loss: 89.0685 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 44\tNet Loss: 93.3352 \tQuestion Loss: 93.3352 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 44\tNet Loss: 87.4711 \tQuestion Loss: 87.4711 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 44\tNet Loss: 90.2020 \tQuestion Loss: 90.2020 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 44\tNet Loss: 94.0185 \tQuestion Loss: 94.0185 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 44\tNet Loss: 94.9794 \tQuestion Loss: 94.9794 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 44\tNet Loss: 91.0453 \tQuestion Loss: 91.0453 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 44\tNet Loss: 94.2744 \tQuestion Loss: 94.2744 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 44\tNet Loss: 91.5445 \tQuestion Loss: 91.5445 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 44\tNet Loss: 90.3914 \tQuestion Loss: 90.3914 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 44\tNet Loss: 92.1750 \tQuestion Loss: 92.1750 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 44\tNet Loss: 93.5674 \tQuestion Loss: 93.5674 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 44\tNet Loss: 90.7947 \tQuestion Loss: 90.7947 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 44\tNet Loss: 93.7266 \tQuestion Loss: 93.7266 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 44\tNet Loss: 92.0920 \tQuestion Loss: 92.0920 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 44\tNet Loss: 97.2291 \tQuestion Loss: 97.2291 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 44\tNet Loss: 86.8537 \tQuestion Loss: 86.8537 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 44\tNet Loss: 94.3572 \tQuestion Loss: 94.3572 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 44\tNet Loss: 92.9538 \tQuestion Loss: 92.9538 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 44\tNet Loss: 88.5342 \tQuestion Loss: 88.5342 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 44\tNet Loss: 89.5917 \tQuestion Loss: 89.5917 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 53 \t Epoch : 44\tNet Loss: 90.0189 \tQuestion Loss: 90.0189 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 44\tNet Loss: 96.4555 \tQuestion Loss: 96.4555 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 44\tNet Loss: 93.1520 \tQuestion Loss: 93.1520 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 44\tNet Loss: 94.4269 \tQuestion Loss: 94.4269 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 44\tNet Loss: 96.6639 \tQuestion Loss: 96.6639 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 44\tNet Loss: 87.5359 \tQuestion Loss: 87.5359 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 44\tNet Loss: 92.1166 \tQuestion Loss: 92.1166 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 44\tNet Loss: 89.1031 \tQuestion Loss: 89.1031 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 44\tNet Loss: 95.3173 \tQuestion Loss: 95.3173 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 44\tNet Loss: 88.8805 \tQuestion Loss: 88.8805 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 44\tNet Loss: 93.4801 \tQuestion Loss: 93.4801 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 44\tNet Loss: 92.0883 \tQuestion Loss: 92.0883 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 44\tNet Loss: 90.4528 \tQuestion Loss: 90.4528 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 44\tNet Loss: 92.5008 \tQuestion Loss: 92.5008 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 44\tNet Loss: 92.0398 \tQuestion Loss: 92.0398 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 44\tNet Loss: 90.7653 \tQuestion Loss: 90.7653 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 44\tNet Loss: 95.8946 \tQuestion Loss: 95.8946 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 44\tNet Loss: 93.2103 \tQuestion Loss: 93.2103 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 44\tNet Loss: 92.9061 \tQuestion Loss: 92.9061 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 44\tNet Loss: 95.4596 \tQuestion Loss: 95.4596 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 44\tNet Loss: 93.3108 \tQuestion Loss: 93.3108 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 44\tNet Loss: 94.8184 \tQuestion Loss: 94.8184 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 44\tNet Loss: 93.5501 \tQuestion Loss: 93.5501 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 44\tNet Loss: 86.7855 \tQuestion Loss: 86.7855 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 44\tNet Loss: 91.4299 \tQuestion Loss: 91.4299 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 44\tNet Loss: 97.1717 \tQuestion Loss: 97.1717 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 44\tNet Loss: 92.9236 \tQuestion Loss: 92.9236 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 44\tNet Loss: 94.3426 \tQuestion Loss: 94.3426 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 44\tNet Loss: 92.0755 \tQuestion Loss: 92.0755 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 44\tNet Loss: 89.6362 \tQuestion Loss: 89.6362 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 44\tNet Loss: 88.6953 \tQuestion Loss: 88.6953 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 44\tNet Loss: 91.9587 \tQuestion Loss: 91.9587 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 44\tNet Loss: 90.0606 \tQuestion Loss: 90.0606 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 44\tNet Loss: 91.6642 \tQuestion Loss: 91.6642 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 44\tNet Loss: 91.6760 \tQuestion Loss: 91.6760 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 44\tNet Loss: 94.5369 \tQuestion Loss: 94.5369 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 44\tNet Loss: 90.1404 \tQuestion Loss: 90.1404 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 44\tNet Loss: 95.3001 \tQuestion Loss: 95.3001 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 44\tNet Loss: 92.9548 \tQuestion Loss: 92.9548 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 44\tNet Loss: 91.4710 \tQuestion Loss: 91.4710 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 44\tNet Loss: 91.8456 \tQuestion Loss: 91.8456 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 44\tNet Loss: 92.4835 \tQuestion Loss: 92.4835 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 44\tNet Loss: 93.4735 \tQuestion Loss: 93.4735 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 44\tNet Loss: 91.9375 \tQuestion Loss: 91.9375 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 44\tNet Loss: 95.7598 \tQuestion Loss: 95.7598 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 44\tNet Loss: 92.6899 \tQuestion Loss: 92.6899 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 44\tNet Loss: 92.9833 \tQuestion Loss: 92.9833 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 44\tNet Loss: 92.6039 \tQuestion Loss: 92.6039 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 44\tNet Loss: 95.5364 \tQuestion Loss: 95.5364 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 44\tNet Loss: 93.0618 \tQuestion Loss: 93.0618 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 44\tNet Loss: 92.4924 \tQuestion Loss: 92.4924 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 44\tNet Loss: 95.0814 \tQuestion Loss: 95.0814 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 44\tNet Loss: 96.5879 \tQuestion Loss: 96.5879 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 44\tNet Loss: 92.3128 \tQuestion Loss: 92.3128 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 44\tNet Loss: 89.2932 \tQuestion Loss: 89.2932 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 44\tNet Loss: 88.5023 \tQuestion Loss: 88.5023 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 44\tNet Loss: 92.9317 \tQuestion Loss: 92.9317 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 44\tNet Loss: 93.2636 \tQuestion Loss: 93.2636 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 44\tNet Loss: 87.6751 \tQuestion Loss: 87.6751 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 44\tNet Loss: 89.7851 \tQuestion Loss: 89.7851 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 44\tNet Loss: 97.2408 \tQuestion Loss: 97.2408 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 44\tNet Loss: 94.3134 \tQuestion Loss: 94.3134 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 44\tNet Loss: 91.3295 \tQuestion Loss: 91.3295 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 44\tNet Loss: 88.1209 \tQuestion Loss: 88.1209 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 44\tNet Loss: 90.5383 \tQuestion Loss: 90.5383 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 44\tNet Loss: 93.8306 \tQuestion Loss: 93.8306 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 44\tNet Loss: 89.7915 \tQuestion Loss: 89.7915 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 44\tNet Loss: 91.5039 \tQuestion Loss: 91.5039 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 44\tNet Loss: 93.4352 \tQuestion Loss: 93.4352 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 44\tNet Loss: 97.6642 \tQuestion Loss: 97.6642 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 44\tNet Loss: 92.9714 \tQuestion Loss: 92.9714 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 44\tNet Loss: 94.7929 \tQuestion Loss: 94.7929 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 44\tNet Loss: 92.5985 \tQuestion Loss: 92.5985 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 44\tNet Loss: 96.1405 \tQuestion Loss: 96.1405 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 44\tNet Loss: 88.0441 \tQuestion Loss: 88.0441 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 44\tNet Loss: 92.2979 \tQuestion Loss: 92.2979 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 44\tNet Loss: 94.8529 \tQuestion Loss: 94.8529 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 44\tNet Loss: 99.9200 \tQuestion Loss: 99.9200 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 44\tNet Loss: 94.8627 \tQuestion Loss: 94.8627 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 44\tNet Loss: 91.3141 \tQuestion Loss: 91.3141 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 44\tNet Loss: 92.6984 \tQuestion Loss: 92.6984 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 44\tNet Loss: 93.5634 \tQuestion Loss: 93.5634 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 44\tNet Loss: 85.6957 \tQuestion Loss: 85.6957 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 44\tNet Loss: 97.2749 \tQuestion Loss: 97.2749 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 44\tNet Loss: 95.0181 \tQuestion Loss: 95.0181 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 44\tNet Loss: 93.2333 \tQuestion Loss: 93.2333 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 44\tNet Loss: 92.1360 \tQuestion Loss: 92.1360 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 44\tNet Loss: 89.2494 \tQuestion Loss: 89.2494 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 44\tNet Loss: 89.7188 \tQuestion Loss: 89.7188 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 44\tNet Loss: 94.5896 \tQuestion Loss: 94.5896 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 44\tNet Loss: 89.3619 \tQuestion Loss: 89.3619 \t Time Taken: 1 seconds\n",
      "Batch: 144 \t Epoch : 44\tNet Loss: 86.8969 \tQuestion Loss: 86.8969 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 145 \t Epoch : 44\tNet Loss: 95.3450 \tQuestion Loss: 95.3450 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 44\tNet Loss: 98.7200 \tQuestion Loss: 98.7200 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 44\tNet Loss: 87.0029 \tQuestion Loss: 87.0029 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 44\tNet Loss: 96.5592 \tQuestion Loss: 96.5592 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 44\tNet Loss: 90.0601 \tQuestion Loss: 90.0601 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 44\tNet Loss: 91.2187 \tQuestion Loss: 91.2187 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 44\tNet Loss: 98.0731 \tQuestion Loss: 98.0731 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 44\tNet Loss: 93.7908 \tQuestion Loss: 93.7908 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 44\tNet Loss: 94.1502 \tQuestion Loss: 94.1502 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 44\tNet Loss: 85.0857 \tQuestion Loss: 85.0857 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 44\tNet Loss: 90.6533 \tQuestion Loss: 90.6533 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 44\tNet Loss: 93.4821 \tQuestion Loss: 93.4821 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 44\tNet Loss: 88.4503 \tQuestion Loss: 88.4503 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 44\tNet Loss: 93.7468 \tQuestion Loss: 93.7468 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 44\tNet Loss: 94.7086 \tQuestion Loss: 94.7086 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 44\tNet Loss: 95.8726 \tQuestion Loss: 95.8726 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 44\tNet Loss: 94.1397 \tQuestion Loss: 94.1397 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 44\tNet Loss: 95.8635 \tQuestion Loss: 95.8635 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 44\tNet Loss: 89.6763 \tQuestion Loss: 89.6763 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 44\tNet Loss: 88.5652 \tQuestion Loss: 88.5652 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 44\tNet Loss: 89.7143 \tQuestion Loss: 89.7143 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 44\tNet Loss: 87.4557 \tQuestion Loss: 87.4557 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 44\tNet Loss: 92.7853 \tQuestion Loss: 92.7853 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 44\tNet Loss: 91.5132 \tQuestion Loss: 91.5132 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 44\tNet Loss: 95.7501 \tQuestion Loss: 95.7501 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 44\tNet Loss: 93.5633 \tQuestion Loss: 93.5633 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 44\tNet Loss: 92.5577 \tQuestion Loss: 92.5577 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 44\tNet Loss: 89.2865 \tQuestion Loss: 89.2865 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 44\tNet Loss: 90.6394 \tQuestion Loss: 90.6394 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 44\tNet Loss: 94.6763 \tQuestion Loss: 94.6763 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 44\tNet Loss: 97.5996 \tQuestion Loss: 97.5996 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 44\tNet Loss: 90.1254 \tQuestion Loss: 90.1254 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 44\tNet Loss: 91.5185 \tQuestion Loss: 91.5185 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 44\tNet Loss: 90.5231 \tQuestion Loss: 90.5231 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 44\tNet Loss: 91.5928 \tQuestion Loss: 91.5928 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 44\tNet Loss: 90.0854 \tQuestion Loss: 90.0854 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 44\tNet Loss: 92.3902 \tQuestion Loss: 92.3902 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 44\tNet Loss: 94.3414 \tQuestion Loss: 94.3414 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 44\tNet Loss: 89.3588 \tQuestion Loss: 89.3588 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 44\tNet Loss: 93.3180 \tQuestion Loss: 93.3180 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 44\tNet Loss: 91.6610 \tQuestion Loss: 91.6610 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 44\tNet Loss: 93.9027 \tQuestion Loss: 93.9027 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 44\tNet Loss: 94.9991 \tQuestion Loss: 94.9991 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 44\tNet Loss: 90.0145 \tQuestion Loss: 90.0145 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 44\tNet Loss: 89.4784 \tQuestion Loss: 89.4784 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 44\tNet Loss: 91.1016 \tQuestion Loss: 91.1016 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 44\tNet Loss: 90.0584 \tQuestion Loss: 90.0584 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 44\tNet Loss: 94.9109 \tQuestion Loss: 94.9109 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 44\tNet Loss: 93.4172 \tQuestion Loss: 93.4172 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 44\tNet Loss: 95.2073 \tQuestion Loss: 95.2073 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 44\tNet Loss: 87.9641 \tQuestion Loss: 87.9641 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 44\tNet Loss: 89.2425 \tQuestion Loss: 89.2425 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 44\tNet Loss: 94.6354 \tQuestion Loss: 94.6354 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 44\tNet Loss: 87.7220 \tQuestion Loss: 87.7220 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 44\tNet Loss: 92.4459 \tQuestion Loss: 92.4459 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 44 : 92.3336 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 45\tNet Loss: 92.7361 \tQuestion Loss: 92.7361 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 45\tNet Loss: 88.9749 \tQuestion Loss: 88.9749 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 45\tNet Loss: 94.2856 \tQuestion Loss: 94.2856 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 45\tNet Loss: 93.7603 \tQuestion Loss: 93.7603 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 45\tNet Loss: 93.6886 \tQuestion Loss: 93.6886 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 45\tNet Loss: 93.5895 \tQuestion Loss: 93.5895 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 45\tNet Loss: 88.7898 \tQuestion Loss: 88.7898 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 45\tNet Loss: 94.8709 \tQuestion Loss: 94.8709 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 45\tNet Loss: 93.5614 \tQuestion Loss: 93.5614 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 45\tNet Loss: 94.6982 \tQuestion Loss: 94.6982 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 45\tNet Loss: 91.0255 \tQuestion Loss: 91.0255 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 45\tNet Loss: 95.6521 \tQuestion Loss: 95.6521 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 45\tNet Loss: 90.3889 \tQuestion Loss: 90.3889 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 45\tNet Loss: 92.6568 \tQuestion Loss: 92.6568 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 45\tNet Loss: 92.8833 \tQuestion Loss: 92.8833 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 45\tNet Loss: 95.5272 \tQuestion Loss: 95.5272 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 45\tNet Loss: 90.5565 \tQuestion Loss: 90.5565 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 45\tNet Loss: 92.3873 \tQuestion Loss: 92.3873 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 45\tNet Loss: 90.3909 \tQuestion Loss: 90.3909 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 45\tNet Loss: 86.7323 \tQuestion Loss: 86.7323 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 45\tNet Loss: 96.1079 \tQuestion Loss: 96.1079 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 45\tNet Loss: 88.1869 \tQuestion Loss: 88.1869 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 45\tNet Loss: 92.8632 \tQuestion Loss: 92.8632 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 45\tNet Loss: 95.9331 \tQuestion Loss: 95.9331 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 45\tNet Loss: 89.8930 \tQuestion Loss: 89.8930 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 45\tNet Loss: 93.2935 \tQuestion Loss: 93.2935 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 45\tNet Loss: 98.4328 \tQuestion Loss: 98.4328 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 45\tNet Loss: 91.8873 \tQuestion Loss: 91.8873 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 45\tNet Loss: 86.9315 \tQuestion Loss: 86.9315 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 45\tNet Loss: 89.8730 \tQuestion Loss: 89.8730 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 45\tNet Loss: 91.6722 \tQuestion Loss: 91.6722 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 45\tNet Loss: 90.4446 \tQuestion Loss: 90.4446 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 45\tNet Loss: 88.8087 \tQuestion Loss: 88.8087 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 45\tNet Loss: 93.3052 \tQuestion Loss: 93.3052 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 45\tNet Loss: 87.3236 \tQuestion Loss: 87.3236 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 45\tNet Loss: 90.1003 \tQuestion Loss: 90.1003 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 36 \t Epoch : 45\tNet Loss: 93.7806 \tQuestion Loss: 93.7806 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 45\tNet Loss: 95.3964 \tQuestion Loss: 95.3964 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 45\tNet Loss: 91.2888 \tQuestion Loss: 91.2888 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 45\tNet Loss: 93.8362 \tQuestion Loss: 93.8362 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 45\tNet Loss: 91.5319 \tQuestion Loss: 91.5319 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 45\tNet Loss: 90.1692 \tQuestion Loss: 90.1692 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 45\tNet Loss: 92.0027 \tQuestion Loss: 92.0027 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 45\tNet Loss: 93.3492 \tQuestion Loss: 93.3492 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 45\tNet Loss: 90.5850 \tQuestion Loss: 90.5850 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 45\tNet Loss: 93.6532 \tQuestion Loss: 93.6532 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 45\tNet Loss: 92.2901 \tQuestion Loss: 92.2901 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 45\tNet Loss: 97.3223 \tQuestion Loss: 97.3223 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 45\tNet Loss: 86.0971 \tQuestion Loss: 86.0971 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 45\tNet Loss: 93.9468 \tQuestion Loss: 93.9468 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 45\tNet Loss: 92.9460 \tQuestion Loss: 92.9460 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 45\tNet Loss: 88.5095 \tQuestion Loss: 88.5095 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 45\tNet Loss: 89.5097 \tQuestion Loss: 89.5097 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 45\tNet Loss: 89.8011 \tQuestion Loss: 89.8011 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 45\tNet Loss: 96.4677 \tQuestion Loss: 96.4677 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 45\tNet Loss: 93.2769 \tQuestion Loss: 93.2769 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 45\tNet Loss: 94.3947 \tQuestion Loss: 94.3947 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 45\tNet Loss: 96.6042 \tQuestion Loss: 96.6042 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 45\tNet Loss: 87.6491 \tQuestion Loss: 87.6491 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 45\tNet Loss: 92.1220 \tQuestion Loss: 92.1220 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 45\tNet Loss: 89.1120 \tQuestion Loss: 89.1120 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 45\tNet Loss: 95.3041 \tQuestion Loss: 95.3041 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 45\tNet Loss: 88.8335 \tQuestion Loss: 88.8335 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 45\tNet Loss: 93.5020 \tQuestion Loss: 93.5020 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 45\tNet Loss: 92.1721 \tQuestion Loss: 92.1721 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 45\tNet Loss: 90.7772 \tQuestion Loss: 90.7772 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 45\tNet Loss: 92.1112 \tQuestion Loss: 92.1112 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 45\tNet Loss: 91.3564 \tQuestion Loss: 91.3564 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 45\tNet Loss: 90.8984 \tQuestion Loss: 90.8984 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 45\tNet Loss: 95.6403 \tQuestion Loss: 95.6403 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 45\tNet Loss: 93.3163 \tQuestion Loss: 93.3163 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 45\tNet Loss: 92.6550 \tQuestion Loss: 92.6550 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 45\tNet Loss: 95.7101 \tQuestion Loss: 95.7101 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 45\tNet Loss: 94.0097 \tQuestion Loss: 94.0097 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 45\tNet Loss: 94.9101 \tQuestion Loss: 94.9101 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 45\tNet Loss: 93.5386 \tQuestion Loss: 93.5386 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 45\tNet Loss: 86.8091 \tQuestion Loss: 86.8091 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 45\tNet Loss: 91.6642 \tQuestion Loss: 91.6642 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 45\tNet Loss: 96.1172 \tQuestion Loss: 96.1172 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 45\tNet Loss: 92.9331 \tQuestion Loss: 92.9331 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 45\tNet Loss: 94.2103 \tQuestion Loss: 94.2103 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 45\tNet Loss: 92.1611 \tQuestion Loss: 92.1611 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 45\tNet Loss: 89.6379 \tQuestion Loss: 89.6379 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 45\tNet Loss: 88.6315 \tQuestion Loss: 88.6315 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 45\tNet Loss: 92.0079 \tQuestion Loss: 92.0079 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 45\tNet Loss: 90.5765 \tQuestion Loss: 90.5765 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 45\tNet Loss: 92.6994 \tQuestion Loss: 92.6994 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 45\tNet Loss: 91.6917 \tQuestion Loss: 91.6917 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 45\tNet Loss: 94.4133 \tQuestion Loss: 94.4133 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 45\tNet Loss: 90.0226 \tQuestion Loss: 90.0226 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 45\tNet Loss: 95.3607 \tQuestion Loss: 95.3607 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 45\tNet Loss: 92.9072 \tQuestion Loss: 92.9072 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 45\tNet Loss: 91.3415 \tQuestion Loss: 91.3415 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 45\tNet Loss: 92.0218 \tQuestion Loss: 92.0218 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 45\tNet Loss: 92.7007 \tQuestion Loss: 92.7007 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 45\tNet Loss: 93.5211 \tQuestion Loss: 93.5211 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 45\tNet Loss: 91.8926 \tQuestion Loss: 91.8926 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 45\tNet Loss: 95.7840 \tQuestion Loss: 95.7840 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 45\tNet Loss: 92.7356 \tQuestion Loss: 92.7356 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 45\tNet Loss: 93.1826 \tQuestion Loss: 93.1826 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 45\tNet Loss: 92.4430 \tQuestion Loss: 92.4430 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 45\tNet Loss: 95.5831 \tQuestion Loss: 95.5831 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 45\tNet Loss: 93.0201 \tQuestion Loss: 93.0201 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 45\tNet Loss: 92.5538 \tQuestion Loss: 92.5538 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 45\tNet Loss: 95.1492 \tQuestion Loss: 95.1492 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 45\tNet Loss: 96.5444 \tQuestion Loss: 96.5444 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 45\tNet Loss: 92.1656 \tQuestion Loss: 92.1656 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 45\tNet Loss: 89.2161 \tQuestion Loss: 89.2161 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 45\tNet Loss: 88.3648 \tQuestion Loss: 88.3648 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 45\tNet Loss: 92.8896 \tQuestion Loss: 92.8896 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 45\tNet Loss: 93.3130 \tQuestion Loss: 93.3130 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 45\tNet Loss: 87.8381 \tQuestion Loss: 87.8381 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 45\tNet Loss: 89.7387 \tQuestion Loss: 89.7387 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 45\tNet Loss: 97.0967 \tQuestion Loss: 97.0967 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 45\tNet Loss: 94.0893 \tQuestion Loss: 94.0893 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 45\tNet Loss: 91.0746 \tQuestion Loss: 91.0746 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 45\tNet Loss: 88.4556 \tQuestion Loss: 88.4556 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 45\tNet Loss: 90.6069 \tQuestion Loss: 90.6069 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 45\tNet Loss: 93.8794 \tQuestion Loss: 93.8794 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 45\tNet Loss: 89.6743 \tQuestion Loss: 89.6743 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 45\tNet Loss: 91.4685 \tQuestion Loss: 91.4685 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 45\tNet Loss: 93.9426 \tQuestion Loss: 93.9426 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 45\tNet Loss: 96.8896 \tQuestion Loss: 96.8896 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 45\tNet Loss: 92.7708 \tQuestion Loss: 92.7708 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 45\tNet Loss: 94.6883 \tQuestion Loss: 94.6883 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 45\tNet Loss: 92.7422 \tQuestion Loss: 92.7422 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 45\tNet Loss: 96.3983 \tQuestion Loss: 96.3983 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 45\tNet Loss: 88.1004 \tQuestion Loss: 88.1004 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 128 \t Epoch : 45\tNet Loss: 93.2931 \tQuestion Loss: 93.2931 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 45\tNet Loss: 93.0606 \tQuestion Loss: 93.0606 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 45\tNet Loss: 100.1510 \tQuestion Loss: 100.1510 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 45\tNet Loss: 94.8863 \tQuestion Loss: 94.8863 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 45\tNet Loss: 91.1580 \tQuestion Loss: 91.1580 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 45\tNet Loss: 92.7560 \tQuestion Loss: 92.7560 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 45\tNet Loss: 92.8087 \tQuestion Loss: 92.8087 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 45\tNet Loss: 86.4755 \tQuestion Loss: 86.4755 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 45\tNet Loss: 97.3460 \tQuestion Loss: 97.3460 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 45\tNet Loss: 95.0648 \tQuestion Loss: 95.0648 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 45\tNet Loss: 93.4328 \tQuestion Loss: 93.4328 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 45\tNet Loss: 92.1597 \tQuestion Loss: 92.1597 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 45\tNet Loss: 89.3228 \tQuestion Loss: 89.3228 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 45\tNet Loss: 89.8101 \tQuestion Loss: 89.8101 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 45\tNet Loss: 94.4376 \tQuestion Loss: 94.4376 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 45\tNet Loss: 89.4419 \tQuestion Loss: 89.4419 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 45\tNet Loss: 87.0218 \tQuestion Loss: 87.0218 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 45\tNet Loss: 95.7103 \tQuestion Loss: 95.7103 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 45\tNet Loss: 98.7831 \tQuestion Loss: 98.7831 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 45\tNet Loss: 87.1345 \tQuestion Loss: 87.1345 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 45\tNet Loss: 96.4371 \tQuestion Loss: 96.4371 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 45\tNet Loss: 90.0555 \tQuestion Loss: 90.0555 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 45\tNet Loss: 91.2062 \tQuestion Loss: 91.2062 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 45\tNet Loss: 98.0018 \tQuestion Loss: 98.0018 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 45\tNet Loss: 93.7464 \tQuestion Loss: 93.7464 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 45\tNet Loss: 94.3642 \tQuestion Loss: 94.3642 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 45\tNet Loss: 85.5100 \tQuestion Loss: 85.5100 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 45\tNet Loss: 90.5716 \tQuestion Loss: 90.5716 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 45\tNet Loss: 93.5717 \tQuestion Loss: 93.5717 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 45\tNet Loss: 88.3043 \tQuestion Loss: 88.3043 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 45\tNet Loss: 93.7440 \tQuestion Loss: 93.7440 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 45\tNet Loss: 94.8314 \tQuestion Loss: 94.8314 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 45\tNet Loss: 95.9598 \tQuestion Loss: 95.9598 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 45\tNet Loss: 94.2299 \tQuestion Loss: 94.2299 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 45\tNet Loss: 95.9717 \tQuestion Loss: 95.9717 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 45\tNet Loss: 89.7638 \tQuestion Loss: 89.7638 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 45\tNet Loss: 88.5546 \tQuestion Loss: 88.5546 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 45\tNet Loss: 89.7183 \tQuestion Loss: 89.7183 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 45\tNet Loss: 87.5493 \tQuestion Loss: 87.5493 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 45\tNet Loss: 93.1243 \tQuestion Loss: 93.1243 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 45\tNet Loss: 91.4955 \tQuestion Loss: 91.4955 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 45\tNet Loss: 95.8450 \tQuestion Loss: 95.8450 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 45\tNet Loss: 93.6024 \tQuestion Loss: 93.6024 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 45\tNet Loss: 92.6447 \tQuestion Loss: 92.6447 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 45\tNet Loss: 88.9744 \tQuestion Loss: 88.9744 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 45\tNet Loss: 90.6134 \tQuestion Loss: 90.6134 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 45\tNet Loss: 95.0763 \tQuestion Loss: 95.0763 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 45\tNet Loss: 97.8059 \tQuestion Loss: 97.8059 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 45\tNet Loss: 90.1147 \tQuestion Loss: 90.1147 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 45\tNet Loss: 91.6810 \tQuestion Loss: 91.6810 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 45\tNet Loss: 90.5374 \tQuestion Loss: 90.5374 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 45\tNet Loss: 92.1061 \tQuestion Loss: 92.1061 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 45\tNet Loss: 90.1721 \tQuestion Loss: 90.1721 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 45\tNet Loss: 92.4560 \tQuestion Loss: 92.4560 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 45\tNet Loss: 94.3534 \tQuestion Loss: 94.3534 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 45\tNet Loss: 89.6543 \tQuestion Loss: 89.6543 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 45\tNet Loss: 93.4945 \tQuestion Loss: 93.4945 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 45\tNet Loss: 91.6099 \tQuestion Loss: 91.6099 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 45\tNet Loss: 94.4259 \tQuestion Loss: 94.4259 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 45\tNet Loss: 95.3236 \tQuestion Loss: 95.3236 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 45\tNet Loss: 89.9630 \tQuestion Loss: 89.9630 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 45\tNet Loss: 90.1454 \tQuestion Loss: 90.1454 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 45\tNet Loss: 91.1822 \tQuestion Loss: 91.1822 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 45\tNet Loss: 90.1024 \tQuestion Loss: 90.1024 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 45\tNet Loss: 95.0213 \tQuestion Loss: 95.0213 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 45\tNet Loss: 93.3955 \tQuestion Loss: 93.3955 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 45\tNet Loss: 95.0501 \tQuestion Loss: 95.0501 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 45\tNet Loss: 87.9995 \tQuestion Loss: 87.9995 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 45\tNet Loss: 89.2760 \tQuestion Loss: 89.2760 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 45\tNet Loss: 94.6622 \tQuestion Loss: 94.6622 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 45\tNet Loss: 87.9919 \tQuestion Loss: 87.9919 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 45\tNet Loss: 92.3730 \tQuestion Loss: 92.3730 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 45 : 92.3347 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 46\tNet Loss: 92.7749 \tQuestion Loss: 92.7749 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 46\tNet Loss: 89.2226 \tQuestion Loss: 89.2226 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 46\tNet Loss: 94.3700 \tQuestion Loss: 94.3700 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 46\tNet Loss: 93.3772 \tQuestion Loss: 93.3772 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 46\tNet Loss: 93.9075 \tQuestion Loss: 93.9075 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 46\tNet Loss: 93.7862 \tQuestion Loss: 93.7862 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 46\tNet Loss: 88.8755 \tQuestion Loss: 88.8755 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 46\tNet Loss: 94.9950 \tQuestion Loss: 94.9950 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 46\tNet Loss: 93.3890 \tQuestion Loss: 93.3890 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 46\tNet Loss: 95.3207 \tQuestion Loss: 95.3207 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 46\tNet Loss: 91.2560 \tQuestion Loss: 91.2560 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 46\tNet Loss: 95.6448 \tQuestion Loss: 95.6448 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 46\tNet Loss: 90.4442 \tQuestion Loss: 90.4442 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 46\tNet Loss: 92.7639 \tQuestion Loss: 92.7639 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 46\tNet Loss: 93.1782 \tQuestion Loss: 93.1782 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 46\tNet Loss: 95.4249 \tQuestion Loss: 95.4249 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 46\tNet Loss: 90.4459 \tQuestion Loss: 90.4459 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 46\tNet Loss: 92.5531 \tQuestion Loss: 92.5531 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 46\tNet Loss: 91.2141 \tQuestion Loss: 91.2141 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 19 \t Epoch : 46\tNet Loss: 86.9098 \tQuestion Loss: 86.9098 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 46\tNet Loss: 96.1453 \tQuestion Loss: 96.1453 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 46\tNet Loss: 88.2509 \tQuestion Loss: 88.2509 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 46\tNet Loss: 92.8727 \tQuestion Loss: 92.8727 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 46\tNet Loss: 95.8605 \tQuestion Loss: 95.8605 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 46\tNet Loss: 89.7781 \tQuestion Loss: 89.7781 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 46\tNet Loss: 93.2144 \tQuestion Loss: 93.2144 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 46\tNet Loss: 98.3632 \tQuestion Loss: 98.3632 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 46\tNet Loss: 91.9325 \tQuestion Loss: 91.9325 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 46\tNet Loss: 87.0397 \tQuestion Loss: 87.0397 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 46\tNet Loss: 89.9394 \tQuestion Loss: 89.9394 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 46\tNet Loss: 91.5816 \tQuestion Loss: 91.5816 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 46\tNet Loss: 90.3589 \tQuestion Loss: 90.3589 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 46\tNet Loss: 89.0347 \tQuestion Loss: 89.0347 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 46\tNet Loss: 93.2855 \tQuestion Loss: 93.2855 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 46\tNet Loss: 87.4687 \tQuestion Loss: 87.4687 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 46\tNet Loss: 90.2151 \tQuestion Loss: 90.2151 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 46\tNet Loss: 94.0039 \tQuestion Loss: 94.0039 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 46\tNet Loss: 94.9741 \tQuestion Loss: 94.9741 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 46\tNet Loss: 91.0074 \tQuestion Loss: 91.0074 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 46\tNet Loss: 94.2215 \tQuestion Loss: 94.2215 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 46\tNet Loss: 91.5256 \tQuestion Loss: 91.5256 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 46\tNet Loss: 90.3985 \tQuestion Loss: 90.3985 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 46\tNet Loss: 92.1749 \tQuestion Loss: 92.1749 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 46\tNet Loss: 93.5539 \tQuestion Loss: 93.5539 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 46\tNet Loss: 90.7798 \tQuestion Loss: 90.7798 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 46\tNet Loss: 93.6772 \tQuestion Loss: 93.6772 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 46\tNet Loss: 92.0956 \tQuestion Loss: 92.0956 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 46\tNet Loss: 97.2156 \tQuestion Loss: 97.2156 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 46\tNet Loss: 86.8686 \tQuestion Loss: 86.8686 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 46\tNet Loss: 94.3926 \tQuestion Loss: 94.3926 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 46\tNet Loss: 92.9570 \tQuestion Loss: 92.9570 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 46\tNet Loss: 88.5367 \tQuestion Loss: 88.5367 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 46\tNet Loss: 89.5989 \tQuestion Loss: 89.5989 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 46\tNet Loss: 90.0176 \tQuestion Loss: 90.0176 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 46\tNet Loss: 96.4428 \tQuestion Loss: 96.4428 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 46\tNet Loss: 93.1355 \tQuestion Loss: 93.1355 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 46\tNet Loss: 94.4207 \tQuestion Loss: 94.4207 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 46\tNet Loss: 96.7540 \tQuestion Loss: 96.7540 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 46\tNet Loss: 87.4929 \tQuestion Loss: 87.4929 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 46\tNet Loss: 92.2133 \tQuestion Loss: 92.2133 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 46\tNet Loss: 89.1014 \tQuestion Loss: 89.1014 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 46\tNet Loss: 95.2680 \tQuestion Loss: 95.2680 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 46\tNet Loss: 88.8416 \tQuestion Loss: 88.8416 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 46\tNet Loss: 93.4699 \tQuestion Loss: 93.4699 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 46\tNet Loss: 92.0833 \tQuestion Loss: 92.0833 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 46\tNet Loss: 90.4273 \tQuestion Loss: 90.4273 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 46\tNet Loss: 92.5035 \tQuestion Loss: 92.5035 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 46\tNet Loss: 92.0794 \tQuestion Loss: 92.0794 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 46\tNet Loss: 90.7759 \tQuestion Loss: 90.7759 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 46\tNet Loss: 95.8963 \tQuestion Loss: 95.8963 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 46\tNet Loss: 93.2051 \tQuestion Loss: 93.2051 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 46\tNet Loss: 92.8768 \tQuestion Loss: 92.8768 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 46\tNet Loss: 95.4730 \tQuestion Loss: 95.4730 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 46\tNet Loss: 93.3167 \tQuestion Loss: 93.3167 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 46\tNet Loss: 94.7528 \tQuestion Loss: 94.7528 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 46\tNet Loss: 93.4928 \tQuestion Loss: 93.4928 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 46\tNet Loss: 86.7804 \tQuestion Loss: 86.7804 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 46\tNet Loss: 91.3695 \tQuestion Loss: 91.3695 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 46\tNet Loss: 97.1836 \tQuestion Loss: 97.1836 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 46\tNet Loss: 92.8988 \tQuestion Loss: 92.8988 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 46\tNet Loss: 94.3186 \tQuestion Loss: 94.3186 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 46\tNet Loss: 92.0468 \tQuestion Loss: 92.0468 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 46\tNet Loss: 89.6184 \tQuestion Loss: 89.6184 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 46\tNet Loss: 88.6687 \tQuestion Loss: 88.6687 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 46\tNet Loss: 91.9533 \tQuestion Loss: 91.9533 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 46\tNet Loss: 90.0491 \tQuestion Loss: 90.0491 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 46\tNet Loss: 91.6539 \tQuestion Loss: 91.6539 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 46\tNet Loss: 91.6664 \tQuestion Loss: 91.6664 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 46\tNet Loss: 94.5031 \tQuestion Loss: 94.5031 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 46\tNet Loss: 90.1068 \tQuestion Loss: 90.1068 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 46\tNet Loss: 95.2936 \tQuestion Loss: 95.2936 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 46\tNet Loss: 92.9122 \tQuestion Loss: 92.9122 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 46\tNet Loss: 91.4517 \tQuestion Loss: 91.4517 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 46\tNet Loss: 91.8720 \tQuestion Loss: 91.8720 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 46\tNet Loss: 92.4821 \tQuestion Loss: 92.4821 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 46\tNet Loss: 93.4331 \tQuestion Loss: 93.4331 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 46\tNet Loss: 91.8836 \tQuestion Loss: 91.8836 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 46\tNet Loss: 95.7700 \tQuestion Loss: 95.7700 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 46\tNet Loss: 92.7230 \tQuestion Loss: 92.7230 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 46\tNet Loss: 92.9568 \tQuestion Loss: 92.9568 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 46\tNet Loss: 92.5217 \tQuestion Loss: 92.5217 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 46\tNet Loss: 95.5324 \tQuestion Loss: 95.5324 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 46\tNet Loss: 93.0495 \tQuestion Loss: 93.0495 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 46\tNet Loss: 92.4970 \tQuestion Loss: 92.4970 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 46\tNet Loss: 95.0728 \tQuestion Loss: 95.0728 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 46\tNet Loss: 96.5684 \tQuestion Loss: 96.5684 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 46\tNet Loss: 92.2877 \tQuestion Loss: 92.2877 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 46\tNet Loss: 89.2733 \tQuestion Loss: 89.2733 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 46\tNet Loss: 88.4110 \tQuestion Loss: 88.4110 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 46\tNet Loss: 92.9520 \tQuestion Loss: 92.9520 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 46\tNet Loss: 93.3712 \tQuestion Loss: 93.3712 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 111 \t Epoch : 46\tNet Loss: 87.6957 \tQuestion Loss: 87.6957 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 46\tNet Loss: 89.7773 \tQuestion Loss: 89.7773 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 46\tNet Loss: 97.2689 \tQuestion Loss: 97.2689 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 46\tNet Loss: 94.3053 \tQuestion Loss: 94.3053 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 46\tNet Loss: 91.3338 \tQuestion Loss: 91.3338 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 46\tNet Loss: 88.1024 \tQuestion Loss: 88.1024 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 46\tNet Loss: 90.5219 \tQuestion Loss: 90.5219 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 46\tNet Loss: 93.9068 \tQuestion Loss: 93.9068 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 46\tNet Loss: 89.7548 \tQuestion Loss: 89.7548 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 46\tNet Loss: 91.4545 \tQuestion Loss: 91.4545 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 46\tNet Loss: 93.4908 \tQuestion Loss: 93.4908 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 46\tNet Loss: 97.6495 \tQuestion Loss: 97.6495 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 46\tNet Loss: 92.9839 \tQuestion Loss: 92.9839 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 46\tNet Loss: 94.7373 \tQuestion Loss: 94.7373 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 46\tNet Loss: 92.5827 \tQuestion Loss: 92.5827 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 46\tNet Loss: 96.1722 \tQuestion Loss: 96.1722 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 46\tNet Loss: 88.0461 \tQuestion Loss: 88.0461 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 46\tNet Loss: 92.2107 \tQuestion Loss: 92.2107 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 46\tNet Loss: 94.7978 \tQuestion Loss: 94.7978 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 46\tNet Loss: 99.9202 \tQuestion Loss: 99.9202 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 46\tNet Loss: 94.8941 \tQuestion Loss: 94.8941 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 46\tNet Loss: 91.2861 \tQuestion Loss: 91.2861 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 46\tNet Loss: 92.6891 \tQuestion Loss: 92.6891 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 46\tNet Loss: 93.6291 \tQuestion Loss: 93.6291 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 46\tNet Loss: 85.7165 \tQuestion Loss: 85.7165 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 46\tNet Loss: 97.2668 \tQuestion Loss: 97.2668 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 46\tNet Loss: 94.9600 \tQuestion Loss: 94.9600 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 46\tNet Loss: 93.2373 \tQuestion Loss: 93.2373 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 46\tNet Loss: 92.0954 \tQuestion Loss: 92.0954 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 46\tNet Loss: 89.2579 \tQuestion Loss: 89.2579 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 46\tNet Loss: 89.7297 \tQuestion Loss: 89.7297 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 46\tNet Loss: 94.5866 \tQuestion Loss: 94.5866 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 46\tNet Loss: 89.3460 \tQuestion Loss: 89.3460 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 46\tNet Loss: 86.8785 \tQuestion Loss: 86.8785 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 46\tNet Loss: 95.3424 \tQuestion Loss: 95.3424 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 46\tNet Loss: 98.7011 \tQuestion Loss: 98.7011 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 46\tNet Loss: 87.0118 \tQuestion Loss: 87.0118 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 46\tNet Loss: 96.5484 \tQuestion Loss: 96.5484 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 46\tNet Loss: 90.0690 \tQuestion Loss: 90.0690 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 46\tNet Loss: 91.2258 \tQuestion Loss: 91.2258 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 46\tNet Loss: 98.0233 \tQuestion Loss: 98.0233 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 46\tNet Loss: 93.7352 \tQuestion Loss: 93.7352 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 46\tNet Loss: 94.1488 \tQuestion Loss: 94.1488 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 46\tNet Loss: 85.1093 \tQuestion Loss: 85.1093 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 46\tNet Loss: 90.5848 \tQuestion Loss: 90.5848 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 46\tNet Loss: 93.4675 \tQuestion Loss: 93.4675 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 46\tNet Loss: 88.4574 \tQuestion Loss: 88.4574 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 46\tNet Loss: 93.7709 \tQuestion Loss: 93.7709 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 46\tNet Loss: 94.6641 \tQuestion Loss: 94.6641 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 46\tNet Loss: 95.8635 \tQuestion Loss: 95.8635 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 46\tNet Loss: 94.1553 \tQuestion Loss: 94.1553 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 46\tNet Loss: 95.8806 \tQuestion Loss: 95.8806 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 46\tNet Loss: 89.6806 \tQuestion Loss: 89.6806 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 46\tNet Loss: 88.5087 \tQuestion Loss: 88.5087 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 46\tNet Loss: 89.7293 \tQuestion Loss: 89.7293 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 46\tNet Loss: 87.5037 \tQuestion Loss: 87.5037 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 46\tNet Loss: 92.7442 \tQuestion Loss: 92.7442 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 46\tNet Loss: 91.4773 \tQuestion Loss: 91.4773 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 46\tNet Loss: 95.7306 \tQuestion Loss: 95.7306 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 46\tNet Loss: 93.5641 \tQuestion Loss: 93.5641 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 46\tNet Loss: 92.5176 \tQuestion Loss: 92.5176 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 46\tNet Loss: 89.2573 \tQuestion Loss: 89.2573 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 46\tNet Loss: 90.6358 \tQuestion Loss: 90.6358 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 46\tNet Loss: 94.6786 \tQuestion Loss: 94.6786 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 46\tNet Loss: 97.6059 \tQuestion Loss: 97.6059 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 46\tNet Loss: 90.1134 \tQuestion Loss: 90.1134 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 46\tNet Loss: 91.5212 \tQuestion Loss: 91.5212 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 46\tNet Loss: 90.5450 \tQuestion Loss: 90.5450 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 46\tNet Loss: 91.6118 \tQuestion Loss: 91.6118 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 46\tNet Loss: 90.0697 \tQuestion Loss: 90.0697 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 46\tNet Loss: 92.3749 \tQuestion Loss: 92.3749 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 46\tNet Loss: 94.3423 \tQuestion Loss: 94.3423 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 46\tNet Loss: 89.3822 \tQuestion Loss: 89.3822 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 46\tNet Loss: 93.2989 \tQuestion Loss: 93.2989 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 46\tNet Loss: 91.6551 \tQuestion Loss: 91.6551 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 46\tNet Loss: 93.9303 \tQuestion Loss: 93.9303 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 46\tNet Loss: 95.0469 \tQuestion Loss: 95.0469 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 46\tNet Loss: 90.0337 \tQuestion Loss: 90.0337 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 46\tNet Loss: 89.4498 \tQuestion Loss: 89.4498 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 46\tNet Loss: 91.1288 \tQuestion Loss: 91.1288 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 46\tNet Loss: 90.0555 \tQuestion Loss: 90.0555 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 46\tNet Loss: 94.9105 \tQuestion Loss: 94.9105 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 46\tNet Loss: 93.3959 \tQuestion Loss: 93.3959 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 46\tNet Loss: 95.2269 \tQuestion Loss: 95.2269 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 46\tNet Loss: 88.0135 \tQuestion Loss: 88.0135 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 46\tNet Loss: 89.2504 \tQuestion Loss: 89.2504 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 46\tNet Loss: 94.6185 \tQuestion Loss: 94.6185 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 46\tNet Loss: 87.7008 \tQuestion Loss: 87.7008 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 46\tNet Loss: 92.4455 \tQuestion Loss: 92.4455 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 46 : 92.3257 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 47\tNet Loss: 92.7218 \tQuestion Loss: 92.7218 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 47\tNet Loss: 88.9858 \tQuestion Loss: 88.9858 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 2 \t Epoch : 47\tNet Loss: 94.2962 \tQuestion Loss: 94.2962 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 47\tNet Loss: 93.7563 \tQuestion Loss: 93.7563 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 47\tNet Loss: 93.7285 \tQuestion Loss: 93.7285 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 47\tNet Loss: 93.6406 \tQuestion Loss: 93.6406 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 47\tNet Loss: 88.7925 \tQuestion Loss: 88.7925 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 47\tNet Loss: 94.9190 \tQuestion Loss: 94.9190 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 47\tNet Loss: 93.6025 \tQuestion Loss: 93.6025 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 47\tNet Loss: 94.6746 \tQuestion Loss: 94.6746 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 47\tNet Loss: 91.0188 \tQuestion Loss: 91.0188 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 47\tNet Loss: 95.6402 \tQuestion Loss: 95.6402 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 47\tNet Loss: 90.4128 \tQuestion Loss: 90.4128 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 47\tNet Loss: 92.7200 \tQuestion Loss: 92.7200 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 47\tNet Loss: 92.9003 \tQuestion Loss: 92.9003 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 47\tNet Loss: 95.5443 \tQuestion Loss: 95.5443 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 47\tNet Loss: 90.5858 \tQuestion Loss: 90.5858 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 47\tNet Loss: 92.4099 \tQuestion Loss: 92.4099 \t Time Taken: 1 seconds\n",
      "Batch: 18 \t Epoch : 47\tNet Loss: 90.3463 \tQuestion Loss: 90.3463 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 47\tNet Loss: 86.7206 \tQuestion Loss: 86.7206 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 47\tNet Loss: 96.0898 \tQuestion Loss: 96.0898 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 47\tNet Loss: 88.1924 \tQuestion Loss: 88.1924 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 47\tNet Loss: 92.8544 \tQuestion Loss: 92.8544 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 47\tNet Loss: 95.9352 \tQuestion Loss: 95.9352 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 47\tNet Loss: 89.8871 \tQuestion Loss: 89.8871 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 47\tNet Loss: 93.3012 \tQuestion Loss: 93.3012 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 47\tNet Loss: 98.4493 \tQuestion Loss: 98.4493 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 47\tNet Loss: 91.8826 \tQuestion Loss: 91.8826 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 47\tNet Loss: 86.9636 \tQuestion Loss: 86.9636 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 47\tNet Loss: 89.8987 \tQuestion Loss: 89.8987 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 47\tNet Loss: 91.6707 \tQuestion Loss: 91.6707 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 47\tNet Loss: 90.4466 \tQuestion Loss: 90.4466 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 47\tNet Loss: 88.8278 \tQuestion Loss: 88.8278 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 47\tNet Loss: 93.3267 \tQuestion Loss: 93.3267 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 47\tNet Loss: 87.3527 \tQuestion Loss: 87.3527 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 47\tNet Loss: 90.1074 \tQuestion Loss: 90.1074 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 47\tNet Loss: 93.7779 \tQuestion Loss: 93.7779 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 47\tNet Loss: 95.4147 \tQuestion Loss: 95.4147 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 47\tNet Loss: 91.2768 \tQuestion Loss: 91.2768 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 47\tNet Loss: 93.8670 \tQuestion Loss: 93.8670 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 47\tNet Loss: 91.5361 \tQuestion Loss: 91.5361 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 47\tNet Loss: 90.2097 \tQuestion Loss: 90.2097 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 47\tNet Loss: 92.0042 \tQuestion Loss: 92.0042 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 47\tNet Loss: 93.3644 \tQuestion Loss: 93.3644 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 47\tNet Loss: 90.5816 \tQuestion Loss: 90.5816 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 47\tNet Loss: 93.6644 \tQuestion Loss: 93.6644 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 47\tNet Loss: 92.3242 \tQuestion Loss: 92.3242 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 47\tNet Loss: 97.3443 \tQuestion Loss: 97.3443 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 47\tNet Loss: 86.0833 \tQuestion Loss: 86.0833 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 47\tNet Loss: 93.9454 \tQuestion Loss: 93.9454 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 47\tNet Loss: 92.9464 \tQuestion Loss: 92.9464 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 47\tNet Loss: 88.5385 \tQuestion Loss: 88.5385 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 47\tNet Loss: 89.5307 \tQuestion Loss: 89.5307 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 47\tNet Loss: 89.7452 \tQuestion Loss: 89.7452 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 47\tNet Loss: 96.4773 \tQuestion Loss: 96.4773 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 47\tNet Loss: 93.3254 \tQuestion Loss: 93.3254 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 47\tNet Loss: 94.4147 \tQuestion Loss: 94.4147 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 47\tNet Loss: 96.5883 \tQuestion Loss: 96.5883 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 47\tNet Loss: 87.6506 \tQuestion Loss: 87.6506 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 47\tNet Loss: 92.2230 \tQuestion Loss: 92.2230 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 47\tNet Loss: 89.1405 \tQuestion Loss: 89.1405 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 47\tNet Loss: 95.2998 \tQuestion Loss: 95.2998 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 47\tNet Loss: 88.8197 \tQuestion Loss: 88.8197 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 47\tNet Loss: 93.5414 \tQuestion Loss: 93.5414 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 47\tNet Loss: 92.1901 \tQuestion Loss: 92.1901 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 47\tNet Loss: 90.7947 \tQuestion Loss: 90.7947 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 47\tNet Loss: 92.0836 \tQuestion Loss: 92.0836 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 47\tNet Loss: 91.3632 \tQuestion Loss: 91.3632 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 47\tNet Loss: 90.9107 \tQuestion Loss: 90.9107 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 47\tNet Loss: 95.6294 \tQuestion Loss: 95.6294 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 47\tNet Loss: 93.2767 \tQuestion Loss: 93.2767 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 47\tNet Loss: 92.6621 \tQuestion Loss: 92.6621 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 47\tNet Loss: 95.7160 \tQuestion Loss: 95.7160 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 47\tNet Loss: 94.0006 \tQuestion Loss: 94.0006 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 47\tNet Loss: 94.9125 \tQuestion Loss: 94.9125 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 47\tNet Loss: 93.5177 \tQuestion Loss: 93.5177 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 47\tNet Loss: 86.8344 \tQuestion Loss: 86.8344 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 47\tNet Loss: 91.6938 \tQuestion Loss: 91.6938 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 47\tNet Loss: 96.0987 \tQuestion Loss: 96.0987 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 47\tNet Loss: 92.9065 \tQuestion Loss: 92.9065 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 47\tNet Loss: 94.2177 \tQuestion Loss: 94.2177 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 47\tNet Loss: 92.2036 \tQuestion Loss: 92.2036 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 47\tNet Loss: 89.6262 \tQuestion Loss: 89.6262 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 47\tNet Loss: 88.5910 \tQuestion Loss: 88.5910 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 47\tNet Loss: 91.9933 \tQuestion Loss: 91.9933 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 47\tNet Loss: 90.5884 \tQuestion Loss: 90.5884 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 47\tNet Loss: 92.7114 \tQuestion Loss: 92.7114 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 47\tNet Loss: 91.7000 \tQuestion Loss: 91.7000 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 47\tNet Loss: 94.4088 \tQuestion Loss: 94.4088 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 47\tNet Loss: 90.0329 \tQuestion Loss: 90.0329 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 47\tNet Loss: 95.3943 \tQuestion Loss: 95.3943 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 47\tNet Loss: 92.8988 \tQuestion Loss: 92.8988 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 47\tNet Loss: 91.3497 \tQuestion Loss: 91.3497 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 47\tNet Loss: 92.0187 \tQuestion Loss: 92.0187 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 47\tNet Loss: 92.7175 \tQuestion Loss: 92.7175 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 95 \t Epoch : 47\tNet Loss: 93.5269 \tQuestion Loss: 93.5269 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 47\tNet Loss: 91.9064 \tQuestion Loss: 91.9064 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 47\tNet Loss: 95.7929 \tQuestion Loss: 95.7929 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 47\tNet Loss: 92.7661 \tQuestion Loss: 92.7661 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 47\tNet Loss: 93.1840 \tQuestion Loss: 93.1840 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 47\tNet Loss: 92.4558 \tQuestion Loss: 92.4558 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 47\tNet Loss: 95.6106 \tQuestion Loss: 95.6106 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 47\tNet Loss: 93.0690 \tQuestion Loss: 93.0690 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 47\tNet Loss: 92.5329 \tQuestion Loss: 92.5329 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 47\tNet Loss: 95.1720 \tQuestion Loss: 95.1720 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 47\tNet Loss: 96.6033 \tQuestion Loss: 96.6033 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 47\tNet Loss: 92.1741 \tQuestion Loss: 92.1741 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 47\tNet Loss: 89.2120 \tQuestion Loss: 89.2120 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 47\tNet Loss: 88.4144 \tQuestion Loss: 88.4144 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 47\tNet Loss: 92.8629 \tQuestion Loss: 92.8629 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 47\tNet Loss: 93.3039 \tQuestion Loss: 93.3039 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 47\tNet Loss: 87.8473 \tQuestion Loss: 87.8473 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 47\tNet Loss: 89.7369 \tQuestion Loss: 89.7369 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 47\tNet Loss: 97.1317 \tQuestion Loss: 97.1317 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 47\tNet Loss: 94.1174 \tQuestion Loss: 94.1174 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 47\tNet Loss: 91.0742 \tQuestion Loss: 91.0742 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 47\tNet Loss: 88.4686 \tQuestion Loss: 88.4686 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 47\tNet Loss: 90.6084 \tQuestion Loss: 90.6084 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 47\tNet Loss: 93.9101 \tQuestion Loss: 93.9101 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 47\tNet Loss: 89.6903 \tQuestion Loss: 89.6903 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 47\tNet Loss: 91.4999 \tQuestion Loss: 91.4999 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 47\tNet Loss: 93.9130 \tQuestion Loss: 93.9130 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 47\tNet Loss: 96.9336 \tQuestion Loss: 96.9336 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 47\tNet Loss: 92.7559 \tQuestion Loss: 92.7559 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 47\tNet Loss: 94.6679 \tQuestion Loss: 94.6679 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 47\tNet Loss: 92.7631 \tQuestion Loss: 92.7631 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 47\tNet Loss: 96.4623 \tQuestion Loss: 96.4623 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 47\tNet Loss: 88.0843 \tQuestion Loss: 88.0843 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 47\tNet Loss: 93.2951 \tQuestion Loss: 93.2951 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 47\tNet Loss: 93.1339 \tQuestion Loss: 93.1339 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 47\tNet Loss: 100.2172 \tQuestion Loss: 100.2172 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 47\tNet Loss: 94.8628 \tQuestion Loss: 94.8628 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 47\tNet Loss: 91.1298 \tQuestion Loss: 91.1298 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 47\tNet Loss: 92.7872 \tQuestion Loss: 92.7872 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 47\tNet Loss: 92.7831 \tQuestion Loss: 92.7831 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 47\tNet Loss: 86.5178 \tQuestion Loss: 86.5178 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 47\tNet Loss: 97.3474 \tQuestion Loss: 97.3474 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 47\tNet Loss: 95.1429 \tQuestion Loss: 95.1429 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 47\tNet Loss: 93.5261 \tQuestion Loss: 93.5261 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 47\tNet Loss: 92.1334 \tQuestion Loss: 92.1334 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 47\tNet Loss: 89.3156 \tQuestion Loss: 89.3156 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 47\tNet Loss: 89.8236 \tQuestion Loss: 89.8236 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 47\tNet Loss: 94.4664 \tQuestion Loss: 94.4664 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 47\tNet Loss: 89.4289 \tQuestion Loss: 89.4289 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 47\tNet Loss: 87.0036 \tQuestion Loss: 87.0036 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 47\tNet Loss: 95.7262 \tQuestion Loss: 95.7262 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 47\tNet Loss: 98.8264 \tQuestion Loss: 98.8264 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 47\tNet Loss: 87.1166 \tQuestion Loss: 87.1166 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 47\tNet Loss: 96.4659 \tQuestion Loss: 96.4659 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 47\tNet Loss: 90.0823 \tQuestion Loss: 90.0823 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 47\tNet Loss: 91.2456 \tQuestion Loss: 91.2456 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 47\tNet Loss: 98.0041 \tQuestion Loss: 98.0041 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 47\tNet Loss: 93.7639 \tQuestion Loss: 93.7639 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 47\tNet Loss: 94.3585 \tQuestion Loss: 94.3585 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 47\tNet Loss: 85.5552 \tQuestion Loss: 85.5552 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 47\tNet Loss: 90.6120 \tQuestion Loss: 90.6120 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 47\tNet Loss: 93.5584 \tQuestion Loss: 93.5584 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 47\tNet Loss: 88.3067 \tQuestion Loss: 88.3067 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 47\tNet Loss: 93.7766 \tQuestion Loss: 93.7766 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 47\tNet Loss: 94.8304 \tQuestion Loss: 94.8304 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 47\tNet Loss: 95.9995 \tQuestion Loss: 95.9995 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 47\tNet Loss: 94.2245 \tQuestion Loss: 94.2245 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 47\tNet Loss: 95.9832 \tQuestion Loss: 95.9832 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 47\tNet Loss: 89.7682 \tQuestion Loss: 89.7682 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 47\tNet Loss: 88.5778 \tQuestion Loss: 88.5778 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 47\tNet Loss: 89.7341 \tQuestion Loss: 89.7341 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 47\tNet Loss: 87.5496 \tQuestion Loss: 87.5496 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 47\tNet Loss: 93.1702 \tQuestion Loss: 93.1702 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 47\tNet Loss: 91.4860 \tQuestion Loss: 91.4860 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 47\tNet Loss: 95.8528 \tQuestion Loss: 95.8528 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 47\tNet Loss: 93.6142 \tQuestion Loss: 93.6142 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 47\tNet Loss: 92.6678 \tQuestion Loss: 92.6678 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 47\tNet Loss: 88.9674 \tQuestion Loss: 88.9674 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 47\tNet Loss: 90.6329 \tQuestion Loss: 90.6329 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 47\tNet Loss: 95.0797 \tQuestion Loss: 95.0797 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 47\tNet Loss: 97.8028 \tQuestion Loss: 97.8028 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 47\tNet Loss: 90.1038 \tQuestion Loss: 90.1038 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 47\tNet Loss: 91.6729 \tQuestion Loss: 91.6729 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 47\tNet Loss: 90.5254 \tQuestion Loss: 90.5254 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 47\tNet Loss: 92.1204 \tQuestion Loss: 92.1204 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 47\tNet Loss: 90.1566 \tQuestion Loss: 90.1566 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 47\tNet Loss: 92.4572 \tQuestion Loss: 92.4572 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 47\tNet Loss: 94.3436 \tQuestion Loss: 94.3436 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 47\tNet Loss: 89.6266 \tQuestion Loss: 89.6266 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 47\tNet Loss: 93.4673 \tQuestion Loss: 93.4673 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 47\tNet Loss: 91.6153 \tQuestion Loss: 91.6153 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 47\tNet Loss: 94.4179 \tQuestion Loss: 94.4179 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 187 \t Epoch : 47\tNet Loss: 95.3183 \tQuestion Loss: 95.3183 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 47\tNet Loss: 89.9465 \tQuestion Loss: 89.9465 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 47\tNet Loss: 90.1486 \tQuestion Loss: 90.1486 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 47\tNet Loss: 91.1899 \tQuestion Loss: 91.1899 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 47\tNet Loss: 90.0909 \tQuestion Loss: 90.0909 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 47\tNet Loss: 94.9919 \tQuestion Loss: 94.9919 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 47\tNet Loss: 93.3870 \tQuestion Loss: 93.3870 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 47\tNet Loss: 95.0688 \tQuestion Loss: 95.0688 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 47\tNet Loss: 88.0062 \tQuestion Loss: 88.0062 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 47\tNet Loss: 89.2584 \tQuestion Loss: 89.2584 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 47\tNet Loss: 94.6605 \tQuestion Loss: 94.6605 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 47\tNet Loss: 87.9987 \tQuestion Loss: 87.9987 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 47\tNet Loss: 92.3809 \tQuestion Loss: 92.3809 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 47 : 92.3437 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 48\tNet Loss: 92.7488 \tQuestion Loss: 92.7488 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 48\tNet Loss: 89.1993 \tQuestion Loss: 89.1993 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 48\tNet Loss: 94.3630 \tQuestion Loss: 94.3630 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 48\tNet Loss: 93.3734 \tQuestion Loss: 93.3734 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 48\tNet Loss: 93.8778 \tQuestion Loss: 93.8778 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 48\tNet Loss: 93.7178 \tQuestion Loss: 93.7178 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 48\tNet Loss: 88.8782 \tQuestion Loss: 88.8782 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 48\tNet Loss: 94.9528 \tQuestion Loss: 94.9528 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 48\tNet Loss: 93.3499 \tQuestion Loss: 93.3499 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 48\tNet Loss: 95.3331 \tQuestion Loss: 95.3331 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 48\tNet Loss: 91.2609 \tQuestion Loss: 91.2609 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 48\tNet Loss: 95.6608 \tQuestion Loss: 95.6608 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 48\tNet Loss: 90.4303 \tQuestion Loss: 90.4303 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 48\tNet Loss: 92.7264 \tQuestion Loss: 92.7264 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 48\tNet Loss: 93.1830 \tQuestion Loss: 93.1830 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 48\tNet Loss: 95.4024 \tQuestion Loss: 95.4024 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 48\tNet Loss: 90.4311 \tQuestion Loss: 90.4311 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 48\tNet Loss: 92.5079 \tQuestion Loss: 92.5079 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 48\tNet Loss: 91.2204 \tQuestion Loss: 91.2204 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 48\tNet Loss: 86.9138 \tQuestion Loss: 86.9138 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 48\tNet Loss: 96.1371 \tQuestion Loss: 96.1371 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 48\tNet Loss: 88.2407 \tQuestion Loss: 88.2407 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 48\tNet Loss: 92.8654 \tQuestion Loss: 92.8654 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 48\tNet Loss: 95.8613 \tQuestion Loss: 95.8613 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 48\tNet Loss: 89.7920 \tQuestion Loss: 89.7920 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 48\tNet Loss: 93.2021 \tQuestion Loss: 93.2021 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 48\tNet Loss: 98.3601 \tQuestion Loss: 98.3601 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 48\tNet Loss: 91.9353 \tQuestion Loss: 91.9353 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 48\tNet Loss: 87.0493 \tQuestion Loss: 87.0493 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 48\tNet Loss: 89.9250 \tQuestion Loss: 89.9250 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 48\tNet Loss: 91.5643 \tQuestion Loss: 91.5643 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 48\tNet Loss: 90.3540 \tQuestion Loss: 90.3540 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 48\tNet Loss: 89.0456 \tQuestion Loss: 89.0456 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 48\tNet Loss: 93.2651 \tQuestion Loss: 93.2651 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 48\tNet Loss: 87.4568 \tQuestion Loss: 87.4568 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 48\tNet Loss: 90.2055 \tQuestion Loss: 90.2055 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 48\tNet Loss: 94.0077 \tQuestion Loss: 94.0077 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 48\tNet Loss: 94.9770 \tQuestion Loss: 94.9770 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 48\tNet Loss: 91.0298 \tQuestion Loss: 91.0298 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 48\tNet Loss: 94.2078 \tQuestion Loss: 94.2078 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 48\tNet Loss: 91.5297 \tQuestion Loss: 91.5297 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 48\tNet Loss: 90.2969 \tQuestion Loss: 90.2969 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 48\tNet Loss: 92.1554 \tQuestion Loss: 92.1554 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 48\tNet Loss: 93.5492 \tQuestion Loss: 93.5492 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 48\tNet Loss: 90.7820 \tQuestion Loss: 90.7820 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 48\tNet Loss: 93.6743 \tQuestion Loss: 93.6743 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 48\tNet Loss: 92.0940 \tQuestion Loss: 92.0940 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 48\tNet Loss: 97.1982 \tQuestion Loss: 97.1982 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 48\tNet Loss: 86.8863 \tQuestion Loss: 86.8863 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 48\tNet Loss: 94.3817 \tQuestion Loss: 94.3817 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 48\tNet Loss: 92.9544 \tQuestion Loss: 92.9544 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 48\tNet Loss: 88.5407 \tQuestion Loss: 88.5407 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 48\tNet Loss: 89.5760 \tQuestion Loss: 89.5760 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 48\tNet Loss: 90.0288 \tQuestion Loss: 90.0288 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 48\tNet Loss: 96.4626 \tQuestion Loss: 96.4626 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 48\tNet Loss: 93.1294 \tQuestion Loss: 93.1294 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 48\tNet Loss: 94.3977 \tQuestion Loss: 94.3977 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 48\tNet Loss: 96.6282 \tQuestion Loss: 96.6282 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 48\tNet Loss: 87.5040 \tQuestion Loss: 87.5040 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 48\tNet Loss: 92.0576 \tQuestion Loss: 92.0576 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 48\tNet Loss: 89.1057 \tQuestion Loss: 89.1057 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 48\tNet Loss: 95.2384 \tQuestion Loss: 95.2384 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 48\tNet Loss: 88.8273 \tQuestion Loss: 88.8273 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 48\tNet Loss: 93.4690 \tQuestion Loss: 93.4690 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 48\tNet Loss: 92.0538 \tQuestion Loss: 92.0538 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 48\tNet Loss: 90.3943 \tQuestion Loss: 90.3943 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 48\tNet Loss: 92.5075 \tQuestion Loss: 92.5075 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 48\tNet Loss: 92.0830 \tQuestion Loss: 92.0830 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 48\tNet Loss: 90.8311 \tQuestion Loss: 90.8311 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 48\tNet Loss: 95.9058 \tQuestion Loss: 95.9058 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 48\tNet Loss: 93.1697 \tQuestion Loss: 93.1697 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 48\tNet Loss: 92.8870 \tQuestion Loss: 92.8870 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 48\tNet Loss: 95.4939 \tQuestion Loss: 95.4939 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 48\tNet Loss: 93.3169 \tQuestion Loss: 93.3169 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 48\tNet Loss: 94.7262 \tQuestion Loss: 94.7262 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 48\tNet Loss: 93.4918 \tQuestion Loss: 93.4918 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 48\tNet Loss: 86.7781 \tQuestion Loss: 86.7781 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 48\tNet Loss: 91.3833 \tQuestion Loss: 91.3833 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 48\tNet Loss: 97.1869 \tQuestion Loss: 97.1869 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 79 \t Epoch : 48\tNet Loss: 92.9015 \tQuestion Loss: 92.9015 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 48\tNet Loss: 94.3060 \tQuestion Loss: 94.3060 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 48\tNet Loss: 92.0569 \tQuestion Loss: 92.0569 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 48\tNet Loss: 89.6021 \tQuestion Loss: 89.6021 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 48\tNet Loss: 88.6845 \tQuestion Loss: 88.6845 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 48\tNet Loss: 91.9774 \tQuestion Loss: 91.9774 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 48\tNet Loss: 90.0754 \tQuestion Loss: 90.0754 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 48\tNet Loss: 91.6320 \tQuestion Loss: 91.6320 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 48\tNet Loss: 91.6725 \tQuestion Loss: 91.6725 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 48\tNet Loss: 94.5198 \tQuestion Loss: 94.5198 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 48\tNet Loss: 90.1413 \tQuestion Loss: 90.1413 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 48\tNet Loss: 95.2677 \tQuestion Loss: 95.2677 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 48\tNet Loss: 92.8877 \tQuestion Loss: 92.8877 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 48\tNet Loss: 91.4565 \tQuestion Loss: 91.4565 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 48\tNet Loss: 91.8841 \tQuestion Loss: 91.8841 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 48\tNet Loss: 92.4815 \tQuestion Loss: 92.4815 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 48\tNet Loss: 93.4047 \tQuestion Loss: 93.4047 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 48\tNet Loss: 91.8806 \tQuestion Loss: 91.8806 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 48\tNet Loss: 95.7802 \tQuestion Loss: 95.7802 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 48\tNet Loss: 92.6862 \tQuestion Loss: 92.6862 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 48\tNet Loss: 92.9339 \tQuestion Loss: 92.9339 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 48\tNet Loss: 92.5487 \tQuestion Loss: 92.5487 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 48\tNet Loss: 95.5463 \tQuestion Loss: 95.5463 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 48\tNet Loss: 93.0078 \tQuestion Loss: 93.0078 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 48\tNet Loss: 92.4916 \tQuestion Loss: 92.4916 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 48\tNet Loss: 95.0835 \tQuestion Loss: 95.0835 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 48\tNet Loss: 96.5454 \tQuestion Loss: 96.5454 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 48\tNet Loss: 92.2669 \tQuestion Loss: 92.2669 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 48\tNet Loss: 89.2475 \tQuestion Loss: 89.2475 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 48\tNet Loss: 88.4044 \tQuestion Loss: 88.4044 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 48\tNet Loss: 92.9908 \tQuestion Loss: 92.9908 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 48\tNet Loss: 93.3496 \tQuestion Loss: 93.3496 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 48\tNet Loss: 87.6795 \tQuestion Loss: 87.6795 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 48\tNet Loss: 89.7922 \tQuestion Loss: 89.7922 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 48\tNet Loss: 97.2928 \tQuestion Loss: 97.2928 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 48\tNet Loss: 94.3194 \tQuestion Loss: 94.3194 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 48\tNet Loss: 91.3294 \tQuestion Loss: 91.3294 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 48\tNet Loss: 88.1065 \tQuestion Loss: 88.1065 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 48\tNet Loss: 90.5625 \tQuestion Loss: 90.5625 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 48\tNet Loss: 93.8939 \tQuestion Loss: 93.8939 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 48\tNet Loss: 89.7410 \tQuestion Loss: 89.7410 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 48\tNet Loss: 91.4663 \tQuestion Loss: 91.4663 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 48\tNet Loss: 93.5458 \tQuestion Loss: 93.5458 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 48\tNet Loss: 97.6776 \tQuestion Loss: 97.6776 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 48\tNet Loss: 92.9418 \tQuestion Loss: 92.9418 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 48\tNet Loss: 94.7305 \tQuestion Loss: 94.7305 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 48\tNet Loss: 92.5843 \tQuestion Loss: 92.5843 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 48\tNet Loss: 96.1703 \tQuestion Loss: 96.1703 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 48\tNet Loss: 88.0061 \tQuestion Loss: 88.0061 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 48\tNet Loss: 92.2509 \tQuestion Loss: 92.2509 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 48\tNet Loss: 94.8269 \tQuestion Loss: 94.8269 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 48\tNet Loss: 99.9306 \tQuestion Loss: 99.9306 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 48\tNet Loss: 94.8445 \tQuestion Loss: 94.8445 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 48\tNet Loss: 91.2724 \tQuestion Loss: 91.2724 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 48\tNet Loss: 92.7195 \tQuestion Loss: 92.7195 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 48\tNet Loss: 93.6733 \tQuestion Loss: 93.6733 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 48\tNet Loss: 85.6364 \tQuestion Loss: 85.6364 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 48\tNet Loss: 97.3018 \tQuestion Loss: 97.3018 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 48\tNet Loss: 94.9894 \tQuestion Loss: 94.9894 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 48\tNet Loss: 93.3683 \tQuestion Loss: 93.3683 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 48\tNet Loss: 92.0650 \tQuestion Loss: 92.0650 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 48\tNet Loss: 89.2875 \tQuestion Loss: 89.2875 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 48\tNet Loss: 89.7480 \tQuestion Loss: 89.7480 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 48\tNet Loss: 94.5724 \tQuestion Loss: 94.5724 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 48\tNet Loss: 89.3097 \tQuestion Loss: 89.3097 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 48\tNet Loss: 86.9035 \tQuestion Loss: 86.9035 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 48\tNet Loss: 95.3423 \tQuestion Loss: 95.3423 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 48\tNet Loss: 98.6984 \tQuestion Loss: 98.6984 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 48\tNet Loss: 86.9735 \tQuestion Loss: 86.9735 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 48\tNet Loss: 96.5838 \tQuestion Loss: 96.5838 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 48\tNet Loss: 90.0702 \tQuestion Loss: 90.0702 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 48\tNet Loss: 91.2133 \tQuestion Loss: 91.2133 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 48\tNet Loss: 97.9625 \tQuestion Loss: 97.9625 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 48\tNet Loss: 93.7334 \tQuestion Loss: 93.7334 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 48\tNet Loss: 94.1550 \tQuestion Loss: 94.1550 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 48\tNet Loss: 85.1192 \tQuestion Loss: 85.1192 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 48\tNet Loss: 90.6054 \tQuestion Loss: 90.6054 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 48\tNet Loss: 93.4708 \tQuestion Loss: 93.4708 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 48\tNet Loss: 88.4508 \tQuestion Loss: 88.4508 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 48\tNet Loss: 93.7919 \tQuestion Loss: 93.7919 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 48\tNet Loss: 94.6067 \tQuestion Loss: 94.6067 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 48\tNet Loss: 95.9091 \tQuestion Loss: 95.9091 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 48\tNet Loss: 94.1922 \tQuestion Loss: 94.1922 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 48\tNet Loss: 95.8935 \tQuestion Loss: 95.8935 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 48\tNet Loss: 89.6264 \tQuestion Loss: 89.6264 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 48\tNet Loss: 88.5051 \tQuestion Loss: 88.5051 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 48\tNet Loss: 89.7567 \tQuestion Loss: 89.7567 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 48\tNet Loss: 87.5258 \tQuestion Loss: 87.5258 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 48\tNet Loss: 92.6665 \tQuestion Loss: 92.6665 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 48\tNet Loss: 91.4820 \tQuestion Loss: 91.4820 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 48\tNet Loss: 95.7574 \tQuestion Loss: 95.7574 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 48\tNet Loss: 93.5756 \tQuestion Loss: 93.5756 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 171 \t Epoch : 48\tNet Loss: 92.4913 \tQuestion Loss: 92.4913 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 48\tNet Loss: 89.2487 \tQuestion Loss: 89.2487 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 48\tNet Loss: 90.6595 \tQuestion Loss: 90.6595 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 48\tNet Loss: 94.6858 \tQuestion Loss: 94.6858 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 48\tNet Loss: 97.5947 \tQuestion Loss: 97.5947 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 48\tNet Loss: 90.0944 \tQuestion Loss: 90.0944 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 48\tNet Loss: 91.5327 \tQuestion Loss: 91.5327 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 48\tNet Loss: 90.5476 \tQuestion Loss: 90.5476 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 48\tNet Loss: 91.6440 \tQuestion Loss: 91.6440 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 48\tNet Loss: 90.0601 \tQuestion Loss: 90.0601 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 48\tNet Loss: 92.3872 \tQuestion Loss: 92.3872 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 48\tNet Loss: 94.3675 \tQuestion Loss: 94.3675 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 48\tNet Loss: 89.3816 \tQuestion Loss: 89.3816 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 48\tNet Loss: 93.2964 \tQuestion Loss: 93.2964 \t Time Taken: 1 seconds\n",
      "Batch: 185 \t Epoch : 48\tNet Loss: 91.6700 \tQuestion Loss: 91.6700 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 48\tNet Loss: 93.9593 \tQuestion Loss: 93.9593 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 48\tNet Loss: 95.0460 \tQuestion Loss: 95.0460 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 48\tNet Loss: 89.9989 \tQuestion Loss: 89.9989 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 48\tNet Loss: 89.4642 \tQuestion Loss: 89.4642 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 48\tNet Loss: 91.1368 \tQuestion Loss: 91.1368 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 48\tNet Loss: 90.0875 \tQuestion Loss: 90.0875 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 48\tNet Loss: 94.9046 \tQuestion Loss: 94.9046 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 48\tNet Loss: 93.3898 \tQuestion Loss: 93.3898 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 48\tNet Loss: 95.2360 \tQuestion Loss: 95.2360 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 48\tNet Loss: 88.0116 \tQuestion Loss: 88.0116 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 48\tNet Loss: 89.2395 \tQuestion Loss: 89.2395 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 48\tNet Loss: 94.6271 \tQuestion Loss: 94.6271 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 48\tNet Loss: 87.7189 \tQuestion Loss: 87.7189 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 48\tNet Loss: 92.4446 \tQuestion Loss: 92.4446 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 48 : 92.3227 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 49\tNet Loss: 92.7221 \tQuestion Loss: 92.7221 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 49\tNet Loss: 88.9935 \tQuestion Loss: 88.9935 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 49\tNet Loss: 94.3057 \tQuestion Loss: 94.3057 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 49\tNet Loss: 93.7749 \tQuestion Loss: 93.7749 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 49\tNet Loss: 93.7349 \tQuestion Loss: 93.7349 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 49\tNet Loss: 93.6556 \tQuestion Loss: 93.6556 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 49\tNet Loss: 88.8015 \tQuestion Loss: 88.8015 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 49\tNet Loss: 94.8982 \tQuestion Loss: 94.8982 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 49\tNet Loss: 93.6024 \tQuestion Loss: 93.6024 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 49\tNet Loss: 94.6503 \tQuestion Loss: 94.6503 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 49\tNet Loss: 91.0398 \tQuestion Loss: 91.0398 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 49\tNet Loss: 95.6594 \tQuestion Loss: 95.6594 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 49\tNet Loss: 90.4076 \tQuestion Loss: 90.4076 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 49\tNet Loss: 92.7154 \tQuestion Loss: 92.7154 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 49\tNet Loss: 92.9047 \tQuestion Loss: 92.9047 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 49\tNet Loss: 95.5562 \tQuestion Loss: 95.5562 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 49\tNet Loss: 90.5820 \tQuestion Loss: 90.5820 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 49\tNet Loss: 92.4305 \tQuestion Loss: 92.4305 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 49\tNet Loss: 90.3736 \tQuestion Loss: 90.3736 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 49\tNet Loss: 86.7244 \tQuestion Loss: 86.7244 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 49\tNet Loss: 96.0779 \tQuestion Loss: 96.0779 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 49\tNet Loss: 88.1938 \tQuestion Loss: 88.1938 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 49\tNet Loss: 92.8682 \tQuestion Loss: 92.8682 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 49\tNet Loss: 95.9355 \tQuestion Loss: 95.9355 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 49\tNet Loss: 89.8937 \tQuestion Loss: 89.8937 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 49\tNet Loss: 93.3093 \tQuestion Loss: 93.3093 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 49\tNet Loss: 98.4521 \tQuestion Loss: 98.4521 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 49\tNet Loss: 91.8979 \tQuestion Loss: 91.8979 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 49\tNet Loss: 86.9546 \tQuestion Loss: 86.9546 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 49\tNet Loss: 89.9037 \tQuestion Loss: 89.9037 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 49\tNet Loss: 91.6724 \tQuestion Loss: 91.6724 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 49\tNet Loss: 90.4670 \tQuestion Loss: 90.4670 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 49\tNet Loss: 88.8119 \tQuestion Loss: 88.8119 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 49\tNet Loss: 93.3315 \tQuestion Loss: 93.3315 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 49\tNet Loss: 87.3391 \tQuestion Loss: 87.3391 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 49\tNet Loss: 90.1191 \tQuestion Loss: 90.1191 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 49\tNet Loss: 93.7732 \tQuestion Loss: 93.7732 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 49\tNet Loss: 95.4300 \tQuestion Loss: 95.4300 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 49\tNet Loss: 91.3118 \tQuestion Loss: 91.3118 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 49\tNet Loss: 93.8904 \tQuestion Loss: 93.8904 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 49\tNet Loss: 91.5233 \tQuestion Loss: 91.5233 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 49\tNet Loss: 90.2625 \tQuestion Loss: 90.2625 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 49\tNet Loss: 92.0516 \tQuestion Loss: 92.0516 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 49\tNet Loss: 93.3585 \tQuestion Loss: 93.3585 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 49\tNet Loss: 90.5796 \tQuestion Loss: 90.5796 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 49\tNet Loss: 93.6533 \tQuestion Loss: 93.6533 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 49\tNet Loss: 92.3293 \tQuestion Loss: 92.3293 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 49\tNet Loss: 97.3631 \tQuestion Loss: 97.3631 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 49\tNet Loss: 86.0601 \tQuestion Loss: 86.0601 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 49\tNet Loss: 93.9334 \tQuestion Loss: 93.9334 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 49\tNet Loss: 92.9516 \tQuestion Loss: 92.9516 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 49\tNet Loss: 88.5463 \tQuestion Loss: 88.5463 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 49\tNet Loss: 89.5426 \tQuestion Loss: 89.5426 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 49\tNet Loss: 89.7749 \tQuestion Loss: 89.7749 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 49\tNet Loss: 96.4717 \tQuestion Loss: 96.4717 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 49\tNet Loss: 93.3338 \tQuestion Loss: 93.3338 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 49\tNet Loss: 94.4204 \tQuestion Loss: 94.4204 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 49\tNet Loss: 96.6059 \tQuestion Loss: 96.6059 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 49\tNet Loss: 87.6173 \tQuestion Loss: 87.6173 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 49\tNet Loss: 92.2555 \tQuestion Loss: 92.2555 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 49\tNet Loss: 89.1527 \tQuestion Loss: 89.1527 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 49\tNet Loss: 95.2980 \tQuestion Loss: 95.2980 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 49\tNet Loss: 88.8144 \tQuestion Loss: 88.8144 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 63 \t Epoch : 49\tNet Loss: 93.5558 \tQuestion Loss: 93.5558 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 49\tNet Loss: 92.2120 \tQuestion Loss: 92.2120 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 49\tNet Loss: 90.8455 \tQuestion Loss: 90.8455 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 49\tNet Loss: 92.0798 \tQuestion Loss: 92.0798 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 49\tNet Loss: 91.3629 \tQuestion Loss: 91.3629 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 49\tNet Loss: 90.8953 \tQuestion Loss: 90.8953 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 49\tNet Loss: 95.6462 \tQuestion Loss: 95.6462 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 49\tNet Loss: 93.3122 \tQuestion Loss: 93.3122 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 49\tNet Loss: 92.6782 \tQuestion Loss: 92.6782 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 49\tNet Loss: 95.7259 \tQuestion Loss: 95.7259 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 49\tNet Loss: 93.9861 \tQuestion Loss: 93.9861 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 49\tNet Loss: 94.8961 \tQuestion Loss: 94.8961 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 49\tNet Loss: 93.5210 \tQuestion Loss: 93.5210 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 49\tNet Loss: 86.8534 \tQuestion Loss: 86.8534 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 49\tNet Loss: 91.6798 \tQuestion Loss: 91.6798 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 49\tNet Loss: 96.1054 \tQuestion Loss: 96.1054 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 49\tNet Loss: 92.9054 \tQuestion Loss: 92.9054 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 49\tNet Loss: 94.2323 \tQuestion Loss: 94.2323 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 49\tNet Loss: 92.1787 \tQuestion Loss: 92.1787 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 49\tNet Loss: 89.6411 \tQuestion Loss: 89.6411 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 49\tNet Loss: 88.5874 \tQuestion Loss: 88.5874 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 49\tNet Loss: 91.9794 \tQuestion Loss: 91.9794 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 49\tNet Loss: 90.5818 \tQuestion Loss: 90.5818 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 49\tNet Loss: 92.7420 \tQuestion Loss: 92.7420 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 49\tNet Loss: 91.6817 \tQuestion Loss: 91.6817 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 49\tNet Loss: 94.3948 \tQuestion Loss: 94.3948 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 49\tNet Loss: 90.0703 \tQuestion Loss: 90.0703 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 49\tNet Loss: 95.3999 \tQuestion Loss: 95.3999 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 49\tNet Loss: 92.9207 \tQuestion Loss: 92.9207 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 49\tNet Loss: 91.3403 \tQuestion Loss: 91.3403 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 49\tNet Loss: 92.0032 \tQuestion Loss: 92.0032 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 49\tNet Loss: 92.7042 \tQuestion Loss: 92.7042 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 49\tNet Loss: 93.5403 \tQuestion Loss: 93.5403 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 49\tNet Loss: 91.9117 \tQuestion Loss: 91.9117 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 49\tNet Loss: 95.7966 \tQuestion Loss: 95.7966 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 49\tNet Loss: 92.7795 \tQuestion Loss: 92.7795 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 49\tNet Loss: 93.1914 \tQuestion Loss: 93.1914 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 49\tNet Loss: 92.4267 \tQuestion Loss: 92.4267 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 49\tNet Loss: 95.6114 \tQuestion Loss: 95.6114 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 49\tNet Loss: 93.0593 \tQuestion Loss: 93.0593 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 49\tNet Loss: 92.5354 \tQuestion Loss: 92.5354 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 49\tNet Loss: 95.1873 \tQuestion Loss: 95.1873 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 49\tNet Loss: 96.6150 \tQuestion Loss: 96.6150 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 49\tNet Loss: 92.2272 \tQuestion Loss: 92.2272 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 49\tNet Loss: 89.2200 \tQuestion Loss: 89.2200 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 49\tNet Loss: 88.3756 \tQuestion Loss: 88.3756 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 49\tNet Loss: 92.8629 \tQuestion Loss: 92.8629 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 49\tNet Loss: 93.4417 \tQuestion Loss: 93.4417 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 49\tNet Loss: 87.8325 \tQuestion Loss: 87.8325 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 49\tNet Loss: 89.7192 \tQuestion Loss: 89.7192 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 49\tNet Loss: 97.1251 \tQuestion Loss: 97.1251 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 49\tNet Loss: 94.1236 \tQuestion Loss: 94.1236 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 49\tNet Loss: 91.0470 \tQuestion Loss: 91.0470 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 49\tNet Loss: 88.4546 \tQuestion Loss: 88.4546 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 49\tNet Loss: 90.6097 \tQuestion Loss: 90.6097 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 49\tNet Loss: 93.9018 \tQuestion Loss: 93.9018 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 49\tNet Loss: 89.6509 \tQuestion Loss: 89.6509 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 49\tNet Loss: 91.5215 \tQuestion Loss: 91.5215 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 49\tNet Loss: 93.8892 \tQuestion Loss: 93.8892 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 49\tNet Loss: 96.8860 \tQuestion Loss: 96.8860 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 49\tNet Loss: 92.7303 \tQuestion Loss: 92.7303 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 49\tNet Loss: 94.6715 \tQuestion Loss: 94.6715 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 49\tNet Loss: 92.7877 \tQuestion Loss: 92.7877 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 49\tNet Loss: 96.4433 \tQuestion Loss: 96.4433 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 49\tNet Loss: 88.0503 \tQuestion Loss: 88.0503 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 49\tNet Loss: 93.2663 \tQuestion Loss: 93.2663 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 49\tNet Loss: 93.1106 \tQuestion Loss: 93.1106 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 49\tNet Loss: 100.1977 \tQuestion Loss: 100.1977 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 49\tNet Loss: 94.8367 \tQuestion Loss: 94.8367 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 49\tNet Loss: 91.1308 \tQuestion Loss: 91.1308 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 49\tNet Loss: 92.7741 \tQuestion Loss: 92.7741 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 49\tNet Loss: 92.7277 \tQuestion Loss: 92.7277 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 49\tNet Loss: 86.5669 \tQuestion Loss: 86.5669 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 49\tNet Loss: 97.3626 \tQuestion Loss: 97.3626 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 49\tNet Loss: 95.1636 \tQuestion Loss: 95.1636 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 49\tNet Loss: 93.5376 \tQuestion Loss: 93.5376 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 49\tNet Loss: 92.1042 \tQuestion Loss: 92.1042 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 49\tNet Loss: 89.3212 \tQuestion Loss: 89.3212 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 49\tNet Loss: 89.8477 \tQuestion Loss: 89.8477 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 49\tNet Loss: 94.4627 \tQuestion Loss: 94.4627 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 49\tNet Loss: 89.3956 \tQuestion Loss: 89.3956 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 49\tNet Loss: 86.9956 \tQuestion Loss: 86.9956 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 49\tNet Loss: 95.7410 \tQuestion Loss: 95.7410 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 49\tNet Loss: 98.8436 \tQuestion Loss: 98.8436 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 49\tNet Loss: 87.0712 \tQuestion Loss: 87.0712 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 49\tNet Loss: 96.5244 \tQuestion Loss: 96.5244 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 49\tNet Loss: 90.1028 \tQuestion Loss: 90.1028 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 49\tNet Loss: 91.2735 \tQuestion Loss: 91.2735 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 49\tNet Loss: 97.9942 \tQuestion Loss: 97.9942 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 49\tNet Loss: 93.7742 \tQuestion Loss: 93.7742 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 49\tNet Loss: 94.3596 \tQuestion Loss: 94.3596 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 49\tNet Loss: 85.5920 \tQuestion Loss: 85.5920 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 155 \t Epoch : 49\tNet Loss: 90.5487 \tQuestion Loss: 90.5487 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 49\tNet Loss: 93.5661 \tQuestion Loss: 93.5661 \t Time Taken: 0 seconds\n",
      "Batch: 157 \t Epoch : 49\tNet Loss: 88.3341 \tQuestion Loss: 88.3341 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 49\tNet Loss: 93.7746 \tQuestion Loss: 93.7746 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 49\tNet Loss: 94.8176 \tQuestion Loss: 94.8176 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 49\tNet Loss: 95.9773 \tQuestion Loss: 95.9773 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 49\tNet Loss: 94.2377 \tQuestion Loss: 94.2377 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 49\tNet Loss: 95.9817 \tQuestion Loss: 95.9817 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 49\tNet Loss: 89.7281 \tQuestion Loss: 89.7281 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 49\tNet Loss: 88.5813 \tQuestion Loss: 88.5813 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 49\tNet Loss: 89.7436 \tQuestion Loss: 89.7436 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 49\tNet Loss: 87.5534 \tQuestion Loss: 87.5534 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 49\tNet Loss: 93.2181 \tQuestion Loss: 93.2181 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 49\tNet Loss: 91.4789 \tQuestion Loss: 91.4789 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 49\tNet Loss: 95.8599 \tQuestion Loss: 95.8599 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 49\tNet Loss: 93.6370 \tQuestion Loss: 93.6370 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 49\tNet Loss: 92.6447 \tQuestion Loss: 92.6447 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 49\tNet Loss: 88.9698 \tQuestion Loss: 88.9698 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 49\tNet Loss: 90.6376 \tQuestion Loss: 90.6376 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 49\tNet Loss: 95.0904 \tQuestion Loss: 95.0904 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 49\tNet Loss: 97.7806 \tQuestion Loss: 97.7806 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 49\tNet Loss: 90.0951 \tQuestion Loss: 90.0951 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 49\tNet Loss: 91.6665 \tQuestion Loss: 91.6665 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 49\tNet Loss: 90.5269 \tQuestion Loss: 90.5269 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 49\tNet Loss: 92.0432 \tQuestion Loss: 92.0432 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 49\tNet Loss: 90.1529 \tQuestion Loss: 90.1529 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 49\tNet Loss: 92.4551 \tQuestion Loss: 92.4551 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 49\tNet Loss: 94.3393 \tQuestion Loss: 94.3393 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 49\tNet Loss: 89.6323 \tQuestion Loss: 89.6323 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 49\tNet Loss: 93.4693 \tQuestion Loss: 93.4693 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 49\tNet Loss: 91.6251 \tQuestion Loss: 91.6251 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 49\tNet Loss: 94.4114 \tQuestion Loss: 94.4114 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 49\tNet Loss: 95.3110 \tQuestion Loss: 95.3110 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 49\tNet Loss: 89.9221 \tQuestion Loss: 89.9221 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 49\tNet Loss: 90.1505 \tQuestion Loss: 90.1505 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 49\tNet Loss: 91.2009 \tQuestion Loss: 91.2009 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 49\tNet Loss: 90.1085 \tQuestion Loss: 90.1085 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 49\tNet Loss: 94.9903 \tQuestion Loss: 94.9903 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 49\tNet Loss: 93.3871 \tQuestion Loss: 93.3871 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 49\tNet Loss: 95.0710 \tQuestion Loss: 95.0710 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 49\tNet Loss: 87.9925 \tQuestion Loss: 87.9925 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 49\tNet Loss: 89.2388 \tQuestion Loss: 89.2388 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 49\tNet Loss: 94.6790 \tQuestion Loss: 94.6790 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 49\tNet Loss: 88.0241 \tQuestion Loss: 88.0241 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 49\tNet Loss: 92.3844 \tQuestion Loss: 92.3844 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 49 : 92.3456 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 50\tNet Loss: 92.7371 \tQuestion Loss: 92.7371 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 50\tNet Loss: 89.2026 \tQuestion Loss: 89.2026 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 50\tNet Loss: 94.3604 \tQuestion Loss: 94.3604 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 50\tNet Loss: 93.3654 \tQuestion Loss: 93.3654 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 50\tNet Loss: 93.8558 \tQuestion Loss: 93.8558 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 50\tNet Loss: 93.7104 \tQuestion Loss: 93.7104 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 50\tNet Loss: 88.8800 \tQuestion Loss: 88.8800 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 50\tNet Loss: 94.9442 \tQuestion Loss: 94.9442 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 50\tNet Loss: 93.3349 \tQuestion Loss: 93.3349 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 50\tNet Loss: 95.3485 \tQuestion Loss: 95.3485 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 50\tNet Loss: 91.2317 \tQuestion Loss: 91.2317 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 50\tNet Loss: 95.6577 \tQuestion Loss: 95.6577 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 50\tNet Loss: 90.4255 \tQuestion Loss: 90.4255 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 50\tNet Loss: 92.7118 \tQuestion Loss: 92.7118 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 50\tNet Loss: 93.1845 \tQuestion Loss: 93.1845 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 50\tNet Loss: 95.3993 \tQuestion Loss: 95.3993 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 50\tNet Loss: 90.4143 \tQuestion Loss: 90.4143 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 50\tNet Loss: 92.4830 \tQuestion Loss: 92.4830 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 50\tNet Loss: 91.2151 \tQuestion Loss: 91.2151 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 50\tNet Loss: 86.9170 \tQuestion Loss: 86.9170 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 50\tNet Loss: 96.1429 \tQuestion Loss: 96.1429 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 50\tNet Loss: 88.2402 \tQuestion Loss: 88.2402 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 50\tNet Loss: 92.8663 \tQuestion Loss: 92.8663 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 50\tNet Loss: 95.8540 \tQuestion Loss: 95.8540 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 50\tNet Loss: 89.7796 \tQuestion Loss: 89.7796 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 50\tNet Loss: 93.1779 \tQuestion Loss: 93.1779 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 50\tNet Loss: 98.3989 \tQuestion Loss: 98.3989 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 50\tNet Loss: 91.9286 \tQuestion Loss: 91.9286 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 50\tNet Loss: 87.0874 \tQuestion Loss: 87.0874 \t Time Taken: 0 seconds\n",
      "Batch: 29 \t Epoch : 50\tNet Loss: 89.9273 \tQuestion Loss: 89.9273 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 50\tNet Loss: 91.6051 \tQuestion Loss: 91.6051 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 50\tNet Loss: 90.3501 \tQuestion Loss: 90.3501 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 50\tNet Loss: 89.0753 \tQuestion Loss: 89.0753 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 50\tNet Loss: 93.2875 \tQuestion Loss: 93.2875 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 50\tNet Loss: 87.4550 \tQuestion Loss: 87.4550 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 50\tNet Loss: 90.1913 \tQuestion Loss: 90.1913 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 50\tNet Loss: 93.9779 \tQuestion Loss: 93.9779 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 50\tNet Loss: 94.9475 \tQuestion Loss: 94.9475 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 50\tNet Loss: 91.0055 \tQuestion Loss: 91.0055 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 50\tNet Loss: 94.2059 \tQuestion Loss: 94.2059 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 50\tNet Loss: 91.5507 \tQuestion Loss: 91.5507 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 50\tNet Loss: 90.3288 \tQuestion Loss: 90.3288 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 50\tNet Loss: 92.1459 \tQuestion Loss: 92.1459 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 50\tNet Loss: 93.5606 \tQuestion Loss: 93.5606 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 50\tNet Loss: 90.7710 \tQuestion Loss: 90.7710 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 50\tNet Loss: 93.6600 \tQuestion Loss: 93.6600 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 46 \t Epoch : 50\tNet Loss: 92.1037 \tQuestion Loss: 92.1037 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 50\tNet Loss: 97.1831 \tQuestion Loss: 97.1831 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 50\tNet Loss: 86.9021 \tQuestion Loss: 86.9021 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 50\tNet Loss: 94.4067 \tQuestion Loss: 94.4067 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 50\tNet Loss: 92.9496 \tQuestion Loss: 92.9496 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 50\tNet Loss: 88.5372 \tQuestion Loss: 88.5372 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 50\tNet Loss: 89.5697 \tQuestion Loss: 89.5697 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 50\tNet Loss: 90.0012 \tQuestion Loss: 90.0012 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 50\tNet Loss: 96.4641 \tQuestion Loss: 96.4641 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 50\tNet Loss: 93.1361 \tQuestion Loss: 93.1361 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 50\tNet Loss: 94.3989 \tQuestion Loss: 94.3989 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 50\tNet Loss: 96.5903 \tQuestion Loss: 96.5903 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 50\tNet Loss: 87.5358 \tQuestion Loss: 87.5358 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 50\tNet Loss: 92.0585 \tQuestion Loss: 92.0585 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 50\tNet Loss: 89.0885 \tQuestion Loss: 89.0885 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 50\tNet Loss: 95.2204 \tQuestion Loss: 95.2204 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 50\tNet Loss: 88.8316 \tQuestion Loss: 88.8316 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 50\tNet Loss: 93.4719 \tQuestion Loss: 93.4719 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 50\tNet Loss: 92.0483 \tQuestion Loss: 92.0483 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 50\tNet Loss: 90.2982 \tQuestion Loss: 90.2982 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 50\tNet Loss: 92.5219 \tQuestion Loss: 92.5219 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 50\tNet Loss: 92.0929 \tQuestion Loss: 92.0929 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 50\tNet Loss: 90.8247 \tQuestion Loss: 90.8247 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 50\tNet Loss: 95.8882 \tQuestion Loss: 95.8882 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 50\tNet Loss: 93.2685 \tQuestion Loss: 93.2685 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 50\tNet Loss: 92.9016 \tQuestion Loss: 92.9016 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 50\tNet Loss: 95.5002 \tQuestion Loss: 95.5002 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 50\tNet Loss: 93.3180 \tQuestion Loss: 93.3180 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 50\tNet Loss: 94.7438 \tQuestion Loss: 94.7438 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 50\tNet Loss: 93.5094 \tQuestion Loss: 93.5094 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 50\tNet Loss: 86.7831 \tQuestion Loss: 86.7831 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 50\tNet Loss: 91.3264 \tQuestion Loss: 91.3264 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 50\tNet Loss: 97.1925 \tQuestion Loss: 97.1925 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 50\tNet Loss: 92.9210 \tQuestion Loss: 92.9210 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 50\tNet Loss: 94.3094 \tQuestion Loss: 94.3094 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 50\tNet Loss: 92.0418 \tQuestion Loss: 92.0418 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 50\tNet Loss: 89.5872 \tQuestion Loss: 89.5872 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 50\tNet Loss: 88.7026 \tQuestion Loss: 88.7026 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 50\tNet Loss: 92.0064 \tQuestion Loss: 92.0064 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 50\tNet Loss: 90.0424 \tQuestion Loss: 90.0424 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 50\tNet Loss: 91.6067 \tQuestion Loss: 91.6067 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 50\tNet Loss: 91.6545 \tQuestion Loss: 91.6545 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 50\tNet Loss: 94.5274 \tQuestion Loss: 94.5274 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 50\tNet Loss: 90.1193 \tQuestion Loss: 90.1193 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 50\tNet Loss: 95.2546 \tQuestion Loss: 95.2546 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 50\tNet Loss: 92.8935 \tQuestion Loss: 92.8935 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 50\tNet Loss: 91.4734 \tQuestion Loss: 91.4734 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 50\tNet Loss: 91.8941 \tQuestion Loss: 91.8941 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 50\tNet Loss: 92.4415 \tQuestion Loss: 92.4415 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 50\tNet Loss: 93.4146 \tQuestion Loss: 93.4146 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 50\tNet Loss: 91.8802 \tQuestion Loss: 91.8802 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 50\tNet Loss: 95.7790 \tQuestion Loss: 95.7790 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 50\tNet Loss: 92.6678 \tQuestion Loss: 92.6678 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 50\tNet Loss: 92.9207 \tQuestion Loss: 92.9207 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 50\tNet Loss: 92.5316 \tQuestion Loss: 92.5316 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 50\tNet Loss: 95.5538 \tQuestion Loss: 95.5538 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 50\tNet Loss: 93.0184 \tQuestion Loss: 93.0184 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 50\tNet Loss: 92.4910 \tQuestion Loss: 92.4910 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 50\tNet Loss: 95.0580 \tQuestion Loss: 95.0580 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 50\tNet Loss: 96.5281 \tQuestion Loss: 96.5281 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 50\tNet Loss: 92.2581 \tQuestion Loss: 92.2581 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 50\tNet Loss: 89.2299 \tQuestion Loss: 89.2299 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 50\tNet Loss: 88.4183 \tQuestion Loss: 88.4183 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 50\tNet Loss: 92.9684 \tQuestion Loss: 92.9684 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 50\tNet Loss: 93.1803 \tQuestion Loss: 93.1803 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 50\tNet Loss: 87.6411 \tQuestion Loss: 87.6411 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 50\tNet Loss: 89.8106 \tQuestion Loss: 89.8106 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 50\tNet Loss: 97.2835 \tQuestion Loss: 97.2835 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 50\tNet Loss: 94.2689 \tQuestion Loss: 94.2689 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 50\tNet Loss: 91.2918 \tQuestion Loss: 91.2918 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 50\tNet Loss: 88.1206 \tQuestion Loss: 88.1206 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 50\tNet Loss: 90.5743 \tQuestion Loss: 90.5743 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 50\tNet Loss: 93.8947 \tQuestion Loss: 93.8947 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 50\tNet Loss: 89.7179 \tQuestion Loss: 89.7179 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 50\tNet Loss: 91.4796 \tQuestion Loss: 91.4796 \t Time Taken: 0 seconds\n",
      "Batch: 121 \t Epoch : 50\tNet Loss: 93.5734 \tQuestion Loss: 93.5734 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 50\tNet Loss: 97.6935 \tQuestion Loss: 97.6935 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 50\tNet Loss: 92.9236 \tQuestion Loss: 92.9236 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 50\tNet Loss: 94.7263 \tQuestion Loss: 94.7263 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 50\tNet Loss: 92.6013 \tQuestion Loss: 92.6013 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 50\tNet Loss: 96.1724 \tQuestion Loss: 96.1724 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 50\tNet Loss: 87.9976 \tQuestion Loss: 87.9976 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 50\tNet Loss: 92.3173 \tQuestion Loss: 92.3173 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 50\tNet Loss: 94.8131 \tQuestion Loss: 94.8131 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 50\tNet Loss: 99.9234 \tQuestion Loss: 99.9234 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 50\tNet Loss: 94.8144 \tQuestion Loss: 94.8144 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 50\tNet Loss: 91.2927 \tQuestion Loss: 91.2927 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 50\tNet Loss: 92.7266 \tQuestion Loss: 92.7266 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 50\tNet Loss: 93.7493 \tQuestion Loss: 93.7493 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 50\tNet Loss: 85.5840 \tQuestion Loss: 85.5840 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 50\tNet Loss: 97.3409 \tQuestion Loss: 97.3409 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 50\tNet Loss: 95.0107 \tQuestion Loss: 95.0107 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 138 \t Epoch : 50\tNet Loss: 93.3195 \tQuestion Loss: 93.3195 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 50\tNet Loss: 92.0521 \tQuestion Loss: 92.0521 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 50\tNet Loss: 89.3162 \tQuestion Loss: 89.3162 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 50\tNet Loss: 89.7517 \tQuestion Loss: 89.7517 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 50\tNet Loss: 94.5487 \tQuestion Loss: 94.5487 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 50\tNet Loss: 89.2899 \tQuestion Loss: 89.2899 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 50\tNet Loss: 86.9231 \tQuestion Loss: 86.9231 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 50\tNet Loss: 95.3363 \tQuestion Loss: 95.3363 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 50\tNet Loss: 98.6982 \tQuestion Loss: 98.6982 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 50\tNet Loss: 86.9436 \tQuestion Loss: 86.9436 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 50\tNet Loss: 96.5871 \tQuestion Loss: 96.5871 \t Time Taken: 0 seconds\n",
      "Batch: 149 \t Epoch : 50\tNet Loss: 90.0893 \tQuestion Loss: 90.0893 \t Time Taken: 0 seconds\n",
      "Batch: 150 \t Epoch : 50\tNet Loss: 91.2366 \tQuestion Loss: 91.2366 \t Time Taken: 0 seconds\n",
      "Batch: 151 \t Epoch : 50\tNet Loss: 97.9411 \tQuestion Loss: 97.9411 \t Time Taken: 0 seconds\n",
      "Batch: 152 \t Epoch : 50\tNet Loss: 93.7369 \tQuestion Loss: 93.7369 \t Time Taken: 0 seconds\n",
      "Batch: 153 \t Epoch : 50\tNet Loss: 94.1438 \tQuestion Loss: 94.1438 \t Time Taken: 0 seconds\n",
      "Batch: 154 \t Epoch : 50\tNet Loss: 85.0916 \tQuestion Loss: 85.0916 \t Time Taken: 0 seconds\n",
      "Batch: 155 \t Epoch : 50\tNet Loss: 90.5238 \tQuestion Loss: 90.5238 \t Time Taken: 0 seconds\n",
      "Batch: 156 \t Epoch : 50\tNet Loss: 93.5186 \tQuestion Loss: 93.5186 \t Time Taken: 1 seconds\n",
      "Batch: 157 \t Epoch : 50\tNet Loss: 88.5005 \tQuestion Loss: 88.5005 \t Time Taken: 0 seconds\n",
      "Batch: 158 \t Epoch : 50\tNet Loss: 93.7721 \tQuestion Loss: 93.7721 \t Time Taken: 0 seconds\n",
      "Batch: 159 \t Epoch : 50\tNet Loss: 94.5786 \tQuestion Loss: 94.5786 \t Time Taken: 0 seconds\n",
      "Batch: 160 \t Epoch : 50\tNet Loss: 95.9065 \tQuestion Loss: 95.9065 \t Time Taken: 0 seconds\n",
      "Batch: 161 \t Epoch : 50\tNet Loss: 94.2286 \tQuestion Loss: 94.2286 \t Time Taken: 0 seconds\n",
      "Batch: 162 \t Epoch : 50\tNet Loss: 95.8789 \tQuestion Loss: 95.8789 \t Time Taken: 0 seconds\n",
      "Batch: 163 \t Epoch : 50\tNet Loss: 89.6011 \tQuestion Loss: 89.6011 \t Time Taken: 0 seconds\n",
      "Batch: 164 \t Epoch : 50\tNet Loss: 88.5166 \tQuestion Loss: 88.5166 \t Time Taken: 0 seconds\n",
      "Batch: 165 \t Epoch : 50\tNet Loss: 89.7511 \tQuestion Loss: 89.7511 \t Time Taken: 0 seconds\n",
      "Batch: 166 \t Epoch : 50\tNet Loss: 87.5538 \tQuestion Loss: 87.5538 \t Time Taken: 0 seconds\n",
      "Batch: 167 \t Epoch : 50\tNet Loss: 92.6045 \tQuestion Loss: 92.6045 \t Time Taken: 0 seconds\n",
      "Batch: 168 \t Epoch : 50\tNet Loss: 91.4959 \tQuestion Loss: 91.4959 \t Time Taken: 0 seconds\n",
      "Batch: 169 \t Epoch : 50\tNet Loss: 95.7805 \tQuestion Loss: 95.7805 \t Time Taken: 0 seconds\n",
      "Batch: 170 \t Epoch : 50\tNet Loss: 93.5816 \tQuestion Loss: 93.5816 \t Time Taken: 0 seconds\n",
      "Batch: 171 \t Epoch : 50\tNet Loss: 92.4801 \tQuestion Loss: 92.4801 \t Time Taken: 0 seconds\n",
      "Batch: 172 \t Epoch : 50\tNet Loss: 89.2533 \tQuestion Loss: 89.2533 \t Time Taken: 0 seconds\n",
      "Batch: 173 \t Epoch : 50\tNet Loss: 90.6633 \tQuestion Loss: 90.6633 \t Time Taken: 0 seconds\n",
      "Batch: 174 \t Epoch : 50\tNet Loss: 94.6844 \tQuestion Loss: 94.6844 \t Time Taken: 0 seconds\n",
      "Batch: 175 \t Epoch : 50\tNet Loss: 97.5780 \tQuestion Loss: 97.5780 \t Time Taken: 0 seconds\n",
      "Batch: 176 \t Epoch : 50\tNet Loss: 90.0933 \tQuestion Loss: 90.0933 \t Time Taken: 0 seconds\n",
      "Batch: 177 \t Epoch : 50\tNet Loss: 91.5476 \tQuestion Loss: 91.5476 \t Time Taken: 0 seconds\n",
      "Batch: 178 \t Epoch : 50\tNet Loss: 90.5628 \tQuestion Loss: 90.5628 \t Time Taken: 0 seconds\n",
      "Batch: 179 \t Epoch : 50\tNet Loss: 91.6540 \tQuestion Loss: 91.6540 \t Time Taken: 0 seconds\n",
      "Batch: 180 \t Epoch : 50\tNet Loss: 90.0656 \tQuestion Loss: 90.0656 \t Time Taken: 0 seconds\n",
      "Batch: 181 \t Epoch : 50\tNet Loss: 92.4081 \tQuestion Loss: 92.4081 \t Time Taken: 0 seconds\n",
      "Batch: 182 \t Epoch : 50\tNet Loss: 94.3606 \tQuestion Loss: 94.3606 \t Time Taken: 0 seconds\n",
      "Batch: 183 \t Epoch : 50\tNet Loss: 89.3706 \tQuestion Loss: 89.3706 \t Time Taken: 0 seconds\n",
      "Batch: 184 \t Epoch : 50\tNet Loss: 93.2850 \tQuestion Loss: 93.2850 \t Time Taken: 0 seconds\n",
      "Batch: 185 \t Epoch : 50\tNet Loss: 91.6768 \tQuestion Loss: 91.6768 \t Time Taken: 0 seconds\n",
      "Batch: 186 \t Epoch : 50\tNet Loss: 93.9698 \tQuestion Loss: 93.9698 \t Time Taken: 0 seconds\n",
      "Batch: 187 \t Epoch : 50\tNet Loss: 95.0343 \tQuestion Loss: 95.0343 \t Time Taken: 0 seconds\n",
      "Batch: 188 \t Epoch : 50\tNet Loss: 89.9988 \tQuestion Loss: 89.9988 \t Time Taken: 0 seconds\n",
      "Batch: 189 \t Epoch : 50\tNet Loss: 89.4698 \tQuestion Loss: 89.4698 \t Time Taken: 0 seconds\n",
      "Batch: 190 \t Epoch : 50\tNet Loss: 91.1561 \tQuestion Loss: 91.1561 \t Time Taken: 0 seconds\n",
      "Batch: 191 \t Epoch : 50\tNet Loss: 90.0640 \tQuestion Loss: 90.0640 \t Time Taken: 0 seconds\n",
      "Batch: 192 \t Epoch : 50\tNet Loss: 94.8981 \tQuestion Loss: 94.8981 \t Time Taken: 0 seconds\n",
      "Batch: 193 \t Epoch : 50\tNet Loss: 93.4018 \tQuestion Loss: 93.4018 \t Time Taken: 0 seconds\n",
      "Batch: 194 \t Epoch : 50\tNet Loss: 95.2418 \tQuestion Loss: 95.2418 \t Time Taken: 0 seconds\n",
      "Batch: 195 \t Epoch : 50\tNet Loss: 88.0199 \tQuestion Loss: 88.0199 \t Time Taken: 0 seconds\n",
      "Batch: 196 \t Epoch : 50\tNet Loss: 89.2363 \tQuestion Loss: 89.2363 \t Time Taken: 0 seconds\n",
      "Batch: 197 \t Epoch : 50\tNet Loss: 94.6355 \tQuestion Loss: 94.6355 \t Time Taken: 0 seconds\n",
      "Batch: 198 \t Epoch : 50\tNet Loss: 87.7231 \tQuestion Loss: 87.7231 \t Time Taken: 0 seconds\n",
      "Batch: 199 \t Epoch : 50\tNet Loss: 92.4511 \tQuestion Loss: 92.4511 \t Time Taken: 0 seconds\n",
      "Average Loss after Epoch 50 : 92.3203 Time Taken: 152 seconds\n",
      "Batch: 0 \t Epoch : 51\tNet Loss: 92.7087 \tQuestion Loss: 92.7087 \t Time Taken: 0 seconds\n",
      "Batch: 1 \t Epoch : 51\tNet Loss: 89.0045 \tQuestion Loss: 89.0045 \t Time Taken: 0 seconds\n",
      "Batch: 2 \t Epoch : 51\tNet Loss: 94.3197 \tQuestion Loss: 94.3197 \t Time Taken: 0 seconds\n",
      "Batch: 3 \t Epoch : 51\tNet Loss: 93.7586 \tQuestion Loss: 93.7586 \t Time Taken: 0 seconds\n",
      "Batch: 4 \t Epoch : 51\tNet Loss: 93.7430 \tQuestion Loss: 93.7430 \t Time Taken: 0 seconds\n",
      "Batch: 5 \t Epoch : 51\tNet Loss: 93.6451 \tQuestion Loss: 93.6451 \t Time Taken: 0 seconds\n",
      "Batch: 6 \t Epoch : 51\tNet Loss: 88.8136 \tQuestion Loss: 88.8136 \t Time Taken: 0 seconds\n",
      "Batch: 7 \t Epoch : 51\tNet Loss: 94.9011 \tQuestion Loss: 94.9011 \t Time Taken: 0 seconds\n",
      "Batch: 8 \t Epoch : 51\tNet Loss: 93.6243 \tQuestion Loss: 93.6243 \t Time Taken: 0 seconds\n",
      "Batch: 9 \t Epoch : 51\tNet Loss: 94.6487 \tQuestion Loss: 94.6487 \t Time Taken: 0 seconds\n",
      "Batch: 10 \t Epoch : 51\tNet Loss: 91.0757 \tQuestion Loss: 91.0757 \t Time Taken: 0 seconds\n",
      "Batch: 11 \t Epoch : 51\tNet Loss: 95.6496 \tQuestion Loss: 95.6496 \t Time Taken: 0 seconds\n",
      "Batch: 12 \t Epoch : 51\tNet Loss: 90.4021 \tQuestion Loss: 90.4021 \t Time Taken: 0 seconds\n",
      "Batch: 13 \t Epoch : 51\tNet Loss: 92.7239 \tQuestion Loss: 92.7239 \t Time Taken: 0 seconds\n",
      "Batch: 14 \t Epoch : 51\tNet Loss: 92.9139 \tQuestion Loss: 92.9139 \t Time Taken: 0 seconds\n",
      "Batch: 15 \t Epoch : 51\tNet Loss: 95.5644 \tQuestion Loss: 95.5644 \t Time Taken: 0 seconds\n",
      "Batch: 16 \t Epoch : 51\tNet Loss: 90.5841 \tQuestion Loss: 90.5841 \t Time Taken: 0 seconds\n",
      "Batch: 17 \t Epoch : 51\tNet Loss: 92.4443 \tQuestion Loss: 92.4443 \t Time Taken: 0 seconds\n",
      "Batch: 18 \t Epoch : 51\tNet Loss: 90.3819 \tQuestion Loss: 90.3819 \t Time Taken: 0 seconds\n",
      "Batch: 19 \t Epoch : 51\tNet Loss: 86.7269 \tQuestion Loss: 86.7269 \t Time Taken: 0 seconds\n",
      "Batch: 20 \t Epoch : 51\tNet Loss: 96.0816 \tQuestion Loss: 96.0816 \t Time Taken: 0 seconds\n",
      "Batch: 21 \t Epoch : 51\tNet Loss: 88.1931 \tQuestion Loss: 88.1931 \t Time Taken: 0 seconds\n",
      "Batch: 22 \t Epoch : 51\tNet Loss: 92.8728 \tQuestion Loss: 92.8728 \t Time Taken: 0 seconds\n",
      "Batch: 23 \t Epoch : 51\tNet Loss: 95.9385 \tQuestion Loss: 95.9385 \t Time Taken: 0 seconds\n",
      "Batch: 24 \t Epoch : 51\tNet Loss: 89.8880 \tQuestion Loss: 89.8880 \t Time Taken: 0 seconds\n",
      "Batch: 25 \t Epoch : 51\tNet Loss: 93.3201 \tQuestion Loss: 93.3201 \t Time Taken: 0 seconds\n",
      "Batch: 26 \t Epoch : 51\tNet Loss: 98.4366 \tQuestion Loss: 98.4366 \t Time Taken: 0 seconds\n",
      "Batch: 27 \t Epoch : 51\tNet Loss: 91.9091 \tQuestion Loss: 91.9091 \t Time Taken: 0 seconds\n",
      "Batch: 28 \t Epoch : 51\tNet Loss: 86.9308 \tQuestion Loss: 86.9308 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 29 \t Epoch : 51\tNet Loss: 89.9133 \tQuestion Loss: 89.9133 \t Time Taken: 0 seconds\n",
      "Batch: 30 \t Epoch : 51\tNet Loss: 91.6474 \tQuestion Loss: 91.6474 \t Time Taken: 0 seconds\n",
      "Batch: 31 \t Epoch : 51\tNet Loss: 90.4685 \tQuestion Loss: 90.4685 \t Time Taken: 0 seconds\n",
      "Batch: 32 \t Epoch : 51\tNet Loss: 88.7984 \tQuestion Loss: 88.7984 \t Time Taken: 0 seconds\n",
      "Batch: 33 \t Epoch : 51\tNet Loss: 93.3062 \tQuestion Loss: 93.3062 \t Time Taken: 0 seconds\n",
      "Batch: 34 \t Epoch : 51\tNet Loss: 87.3558 \tQuestion Loss: 87.3558 \t Time Taken: 0 seconds\n",
      "Batch: 35 \t Epoch : 51\tNet Loss: 90.1345 \tQuestion Loss: 90.1345 \t Time Taken: 0 seconds\n",
      "Batch: 36 \t Epoch : 51\tNet Loss: 93.7949 \tQuestion Loss: 93.7949 \t Time Taken: 0 seconds\n",
      "Batch: 37 \t Epoch : 51\tNet Loss: 95.4328 \tQuestion Loss: 95.4328 \t Time Taken: 0 seconds\n",
      "Batch: 38 \t Epoch : 51\tNet Loss: 91.2832 \tQuestion Loss: 91.2832 \t Time Taken: 0 seconds\n",
      "Batch: 39 \t Epoch : 51\tNet Loss: 93.8742 \tQuestion Loss: 93.8742 \t Time Taken: 0 seconds\n",
      "Batch: 40 \t Epoch : 51\tNet Loss: 91.5454 \tQuestion Loss: 91.5454 \t Time Taken: 0 seconds\n",
      "Batch: 41 \t Epoch : 51\tNet Loss: 90.1817 \tQuestion Loss: 90.1817 \t Time Taken: 0 seconds\n",
      "Batch: 42 \t Epoch : 51\tNet Loss: 92.0190 \tQuestion Loss: 92.0190 \t Time Taken: 0 seconds\n",
      "Batch: 43 \t Epoch : 51\tNet Loss: 93.3612 \tQuestion Loss: 93.3612 \t Time Taken: 0 seconds\n",
      "Batch: 44 \t Epoch : 51\tNet Loss: 90.5853 \tQuestion Loss: 90.5853 \t Time Taken: 0 seconds\n",
      "Batch: 45 \t Epoch : 51\tNet Loss: 93.6583 \tQuestion Loss: 93.6583 \t Time Taken: 0 seconds\n",
      "Batch: 46 \t Epoch : 51\tNet Loss: 92.3437 \tQuestion Loss: 92.3437 \t Time Taken: 0 seconds\n",
      "Batch: 47 \t Epoch : 51\tNet Loss: 97.3729 \tQuestion Loss: 97.3729 \t Time Taken: 0 seconds\n",
      "Batch: 48 \t Epoch : 51\tNet Loss: 86.0374 \tQuestion Loss: 86.0374 \t Time Taken: 0 seconds\n",
      "Batch: 49 \t Epoch : 51\tNet Loss: 93.9494 \tQuestion Loss: 93.9494 \t Time Taken: 0 seconds\n",
      "Batch: 50 \t Epoch : 51\tNet Loss: 92.9567 \tQuestion Loss: 92.9567 \t Time Taken: 0 seconds\n",
      "Batch: 51 \t Epoch : 51\tNet Loss: 88.5527 \tQuestion Loss: 88.5527 \t Time Taken: 0 seconds\n",
      "Batch: 52 \t Epoch : 51\tNet Loss: 89.5606 \tQuestion Loss: 89.5606 \t Time Taken: 0 seconds\n",
      "Batch: 53 \t Epoch : 51\tNet Loss: 89.7454 \tQuestion Loss: 89.7454 \t Time Taken: 0 seconds\n",
      "Batch: 54 \t Epoch : 51\tNet Loss: 96.4922 \tQuestion Loss: 96.4922 \t Time Taken: 0 seconds\n",
      "Batch: 55 \t Epoch : 51\tNet Loss: 93.3477 \tQuestion Loss: 93.3477 \t Time Taken: 0 seconds\n",
      "Batch: 56 \t Epoch : 51\tNet Loss: 94.4224 \tQuestion Loss: 94.4224 \t Time Taken: 0 seconds\n",
      "Batch: 57 \t Epoch : 51\tNet Loss: 96.6027 \tQuestion Loss: 96.6027 \t Time Taken: 0 seconds\n",
      "Batch: 58 \t Epoch : 51\tNet Loss: 87.6102 \tQuestion Loss: 87.6102 \t Time Taken: 0 seconds\n",
      "Batch: 59 \t Epoch : 51\tNet Loss: 92.2634 \tQuestion Loss: 92.2634 \t Time Taken: 0 seconds\n",
      "Batch: 60 \t Epoch : 51\tNet Loss: 89.1597 \tQuestion Loss: 89.1597 \t Time Taken: 0 seconds\n",
      "Batch: 61 \t Epoch : 51\tNet Loss: 95.3081 \tQuestion Loss: 95.3081 \t Time Taken: 0 seconds\n",
      "Batch: 62 \t Epoch : 51\tNet Loss: 88.8155 \tQuestion Loss: 88.8155 \t Time Taken: 0 seconds\n",
      "Batch: 63 \t Epoch : 51\tNet Loss: 93.5566 \tQuestion Loss: 93.5566 \t Time Taken: 0 seconds\n",
      "Batch: 64 \t Epoch : 51\tNet Loss: 92.2108 \tQuestion Loss: 92.2108 \t Time Taken: 0 seconds\n",
      "Batch: 65 \t Epoch : 51\tNet Loss: 90.9041 \tQuestion Loss: 90.9041 \t Time Taken: 0 seconds\n",
      "Batch: 66 \t Epoch : 51\tNet Loss: 92.0812 \tQuestion Loss: 92.0812 \t Time Taken: 0 seconds\n",
      "Batch: 67 \t Epoch : 51\tNet Loss: 91.3618 \tQuestion Loss: 91.3618 \t Time Taken: 0 seconds\n",
      "Batch: 68 \t Epoch : 51\tNet Loss: 90.9101 \tQuestion Loss: 90.9101 \t Time Taken: 0 seconds\n",
      "Batch: 69 \t Epoch : 51\tNet Loss: 95.6278 \tQuestion Loss: 95.6278 \t Time Taken: 0 seconds\n",
      "Batch: 70 \t Epoch : 51\tNet Loss: 93.2880 \tQuestion Loss: 93.2880 \t Time Taken: 0 seconds\n",
      "Batch: 71 \t Epoch : 51\tNet Loss: 92.6796 \tQuestion Loss: 92.6796 \t Time Taken: 0 seconds\n",
      "Batch: 72 \t Epoch : 51\tNet Loss: 95.7252 \tQuestion Loss: 95.7252 \t Time Taken: 0 seconds\n",
      "Batch: 73 \t Epoch : 51\tNet Loss: 93.9907 \tQuestion Loss: 93.9907 \t Time Taken: 0 seconds\n",
      "Batch: 74 \t Epoch : 51\tNet Loss: 94.9051 \tQuestion Loss: 94.9051 \t Time Taken: 0 seconds\n",
      "Batch: 75 \t Epoch : 51\tNet Loss: 93.5158 \tQuestion Loss: 93.5158 \t Time Taken: 0 seconds\n",
      "Batch: 76 \t Epoch : 51\tNet Loss: 86.8557 \tQuestion Loss: 86.8557 \t Time Taken: 0 seconds\n",
      "Batch: 77 \t Epoch : 51\tNet Loss: 91.7492 \tQuestion Loss: 91.7492 \t Time Taken: 0 seconds\n",
      "Batch: 78 \t Epoch : 51\tNet Loss: 96.0905 \tQuestion Loss: 96.0905 \t Time Taken: 0 seconds\n",
      "Batch: 79 \t Epoch : 51\tNet Loss: 92.8974 \tQuestion Loss: 92.8974 \t Time Taken: 0 seconds\n",
      "Batch: 80 \t Epoch : 51\tNet Loss: 94.2232 \tQuestion Loss: 94.2232 \t Time Taken: 0 seconds\n",
      "Batch: 81 \t Epoch : 51\tNet Loss: 92.1905 \tQuestion Loss: 92.1905 \t Time Taken: 0 seconds\n",
      "Batch: 82 \t Epoch : 51\tNet Loss: 89.6083 \tQuestion Loss: 89.6083 \t Time Taken: 0 seconds\n",
      "Batch: 83 \t Epoch : 51\tNet Loss: 88.5793 \tQuestion Loss: 88.5793 \t Time Taken: 0 seconds\n",
      "Batch: 84 \t Epoch : 51\tNet Loss: 91.9524 \tQuestion Loss: 91.9524 \t Time Taken: 0 seconds\n",
      "Batch: 85 \t Epoch : 51\tNet Loss: 90.6251 \tQuestion Loss: 90.6251 \t Time Taken: 0 seconds\n",
      "Batch: 86 \t Epoch : 51\tNet Loss: 92.7639 \tQuestion Loss: 92.7639 \t Time Taken: 0 seconds\n",
      "Batch: 87 \t Epoch : 51\tNet Loss: 91.6990 \tQuestion Loss: 91.6990 \t Time Taken: 0 seconds\n",
      "Batch: 88 \t Epoch : 51\tNet Loss: 94.3828 \tQuestion Loss: 94.3828 \t Time Taken: 0 seconds\n",
      "Batch: 89 \t Epoch : 51\tNet Loss: 90.0690 \tQuestion Loss: 90.0690 \t Time Taken: 0 seconds\n",
      "Batch: 90 \t Epoch : 51\tNet Loss: 95.4117 \tQuestion Loss: 95.4117 \t Time Taken: 0 seconds\n",
      "Batch: 91 \t Epoch : 51\tNet Loss: 92.9136 \tQuestion Loss: 92.9136 \t Time Taken: 0 seconds\n",
      "Batch: 92 \t Epoch : 51\tNet Loss: 91.3554 \tQuestion Loss: 91.3554 \t Time Taken: 0 seconds\n",
      "Batch: 93 \t Epoch : 51\tNet Loss: 92.0045 \tQuestion Loss: 92.0045 \t Time Taken: 0 seconds\n",
      "Batch: 94 \t Epoch : 51\tNet Loss: 92.7341 \tQuestion Loss: 92.7341 \t Time Taken: 0 seconds\n",
      "Batch: 95 \t Epoch : 51\tNet Loss: 93.5289 \tQuestion Loss: 93.5289 \t Time Taken: 0 seconds\n",
      "Batch: 96 \t Epoch : 51\tNet Loss: 91.9178 \tQuestion Loss: 91.9178 \t Time Taken: 0 seconds\n",
      "Batch: 97 \t Epoch : 51\tNet Loss: 95.7972 \tQuestion Loss: 95.7972 \t Time Taken: 0 seconds\n",
      "Batch: 98 \t Epoch : 51\tNet Loss: 92.8103 \tQuestion Loss: 92.8103 \t Time Taken: 0 seconds\n",
      "Batch: 99 \t Epoch : 51\tNet Loss: 93.1802 \tQuestion Loss: 93.1802 \t Time Taken: 0 seconds\n",
      "Batch: 100 \t Epoch : 51\tNet Loss: 92.4457 \tQuestion Loss: 92.4457 \t Time Taken: 0 seconds\n",
      "Batch: 101 \t Epoch : 51\tNet Loss: 95.6040 \tQuestion Loss: 95.6040 \t Time Taken: 0 seconds\n",
      "Batch: 102 \t Epoch : 51\tNet Loss: 93.0749 \tQuestion Loss: 93.0749 \t Time Taken: 0 seconds\n",
      "Batch: 103 \t Epoch : 51\tNet Loss: 92.5391 \tQuestion Loss: 92.5391 \t Time Taken: 0 seconds\n",
      "Batch: 104 \t Epoch : 51\tNet Loss: 95.1901 \tQuestion Loss: 95.1901 \t Time Taken: 0 seconds\n",
      "Batch: 105 \t Epoch : 51\tNet Loss: 96.6488 \tQuestion Loss: 96.6488 \t Time Taken: 0 seconds\n",
      "Batch: 106 \t Epoch : 51\tNet Loss: 92.1955 \tQuestion Loss: 92.1955 \t Time Taken: 0 seconds\n",
      "Batch: 107 \t Epoch : 51\tNet Loss: 89.2051 \tQuestion Loss: 89.2051 \t Time Taken: 0 seconds\n",
      "Batch: 108 \t Epoch : 51\tNet Loss: 88.4076 \tQuestion Loss: 88.4076 \t Time Taken: 0 seconds\n",
      "Batch: 109 \t Epoch : 51\tNet Loss: 92.8662 \tQuestion Loss: 92.8662 \t Time Taken: 0 seconds\n",
      "Batch: 110 \t Epoch : 51\tNet Loss: 93.5022 \tQuestion Loss: 93.5022 \t Time Taken: 0 seconds\n",
      "Batch: 111 \t Epoch : 51\tNet Loss: 87.8506 \tQuestion Loss: 87.8506 \t Time Taken: 0 seconds\n",
      "Batch: 112 \t Epoch : 51\tNet Loss: 89.7274 \tQuestion Loss: 89.7274 \t Time Taken: 0 seconds\n",
      "Batch: 113 \t Epoch : 51\tNet Loss: 97.1034 \tQuestion Loss: 97.1034 \t Time Taken: 0 seconds\n",
      "Batch: 114 \t Epoch : 51\tNet Loss: 94.1433 \tQuestion Loss: 94.1433 \t Time Taken: 0 seconds\n",
      "Batch: 115 \t Epoch : 51\tNet Loss: 91.0735 \tQuestion Loss: 91.0735 \t Time Taken: 0 seconds\n",
      "Batch: 116 \t Epoch : 51\tNet Loss: 88.4794 \tQuestion Loss: 88.4794 \t Time Taken: 0 seconds\n",
      "Batch: 117 \t Epoch : 51\tNet Loss: 90.6076 \tQuestion Loss: 90.6076 \t Time Taken: 0 seconds\n",
      "Batch: 118 \t Epoch : 51\tNet Loss: 93.9273 \tQuestion Loss: 93.9273 \t Time Taken: 0 seconds\n",
      "Batch: 119 \t Epoch : 51\tNet Loss: 89.6391 \tQuestion Loss: 89.6391 \t Time Taken: 0 seconds\n",
      "Batch: 120 \t Epoch : 51\tNet Loss: 91.5247 \tQuestion Loss: 91.5247 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 121 \t Epoch : 51\tNet Loss: 93.8885 \tQuestion Loss: 93.8885 \t Time Taken: 0 seconds\n",
      "Batch: 122 \t Epoch : 51\tNet Loss: 96.8583 \tQuestion Loss: 96.8583 \t Time Taken: 0 seconds\n",
      "Batch: 123 \t Epoch : 51\tNet Loss: 92.7146 \tQuestion Loss: 92.7146 \t Time Taken: 0 seconds\n",
      "Batch: 124 \t Epoch : 51\tNet Loss: 94.6840 \tQuestion Loss: 94.6840 \t Time Taken: 0 seconds\n",
      "Batch: 125 \t Epoch : 51\tNet Loss: 92.8001 \tQuestion Loss: 92.8001 \t Time Taken: 0 seconds\n",
      "Batch: 126 \t Epoch : 51\tNet Loss: 96.4561 \tQuestion Loss: 96.4561 \t Time Taken: 0 seconds\n",
      "Batch: 127 \t Epoch : 51\tNet Loss: 88.0429 \tQuestion Loss: 88.0429 \t Time Taken: 0 seconds\n",
      "Batch: 128 \t Epoch : 51\tNet Loss: 93.2485 \tQuestion Loss: 93.2485 \t Time Taken: 0 seconds\n",
      "Batch: 129 \t Epoch : 51\tNet Loss: 93.1616 \tQuestion Loss: 93.1616 \t Time Taken: 0 seconds\n",
      "Batch: 130 \t Epoch : 51\tNet Loss: 100.1947 \tQuestion Loss: 100.1947 \t Time Taken: 0 seconds\n",
      "Batch: 131 \t Epoch : 51\tNet Loss: 94.8028 \tQuestion Loss: 94.8028 \t Time Taken: 0 seconds\n",
      "Batch: 132 \t Epoch : 51\tNet Loss: 91.1218 \tQuestion Loss: 91.1218 \t Time Taken: 0 seconds\n",
      "Batch: 133 \t Epoch : 51\tNet Loss: 92.7665 \tQuestion Loss: 92.7665 \t Time Taken: 0 seconds\n",
      "Batch: 134 \t Epoch : 51\tNet Loss: 92.6641 \tQuestion Loss: 92.6641 \t Time Taken: 0 seconds\n",
      "Batch: 135 \t Epoch : 51\tNet Loss: 86.5795 \tQuestion Loss: 86.5795 \t Time Taken: 0 seconds\n",
      "Batch: 136 \t Epoch : 51\tNet Loss: 97.3702 \tQuestion Loss: 97.3702 \t Time Taken: 0 seconds\n",
      "Batch: 137 \t Epoch : 51\tNet Loss: 95.2050 \tQuestion Loss: 95.2050 \t Time Taken: 0 seconds\n",
      "Batch: 138 \t Epoch : 51\tNet Loss: 93.5684 \tQuestion Loss: 93.5684 \t Time Taken: 0 seconds\n",
      "Batch: 139 \t Epoch : 51\tNet Loss: 92.0939 \tQuestion Loss: 92.0939 \t Time Taken: 0 seconds\n",
      "Batch: 140 \t Epoch : 51\tNet Loss: 89.3429 \tQuestion Loss: 89.3429 \t Time Taken: 0 seconds\n",
      "Batch: 141 \t Epoch : 51\tNet Loss: 89.8705 \tQuestion Loss: 89.8705 \t Time Taken: 0 seconds\n",
      "Batch: 142 \t Epoch : 51\tNet Loss: 94.4574 \tQuestion Loss: 94.4574 \t Time Taken: 0 seconds\n",
      "Batch: 143 \t Epoch : 51\tNet Loss: 89.3649 \tQuestion Loss: 89.3649 \t Time Taken: 0 seconds\n",
      "Batch: 144 \t Epoch : 51\tNet Loss: 87.0221 \tQuestion Loss: 87.0221 \t Time Taken: 0 seconds\n",
      "Batch: 145 \t Epoch : 51\tNet Loss: 95.7638 \tQuestion Loss: 95.7638 \t Time Taken: 0 seconds\n",
      "Batch: 146 \t Epoch : 51\tNet Loss: 98.8455 \tQuestion Loss: 98.8455 \t Time Taken: 0 seconds\n",
      "Batch: 147 \t Epoch : 51\tNet Loss: 87.0438 \tQuestion Loss: 87.0438 \t Time Taken: 0 seconds\n",
      "Batch: 148 \t Epoch : 51\tNet Loss: 96.5543 \tQuestion Loss: 96.5543 \t Time Taken: 0 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-232-14cc594c2406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainEpoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mverboseBatchPrinting\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-231-0e91fd0f26dc>\u001b[0m in \u001b[0;36mtrainEpoch\u001b[0;34m(verboseBatchPrinting)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0mnet_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestion_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mnet_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/ra2630/miniconda3/envs/qgen/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data/ra2630/miniconda3/envs/qgen/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "verboseBatchPrinting = True\n",
    "averageBatchLossPrinting = True\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    avg_loss, time_taken = trainEpoch(verboseBatchPrinting)\n",
    "    \n",
    "\n",
    "    if averageBatchLossPrinting:\n",
    "        print('Average Loss after Epoch %d : %.4f Time Taken: %d seconds'\n",
    "                   %(epoch, avg_loss, time_taken))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(path):\n",
    "    d = dict()\n",
    "    documentEncoder.train()\n",
    "    questionDecoder.train()\n",
    "    questionGenerator.train()\n",
    "    attention.train()\n",
    "    d['documentEncoder'] = documentEncoder.state_dict()\n",
    "    d['questionDecoder'] = questionDecoder.state_dict()\n",
    "    d['questionGenerator'] = questionGenerator.state_dict()\n",
    "    d['attention'] = attention.state_dict()\n",
    "    d['optimizer'] = optimizer.state_dict()\n",
    "    torch.save(d, path)\n",
    "save(\"/data/ra2630/32k_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/data/ra2630/3k_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reduced_glove.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareSetForInferenceFromBatch(batch_number):\n",
    "    data_dict = batch_input[batch_number]\n",
    "    data_dict['answer_mask_1'] = data_dict['answer_labels']\n",
    "    data_dict['answer_mask_2'] = 1 - data_dict['answer_mask_1']\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareSetForInference(context, answer, maxDocLength, batch_size):\n",
    "    \n",
    "    passage = word_tokenize(context.lower())\n",
    "    doc_len = len(passage)\n",
    "    \n",
    "    ans_condition = word_tokenize(answer.lower())\n",
    "    \n",
    "    ans_len = len(ans_condition)\n",
    "        \n",
    "    start,end = find_sub_list(ans_condition,passage)\n",
    "\n",
    "    if start == -1:\n",
    "            print(\"Couldn't Find answer in text, Please try again !!\")\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    document_token = np.full((1, maxDocLength), END_TOKEN, dtype=np.int32)\n",
    "    document_length = np.zeros(1, dtype=np.int32)\n",
    "    answer_mask_1 = np.zeros((1,maxDocLength), dtype=np.int32)\n",
    "    answer_mask_2 = np.zeros((1,maxDocLength), dtype=np.int32)\n",
    "    answer_length = np.zeros(1, dtype=np.int32)\n",
    "    \n",
    "    document_length[0] = doc_len\n",
    "    answer_length[0] = ans_len\n",
    "    answer_mask_1[0,start:end] = 1\n",
    "    answer_mask_2 = 1 - answer_mask_1\n",
    "    \n",
    "    \n",
    "    for i in range(doc_len):\n",
    "        document_token[:,i] = look_up_word_reduced(passage[i])\n",
    "    \n",
    "    document_token = document_token.repeat(batch_size, axis = 0)\n",
    "    document_length = document_length.repeat(batch_size, axis = 0)\n",
    "    answer_mask_1 = answer_mask_1.repeat(batch_size, axis = 0)\n",
    "    answer_mask_2 = answer_mask_2.repeat(batch_size, axis = 0)\n",
    "    answer_length = answer_length.repeat(batch_size, axis = 0)\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"document_tokens\" : document_token,\n",
    "        \"document_length\" : document_length,\n",
    "        \"answer_mask_1\" : answer_mask_1,\n",
    "        \"answer_mask_2\" : answer_mask_2,\n",
    "        \"answer_length\" : answer_length,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(data_dict):\n",
    "    \n",
    "    hidden_size = reduced_glove.shape[1]\n",
    "    embedder = Embedder(input_size = reduced_glove.shape[0], output_size = reduced_glove.shape[1])\n",
    "    documentEncoder2 = DocumentEncoderRNN(input_size = hidden_size+1, hidden_size = hidden_size)\n",
    "    questionDecoder2 = QuestionDecoderRNN(input_size=hidden_size, hidden_size=2 * hidden_size)\n",
    "    questionGenerator2 = QuestionGenerationFC(input_size = 2*questionDecoder2.hidden_size, hidden_size=2*hidden_size, output_size=reduced_glove.shape[0])\n",
    "    attention2 = Attention(2 * documentEncoder2.hidden_size, questionDecoder2.hidden_size)\n",
    "    optimizer2 = torch.optim.Adam(train_param)\n",
    "\n",
    "\n",
    "    use_ckpt = False\n",
    "\n",
    "    if use_ckpt:\n",
    "        documentEncoder2.load_state_dict(checkpoint[\"documentEncoder\"])\n",
    "        questionDecoder2.load_state_dict(checkpoint[\"questionDecoder\"])\n",
    "        questionGenerator2.load_state_dict(checkpoint[\"questionGenerator\"])\n",
    "        attention2.load_state_dict(checkpoint[\"attention\"])\n",
    "        optimizer2.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    else:\n",
    "        documentEncoder2 = documentEncoder\n",
    "        questionDecoder2 = questionDecoder\n",
    "        questionGenerator2 = questionGenerator\n",
    "        attention2 = attention\n",
    "        optimizer2 = optimizer\n",
    "\n",
    "\n",
    "    documentEncoder2.eval()\n",
    "    questionDecoder2.eval()\n",
    "    questionGenerator2.eval()\n",
    "    attention2.eval()\n",
    "    \n",
    "    use_attention = True\n",
    "    use_cuda = True\n",
    "\n",
    "    document_encoder_hidden_inf = documentEncoder2.initHidden(batch_size)\n",
    "\n",
    "\n",
    "    inp = Variable(torch.from_numpy(data_dict[\"document_tokens\"]).long())\n",
    "    labels = Variable(torch.from_numpy(data_dict[\"answer_mask_1\"])).long() #Let's see if we want to use or not\n",
    "    embedded_inp = embedder(inp)\n",
    "\n",
    "\n",
    "    if use_cuda:\n",
    "        labels = labels.cuda()\n",
    "        embedded_inp = embedded_inp.cuda()\n",
    "    #Adding answer=0/1 dim to embedded inp  \n",
    "    embedded_inp = torch.cat((embedded_inp,labels.unsqueeze(-1).float()),dim=-1)\n",
    "    answer_tags, answer_outputs, document_encoder_hidden_inf = documentEncoder2(embedded_inp, document_encoder_hidden_inf)\n",
    "\n",
    "    if use_cuda:\n",
    "        answer_outputs = answer_outputs.cuda()\n",
    "        answer_tags = answer_tags.cuda()\n",
    "\n",
    "      \n",
    "    maxGenQuesLen = 20\n",
    "    currGenQuestionLen = 0\n",
    "    \n",
    "    question_decoder_hidden_inf = document_encoder_hidden_inf.view(1,document_encoder_hidden_inf.shape[1],2*document_encoder_hidden_inf.shape[2])\n",
    "\n",
    "    current_question_token = torch.from_numpy(np.full((batch_size), START_TOKEN)).long()\n",
    "    questionGenerated = []\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        questionGenerated.append([])\n",
    "    \n",
    "    \n",
    "    while currGenQuestionLen < maxGenQuesLen:\n",
    "        question_token_embedding = embedder(current_question_token)\n",
    "        if use_cuda:\n",
    "            question_token_embedding = question_token_embedding.cuda()\n",
    "        \n",
    "        _, question_decoder_hidden_inf = questionDecoder(\n",
    "            question_token_embedding.unsqueeze(1),\n",
    "            question_decoder_hidden_inf)\n",
    "\n",
    "\n",
    "        output_inf = question_decoder_hidden_inf\n",
    "        \n",
    "        #if use_attention:\n",
    "        attn_output_inf = attention(question_decoder_hidden_inf.squeeze(0), answer_outputs)\n",
    "        output_inf = torch.cat((attn_output_inf, output_inf), dim=2)\n",
    "\n",
    "        final_output = questionGenerator2(output_inf.squeeze(0))\n",
    "        current_question_token = np.argmax(final_output.data, axis = 1)\n",
    "        for i in range(batch_size):\n",
    "            questionGenerated[i].append(look_up_token_reduced(current_question_token[i]))\n",
    "        currGenQuestionLen += 1\n",
    "\n",
    "    return answer_tags, questionGenerated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  , shortly after the end of world war ii , harry s. truman signed an executive order dissolving the oss , and by\n",
      "Ground Truth Question:  who signed the order to get rid of the oss ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  harry s. truman\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  written music , transmission was oral , and subject to change every time it was transmitted . with a musical score\n",
      "Ground Truth Question:  what was oral music subject to every time is was transmitted ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  change\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  the term hokkien (  ; hkkn ) is itself a term not\n",
      "Ground Truth Question:  in south east asia , what term is commonly used to refer to min-nan dialects ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  hokkien\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  in addition to the principal cast , alessandro cremona was cast as marco sciarra , stephanie sigman was cast\n",
      "Ground Truth Question:  which actor portrayed marco sciarra ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  alessandro cremona\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  modern estonian orthography is based on the newer orthography created by eduard ahrens in the second half of the\n",
      "Ground Truth Question:  in what orthagraphy does modern estonian orthography have its basis ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  newer orthography\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  the cathedral has a heliometer ( solar `` observatory '' ) of 1690 , one of a number built in\n",
      "Ground Truth Question:  what is a heliometer ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  solar `` observatory ''\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  down in the shastras , and is concerned with expressing the macrocosm and the microcosm . in many asian countries , pantheistic religion led to\n",
      "Ground Truth Question:  what does hindu temple architecture try to express ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  the macrocosm and the microcosm\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  this eventually led to lbj 's civil rights act , which came shortly after president kennedy 's assassination .\n",
      "Ground Truth Question:  which piece of legislation quickly followed president kennedy 's assassination ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  civil rights act\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  to the chamber a notice from the secretary to the governor general indicating when the viceroy or a deputy thereof will arrive\n",
      "Ground Truth Question:  which position nods their head to signify assention ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  governor general\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  agree to the plan 's implementation were not successful until 1988 when the transition to independence finally started under a diplomatic\n",
      "Ground Truth Question:  how long did it take for south africa to agree to the implantation of namibia 's new name ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  1988\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  `` into the field , '' which means traveling to a community in its own setting , to do something called `` fieldwork . '' on\n",
      "Ground Truth Question:  to be able to do fieldwork , an anthropologist must first travel to what ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  a community in its own setting\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  konstytucja piotrkowska '' or `` statuty piotrkowskie '' ) , increasing the nobility 's feudal power over serfs . it bound\n",
      "Ground Truth Question:  what happened to the nobilities feudal power ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  increasing\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  as a transition zone from the mountain climate in the western part of the state to the desert climate in the\n",
      "Ground Truth Question:  the mountain climate is found in which part of the state ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  western\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  the number of u.s. military advisors in south vietnam to 900 men . this was due to north vietnam 's support\n",
      "Ground Truth Question:  how many soldiers did eisenhower ultimately send to vietnam ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  900\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  of protestantism . these are , in alphabetical order : adventist , anglican , baptist , calvinist ( reformed ) , lutheran , methodist and pentecostal . a small but historically significant anabaptist branch is also\n",
      "Ground Truth Question:  what are the major branches of protestantism ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  adventist , anglican , baptist , calvinist ( reformed ) , lutheran , methodist and pentecostal\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  a dialect . during the civil war christians often used lebanese arabic officially , and sporadically used the latin script to write\n",
      "Ground Truth Question:  during the lebanese civil war , what language did lebanese christians sometimes use officially ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  lebanese arabic\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  the uneducated majority became popular in the 18th century . the marperger curieuses natur- , kunst- , berg- , gewerkund handlungs-lexicon ( 1712 ) explained terms that usefully described the trades\n",
      "Ground Truth Question:  which work published in 1712 explained terms that usefully described the trades and scientific and commercial education ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  the marperger curieuses natur- , kunst- , berg- , gewerkund handlungs-lexicon\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  office said in january 2008 that west had died of heart disease while suffering `` multiple post-operative factors '' after plastic surgery\n",
      "Ground Truth Question:  what condition along with complications for the plastic surgery caused the death of donda west ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  heart disease\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  of africa comes from cemeteries in somalia dating back to 4th millennium bc . the stone implements from the jalelo site in northern\n",
      "Ground Truth Question:  from what millennium do the oldest cemeteries in the horn of africa date ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  4th millennium bc\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  foreign-born individuals in the city as of 2011 were the dominican republic , china , mexico , guyana , jamaica , ecuador\n",
      "Ground Truth Question:  from what country did the largest number of foreign-born immigrants originate as of 2011 ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  dominican republic\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  . the king was constitutionally bound to support prime minister neville chamberlain 's appeasement of hitler . however , when the king\n",
      "Ground Truth Question:  which prime minister was the king constitutionally bound to support ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  neville chamberlain\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  the show had originally planned on having four judges following the pop idol format ; however , only\n",
      "Ground Truth Question:  how many judges were originally planned for american idol ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  four\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  greece is the country 's most-visited geographical region , with 6.5 million tourists , while central greece is second with 6.3 million\n",
      "Ground Truth Question:  northern greece gets how many visitors ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  6.5 million\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  tabatabaei uses other verses and concludes that those who are purified by god know the interpretation of the quran to a\n",
      "Ground Truth Question:  how must god have treated those who are qualified to know quranic sectets ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  purified\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  developed some different disciplines and practices , underwent its own process of codification , resulting in the code of canons of the eastern\n",
      "Ground Truth Question:  what led to the code of canons of the eastern churches ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  process of codification\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  realizing that war was imminent , prussia preemptively struck saxony and quickly overran it . the result caused uproar across\n",
      "Ground Truth Question:  what area was the site of the first action in the seven years ' war\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  saxony\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  antennas are required by any radio receiver or transmitter to couple\n",
      "Ground Truth Question:  what often inconspicuous part of a laptop computer allows for internet usage ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  antennas\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  's fleet roster is smaller than an all-time high of 5,000 ships in the late 1970s .\n",
      "Ground Truth Question:  how many ships did greece 's navy have in the late 1970s ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  5,000\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  harmony , order , and reasoncharacteristics contrasted with those of dionysus , god of wine , who represents ecstasy and disorder\n",
      "Ground Truth Question:  who was the god of wine ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  dionysus\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  rule of law matter for economic development or not ? constitutional economics is the study of the compatibility of economic and financial\n",
      "Ground Truth Question:  what doctrine seeks to study rules and their relationship with the economy ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  constitutional economics\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  in 1924 and the second hellenic republic was declared . premier georgios kondylis took power in 1935 and effectively abolished the republic by\n",
      "Ground Truth Question:  who became the greek leader in 1935 ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  premier georgios kondylis\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  sites but had a mobile mounting and the unit had 220v 24 kw generators . in 1938 design started on the 12.8 cm flak\n",
      "Ground Truth Question:  what did the flak 39 have ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  220v 24 kw generators\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  industries also made an increase in the number of both italians and eritreans residing in the cities . the number of\n",
      "Ground Truth Question:  besides eritreans , which other nationality increased its city population because of new industries ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  italians\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  . after a trial with canada geese branta canadensis , microlight aircraft were used in the us to teach safe migration routes\n",
      "Ground Truth Question:  what did they use to teach birds a migration route ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  microlight aircraft\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  . his primary objective was to move forces successfully into tunisia , and intending to facilitate that objective , he gave\n",
      "Ground Truth Question:  what geographic area was the primary objective of eisenhower ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  tunisia\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  well in his sculptures , inspired by the study of classical models . as the centre of the movement shifted to rome\n",
      "Ground Truth Question:  what did donatello study that inspired sculptures ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  classical models\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  ruled england and large domains in what was to become modern france . norway came under the influence of england , while\n",
      "Ground Truth Question:  what modern country contains regions in which plantagenet kings controlled large domains ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  modern france\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  the london fire brigade is the statutory fire and rescue service for greater london\n",
      "Ground Truth Question:  what agency provides fire fighting and rescue service in london ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  the london fire brigade\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  in dc circuits and pulsed circuits , current and voltage reversal are affected by the damping\n",
      "Ground Truth Question:  what is another type of circuit in which voltage and current reversal are affected by damping ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  pulsed circuits\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  , which had occupied it , for the sum of 1.2 million thalers . when hostilities were concluded in 1648 with the treaty\n",
      "Ground Truth Question:  how much did france pay for sundgau ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  1.2 million thalers\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  dog park and the postwar-era stars and stripes park . lake stanley draper is the city 's largest and most remote lake .\n",
      "Ground Truth Question:  which lake is the cities largest lake ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  lake stanley draper\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  way of expressing values , architecture can stimulate and influence social life without presuming that , in and of itself , it\n",
      "Ground Truth Question:  what could , in rondanini 's opinion , architecture `` stimulate and influence '' ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  social life\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  . gaddafi insisted that the free officers ' coup represented a revolution , marking the start of widespread change in the socio-economic\n",
      "Ground Truth Question:  what did gaddafi insist that the coup be referred to as ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  a revolution\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  largest producers of energy , serving retail energy customers in nine states . electricity is provided in the richmond area primarily\n",
      "Ground Truth Question:  how many states does dominion virginia power operate in ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  nine\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  corduba have a decidedly byzantine character . they were made between 965 and 970 by local craftsmen , supervised by a master mosaicist from\n",
      "Ground Truth Question:  when were the mosaics in the great mosque in corduba created ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  between 965 and 970\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  save . the following week , unlike previous seasons , colton dixon was the only contestant sent home . sanchez later made\n",
      "Ground Truth Question:  which contestant was sent home the following week ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  colton dixon\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  pradesh . [ a ] occupying 650 square kilometres ( 250 sq mi ) along the banks of the musi river , it\n",
      "Ground Truth Question:  how many square miles does hyderabad cover ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  250 sq mi\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  north africa , eisenhower oversaw the highly successful invasion of sicily . once mussolini , the italian leader , had fallen\n",
      "Ground Truth Question:  what did the allies invade after they conquered north africa ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  sicily\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  school casualties , thousands of school children died due to shoddy construction . in mianyang city , seven schools collapsed , burying\n",
      "Ground Truth Question:  what caused the deaths of many school children ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  shoddy construction\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  theoretical maximum transfer rate , firewire 400 is faster than usb 2.0 hi-bandwidth in real-use , especially in high-bandwidth use such as external hard-drives .\n",
      "Ground Truth Question:  firewire 400 is faster than what ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  usb 2.0 hi-bandwidth in real-use\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  popular concept . in the united states , driven by mtv and modern rock radio stations , a number of post-punk acts had an influence on\n",
      "Ground Truth Question:  what was a driving force behind the revival of the second british invasion of new music to america ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  mtv and modern rock radio stations\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  , but is opposed in some industries , such as postal workers and farmers , and particularly by those living in the\n",
      "Ground Truth Question:  joining farmers , what other kind of workers opposed sdst ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  postal workers\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  only government-owned technological university in thailand that was established ( 1989 ) as such ; while mahanakorn university of technology is\n",
      "Ground Truth Question:  what year was suranaree university of technology founded ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  1989\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  due by 22 march , with a plan to raise $ 23bn from the sale . in june 2010 , royal dutch\n",
      "Ground Truth Question:  how much did shell plan to raise from the sale of its assets ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  $ 23bn\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  arguments for god 's existence raised by such philosophers as immanuel kant , david hume and antony flew , although kant held that the argument from morality was\n",
      "Ground Truth Question:  name three philosophers of the last 100 years arguing for the existence of god ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  immanuel kant , david hume and antony flew\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  ummah.com , a british muslim internet forum , had made a `` hate hit list '' of british jews to be targeted by extremists over the gaza war .\n",
      "Ground Truth Question:  who was said to be targeted in the sun 's front page story ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  a `` hate hit list '' of british jews\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  land ecosystems rely on mutualisms between the plants , which fix carbon from the air , and mycorrhyzal fungi , which help in extracting water\n",
      "Ground Truth Question:  how do plants contribute to terrestrial ecosystems ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  fix carbon from the air\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  beyonc for wearing and using fur in her clothing line house of deron . in 2011 , she appeared on the cover of\n",
      "Ground Truth Question:  beyonce has a clothing line known as what ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  house of deron\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  professional programs . northwestern conferred 2,190 bachelor 's degrees , 3,272 master 's degrees , 565 doctoral degrees , and 444\n",
      "Ground Truth Question:  how many master 's degrees did northwestern confer during the 2012-2013 school term ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  3,272\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  41st week on the bestseller list , it was awarded the pulitzer prize , stunning lee . it also won the brotherhood award\n",
      "Ground Truth Question:  what major award did the book receive in 1961 ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  the pulitzer prize\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  the ultimate substantive legacy of principia mathematica is mixed . it is generally accepted that kurt gdel 's incompleteness\n",
      "Ground Truth Question:  what is the general consensus of the axioms and inference rules declared in principia mathematica ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  mixed\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  language is context-embedded and that it takes a person to comprehend the context of the original text with a reasonable degree of probability . it is certainly\n",
      "Ground Truth Question:  what does it take a person to be able to do ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  comprehend the context of the original text\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  used before the early 20th century . nevin fenneman 's 1916 study , physiographic subdivision of the united states , brought\n",
      "Ground Truth Question:  what year was nevin fenneman 's study ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  1916\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  . under the central intelligence agency act of 1949 , the director of central intelligence is the only federal government employee who can spend ``\n",
      "Ground Truth Question:  who is the only federal employee that can spend un-vouched for money ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  the director of central intelligence\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  was dissolved , and was replaced by two institutions : brigham young high school , and brigham young university . ( the by high school class of 1907 was\n",
      "Ground Truth Question:  in 1903 , which two institutions was brigham young univesity replaced with ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  brigham young high school , and brigham young university\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  hanover 's leading cabaret-stage is the gop variety theatre which is located in the georgs palace . some other\n",
      "Ground Truth Question:  what famous theatre is located in georgs palace ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  gop variety theatre\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  second and most dramatic boom and bust resulted from the klondike gold rush , which ended the depression that had begun with the\n",
      "Ground Truth Question:  after lumbering , what was seattle 's second big economic boom ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  klondike gold rush\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  following the disbandment of destiny 's child in june 2005 , she released her second solo album , b'day (\n",
      "Ground Truth Question:  when did destiny 's child end their group act ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  june 2005\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  william p. hobby airport ( named houston international airport until 1967 ) which operates primarily short- to medium-haul domestic flights .\n",
      "Ground Truth Question:  when was houston international airport renamed to hobby airport ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  1967\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  photochemical . a variety of fuels can be produced by artificial photosynthesis . the multielectron catalytic chemistry involved in making carbon-based fuels\n",
      "Ground Truth Question:  what solar process can be used to produce different fuels ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  artificial photosynthesis\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  it regards the universe as consisting of two realities ; purua ( consciousness ) and prakriti ( matter ) . jiva\n",
      "Ground Truth Question:  what is the samkhya name for consciousness ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  purua\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  fog is fairly common , particularly in spring and early summer , and the occasional tropical storm or hurricane can threaten\n",
      "Ground Truth Question:  during what seasons is fog common in boston ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  spring and early summer\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  and tolerance for exercise . sex differences are apparent as males tend to develop `` larger hearts and lungs , higher\n",
      "Ground Truth Question:  which sex tends to develop larger hearts and lungs ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  males\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  the a cappella musical perfect harmony , a comedy about two high school a cappella groups vying to win\n",
      "Ground Truth Question:  what genre best describes perfect harmony ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  comedy\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  . the known last use of samurai armor occurring in 1877 during the satsuma rebellion . as the last samurai rebellion\n",
      "Ground Truth Question:  when was samurai armor last used ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  1877\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  which largely restored swiss autonomy and introduced a confederation of 19 cantons . henceforth , much of swiss politics would concern\n",
      "Ground Truth Question:  how many cantons were introduced as a confederation by the act of mediation ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  19\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  led by kim il-sung . president rhee 's rgime excluded communists and leftists from southern politics . disenfranchised , they headed for the\n",
      "Ground Truth Question:  what two groups were excluded from the south korean political process ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  communists and leftists\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  during 2009 , the federal government enacted new legislation repealing the canada\n",
      "Ground Truth Question:  when was the canada corporations act , part ii repealed ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  2009\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  magazine . in the same year , the city ranked second on the annual fortune 500 list of company headquarters ,\n",
      "Ground Truth Question:  where did houston place for fortune 500 companies ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  second\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  enhanced through active consolidation . system consolidation takes place during slow-wave sleep ( sws ) . this process implicates that memories are\n",
      "Ground Truth Question:  what does sws stand for ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  slow-wave sleep\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  sentence , the arrangement of words is important to its meaning . a basic sentence follows the subjectverbobject pattern ( i.e\n",
      "Ground Truth Question:  in hokkien dialects , the arrangent of words is important to what ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  meaning\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  around 746 , abu muslim assumed leadership of the hashimiyya in khurasan . in 747\n",
      "Ground Truth Question:  who became leader of the khurasan hashimiyya in approximately 746 ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  abu muslim\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  texts , all hokkien , can be dated back to the 16th century . one example is the doctrina christiana en letra y\n",
      "Ground Truth Question:  when can min nan texts be dated back to ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  the 16th century\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  the period is also noted for the rise of the samurai class , which would eventually take power and start the\n",
      "Ground Truth Question:  what warrior class rose during the heian era ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  samurai\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  the fifth dalai lama and then did the same for the fifth panchen lama in 1713 , officially establishing the titles of the dalai\n",
      "Ground Truth Question:  in 1713 who did the qing emperor grant a title to ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  the fifth panchen lama\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  adder , the grass snake and the smooth snake ; none are native to ireland . in general , great britain has slightly more variation\n",
      "Ground Truth Question:  is the grass nake or the smooth snake native to ireland ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  none are native to ireland\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  jalisco . in 2010 detroit had 48,679 hispanics , including 36,452 mexicans . the number of hispanics was a 70 %\n",
      "Ground Truth Question:  what was detroit 's 2010 mexican population ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  36,452\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  after the war ended , then eventually defected to the soviet union in 1963 .\n",
      "Ground Truth Question:  in 1963 , a notable double agent providing valuable and high-quality reporting for the times during a war in the late 1930s eventually defected to which country ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  soviet union\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  . some indigenous peoples still live in relative isolation from western culture and a few are still counted as uncontacted peoples .\n",
      "Ground Truth Question:  what have some indigenous peoples managed to remain in relative isolation from ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  western culture\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  thought to resemble the acropolis and inspired the nickname of `` hilltop '' campus , renamed the danforth campus in 2006 to honor former\n",
      "Ground Truth Question:  what nickname was given to the new campus site ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  `` hilltop '' campus\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  france 's major upscale department stores are galeries lafayette and le printemps , which both have flagship stores on boulevard haussmann in\n",
      "Ground Truth Question:  what are two main department stores in france ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  galeries lafayette and le printemps\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  position which is sometimes presented outside mexico as the `` mayor '' of mexico city . [ citation needed ] in\n",
      "Ground Truth Question:  what do people call the leader of mexico city ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  mayor\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  it is common for airports to provide moving walkways and buses . the hartsfieldjackson atlanta international airport has a tram that\n",
      "Ground Truth Question:  it is common for airports to provide moving walkways and what ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  buses\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  of chalcedon . today this church is known as the armenian apostolic church , which is a part of the oriental orthodox communion\n",
      "Ground Truth Question:  what is armenia 's church called ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  armenian apostolic church\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  and historic nationality under spanish law . located in the north-west of the iberian peninsula , it comprises the provinces of a corua , lugo\n",
      "Ground Truth Question:  where is its geographic location ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  north-west of the iberian peninsula\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  described by hubbard is referred to within scientology as `` squirreling '' and is said by scientologists to be high treason\n",
      "Ground Truth Question:  what term is used for the act of using church of scientology techniques in contrast to what hubbard envisioned ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  squirreling\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  . in orthography , the uppercase is primarily reserved for special purposes , such as the first letter of a sentence\n",
      "Ground Truth Question:  which kind of purpose is uppercase reserved for in orthography ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  special\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  benefit would have been the use of dogs ' robust sense of smell to assist with the hunt . the relationship between the\n",
      "Ground Truth Question:  what would have been the top benefit for dogs in camps ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  sense of smell\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  another issue has been the use of a hypopodium as a standing platform to support the feet , given\n",
      "Ground Truth Question:  what was said to be used as a platform for crucifixion ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  a hypopodium\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  antecedents include link wray 's instrumental `` rumble '' in 1958 , and the surf rock instrumentals of dick dale ,\n",
      "Ground Truth Question:  when was `` rumble '' released ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  1958\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  was declared `` blessed '' alongside pope pius ix by pope john paul ii , the penultimate step on the road to sainthood after\n",
      "Ground Truth Question:  who declared him `` blessed ? ''\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  pope john paul ii\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  to prof. jeremiah jenks of new york university , compiling macroeconomic data on the american economy and the operations of the us\n",
      "Ground Truth Question:  what was hayek gathering during his time as a research assistant ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  macroeconomic data\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  examples of canons are found in the writings of the 1st-century bce roman architect vitruvius . some of the most important early\n",
      "Ground Truth Question:  when did vitruvius write his canons ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  1st-century bce\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  goal , he sees the rise of life as totally unintelligible .\n",
      "Ground Truth Question:  without whitehead 's proposed purpose , life would be what ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  unintelligible\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  vector for introducing the nif gene responsible for nitrogen fixation in the root nodules of legumes and other plant species . today , genetic\n",
      "Ground Truth Question:  how are genes transferred to a plant by scientists ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  in the root nodules\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  destroyed iraq 's sole nuclear reactor , in order to impede iraq 's nuclear weapons program . the reactor was under construction just outside baghdad .\n",
      "Ground Truth Question:  why did they destroy the nuclear reactor ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  impede iraq 's nuclear weapons program\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  scale from completely heterosexual to completely homosexual . kinsey reported that when the individuals ' behavior as well as their identity are analyzed , most people appeared to be at least somewhat bisexual  i.e. , most people have some attraction to either\n",
      "Ground Truth Question:  what did kinsey find during these studies ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  that when the individuals ' behavior as well as their identity are analyzed , most people appeared to be at least somewhat bisexual\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  the government 's interest could , and invariably would , outvote the two company members . the council was headed by\n",
      "Ground Truth Question:  by having 3 members on the council in calcutta from the british government they were always able to ____ the two eic members ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  outvote\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  '' and refers to the university 's patron saint , the virgin mary . the main campus covers 1,250 acres in a suburban\n",
      "Ground Truth Question:  who is the patron saint of notre dame ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  the virgin mary\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  amassing 100 points . their top scorer once again was lionel messi , who scored 46 goals in the league , including\n",
      "Ground Truth Question:  who was barcelona 's top scorer ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  lionel messi\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  connector for usb devices . the type-c plug connects to both hosts and devices , replacing various type-a and type-b connectors and cables with\n",
      "Ground Truth Question:  what does type-c connect to ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  both hosts and devices\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  robots.txt is used as part of the robots exclusion standard ,\n",
      "Ground Truth Question:  what file is utilized to exercise the rights promoted by the robots exclusion standard ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  robots.txt\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  of willow were being grown commercially on the levels . largely due to the displacement of baskets with plastic bags and cardboard boxes , the industry has severely declined since the 1950s . by the end of the 20th century only about\n",
      "Ground Truth Question:  what did plastic bags result in\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  largely due to the displacement of baskets with plastic bags and cardboard boxes , the industry has severely declined since the 1950s\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  people known as picts and the southern two thirds by britons .\n",
      "Ground Truth Question:  what was the name of the native people that lived in the southern parts of the british isles during the roman empire occupation ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  britons\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  styles of design more generally , came from dissemination through pattern books and inexpensive suites of engravings . this contrasted with earlier styles , which were primarily\n",
      "Ground Truth Question:  how did most georgian design styles disseminate ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  pattern books and inexpensive suites of engravings\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  ranged from hundreds of people in san francisco , to effectively none in pyongyang , forced the path of the torch relay\n",
      "Ground Truth Question:  how many people protested at the pyongyang torch route ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  effectively none\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  were used . in 1815 it was rejoined with the austrian netherlands , luxembourg and lige ( the `` southern provinces '' ) to become the\n",
      "Ground Truth Question:  the kingdom of the netherlands was formed by which countries ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  austrian netherlands , luxembourg and lige\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  central catalan is considered the standard pronunciation of the language . the descriptions below are mostly for\n",
      "Ground Truth Question:  what are the descriptions for ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  standard pronunciation\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  main spiritual and philosophical systems which continued to be in hinduism , buddhism and jainism . emperor harsha of kannauj succeeded in reuniting northern india\n",
      "Ground Truth Question:  what philosophical traditions developed during the period of the 5th to the 13th centuries ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  hinduism , buddhism and jainism\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  service , but this call must be ratified by the local presbytery .\n",
      "Ground Truth Question:  when the congregation issues a call for service by a pastor , who has to ratify it ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  local presbytery\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  reformulation of gasoline and diesel fuels . the introduction of metrobs bus rapid transit and the ecobici bike-sharing were among efforts\n",
      "Ground Truth Question:  what is the bus system called in mexico city ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  metrobs\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  such as sushi . notable restaurants in montevideo include arcadia atop the plaza victoria , widely regarded to be the finest restaurant in the\n",
      "Ground Truth Question:  where is arcadia located ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  atop the plaza victoria\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  season four premiered on january 18 , 2005 ; this was the\n",
      "Ground Truth Question:  in which season was bo bice a contestant on american idol ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  season four\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  depending on core assumptions such as the growth rate . recycling is a major source of copper in the modern world\n",
      "Ground Truth Question:  what is a major source of cooper in modern times ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  recycling\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  the former comecon area . [ citation needed ] in 1994 , based on the economic theories of milton friedman ,\n",
      "Ground Truth Question:  what year did estonia establish a flat tax ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  1994\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  along , would change my viewpoint '' . therefore , individuals classified as homosexual in one study might not be identified the same way in another depending on which components are assessed and when the assessment is made making it difficult to\n",
      "Ground Truth Question:  what makes it difficult to determine if someone is homosexual or not ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  individuals classified as homosexual in one study might not be identified the same way in another depending on which components are assessed\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  with the premier league in 2015 to broadcast the league through the 202122 season in a deal valued at $ 1 billion ( 640\n",
      "Ground Truth Question:  when will this extension end ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  through the 202122 season\n",
      "------------------------------------------------------------------------------------------\n",
      "------------------------------------------------------------------------------------------\n",
      "Comprehension :  radziwi , zamoyski , potocki or lubomirski often rivalled the estates of the king and were important power bases for the magnates .\n",
      "Ground Truth Question:  what was an important power basis for the magnates ?\n",
      "Question Generated :  ['<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>', '<END>']\n",
      "Answer :  estates of the king\n",
      "------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "b_n = 1\n",
    "context = ' '.join(X_train_comp_all_shuffled[10])\n",
    "answer = ' '.join(X_train_ans_all_shuffled[10])\n",
    "\n",
    "data_dict1 = prepareSetForInference(context, answer, max_document_len, batch_size)\n",
    "data_dict2 = prepareSetForInferenceFromBatch(b_n)\n",
    "    \n",
    "a_tags1, questionGenerated1 = inference(data_dict1)\n",
    "a_tags2, questionGenerated2 = inference(data_dict2)\n",
    "\n",
    "for i in range(batch_size):\n",
    "    print(\"------------------------------------------------------------------------------------------\")\n",
    "    print(\"Comprehension : \", ' '.join(X_train_comp_all_shuffled[32*b_n + i]))\n",
    "    print(\"Ground Truth Question: \", ' '.join(Y_train_ques_all_shuffled[32*b_n + i]))\n",
    "    print(\"Question Generated : \", questionGenerated2[i])\n",
    "    print(\"Answer : \", ' '.join(X_train_ans_all_shuffled[32*b_n + i]))\n",
    "    print(\"------------------------------------------------------------------------------------------\")     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
