{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export LC_ALL=en_US.UTF-8\n",
    "!CUDA_VISIBLE_DEVICES=0,1,2,3\n",
    "from embedding import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pprint import pprint\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "try:\n",
    "    import nltk\n",
    "except:\n",
    "    !pip install nltk\n",
    "    import nltk\n",
    "    nltk.download('punkt')\n",
    "    nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('../train-v1.1.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/tshrjn/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /Users/tshrjn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltkStopWords = stopwords.words('english')\n",
    "punctuations = [',', '?', '.', '-',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(data):\n",
    "    contexts = []\n",
    "    qas = []\n",
    "    for i in range(len(data[\"data\"])):\n",
    "        for j in range(len(data[\"data\"][i][\"paragraphs\"])):\n",
    "            contexts.append(data[\"data\"][i][\"paragraphs\"][j][\"context\"])\n",
    "            qas.append(data[\"data\"][i][\"paragraphs\"][j][\"qas\"])\n",
    "    return (contexts,qas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "CapPassage = False\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "contexts,qas = extractor(data)\n",
    "\n",
    "def find_sub_list(sl,l):\n",
    "    sll=len(sl)\n",
    "    for ind in (i for i,e in enumerate(l) if e==sl[0]):\n",
    "        if l[ind:ind+sll]==sl:\n",
    "            return ind,ind+sll\n",
    "    return (-1,-1)\n",
    "\n",
    "def capPassage(passage,answer,cap_length = 30):\n",
    "    y = np.zeros(cap_length)\n",
    "    left,right = find_sub_list(answer,passage)\n",
    "    if(left==-1):\n",
    "        return passage[0:cap_length]\n",
    "    left = left - int((cap_length - len(answer))/2)\n",
    "    right = right + int((cap_length + len(answer))/2)\n",
    "    if(left < 0):\n",
    "        left = 0\n",
    "    if(right > len(passage)):\n",
    "        right = len(passage)\n",
    "    return passage[left:right]\n",
    "    \n",
    "def findAnsVec(answer,passage):\n",
    "    ans = np.zeros((len(passage)))\n",
    "    start,end = find_sub_list(answer,passage)\n",
    "    if(start==-1):\n",
    "        start = passage.index(answer[0])\n",
    "        end = start + len(answer)\n",
    "    ans[start:end] = 1\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_comp_all = []\n",
    "X_train_comp_with_answer_marked_all = []\n",
    "X_train_ans_all = []\n",
    "X_train_comp_answer_label_all = []\n",
    "Y_train_ques_all = []\n",
    "invalid = 0\n",
    "for i,context in enumerate(contexts):\n",
    "    passage = word_tokenize(context.lower())\n",
    "    \n",
    "    a_lab = np.zeros(len(passage))\n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "        start,end = find_sub_list(answer,passage)\n",
    "        if start == -1:\n",
    "            invalid=invalid+1\n",
    "            continue\n",
    "        a_lab[start:end] = 1\n",
    "        \n",
    "              \n",
    "    for j,_ in enumerate(qas[i]):\n",
    "        try:\n",
    "            question = word_tokenize(qas[i][j]['question'].lower())\n",
    "            answer = word_tokenize(qas[i][j][\"answers\"][0]['text'].lower())\n",
    "            start,end = find_sub_list(answer,passage)\n",
    "            if start == -1:\n",
    "                invalid = invalid+1\n",
    "                continue\n",
    "            marked_comp = np.zeros(len(passage))\n",
    "            marked_comp[start:end] = 1\n",
    "            \n",
    "            if CapPassage:\n",
    "                cappedPassage = capPassage(passage,answer)\n",
    "            else:\n",
    "                cappedPassage = passage\n",
    "            \n",
    "            X_train_comp_all.append(cappedPassage)\n",
    "            X_train_comp_with_answer_marked_all.append(marked_comp)\n",
    "            X_train_ans_all.append(answer)\n",
    "            X_train_comp_answer_label_all.append(a_lab)\n",
    "            Y_train_ques_all.append(question)\n",
    "        except Exception as e:\n",
    "            invalid = invalid+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import operator\n",
    "def findKMostFrequentWords(k):\n",
    "    ctr = Counter([item for sublist in X_train_comp_all for item in sublist] + [item for sublist in Y_train_ques_all for item in sublist])\n",
    "    sorted_ctr = sorted(ctr.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    return [item[0] for item in sorted_ctr[0:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordToTake = 2000\n",
    "words = findKMostFrequentWords(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_word_to_idx_reduced = {}\n",
    "_idx_to_word_reduced = []\n",
    "\n",
    "\n",
    "def _add_word_reduced(word):\n",
    "    idx = len(_idx_to_word_reduced)\n",
    "    _word_to_idx_reduced[word] = idx\n",
    "    _idx_to_word_reduced.append(word)\n",
    "    return idx\n",
    "\n",
    "\n",
    "UNKNOWN_TOKEN = _add_word_reduced(UNKNOWN_WORD)\n",
    "START_TOKEN = _add_word_reduced(START_WORD)\n",
    "END_TOKEN = _add_word_reduced(END_WORD)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dimensions = glove.shape[1]\n",
    "reduced_glove = []\n",
    "reduced_glove.append(np.zeros(dimensions))\n",
    "reduced_glove.append(-np.ones(dimensions))\n",
    "reduced_glove.append(np.ones(dimensions))\n",
    "\n",
    "for word in words:\n",
    "    l = look_up_word(word)\n",
    "    if(l != UNKNOWN_TOKEN):\n",
    "        idx = _add_word_reduced(word)\n",
    "        reduced_glove.append(glove[l])\n",
    "        if(len(reduced_glove) == wordToTake):\n",
    "            break\n",
    "        \n",
    "def look_up_word_reduced(word):\n",
    "    return _word_to_idx_reduced.get(word, UNKNOWN_TOKEN)\n",
    "\n",
    "\n",
    "def look_up_token_reduced(token):\n",
    "    return _idx_to_word_reduced[token]\n",
    "\n",
    "reduced_glove = np.array(reduced_glove)\n",
    "reduced_glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JackSparrow.local\n",
      "4858\n"
     ]
    }
   ],
   "source": [
    "!hostname\n",
    "print(invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(zip(X_train_comp_all,X_train_ans_all, Y_train_ques_all,X_train_comp_with_answer_marked_all, X_train_comp_answer_label_all))\n",
    "np.random.shuffle(c)\n",
    "X_train_comp_all_shuffled,X_train_ans_all_shuffled, Y_train_ques_all_shuffled, X_train_comp_with_answer_marked_all_shuffled, X_train_comp_answer_label_shuffled = zip(*c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_to_take_train = 160\n",
    "\n",
    "X_train_comp = X_train_comp_all_shuffled[0:examples_to_take_train]\n",
    "X_train_ans = X_train_ans_all_shuffled[0:examples_to_take_train]\n",
    "Y_train_ques = Y_train_ques_all_shuffled[0:examples_to_take_train]\n",
    "X_train_comp_with_answer_marked = X_train_comp_with_answer_marked_all_shuffled[0:examples_to_take_train]\n",
    "X_train_comp_answer_label = X_train_comp_answer_label_shuffled[0:examples_to_take_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_document_len = len(max(X_train_comp,key=len))\n",
    "max_answer_len = len(max(X_train_ans,key=len))\n",
    "max_question_len = len(max(Y_train_ques,key=len)) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]), 618)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_comp_with_answer_marked[0], max_document_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = np.full((examples_to_take_train, max_document_len), END_TOKEN,dtype=np.int32)\n",
    "document_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "\n",
    "answer_labels_all = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "answer_labels = np.zeros((examples_to_take_train, max_document_len), dtype=np.int32)\n",
    "answer_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n",
    "\n",
    "question_input_tokens = np.full((examples_to_take_train, max_question_len), END_TOKEN, dtype=np.int32)\n",
    "question_output_tokens = np.full((examples_to_take_train, max_question_len), END_TOKEN, dtype=np.int32)\n",
    "question_lengths = np.zeros(examples_to_take_train, dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(examples_to_take_train):\n",
    "    answer_labels_all[i,0:len(X_train_comp_answer_label[i])] = X_train_comp_answer_label[i]\n",
    "    answer_labels[i, 0:len(X_train_comp_with_answer_marked[i])] = X_train_comp_with_answer_marked[i]\n",
    "    for j, word in enumerate(X_train_comp[i]):\n",
    "        document_tokens[i, j] = look_up_word_reduced(word)\n",
    "    document_lengths[i] = len(X_train_comp[i])\n",
    "\n",
    "    answer_lengths[i] = len(X_train_ans[i])\n",
    "    \n",
    "    question_input_words = ([START_WORD] + Y_train_ques[i])\n",
    "    question_output_words = (Y_train_ques[i] + [END_WORD])\n",
    "\n",
    "    for j, word in enumerate(question_input_words):\n",
    "            question_input_tokens[i, j] = look_up_word_reduced(word)\n",
    "    for j, word in enumerate(question_output_words):\n",
    "        question_output_tokens[i, j] = look_up_word_reduced(word)\n",
    "    question_lengths[i] = len(question_input_words)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(data):\n",
    "    flat_list = [item for sublist in data for item in sublist]\n",
    "    vocabulary = sorted(set(flat_list))\n",
    "    vocabulary.append(\"<UNK>\")\n",
    "    vocabulary.append(\"unk\")\n",
    "    vocabulary.append(\"eos\")\n",
    "    vocabulary = [\"<EOS>\"] + vocabulary\n",
    "    word_to_index = { word:i for i,word in enumerate(vocabulary) }\n",
    "    index_to_word = { i:word for i,word in enumerate(vocabulary) }\n",
    "    return (vocabulary,word_to_index,index_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 300)\n",
      "5959\n",
      "406\n",
      "5800\n"
     ]
    }
   ],
   "source": [
    "print(reduced_glove.shape)\n",
    "vocabulary_comp,word_to_index_comp,index_to_word_comp = create_vocabulary(X_train_comp + Y_train_ques)\n",
    "print(len(vocabulary_comp))\n",
    "print(word_to_index_comp[\"?\"])\n",
    "print(word_to_index_comp[\"what\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_one_hot_vector(data,vocabulary,word_to_index,index_to_word, maxLen):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,word in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        if(word not in word_to_index):\n",
    "            word = \"<UNK>\"\n",
    "        one_hot[i][word_to_index[word]] = 1\n",
    "    return one_hot\n",
    "\n",
    "def create_one_hot_vector_from_indices(data,maxLen,vocabulary):\n",
    "    one_hot = np.zeros([maxLen,len(vocabulary)])\n",
    "    for i,indice in enumerate(data):\n",
    "        if i >= maxLen:\n",
    "            break\n",
    "        one_hot[i][int(indice)] = 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def create_one_hot_training_Set(data,maxLen,vocabulary):\n",
    "    one_hot_data = np.zeros([data.shape[0],maxLen,len(vocabulary)])\n",
    "    for i in range(data.shape[0]):\n",
    "        one_hot_data[i] = create_one_hot_vector_from_indices(data[i],maxLen,vocabulary)\n",
    "    return one_hot_data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_to_indices_glove(X,max_len):\n",
    "    \n",
    "    m = len(X)                                 \n",
    "    \n",
    "    X_indices = np.full([m,max_len],look_up_word_reduced(END_WORD))\n",
    "    \n",
    "    for i in range(m):\n",
    "        j = 0\n",
    "        for w in X[i]:\n",
    "            if(j>=max_len):\n",
    "                break;\n",
    "            \n",
    "            X_indices[i, j] = look_up_word_reduced(w)\n",
    "            j = j+1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_tokens = context_to_indices_glove(X_train_comp, max_document_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "618"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_document_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_question_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def createBatch(inputs,batch_size,shuffle=False):\n",
    "    outputs = []\n",
    "    \n",
    "    num_batches = math.ceil(len(inputs[0])/batch_size)\n",
    "\n",
    "    for i in range(num_batches-1):\n",
    "        start = 0\n",
    "        \n",
    "        output = {'document_tokens':[],\n",
    "                    'document_lengths':[],\n",
    "                    'answer_labels_all':[],\n",
    "                    'answer_labels':[],\n",
    "                    'answer_lengths': [],\n",
    "                    'question_input_tokens':[],\n",
    "                    'question_output_tokens':[],\n",
    "                    'question_lengths':[]}\n",
    "        \n",
    "        for index,inp in enumerate(inputs):\n",
    "            #maxD = max(inputs[1][start:start+batch_size])\n",
    "            maxD = max_document_len\n",
    "            maxA = max(inputs[4][start:start+batch_size])\n",
    "            maxQ = max_question_len\n",
    "            \n",
    "            if index == 0:\n",
    "                output['document_tokens'].append(inp[start:start+batch_size,0:maxD])\n",
    "            elif index==1:\n",
    "                output['document_lengths'].append(inp[start:start+batch_size])\n",
    "            elif index==2:\n",
    "                output['answer_labels_all'].append(inp[start:start+batch_size,0:maxD])\n",
    "            elif index==3:\n",
    "                output['answer_labels'].append(inp[start:start+batch_size,0:maxD])\n",
    "            elif index==4:\n",
    "                output['answer_lengths'].append(inp[start:start+batch_size])\n",
    "            elif index==5:\n",
    "                output['question_input_tokens'].append(inp[start:start+batch_size, 0:maxQ])\n",
    "            elif index==6:\n",
    "                output['question_output_tokens'].append(inp[start:start+batch_size, 0:maxQ])\n",
    "            elif index==7:\n",
    "                output['question_lengths'].append(inp[start:start+batch_size])\n",
    "        \n",
    "        output[\"document_tokens\"] = np.array(output[\"document_tokens\"])\n",
    "        output[\"document_lengths\"] = np.array(output[\"document_lengths\"])\n",
    "        output[\"answer_labels_all\"] = np.array(output[\"answer_labels_all\"])\n",
    "        output[\"answer_labels\"] = np.array(output[\"answer_labels\"])\n",
    "        output[\"answer_lengths\"] = np.array(output[\"question_lengths\"])\n",
    "        output[\"question_input_tokens\"] = np.array(output[\"question_input_tokens\"])\n",
    "        output[\"question_output_tokens\"] = np.array(output[\"question_output_tokens\"])\n",
    "        output[\"question_lengths\"] = np.array(output[\"question_lengths\"])\n",
    "        outputs.append(output)\n",
    "        start = start + batch_size\n",
    "        \n",
    "    # Remaining training sample i.e. training mod batch_size\n",
    "    #maxD = max(inputs[1][start:])\n",
    "    maxD = max_document_len\n",
    "    maxA = max(inputs[4][start:])\n",
    "    maxQ = max_question_len\n",
    "    output = {'document_tokens':[],\n",
    "                'document_lengths':[],\n",
    "                'answer_labels_all':[],\n",
    "                'answer_labels':[],\n",
    "                'answer_lengths': [],\n",
    "                'question_input_tokens':[],\n",
    "                'question_output_tokens':[],\n",
    "                'question_lengths':[]}\n",
    "    if index == 0:\n",
    "        output['document_tokens'].append(inp[start:,0:maxD])\n",
    "    elif index==1:\n",
    "        output['document_lengths'].append(inp[start:])\n",
    "    elif index==2:\n",
    "        output['answer_labels_all'].append(inp[start:,0:maxD])\n",
    "    elif index==3:\n",
    "        output['answer_labels'].append(inp[start:,0:maxD])\n",
    "    elif index==4:\n",
    "        output['answer_lengths'].append(inp[start:])\n",
    "    elif index==5:\n",
    "        output['question_input_tokens'].append(inp[start:, 0:maxQ])\n",
    "    elif index==6:\n",
    "        output['question_output_tokens'].append(inp[start:, 0:maxQ])\n",
    "    elif index==7:\n",
    "        output['question_lengths'].append(inp[start:])\n",
    "\n",
    "    output[\"document_tokens\"] = np.array(output[\"document_tokens\"])\n",
    "    output[\"document_lengths\"] = np.array(output[\"document_lengths\"])\n",
    "    output[\"answer_labels\"] = np.array(output[\"answer_labels\"])\n",
    "    output[\"answer_labels_all\"] = np.array(output[\"answer_labels_all\"])\n",
    "    output[\"answer_lengths\"] = np.array(output[\"question_lengths\"])\n",
    "    output[\"question_input_tokens\"] = np.array(output[\"question_input_tokens\"])\n",
    "    output[\"question_output_tokens\"] = np.array(output[\"question_output_tokens\"])\n",
    "    output[\"question_lengths\"] = np.array(output[\"question_lengths\"])\n",
    "\n",
    "    outputs.append(output)\n",
    "    \n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches =  5\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "batch_input = createBatch([document_tokens,document_lengths,answer_labels_all,answer_labels,answer_lengths,question_input_tokens,question_output_tokens,question_lengths]\n",
    "                    ,batch_size)\n",
    "for b in batch_input:\n",
    "    for k, v in b.items():\n",
    "        b[k] = v.squeeze()\n",
    "number_of_batches = len(batch_input)\n",
    "print(\"Number of batches = \", number_of_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0], dtype=int32)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input[1]['answer_labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "use_attention = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedder(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Embedder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_size, output_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.from_numpy(reduced_glove).float())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnswerEncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=1):\n",
    "        super(AnswerEncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True, bidirectional=True) #Input_size = Hidden_Size\n",
    "        if use_cuda:\n",
    "            self.gru = self.gru.cuda()\n",
    "        self.fc = nn.Linear(hidden_size*2, 1)\n",
    "        if use_cuda:\n",
    "            self.fc = self.fc.cuda()\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        final_output = self.fc(output)\n",
    "        final_output = F.sigmoid(final_output)\n",
    "        self.hiddenState = hidden\n",
    "        return final_output, output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(2, 32, self.hidden_size)) #2 for BiDirectional\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionEncoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super(QuestionEncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True)\n",
    "        if use_cuda:\n",
    "            self.gru = self.gru.cuda()\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        self.hiddenState = hidden\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        #result = Variable(torch.zeros(1, batch_size, self.hidden_size))\n",
    "        result = Variable(torch.zeros(1, 32, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QuestionDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,input_size, hidden_size):\n",
    "        super(QuestionDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first= True)\n",
    "        if use_cuda:\n",
    "            self.gru = self.gru.cuda()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        output, hidden = self.gru(x, hidden)\n",
    "        self.hiddenState = hidden\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        #result = Variable(torch(1, batch_size, self.hidden_size))\n",
    "        result = Variable(torch.zeros(1, 32, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result\n",
    "    \n",
    "'''\n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self,input_size, output_size):\n",
    "        super(FCLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "'''\n",
    "\n",
    "class QuestionGenerationFC(nn.Module):\n",
    "    def __init__(self,input_size, output_size):\n",
    "        super(QuestionGenerationFC, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        if use_cuda:\n",
    "            self.fc = self.fc.cuda()\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.fc(x)\n",
    "        output = F.log_softmax(output, dim=-1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_size, max_document_len, dropout_p=0.1):\n",
    "        \n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_document_len = max_document_len\n",
    "\n",
    "        self.attn_reduce = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.attn_combine = nn.Linear(self.max_document_len + self.hidden_size, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        \n",
    "        if use_cuda:\n",
    "            self.attn_reduce = self.attn_reduce.cuda()\n",
    "            self.attn_combine = self.attn_combine.cuda()\n",
    "            self.dropout = self.dropout.cuda()\n",
    "        \n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "\n",
    "        hidden = hidden.squeeze(0)\n",
    "        hidden = self.dropout(hidden)\n",
    "        concat = torch.cat((input.unsqueeze(0), hidden.unsqueeze(0)), 1)\n",
    "        concat_reduced = self.attn_reduce(concat)\n",
    "        attn_weights = F.softmax(concat_reduced, dim=1)\n",
    "        attn_applied = torch.mm(encoder_outputs, attn_weights.squeeze(0).unsqueeze(1))\n",
    "        output = torch.cat((hidden.unsqueeze(0), attn_applied.squeeze(1).unsqueeze(0)), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "        output = F.relu(output)\n",
    "        self.hidden_state = hidden\n",
    "        return output, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        result = Variable(torch.zeros(1, 1, self.hidden_size))\n",
    "        if use_cuda:\n",
    "            result = result.cuda()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters =  28\n"
     ]
    }
   ],
   "source": [
    "hidden_size = reduced_glove.shape[1]\n",
    "\n",
    "embedder = Embedder(input_size = reduced_glove.shape[0], output_size = reduced_glove.shape[1])\n",
    "#fcLayer = FCLayer(hidden_size, hidden_size)\n",
    "answerEncoder = AnswerEncoderRNN(input_size = hidden_size, hidden_size=int(hidden_size/2))\n",
    "questionEncoder1 = QuestionEncoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionEncoder2 = QuestionEncoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "\n",
    "questionDecoder = QuestionDecoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionGenerator = QuestionGenerationFC(input_size = hidden_size, output_size=reduced_glove.shape[0])\n",
    "attention = AttnDecoderRNN(hidden_size= hidden_size,max_document_len = max_document_len)\n",
    "\n",
    "answerEncoder.train()\n",
    "questionEncoder1.train()\n",
    "questionEncoder2.train()\n",
    "questionDecoder.train()\n",
    "questionGenerator.train()\n",
    "attention.train()\n",
    "\n",
    "train_param = []\n",
    "\n",
    "for model in [answerEncoder, questionEncoder1, questionEncoder2, questionDecoder, questionGenerator, attention]:\n",
    "    train_param += [p for p in model.parameters() if p.requires_grad]\n",
    "\n",
    "print(\"Number of trainable parameters = \", len(train_param))\n",
    "\n",
    "optimizer = torch.optim.Adam(train_param, 0.01)\n",
    "criterion1 = nn.BCELoss()\n",
    "#criterion2 = nn.CrossEntropyLoss()\n",
    "criterion2 = nn.NLLLoss()\n",
    "#criterion2 = nn.MultiLabelSoftMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repackage_hidden(h):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data)\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 618]) torch.Size([32, 618])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n",
      "torch.Size([32, 2000]) torch.Size([32])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-d9a2a61dbe65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0mnet_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manswer_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mquestion_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0mnet_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "verboseBatchPrinting = True\n",
    "averageBatchLossPrinting = True\n",
    "\n",
    "num_epochs = 100\n",
    "answer_encoder_hidden = answerEncoder.initHidden()\n",
    "question_encoder_1_hidden = questionEncoder1.initHidden()\n",
    "question_encoder_2_hidden = questionEncoder2.initHidden()\n",
    "question_decoder_hidden = None\n",
    "\n",
    "Attention_Weights = None\n",
    "attn_output = None\n",
    "attention_hidden = attention.initHidden()\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    avg_loss = 0\n",
    "    \n",
    "    for batch_num in range(number_of_batches):\n",
    "        \n",
    "        answer_encoder_hidden = answerEncoder.initHidden()\n",
    "        \n",
    "        answer_encoder_hidden = repackage_hidden(answer_encoder_hidden)\n",
    "        question_encoder_1_hidden = repackage_hidden(question_encoder_1_hidden)\n",
    "        question_encoder_2_hidden = repackage_hidden(question_encoder_2_hidden)\n",
    "        if type(question_decoder_hidden) == Variable:\n",
    "            question_decoder_hidden = repackage_hidden(question_decoder_hidden)\n",
    "        if(type(attn_output) == Variable):\n",
    "            attn_output = repackage_hidden(attn_output)\n",
    "        if(type(attention_hidden) == Variable):\n",
    "            attention_hidden = repackage_hidden(attention_hidden)\n",
    "        if(type(Attention_Weights) == Variable):\n",
    "            Attention_Weights = repackage_hidden(Attention_Weights)\n",
    "        \n",
    "        current_batch_size = len(batch_input[batch_num][\"document_tokens\"])\n",
    "        if current_batch_size != batch_size:\n",
    "            continue\n",
    "        \n",
    "        maxDocLenForBatch = max_document_len\n",
    "        mask = np.zeros((current_batch_size, maxDocLenForBatch))\n",
    "        \n",
    "        for i in range(current_batch_size):\n",
    "            mask[i,0:batch_input[batch_num][\"document_lengths\"][i]] = 1\n",
    "            \n",
    "\n",
    "        inp = Variable(torch.from_numpy(batch_input[batch_num][\"document_tokens\"]).long())\n",
    "\n",
    "        labels = Variable(torch.from_numpy(batch_input[batch_num][\"answer_labels_all\"])).long()\n",
    "        if use_cuda:\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        embedded_inp = embedder(inp)\n",
    "        \n",
    "        #answer_outputs = Variable(torch.zeros(current_batch_size, max_document_len, hidden_size))\n",
    "        #answer_tags = Variable(torch.zeros(current_batch_size, max_document_len, 1))\n",
    "        \n",
    "        if use_cuda:\n",
    "            #answer_outputs = answer_outputs.cuda()\n",
    "            #answer_tags = answer_tags.cuda()\n",
    "            embedded_inp = embedded_inp.cuda()\n",
    "\n",
    "        answer_tags, answer_outputs, answer_encoder_hidden = answerEncoder(embedded_inp, answer_encoder_hidden)              \n",
    "\n",
    "        t_document_mask = Variable(torch.from_numpy(mask)).float()\n",
    "        if use_cuda:\n",
    "            t_document_mask = t_document_mask.cuda()\n",
    "        \n",
    "        outputs = torch.mul(answer_tags.squeeze(-1),t_document_mask)\n",
    "\n",
    "        print(outputs.shape, labels.shape)\n",
    "#         break\n",
    "        answer_loss = criterion1(outputs, labels.float())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        answer_mask_1 = Variable(torch.from_numpy(batch_input[batch_num][\"answer_labels\"])).float().unsqueeze(-1)\n",
    "        answer_mask_2 = Variable(torch.from_numpy(1 - batch_input[batch_num][\"answer_labels\"])).float().unsqueeze(-1)\n",
    "        \n",
    "        if use_cuda:\n",
    "            answer_mask_1 = answer_mask_1.cuda()\n",
    "            answer_mask_2 = answer_mask_2.cuda()\n",
    "        \n",
    "        question_encoder_input1 = torch.mul(answer_mask_1, answer_outputs.float())\n",
    "        question_encoder_input2 = torch.mul(answer_mask_2, answer_outputs.float())\n",
    "        \n",
    "        question_encoder_1_outputs , question_encoder_1_hidden = questionEncoder1(question_encoder_input1, question_encoder_1_hidden)\n",
    "        question_encoder_2_outputs , question_encoder_2_hidden = questionEncoder2(question_encoder_input2, question_encoder_2_hidden)\n",
    "            \n",
    "        \n",
    "        \n",
    "\n",
    "        question_loss = 0\n",
    "        question_decoder_hidden = question_encoder_2_hidden\n",
    "        embedded_inputs = embedder(torch.from_numpy(batch_input[batch_num][\"question_input_tokens\"]).long())\n",
    "        output_labels = Variable(torch.from_numpy(batch_input[batch_num][\"question_output_tokens\"]).long())\n",
    "        if use_cuda:\n",
    "            embedded_inputs = embedded_inputs.cuda()\n",
    "            output_labels = output_labels.cuda()\n",
    "\n",
    "        for quesL in range(batch_input[batch_num][\"question_input_tokens\"].shape[1]):\n",
    "            if use_attention:\n",
    "                attn_output, Attention_Weights = attention(question_decoder_hidden.squeeze(0).squeeze(0), attention_hidden.squeeze(0), answer_outputs[i])\n",
    "                decoder_output, attention_hidden = questionDecoder(\n",
    "                    embedded_inputs[quesL:quesL+1].unsqueeze(1), attn_output)\n",
    "            else:\n",
    "                decoder_output, question_decoder_hidden = questionDecoder(\n",
    "                    embedded_inputs[:,quesL:quesL+1,:],\n",
    "                    question_decoder_hidden)\n",
    "\n",
    "            final_output = questionGenerator(decoder_output.squeeze(1))\n",
    "            \n",
    "            '''\n",
    "            idx = np.where (output_labels[:, quesL:quesL+1].squeeze(1).data == END_TOKEN)[0].tolist()\n",
    "            final_copy = final_output.clone()\n",
    "            if len(idx):\n",
    "                print(final_copy.data[idx])\n",
    "                final_copy.data[idx] = 0\n",
    "                final_copy.data[idx][:,END_TOKEN] = 1\n",
    "                print(final_copy[idx])\n",
    "                print(\"IDX\", idx)\n",
    "                print(\"Where:\", np.where(final_copy[idx].data == 1)) \n",
    "            '''\n",
    "            print(final_output.shape, output_labels[:, quesL:quesL+1].squeeze(1).shape)\n",
    "            question_loss += criterion2(final_output,\n",
    "                                       output_labels[:, quesL:quesL+1].squeeze(1))\n",
    "\n",
    "\n",
    "        net_loss = answer_loss + question_loss\n",
    "        net_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        avg_loss+= net_loss.data[0]\n",
    "        if verboseBatchPrinting:\n",
    "            print ('Batch: %d \\t Epoch : %d\\tNet Loss: %.4f \\tAnswer Loss: %.4f \\tQuestion Loss: %.4f'\n",
    "                   %(batch_num, epoch, net_loss.data[0], answer_loss.data[0], question_loss.data[0]))\n",
    "\n",
    "\n",
    "\n",
    "    if averageBatchLossPrinting:\n",
    "        print('Average Loss after Epoch %d : %.4f'\n",
    "                   %(epoch, avg_loss/number_of_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-25d6d9042fb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = [1, 2, 3]\n",
    "final_output[idx][:,2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"/data/ra2630/qgen_base_40k.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerEncoder2 = AnswerEncoderRNN(input_size = hidden_size, hidden_size=int(hidden_size/2))\n",
    "questionEncoder2_1 = QuestionEncoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionEncoder2_2 = QuestionEncoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionDecoder2 = QuestionDecoderRNN(input_size=hidden_size, hidden_size=hidden_size)\n",
    "questionGenerator2 = QuestionGenerationFC(input_size = hidden_size, output_size=reduced_glove.shape[0])\n",
    "attention2 = AttnDecoderRNN(hidden_size= hidden_size,max_document_len = max_document_len)\n",
    "optimizer2 = torch.optim.Adam(train_param, 0.01)\n",
    "\n",
    "\n",
    "#answerEncoder2.load_state_dict(checkpoint[\"answerEncoder\"])\n",
    "#questionEncoder2.load_state_dict(checkpoint[\"questionEncoder\"])\n",
    "#questionDecoder2.load_state_dict(checkpoint[\"questionDecoder\"])\n",
    "#questionGenerator2.load_state_dict(checkpoint[\"questionGenerator\"])\n",
    "#optimizer2.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "answerEncoder2 = answerEncoder\n",
    "questionEncoder2_1 = questionEncoder1\n",
    "questionEncoder2_2 = questionEncoder2\n",
    "questionDecoder2 = questionDecoder\n",
    "questionGenerator2 = questionGenerator\n",
    "attention2 = attention\n",
    "optimizer2 = optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " -3.7828e-02  6.6429e-02 -5.2511e-02  ...  -8.8349e-02 -3.3487e-02  2.0664e-01\n",
       "  5.3192e-04 -2.9515e-02 -2.5246e-01  ...   6.4553e-03 -1.2639e-01 -3.4465e-02\n",
       " -1.2899e-02  4.3461e-02 -1.0208e-01  ...   5.5923e-02  3.2590e-01  1.3529e-01\n",
       "                 ...                                      ...                \n",
       " -1.4746e-01 -1.5048e-03 -1.0250e-01  ...  -1.1302e-01 -1.2740e-01 -2.2424e-01\n",
       "  8.9877e-02  7.3200e-02 -2.5811e-02  ...   1.1611e-01  1.2277e-01  3.6928e-02\n",
       "  9.0432e-02  1.2776e-02  1.7478e-02  ...   1.0188e-01  6.7899e-03  1.3490e-01\n",
       " [torch.cuda.FloatTensor of size 450x300 (GPU 0)], Parameter containing:\n",
       "  1.0948e-01 -5.2877e-03 -1.4122e-01  ...  -6.3701e-02 -4.3667e-03  1.4487e-02\n",
       "  1.7246e-02  2.1329e-01 -1.3400e-01  ...   1.4544e-02  1.1322e-01  9.0475e-02\n",
       "  1.7838e-02 -2.2348e-01 -4.9929e-02  ...   3.1174e-02  2.8664e-02 -1.1325e-01\n",
       "                 ...                                      ...                \n",
       " -3.7944e-02  7.7023e-02 -1.7932e-02  ...   3.3457e-02 -6.5450e-02  1.0796e-01\n",
       "  1.0324e-01 -1.3007e-01 -3.7691e-02  ...   3.5671e-02  3.6250e-02 -4.9155e-02\n",
       "  3.9654e-02 -1.6660e-02 -1.4960e-01  ...   1.0897e-01  1.8781e-01 -1.2768e-01\n",
       " [torch.cuda.FloatTensor of size 450x150 (GPU 0)], Parameter containing:\n",
       " -0.1173\n",
       " -0.0836\n",
       "  0.0044\n",
       "  0.0873\n",
       " -0.0382\n",
       " -0.0337\n",
       " -0.0237\n",
       "  0.0374\n",
       "  0.0479\n",
       " -0.0096\n",
       " -0.0089\n",
       " -0.0415\n",
       " -0.1669\n",
       " -0.0460\n",
       "  0.0434\n",
       " -0.1694\n",
       " -0.1176\n",
       " -0.0713\n",
       " -0.0201\n",
       "  0.0388\n",
       " -0.0305\n",
       "  0.0472\n",
       " -0.1626\n",
       "  0.0219\n",
       " -0.0941\n",
       " -0.0831\n",
       " -0.0139\n",
       "  0.0651\n",
       "  0.0035\n",
       " -0.0902\n",
       " -0.0880\n",
       " -0.0245\n",
       " -0.0372\n",
       "  0.0743\n",
       " -0.1076\n",
       " -0.0735\n",
       " -0.1938\n",
       " -0.0121\n",
       "  0.0731\n",
       " -0.0434\n",
       " -0.0124\n",
       " -0.1327\n",
       " -0.0028\n",
       " -0.1543\n",
       " -0.0753\n",
       "  0.1120\n",
       " -0.0460\n",
       " -0.0998\n",
       "  0.0004\n",
       " -0.1353\n",
       " -0.0022\n",
       "  0.0871\n",
       "  0.1811\n",
       "  0.2185\n",
       "  0.0948\n",
       " -0.0973\n",
       "  0.1139\n",
       "  0.1547\n",
       "  0.0315\n",
       " -0.0727\n",
       " -0.1524\n",
       " -0.1486\n",
       "  0.0555\n",
       " -0.0323\n",
       "  0.0781\n",
       "  0.0380\n",
       "  0.0371\n",
       " -0.1403\n",
       "  0.0438\n",
       " -0.1338\n",
       " -0.0450\n",
       "  0.0719\n",
       "  0.0156\n",
       "  0.2153\n",
       " -0.0873\n",
       " -0.1120\n",
       "  0.1915\n",
       " -0.1557\n",
       "  0.0420\n",
       "  0.2191\n",
       " -0.2066\n",
       " -0.1248\n",
       "  0.0018\n",
       " -0.0899\n",
       " -0.1296\n",
       " -0.1405\n",
       "  0.1133\n",
       " -0.2050\n",
       "  0.0140\n",
       "  0.0948\n",
       "  0.0337\n",
       "  0.0538\n",
       " -0.0724\n",
       "  0.0978\n",
       "  0.0544\n",
       " -0.1679\n",
       "  0.0433\n",
       " -0.1523\n",
       "  0.1792\n",
       "  0.0071\n",
       " -0.0772\n",
       " -0.0522\n",
       " -0.0538\n",
       " -0.0469\n",
       "  0.0963\n",
       "  0.1074\n",
       " -0.0638\n",
       "  0.0806\n",
       " -0.1483\n",
       " -0.0322\n",
       " -0.1181\n",
       "  0.0175\n",
       "  0.0344\n",
       " -0.1301\n",
       "  0.1602\n",
       " -0.0584\n",
       "  0.1217\n",
       "  0.0557\n",
       " -0.0341\n",
       " -0.0273\n",
       "  0.0093\n",
       " -0.1253\n",
       " -0.0495\n",
       " -0.0549\n",
       " -0.0603\n",
       " -0.0594\n",
       "  0.0234\n",
       "  0.1709\n",
       "  0.0017\n",
       " -0.0231\n",
       "  0.0274\n",
       "  0.0669\n",
       "  0.0018\n",
       " -0.1386\n",
       " -0.0810\n",
       " -0.0310\n",
       " -0.0364\n",
       " -0.0795\n",
       "  0.0546\n",
       " -0.0356\n",
       " -0.1302\n",
       "  0.0440\n",
       " -0.0543\n",
       " -0.0015\n",
       " -0.0161\n",
       "  0.0032\n",
       " -0.1261\n",
       " -0.0494\n",
       " -0.1125\n",
       "  0.1441\n",
       " -0.0149\n",
       "  0.0110\n",
       "  0.0415\n",
       "  0.0539\n",
       " -0.1325\n",
       "  0.1800\n",
       " -0.0570\n",
       "  0.2210\n",
       "  0.0988\n",
       " -0.1124\n",
       " -0.0430\n",
       "  0.1660\n",
       "  0.1873\n",
       " -0.1921\n",
       "  0.0919\n",
       "  0.0376\n",
       " -0.0567\n",
       "  0.1567\n",
       "  0.3255\n",
       "  0.0793\n",
       "  0.0084\n",
       "  0.0087\n",
       " -0.0231\n",
       " -0.0486\n",
       "  0.0958\n",
       "  0.0580\n",
       "  0.0951\n",
       "  0.0602\n",
       "  0.0116\n",
       " -0.1514\n",
       " -0.1136\n",
       "  0.0696\n",
       "  0.0913\n",
       "  0.1291\n",
       " -0.0711\n",
       "  0.1199\n",
       "  0.0201\n",
       "  0.2246\n",
       "  0.0007\n",
       "  0.1341\n",
       "  0.1522\n",
       "  0.1031\n",
       "  0.0366\n",
       "  0.1138\n",
       " -0.0704\n",
       " -0.0130\n",
       "  0.0527\n",
       "  0.1440\n",
       "  0.2043\n",
       "  0.0594\n",
       "  0.1149\n",
       " -0.1701\n",
       "  0.3091\n",
       " -0.1508\n",
       "  0.0899\n",
       "  0.3600\n",
       "  0.0067\n",
       " -0.0009\n",
       " -0.0601\n",
       " -0.0464\n",
       "  0.0211\n",
       "  0.1895\n",
       " -0.0743\n",
       "  0.0865\n",
       " -0.0379\n",
       " -0.0796\n",
       " -0.0545\n",
       "  0.4172\n",
       "  0.0764\n",
       "  0.1430\n",
       " -0.0149\n",
       "  0.0754\n",
       "  0.0412\n",
       " -0.0071\n",
       "  0.2423\n",
       " -0.3286\n",
       " -0.1860\n",
       "  0.1576\n",
       " -0.2118\n",
       " -0.2863\n",
       " -0.0215\n",
       " -0.0829\n",
       "  0.2685\n",
       " -0.0743\n",
       " -0.0495\n",
       " -0.0318\n",
       " -0.0602\n",
       " -0.0061\n",
       "  0.1692\n",
       " -0.0296\n",
       "  0.0451\n",
       " -0.1357\n",
       " -0.1786\n",
       " -0.2562\n",
       " -0.0191\n",
       "  0.0155\n",
       "  0.1647\n",
       "  0.1004\n",
       "  0.1094\n",
       "  0.1952\n",
       " -0.0235\n",
       " -0.1013\n",
       " -0.1268\n",
       " -0.0585\n",
       "  0.0957\n",
       "  0.1071\n",
       "  0.0306\n",
       " -0.0665\n",
       "  0.2067\n",
       " -0.1193\n",
       "  0.1165\n",
       " -0.0557\n",
       " -0.0724\n",
       " -0.0514\n",
       " -0.1348\n",
       "  0.0902\n",
       " -0.1486\n",
       " -0.1120\n",
       "  0.1859\n",
       "  0.1546\n",
       "  0.0981\n",
       "  0.0224\n",
       "  0.3118\n",
       " -0.1489\n",
       " -0.0720\n",
       " -0.0931\n",
       " -0.1923\n",
       "  0.0755\n",
       " -0.1054\n",
       "  0.0060\n",
       "  0.0427\n",
       "  0.0623\n",
       "  0.0471\n",
       "  0.1752\n",
       "  0.1105\n",
       " -0.1105\n",
       " -0.0916\n",
       " -0.0217\n",
       "  0.1166\n",
       "  0.0816\n",
       "  0.0298\n",
       "  0.2790\n",
       " -0.0595\n",
       "  0.2253\n",
       "  0.0082\n",
       " -0.0770\n",
       "  0.0008\n",
       "  0.1890\n",
       "  0.3315\n",
       " -0.0261\n",
       "  0.0517\n",
       "  0.0176\n",
       "  0.0598\n",
       "  0.0408\n",
       "  0.1322\n",
       " -0.0168\n",
       " -0.0046\n",
       "  0.1173\n",
       " -0.1392\n",
       " -0.0760\n",
       " -0.1137\n",
       "  0.1805\n",
       "  0.0226\n",
       "  0.0459\n",
       " -0.0099\n",
       "  0.0507\n",
       " -0.0349\n",
       " -0.0115\n",
       "  0.0032\n",
       "  0.1326\n",
       "  0.0881\n",
       "  0.0342\n",
       "  0.1497\n",
       " -0.0568\n",
       "  0.0409\n",
       " -0.0222\n",
       "  0.0579\n",
       " -0.0534\n",
       "  0.1176\n",
       "  0.0320\n",
       "  0.0816\n",
       " -0.1464\n",
       " -0.0937\n",
       "  0.1307\n",
       "  0.0748\n",
       " -0.1288\n",
       " -0.1539\n",
       "  0.0148\n",
       " -0.0333\n",
       "  0.0356\n",
       "  0.0286\n",
       "  0.0145\n",
       "  0.1093\n",
       " -0.0552\n",
       "  0.1347\n",
       " -0.0117\n",
       " -0.1054\n",
       " -0.0077\n",
       "  0.0285\n",
       " -0.1498\n",
       " -0.1041\n",
       " -0.0519\n",
       "  0.0544\n",
       "  0.0334\n",
       "  0.0224\n",
       " -0.0100\n",
       "  0.0376\n",
       " -0.0130\n",
       "  0.1149\n",
       " -0.1024\n",
       "  0.0902\n",
       "  0.0328\n",
       " -0.0494\n",
       " -0.0770\n",
       " -0.1552\n",
       "  0.0751\n",
       "  0.0690\n",
       "  0.0459\n",
       " -0.1056\n",
       " -0.1208\n",
       " -0.1245\n",
       "  0.1271\n",
       "  0.1506\n",
       "  0.0370\n",
       "  0.0883\n",
       "  0.0603\n",
       "  0.0245\n",
       " -0.0255\n",
       "  0.0332\n",
       "  0.0158\n",
       " -0.1231\n",
       " -0.1575\n",
       "  0.0814\n",
       "  0.0514\n",
       " -0.0050\n",
       " -0.0522\n",
       " -0.1344\n",
       "  0.1134\n",
       "  0.0281\n",
       "  0.0356\n",
       "  0.1387\n",
       "  0.0340\n",
       "  0.1165\n",
       "  0.0275\n",
       " -0.0484\n",
       " -0.0657\n",
       " -0.0068\n",
       " -0.0186\n",
       "  0.0118\n",
       " -0.0099\n",
       "  0.0980\n",
       "  0.0815\n",
       " -0.1253\n",
       "  0.0328\n",
       "  0.0872\n",
       "  0.0363\n",
       " -0.0293\n",
       " -0.0673\n",
       "  0.0600\n",
       "  0.0504\n",
       "  0.0051\n",
       " -0.0454\n",
       "  0.1153\n",
       " -0.0911\n",
       " -0.0543\n",
       " -0.0577\n",
       "  0.0428\n",
       "  0.1133\n",
       " -0.1157\n",
       "  0.0355\n",
       "  0.0939\n",
       " -0.1091\n",
       " -0.0290\n",
       "  0.0163\n",
       " -0.0198\n",
       " -0.1325\n",
       " -0.0960\n",
       "  0.0216\n",
       "  0.0318\n",
       "  0.0345\n",
       "  0.1116\n",
       "  0.1130\n",
       " -0.0694\n",
       "  0.0735\n",
       "  0.0377\n",
       " -0.0118\n",
       "  0.0299\n",
       "  0.0367\n",
       "  0.0215\n",
       "  0.0941\n",
       "  0.1018\n",
       " -0.0291\n",
       " -0.1222\n",
       " -0.0698\n",
       "  0.0289\n",
       "  0.1438\n",
       "  0.0539\n",
       " -0.0787\n",
       "  0.0232\n",
       " -0.0436\n",
       " [torch.cuda.FloatTensor of size 450 (GPU 0)], Parameter containing:\n",
       " -0.1503\n",
       " -0.0716\n",
       "  0.0138\n",
       "  0.0443\n",
       " -0.1238\n",
       " -0.0065\n",
       " -0.0735\n",
       " -0.0355\n",
       "  0.0588\n",
       " -0.0179\n",
       " -0.0552\n",
       "  0.0137\n",
       " -0.2036\n",
       "  0.0269\n",
       "  0.0521\n",
       " -0.0999\n",
       " -0.1221\n",
       " -0.0345\n",
       " -0.0605\n",
       "  0.0463\n",
       " -0.0641\n",
       " -0.0457\n",
       " -0.1673\n",
       "  0.0578\n",
       " -0.0218\n",
       "  0.0696\n",
       "  0.0585\n",
       "  0.0180\n",
       " -0.1430\n",
       " -0.0047\n",
       " -0.1141\n",
       " -0.0642\n",
       " -0.0745\n",
       "  0.1078\n",
       " -0.1144\n",
       " -0.0991\n",
       " -0.2009\n",
       "  0.0022\n",
       "  0.0279\n",
       " -0.0656\n",
       " -0.1101\n",
       " -0.1230\n",
       " -0.0091\n",
       " -0.0800\n",
       " -0.0288\n",
       "  0.1987\n",
       " -0.1371\n",
       " -0.0179\n",
       " -0.0180\n",
       " -0.1192\n",
       "  0.0218\n",
       "  0.0463\n",
       "  0.0810\n",
       "  0.2186\n",
       "  0.1835\n",
       "  0.0034\n",
       "  0.1380\n",
       "  0.1627\n",
       "  0.1086\n",
       " -0.0380\n",
       " -0.1732\n",
       " -0.1713\n",
       "  0.0641\n",
       " -0.0265\n",
       " -0.0013\n",
       "  0.0537\n",
       " -0.0977\n",
       " -0.0624\n",
       " -0.0464\n",
       " -0.1698\n",
       "  0.0659\n",
       "  0.0097\n",
       " -0.1192\n",
       "  0.2140\n",
       " -0.0147\n",
       " -0.0594\n",
       "  0.1565\n",
       " -0.0755\n",
       " -0.0142\n",
       "  0.2418\n",
       " -0.0614\n",
       " -0.0794\n",
       "  0.0677\n",
       " -0.0502\n",
       " -0.0885\n",
       " -0.0357\n",
       "  0.0845\n",
       " -0.1052\n",
       " -0.1097\n",
       "  0.0477\n",
       " -0.0289\n",
       "  0.1044\n",
       " -0.0707\n",
       "  0.1904\n",
       "  0.0882\n",
       " -0.0581\n",
       "  0.1525\n",
       " -0.0359\n",
       "  0.1899\n",
       " -0.0009\n",
       " -0.0784\n",
       " -0.0298\n",
       " -0.0633\n",
       " -0.0019\n",
       "  0.0443\n",
       "  0.0518\n",
       "  0.0627\n",
       " -0.0664\n",
       " -0.1706\n",
       " -0.0184\n",
       " -0.1375\n",
       " -0.0207\n",
       "  0.0697\n",
       " -0.0824\n",
       "  0.0978\n",
       " -0.0908\n",
       "  0.0996\n",
       "  0.0622\n",
       " -0.1119\n",
       " -0.0625\n",
       "  0.0830\n",
       " -0.1851\n",
       " -0.0294\n",
       " -0.1568\n",
       " -0.1894\n",
       " -0.0664\n",
       " -0.0387\n",
       "  0.2275\n",
       "  0.0254\n",
       " -0.0582\n",
       "  0.0750\n",
       "  0.0468\n",
       " -0.0750\n",
       " -0.0276\n",
       " -0.1457\n",
       "  0.0637\n",
       "  0.0170\n",
       " -0.0547\n",
       "  0.0265\n",
       " -0.0592\n",
       " -0.0480\n",
       " -0.0583\n",
       " -0.1044\n",
       " -0.0661\n",
       "  0.1156\n",
       "  0.0214\n",
       " -0.0420\n",
       "  0.0239\n",
       " -0.1578\n",
       "  0.1126\n",
       " -0.0507\n",
       "  0.1130\n",
       "  0.0799\n",
       "  0.0843\n",
       " -0.1630\n",
       "  0.1669\n",
       " -0.1047\n",
       "  0.1931\n",
       "  0.0060\n",
       " -0.0467\n",
       " -0.1009\n",
       "  0.0913\n",
       "  0.2102\n",
       " -0.2132\n",
       "  0.2070\n",
       "  0.1109\n",
       " -0.0365\n",
       "  0.1653\n",
       "  0.2815\n",
       "  0.0351\n",
       "  0.0075\n",
       " -0.0181\n",
       "  0.0120\n",
       " -0.0037\n",
       "  0.0381\n",
       "  0.0382\n",
       "  0.0728\n",
       "  0.0109\n",
       "  0.0055\n",
       " -0.1648\n",
       " -0.0609\n",
       "  0.1346\n",
       "  0.0433\n",
       "  0.1054\n",
       " -0.0464\n",
       "  0.1008\n",
       "  0.0285\n",
       "  0.1445\n",
       " -0.0017\n",
       "  0.1424\n",
       "  0.1593\n",
       "  0.1016\n",
       "  0.0404\n",
       "  0.0771\n",
       "  0.0049\n",
       " -0.0666\n",
       " -0.0344\n",
       "  0.1023\n",
       "  0.2373\n",
       "  0.0933\n",
       "  0.0437\n",
       " -0.1386\n",
       "  0.3361\n",
       " -0.1877\n",
       " -0.0332\n",
       "  0.3348\n",
       " -0.0861\n",
       " -0.1002\n",
       " -0.0053\n",
       " -0.0420\n",
       "  0.0381\n",
       "  0.1617\n",
       "  0.0178\n",
       "  0.1141\n",
       "  0.0615\n",
       " -0.0778\n",
       "  0.0650\n",
       "  0.4617\n",
       "  0.0475\n",
       "  0.1294\n",
       "  0.1061\n",
       "  0.0023\n",
       "  0.1080\n",
       "  0.0461\n",
       "  0.1947\n",
       " -0.2190\n",
       " -0.1587\n",
       "  0.1594\n",
       " -0.2112\n",
       " -0.1787\n",
       " -0.1287\n",
       "  0.0043\n",
       "  0.2287\n",
       " -0.1738\n",
       " -0.0842\n",
       " -0.0584\n",
       " -0.0815\n",
       "  0.0105\n",
       "  0.1742\n",
       "  0.0529\n",
       "  0.0852\n",
       " -0.0221\n",
       " -0.2158\n",
       " -0.2802\n",
       " -0.0424\n",
       "  0.0685\n",
       "  0.2717\n",
       "  0.0510\n",
       "  0.2163\n",
       "  0.1364\n",
       " -0.0653\n",
       " -0.0158\n",
       "  0.0031\n",
       " -0.0449\n",
       "  0.0186\n",
       "  0.1472\n",
       "  0.0622\n",
       " -0.2034\n",
       "  0.1343\n",
       " -0.1829\n",
       "  0.0137\n",
       "  0.0498\n",
       " -0.0132\n",
       "  0.0469\n",
       " -0.0166\n",
       "  0.0262\n",
       " -0.1073\n",
       " -0.1074\n",
       "  0.2267\n",
       "  0.1417\n",
       " -0.0052\n",
       "  0.1097\n",
       "  0.1631\n",
       " -0.0773\n",
       " -0.0905\n",
       " -0.0468\n",
       " -0.1951\n",
       " -0.0513\n",
       " -0.1288\n",
       "  0.0714\n",
       " -0.0125\n",
       "  0.1009\n",
       "  0.0472\n",
       "  0.1452\n",
       "  0.1152\n",
       " -0.1751\n",
       " -0.1542\n",
       " -0.0522\n",
       "  0.2191\n",
       "  0.1323\n",
       " -0.0609\n",
       "  0.3717\n",
       " -0.1776\n",
       "  0.1578\n",
       "  0.0031\n",
       " -0.0072\n",
       " -0.0483\n",
       "  0.1521\n",
       "  0.3066\n",
       " -0.0644\n",
       "  0.1527\n",
       "  0.0680\n",
       " -0.1043\n",
       "  0.0192\n",
       "  0.0246\n",
       " -0.1625\n",
       " -0.0748\n",
       "  0.1372\n",
       " -0.0963\n",
       " -0.0515\n",
       "  0.0215\n",
       "  0.1400\n",
       " -0.0641\n",
       "  0.0043\n",
       " -0.1054\n",
       " -0.0107\n",
       " -0.0856\n",
       " -0.0849\n",
       " -0.1077\n",
       "  0.0925\n",
       " -0.0483\n",
       "  0.0321\n",
       "  0.0047\n",
       " -0.0719\n",
       " -0.0636\n",
       " -0.0113\n",
       "  0.0330\n",
       " -0.0027\n",
       "  0.0274\n",
       "  0.0107\n",
       "  0.0579\n",
       " -0.1501\n",
       " -0.1207\n",
       "  0.0950\n",
       "  0.0418\n",
       " -0.1294\n",
       " -0.1989\n",
       " -0.0301\n",
       " -0.0469\n",
       "  0.0985\n",
       "  0.1108\n",
       " -0.0119\n",
       "  0.1114\n",
       "  0.1056\n",
       " -0.0318\n",
       " -0.0433\n",
       " -0.0572\n",
       "  0.0878\n",
       "  0.0602\n",
       " -0.0709\n",
       "  0.0250\n",
       "  0.0276\n",
       "  0.0063\n",
       " -0.0077\n",
       " -0.0290\n",
       " -0.0258\n",
       " -0.0352\n",
       "  0.0289\n",
       "  0.0610\n",
       "  0.0285\n",
       "  0.0513\n",
       " -0.0442\n",
       " -0.1031\n",
       " -0.0205\n",
       " -0.0478\n",
       "  0.0685\n",
       "  0.0725\n",
       "  0.0332\n",
       " -0.0544\n",
       " -0.0880\n",
       " -0.1090\n",
       "  0.0069\n",
       "  0.0155\n",
       "  0.0741\n",
       "  0.0404\n",
       " -0.0279\n",
       " -0.0880\n",
       " -0.0543\n",
       " -0.0661\n",
       " -0.0331\n",
       "  0.0133\n",
       " -0.0052\n",
       "  0.0446\n",
       "  0.0327\n",
       "  0.0833\n",
       "  0.0093\n",
       " -0.0724\n",
       "  0.0817\n",
       "  0.0165\n",
       " -0.0801\n",
       "  0.1025\n",
       "  0.0051\n",
       "  0.0612\n",
       " -0.0664\n",
       "  0.0035\n",
       " -0.0819\n",
       "  0.0210\n",
       "  0.0319\n",
       " -0.0015\n",
       "  0.0194\n",
       "  0.0394\n",
       "  0.0932\n",
       " -0.1512\n",
       " -0.0523\n",
       "  0.0551\n",
       "  0.0897\n",
       "  0.0100\n",
       "  0.0675\n",
       "  0.0734\n",
       "  0.0455\n",
       " -0.0108\n",
       "  0.0058\n",
       " -0.0194\n",
       " -0.0707\n",
       " -0.1313\n",
       " -0.0102\n",
       "  0.0419\n",
       "  0.0997\n",
       " -0.0263\n",
       "  0.0520\n",
       "  0.1289\n",
       " -0.1684\n",
       " -0.1318\n",
       " -0.0633\n",
       " -0.0841\n",
       " -0.0181\n",
       " -0.0484\n",
       " -0.0777\n",
       "  0.0707\n",
       "  0.0858\n",
       "  0.0827\n",
       " -0.1225\n",
       " -0.0514\n",
       "  0.1635\n",
       "  0.0353\n",
       "  0.0068\n",
       "  0.0471\n",
       "  0.1130\n",
       " -0.0346\n",
       " -0.0068\n",
       "  0.0520\n",
       "  0.0108\n",
       " -0.0673\n",
       "  0.0079\n",
       "  0.0193\n",
       "  0.0587\n",
       "  0.0244\n",
       " -0.1289\n",
       "  0.0845\n",
       " -0.0427\n",
       " [torch.cuda.FloatTensor of size 450 (GPU 0)], Parameter containing:\n",
       " -1.8449e-01 -6.9378e-02  4.9238e-02  ...  -1.8847e-01  3.7413e-02 -2.5398e-01\n",
       " -2.2450e-02 -8.9843e-02  6.2058e-02  ...  -3.1173e-02 -2.5211e-02 -9.0317e-04\n",
       "  7.1771e-02  1.0672e-01  3.3782e-02  ...  -4.9267e-02  4.5489e-02  6.6800e-02\n",
       "                 ...                                      ...                \n",
       "  7.2203e-02  1.6257e-01  4.0287e-02  ...   5.4679e-02  1.6596e-01  7.7037e-02\n",
       "  1.0046e-01 -3.7856e-02  4.1439e-02  ...   1.4340e-01  1.7347e-01  6.8893e-02\n",
       "  4.6724e-02  6.4342e-02 -5.4264e-02  ...   8.8685e-02  2.8821e-02 -4.1288e-02\n",
       " [torch.cuda.FloatTensor of size 450x300 (GPU 0)], Parameter containing:\n",
       "  2.7324e-02  5.0179e-02  5.7054e-02  ...   8.9570e-02  8.5425e-04 -9.2849e-02\n",
       " -1.5985e-01  3.2601e-02  2.9365e-02  ...   5.6991e-02 -5.5248e-02  7.9142e-02\n",
       " -9.2814e-02 -2.1816e-02  7.3895e-05  ...   1.9031e-02 -1.0880e-01  7.7200e-02\n",
       "                 ...                                      ...                \n",
       "  1.0578e-01 -9.5375e-02 -1.1025e-01  ...  -1.0880e-01  1.8366e-01  4.4446e-02\n",
       "  1.6681e-01  3.8764e-01 -1.4326e-01  ...  -2.1211e-01 -1.2203e-01  3.4659e-02\n",
       " -1.3320e-01  2.4820e-04 -1.3937e-01  ...  -7.2855e-02 -2.0288e-04  3.2232e-02\n",
       " [torch.cuda.FloatTensor of size 450x150 (GPU 0)], Parameter containing:\n",
       "  0.0114\n",
       "  0.0774\n",
       " -0.0844\n",
       " -0.0316\n",
       " -0.0445\n",
       "  0.0378\n",
       " -0.1714\n",
       " -0.1814\n",
       "  0.0194\n",
       " -0.0922\n",
       "  0.0281\n",
       "  0.0626\n",
       " -0.0737\n",
       " -0.0229\n",
       " -0.0247\n",
       "  0.0169\n",
       " -0.0335\n",
       "  0.0468\n",
       "  0.0015\n",
       " -0.0073\n",
       "  0.1494\n",
       "  0.0411\n",
       " -0.1505\n",
       " -0.0737\n",
       " -0.0849\n",
       " -0.0709\n",
       "  0.0904\n",
       " -0.0051\n",
       " -0.0241\n",
       "  0.0264\n",
       " -0.1247\n",
       "  0.0215\n",
       " -0.0866\n",
       " -0.1058\n",
       " -0.1235\n",
       " -0.0294\n",
       " -0.0601\n",
       "  0.1188\n",
       " -0.0169\n",
       " -0.1374\n",
       " -0.1184\n",
       " -0.2721\n",
       " -0.0206\n",
       " -0.0302\n",
       " -0.2312\n",
       "  0.0241\n",
       " -0.0058\n",
       " -0.1033\n",
       " -0.0880\n",
       " -0.1063\n",
       "  0.1825\n",
       " -0.0684\n",
       "  0.0986\n",
       " -0.0382\n",
       " -0.1708\n",
       " -0.2282\n",
       " -0.0731\n",
       "  0.0540\n",
       " -0.0474\n",
       " -0.1558\n",
       "  0.0067\n",
       " -0.0898\n",
       " -0.1462\n",
       "  0.0015\n",
       "  0.0008\n",
       " -0.1060\n",
       " -0.0472\n",
       " -0.1101\n",
       " -0.1332\n",
       " -0.1430\n",
       "  0.1463\n",
       " -0.1412\n",
       " -0.1461\n",
       " -0.0906\n",
       " -0.0963\n",
       "  0.0631\n",
       " -0.1870\n",
       "  0.0011\n",
       " -0.0919\n",
       " -0.0631\n",
       "  0.1119\n",
       "  0.1203\n",
       " -0.0326\n",
       " -0.0846\n",
       "  0.0803\n",
       " -0.0038\n",
       "  0.1630\n",
       "  0.0227\n",
       " -0.0817\n",
       " -0.1960\n",
       " -0.0612\n",
       "  0.0208\n",
       " -0.1851\n",
       "  0.0002\n",
       " -0.0576\n",
       " -0.0568\n",
       " -0.0877\n",
       "  0.0850\n",
       "  0.0591\n",
       " -0.2072\n",
       "  0.0591\n",
       " -0.0869\n",
       "  0.0008\n",
       " -0.0916\n",
       " -0.2165\n",
       "  0.0740\n",
       " -0.0437\n",
       "  0.0165\n",
       " -0.0590\n",
       " -0.1317\n",
       "  0.0153\n",
       " -0.0469\n",
       "  0.0586\n",
       " -0.1110\n",
       " -0.0494\n",
       " -0.0306\n",
       " -0.2239\n",
       "  0.0229\n",
       " -0.0634\n",
       " -0.1123\n",
       " -0.0683\n",
       "  0.0092\n",
       " -0.1043\n",
       "  0.0931\n",
       "  0.0189\n",
       "  0.0560\n",
       " -0.1614\n",
       " -0.0605\n",
       " -0.0721\n",
       " -0.1054\n",
       "  0.0288\n",
       "  0.0146\n",
       " -0.1163\n",
       " -0.0206\n",
       " -0.0738\n",
       " -0.0900\n",
       "  0.0401\n",
       "  0.0076\n",
       "  0.0609\n",
       " -0.1692\n",
       " -0.1184\n",
       "  0.0375\n",
       " -0.0496\n",
       " -0.1340\n",
       "  0.1017\n",
       " -0.0919\n",
       " -0.0255\n",
       " -0.0058\n",
       "  0.0012\n",
       " -0.0114\n",
       "  0.0761\n",
       " -0.0397\n",
       "  0.0502\n",
       " -0.0575\n",
       "  0.0408\n",
       " -0.1908\n",
       "  0.0474\n",
       "  0.0572\n",
       " -0.1547\n",
       " -0.0651\n",
       "  0.0492\n",
       "  0.0319\n",
       " -0.0056\n",
       " -0.0176\n",
       " -0.0998\n",
       "  0.0677\n",
       "  0.0140\n",
       "  0.0793\n",
       "  0.2007\n",
       " -0.0390\n",
       " -0.1031\n",
       "  0.0217\n",
       " -0.0387\n",
       " -0.0252\n",
       " -0.0101\n",
       "  0.0907\n",
       " -0.0218\n",
       " -0.0866\n",
       " -0.0990\n",
       " -0.0817\n",
       " -0.0117\n",
       " -0.0969\n",
       " -0.0587\n",
       "  0.0214\n",
       "  0.0664\n",
       " -0.1616\n",
       " -0.0311\n",
       " -0.0202\n",
       "  0.1174\n",
       "  0.0671\n",
       " -0.0592\n",
       " -0.0014\n",
       "  0.0668\n",
       " -0.0418\n",
       "  0.0573\n",
       " -0.1419\n",
       "  0.0198\n",
       "  0.0402\n",
       " -0.1050\n",
       "  0.0101\n",
       " -0.1736\n",
       " -0.0044\n",
       "  0.0073\n",
       "  0.1641\n",
       "  0.0414\n",
       "  0.0755\n",
       " -0.1171\n",
       " -0.0467\n",
       "  0.1890\n",
       " -0.0531\n",
       " -0.1522\n",
       " -0.0350\n",
       "  0.1294\n",
       " -0.0816\n",
       " -0.0889\n",
       " -0.0305\n",
       " -0.0749\n",
       " -0.0591\n",
       " -0.0757\n",
       " -0.0238\n",
       " -0.0471\n",
       " -0.0167\n",
       "  0.0777\n",
       " -0.0530\n",
       " -0.1005\n",
       " -0.0171\n",
       "  0.0260\n",
       "  0.0123\n",
       " -0.0351\n",
       " -0.0641\n",
       " -0.0641\n",
       " -0.1411\n",
       "  0.0627\n",
       "  0.1479\n",
       "  0.1180\n",
       "  0.0796\n",
       "  0.0499\n",
       " -0.0771\n",
       "  0.0394\n",
       " -0.0138\n",
       " -0.0126\n",
       "  0.0926\n",
       " -0.0891\n",
       " -0.1174\n",
       " -0.0642\n",
       "  0.0830\n",
       "  0.0148\n",
       " -0.0388\n",
       "  0.0288\n",
       " -0.1342\n",
       "  0.0009\n",
       " -0.0277\n",
       " -0.0483\n",
       " -0.1296\n",
       " -0.0993\n",
       "  0.1632\n",
       "  0.0376\n",
       " -0.0940\n",
       " -0.0046\n",
       " -0.0722\n",
       "  0.0152\n",
       " -0.1072\n",
       " -0.1690\n",
       " -0.0699\n",
       "  0.0380\n",
       "  0.1825\n",
       " -0.0223\n",
       " -0.0287\n",
       "  0.0655\n",
       " -0.0608\n",
       "  0.1216\n",
       " -0.0083\n",
       "  0.0375\n",
       " -0.0510\n",
       " -0.1196\n",
       " -0.0459\n",
       "  0.0435\n",
       " -0.0871\n",
       "  0.0272\n",
       " -0.1101\n",
       "  0.1882\n",
       " -0.0328\n",
       "  0.0011\n",
       " -0.0857\n",
       "  0.0229\n",
       " -0.2070\n",
       " -0.0670\n",
       " -0.0424\n",
       " -0.0061\n",
       "  0.0022\n",
       " -0.0335\n",
       " -0.0458\n",
       " -0.0201\n",
       "  0.0266\n",
       "  0.0342\n",
       "  0.0578\n",
       "  0.1370\n",
       "  0.0659\n",
       " -0.1092\n",
       " -0.0972\n",
       " -0.0489\n",
       " -0.0697\n",
       "  0.0614\n",
       " -0.0305\n",
       " -0.1502\n",
       " -0.0714\n",
       "  0.0386\n",
       " -0.1203\n",
       "  0.0686\n",
       "  0.0297\n",
       " -0.0524\n",
       " -0.0553\n",
       "  0.0769\n",
       " -0.1662\n",
       "  0.0145\n",
       " -0.0186\n",
       " -0.0203\n",
       " -0.1212\n",
       " -0.0296\n",
       " -0.0021\n",
       " -0.0040\n",
       " -0.0147\n",
       "  0.0829\n",
       "  0.0870\n",
       " -0.0862\n",
       " -0.0149\n",
       "  0.0725\n",
       " -0.0053\n",
       "  0.0543\n",
       " -0.0073\n",
       "  0.0055\n",
       "  0.0165\n",
       " -0.1474\n",
       " -0.0602\n",
       " -0.1448\n",
       " -0.0376\n",
       " -0.1335\n",
       "  0.0489\n",
       "  0.0656\n",
       " -0.0979\n",
       " -0.0828\n",
       "  0.0091\n",
       "  0.0636\n",
       " -0.0839\n",
       " -0.0275\n",
       " -0.0621\n",
       " -0.0414\n",
       " -0.0443\n",
       "  0.0195\n",
       " -0.1174\n",
       "  0.0381\n",
       "  0.0021\n",
       " -0.0018\n",
       " -0.0434\n",
       " -0.1103\n",
       "  0.0692\n",
       " -0.1446\n",
       " -0.0831\n",
       "  0.1109\n",
       " -0.0353\n",
       "  0.0776\n",
       "  0.0713\n",
       " -0.0623\n",
       "  0.0196\n",
       "  0.1590\n",
       "  0.0209\n",
       "  0.0634\n",
       " -0.0405\n",
       " -0.1610\n",
       "  0.1350\n",
       "  0.0525\n",
       " -0.0346\n",
       "  0.0428\n",
       "  0.0236\n",
       " -0.0145\n",
       "  0.0785\n",
       "  0.0078\n",
       "  0.0929\n",
       " -0.0257\n",
       " -0.0631\n",
       "  0.0937\n",
       "  0.0372\n",
       " -0.0544\n",
       " -0.0617\n",
       " -0.0539\n",
       " -0.0355\n",
       " -0.0497\n",
       "  0.1668\n",
       "  0.0577\n",
       "  0.0678\n",
       " -0.0559\n",
       " -0.1556\n",
       " -0.1606\n",
       "  0.1603\n",
       " -0.0151\n",
       " -0.1212\n",
       " -0.1185\n",
       "  0.0488\n",
       "  0.0714\n",
       "  0.0781\n",
       " -0.0182\n",
       " -0.0076\n",
       "  0.1383\n",
       " -0.1387\n",
       "  0.1130\n",
       "  0.0135\n",
       "  0.1056\n",
       "  0.0323\n",
       "  0.0838\n",
       " -0.0290\n",
       " -0.1195\n",
       "  0.0709\n",
       " -0.0812\n",
       " -0.0203\n",
       " -0.0182\n",
       "  0.0553\n",
       " -0.1357\n",
       " -0.0863\n",
       "  0.1005\n",
       "  0.0065\n",
       "  0.0242\n",
       " -0.1291\n",
       " -0.0546\n",
       "  0.0141\n",
       "  0.0555\n",
       " -0.0128\n",
       " -0.1245\n",
       " -0.0533\n",
       "  0.0975\n",
       " -0.1636\n",
       "  0.0667\n",
       " -0.0445\n",
       " -0.0436\n",
       " -0.0873\n",
       "  0.0160\n",
       " -0.0003\n",
       " -0.0266\n",
       " -0.1167\n",
       "  0.0037\n",
       "  0.0121\n",
       "  0.0097\n",
       "  0.1495\n",
       "  0.0525\n",
       " -0.0101\n",
       " -0.0050\n",
       "  0.0707\n",
       " -0.1544\n",
       " -0.0341\n",
       " -0.0738\n",
       " -0.0539\n",
       " [torch.cuda.FloatTensor of size 450 (GPU 0)], Parameter containing:\n",
       "  0.0515\n",
       "  0.0244\n",
       "  0.0451\n",
       "  0.0000\n",
       "  0.0225\n",
       "  0.0482\n",
       " -0.1481\n",
       " -0.2438\n",
       "  0.0916\n",
       " -0.0684\n",
       "  0.0341\n",
       "  0.0407\n",
       " -0.1745\n",
       " -0.0616\n",
       " -0.1016\n",
       " -0.0238\n",
       " -0.0532\n",
       " -0.0843\n",
       "  0.0530\n",
       "  0.0550\n",
       "  0.0505\n",
       "  0.0075\n",
       " -0.1423\n",
       " -0.0244\n",
       " -0.1448\n",
       " -0.0988\n",
       "  0.0479\n",
       "  0.0270\n",
       "  0.0010\n",
       " -0.0590\n",
       " -0.1714\n",
       "  0.1171\n",
       " -0.0766\n",
       " -0.0228\n",
       " -0.0902\n",
       " -0.0081\n",
       " -0.1148\n",
       "  0.0291\n",
       " -0.0427\n",
       " -0.1169\n",
       " -0.1698\n",
       " -0.1591\n",
       " -0.0393\n",
       " -0.0945\n",
       " -0.2239\n",
       " -0.0092\n",
       " -0.0044\n",
       " -0.0508\n",
       " -0.0646\n",
       " -0.2625\n",
       "  0.1316\n",
       " -0.1652\n",
       "  0.0250\n",
       " -0.0774\n",
       " -0.0769\n",
       " -0.1919\n",
       "  0.0204\n",
       " -0.0845\n",
       " -0.1796\n",
       " -0.1621\n",
       " -0.0625\n",
       " -0.1331\n",
       " -0.1131\n",
       "  0.0309\n",
       " -0.0815\n",
       " -0.0476\n",
       " -0.0810\n",
       " -0.0237\n",
       " -0.0459\n",
       " -0.1220\n",
       "  0.1283\n",
       " -0.0559\n",
       " -0.1321\n",
       " -0.0978\n",
       " -0.1429\n",
       " -0.0006\n",
       " -0.1002\n",
       " -0.1394\n",
       " -0.0724\n",
       " -0.0959\n",
       "  0.1193\n",
       "  0.1057\n",
       "  0.0984\n",
       " -0.1491\n",
       "  0.0267\n",
       "  0.0136\n",
       "  0.0316\n",
       " -0.0852\n",
       " -0.1170\n",
       " -0.2027\n",
       "  0.0445\n",
       " -0.1188\n",
       " -0.1130\n",
       "  0.0019\n",
       " -0.0568\n",
       "  0.0094\n",
       "  0.0099\n",
       "  0.0244\n",
       " -0.0060\n",
       " -0.1765\n",
       " -0.0287\n",
       " -0.0801\n",
       " -0.0554\n",
       " -0.0345\n",
       " -0.1951\n",
       "  0.0582\n",
       " -0.0279\n",
       "  0.0191\n",
       "  0.0187\n",
       " -0.0186\n",
       " -0.0322\n",
       " -0.0286\n",
       "  0.0370\n",
       " -0.0725\n",
       " -0.0036\n",
       "  0.0459\n",
       " -0.1205\n",
       "  0.0034\n",
       " -0.0819\n",
       " -0.1192\n",
       " -0.1292\n",
       " -0.0517\n",
       " -0.1013\n",
       "  0.0624\n",
       "  0.1423\n",
       " -0.0746\n",
       " -0.0624\n",
       " -0.0101\n",
       "  0.0153\n",
       " -0.0421\n",
       "  0.0258\n",
       "  0.0693\n",
       " -0.1593\n",
       "  0.0600\n",
       "  0.0168\n",
       " -0.0927\n",
       "  0.0213\n",
       " -0.1251\n",
       "  0.0328\n",
       " -0.2107\n",
       " -0.0901\n",
       " -0.0762\n",
       "  0.0385\n",
       " -0.1310\n",
       "  0.0143\n",
       " -0.1373\n",
       " -0.1546\n",
       " -0.0529\n",
       " -0.0139\n",
       " -0.1121\n",
       " -0.0322\n",
       " -0.0480\n",
       " -0.0996\n",
       "  0.0372\n",
       " -0.0333\n",
       " -0.0926\n",
       "  0.0019\n",
       "  0.0620\n",
       " -0.1387\n",
       " -0.0968\n",
       " -0.0042\n",
       " -0.0364\n",
       "  0.0465\n",
       " -0.0663\n",
       "  0.0212\n",
       "  0.0162\n",
       "  0.0520\n",
       "  0.0683\n",
       "  0.1814\n",
       " -0.0218\n",
       "  0.0019\n",
       "  0.0916\n",
       " -0.0184\n",
       " -0.0218\n",
       " -0.0706\n",
       " -0.0571\n",
       "  0.0118\n",
       "  0.0196\n",
       " -0.1152\n",
       "  0.0081\n",
       " -0.0768\n",
       " -0.1216\n",
       " -0.0231\n",
       "  0.0039\n",
       "  0.0038\n",
       " -0.0211\n",
       " -0.0822\n",
       "  0.0351\n",
       " -0.0388\n",
       "  0.0744\n",
       " -0.0804\n",
       " -0.0250\n",
       "  0.0293\n",
       " -0.0604\n",
       "  0.1036\n",
       " -0.1424\n",
       "  0.1027\n",
       " -0.0279\n",
       " -0.1007\n",
       " -0.0757\n",
       " -0.1533\n",
       "  0.0233\n",
       " -0.0790\n",
       "  0.1060\n",
       " -0.0157\n",
       " -0.0588\n",
       " -0.1251\n",
       "  0.0287\n",
       "  0.1776\n",
       " -0.0133\n",
       " -0.0296\n",
       " -0.0644\n",
       "  0.0885\n",
       "  0.0353\n",
       " -0.1237\n",
       "  0.0147\n",
       " -0.0698\n",
       " -0.1619\n",
       "  0.0092\n",
       " -0.0030\n",
       " -0.0128\n",
       "  0.1110\n",
       " -0.0425\n",
       "  0.0092\n",
       " -0.0684\n",
       " -0.0199\n",
       " -0.0283\n",
       "  0.0226\n",
       " -0.0314\n",
       "  0.0526\n",
       " -0.0167\n",
       " -0.1626\n",
       " -0.0022\n",
       "  0.1741\n",
       "  0.1296\n",
       "  0.0906\n",
       "  0.0131\n",
       "  0.0211\n",
       "  0.0570\n",
       "  0.0232\n",
       "  0.0360\n",
       "  0.0490\n",
       "  0.0383\n",
       " -0.0628\n",
       " -0.0470\n",
       "  0.0862\n",
       "  0.0086\n",
       " -0.0226\n",
       "  0.0136\n",
       " -0.1614\n",
       " -0.0154\n",
       " -0.0148\n",
       "  0.0115\n",
       " -0.1206\n",
       "  0.0017\n",
       "  0.0591\n",
       " -0.0133\n",
       " -0.0778\n",
       "  0.0937\n",
       " -0.0580\n",
       "  0.0774\n",
       " -0.0651\n",
       " -0.1199\n",
       "  0.0147\n",
       "  0.0318\n",
       "  0.1487\n",
       " -0.1118\n",
       "  0.0479\n",
       "  0.0532\n",
       " -0.0513\n",
       "  0.1764\n",
       " -0.0845\n",
       " -0.0149\n",
       "  0.0372\n",
       " -0.1202\n",
       " -0.0494\n",
       "  0.0779\n",
       "  0.0103\n",
       " -0.0109\n",
       " -0.0227\n",
       "  0.0626\n",
       " -0.0188\n",
       "  0.0286\n",
       " -0.0889\n",
       "  0.0179\n",
       " -0.1265\n",
       " -0.0546\n",
       "  0.0091\n",
       " -0.0983\n",
       "  0.0485\n",
       " -0.0638\n",
       "  0.0141\n",
       "  0.0283\n",
       "  0.0254\n",
       " -0.0625\n",
       "  0.0083\n",
       "  0.0858\n",
       "  0.1469\n",
       "  0.0312\n",
       " -0.0021\n",
       "  0.0194\n",
       "  0.0218\n",
       " -0.0672\n",
       " -0.1123\n",
       " -0.1438\n",
       "  0.0189\n",
       "  0.1098\n",
       " -0.0672\n",
       "  0.0101\n",
       "  0.0679\n",
       "  0.0485\n",
       "  0.0198\n",
       "  0.0259\n",
       " -0.1484\n",
       "  0.0550\n",
       "  0.0169\n",
       " -0.0639\n",
       " -0.0896\n",
       "  0.0589\n",
       "  0.0727\n",
       " -0.0357\n",
       " -0.0217\n",
       "  0.0336\n",
       "  0.0310\n",
       " -0.1146\n",
       " -0.0773\n",
       " -0.0474\n",
       " -0.0386\n",
       "  0.1265\n",
       " -0.0034\n",
       "  0.0797\n",
       " -0.0425\n",
       " -0.0166\n",
       " -0.0053\n",
       " -0.1440\n",
       " -0.0702\n",
       " -0.0208\n",
       " -0.1441\n",
       "  0.0491\n",
       " -0.1362\n",
       " -0.0835\n",
       "  0.0301\n",
       "  0.1288\n",
       " -0.0202\n",
       "  0.0204\n",
       " -0.0740\n",
       "  0.0772\n",
       " -0.0726\n",
       "  0.0205\n",
       " -0.0814\n",
       "  0.0034\n",
       " -0.0670\n",
       "  0.0518\n",
       " -0.0852\n",
       " -0.1595\n",
       " -0.0192\n",
       " -0.0319\n",
       "  0.0732\n",
       "  0.0570\n",
       " -0.1489\n",
       "  0.1099\n",
       "  0.0934\n",
       " -0.0778\n",
       " -0.0572\n",
       "  0.0643\n",
       "  0.0147\n",
       "  0.0432\n",
       "  0.0128\n",
       " -0.1041\n",
       "  0.0430\n",
       " -0.0493\n",
       "  0.0196\n",
       "  0.1389\n",
       "  0.1640\n",
       " -0.0526\n",
       "  0.1182\n",
       "  0.0347\n",
       " -0.0306\n",
       " -0.0620\n",
       " -0.0358\n",
       "  0.1682\n",
       "  0.0559\n",
       "  0.1022\n",
       " -0.0979\n",
       "  0.0379\n",
       "  0.1213\n",
       "  0.0709\n",
       "  0.1163\n",
       "  0.0223\n",
       "  0.0546\n",
       "  0.0103\n",
       " -0.0320\n",
       " -0.0437\n",
       "  0.0849\n",
       " -0.1453\n",
       " -0.1083\n",
       " -0.0059\n",
       " -0.0224\n",
       " -0.0458\n",
       "  0.1133\n",
       "  0.0419\n",
       " -0.0892\n",
       "  0.1437\n",
       " -0.0796\n",
       "  0.0643\n",
       "  0.0803\n",
       "  0.1660\n",
       "  0.0742\n",
       "  0.0892\n",
       " -0.1329\n",
       " -0.0863\n",
       "  0.0133\n",
       " -0.1248\n",
       " -0.1312\n",
       "  0.0477\n",
       "  0.0214\n",
       " -0.0973\n",
       "  0.0343\n",
       "  0.1238\n",
       "  0.1084\n",
       " -0.0795\n",
       " -0.1463\n",
       " -0.0989\n",
       " -0.0243\n",
       " -0.0501\n",
       "  0.0851\n",
       " -0.0423\n",
       " -0.1013\n",
       " -0.0863\n",
       " -0.1464\n",
       "  0.0266\n",
       "  0.0189\n",
       " -0.0256\n",
       " -0.1655\n",
       "  0.0814\n",
       " -0.0790\n",
       " -0.1600\n",
       " -0.0774\n",
       " -0.0049\n",
       "  0.0892\n",
       "  0.0023\n",
       "  0.0183\n",
       "  0.0225\n",
       "  0.1217\n",
       " -0.0665\n",
       "  0.1814\n",
       " -0.0850\n",
       " -0.0139\n",
       " -0.0342\n",
       " -0.0262\n",
       " [torch.cuda.FloatTensor of size 450 (GPU 0)], Parameter containing:\n",
       " \n",
       " Columns 0 to 9 \n",
       " -0.1256  0.4763 -0.1130 -0.2304 -0.0621  0.0539  0.0235 -0.0489  0.2415  0.3796\n",
       " \n",
       " Columns 10 to 19 \n",
       "  0.0603 -0.0847 -0.2311 -0.6178 -0.1022 -0.2429  0.1202 -0.0354  0.1351 -0.2639\n",
       " \n",
       " Columns 20 to 29 \n",
       " -0.0585 -0.1699 -0.0692  0.3764  0.1808 -0.1415 -0.0145 -0.6275  0.0769  0.2445\n",
       " \n",
       " Columns 30 to 39 \n",
       "  0.2247 -0.0521 -0.1217  0.0133 -0.1652  0.0477  0.1183 -0.0012  0.2359  0.0969\n",
       " \n",
       " Columns 40 to 49 \n",
       "  0.0015  0.3978 -0.2209  0.1679 -0.2148  0.3474  0.2152 -0.4437  0.0191  0.1328\n",
       " \n",
       " Columns 50 to 59 \n",
       "  0.1516  0.1400  0.1477  0.2420 -0.1466  0.1970 -0.3214  0.0210 -0.2697  0.7996\n",
       " \n",
       " Columns 60 to 69 \n",
       "  0.0064  0.0962  0.1527 -0.0342  0.2452 -0.3666 -0.2217  0.2341  0.0515 -0.0931\n",
       " \n",
       " Columns 70 to 79 \n",
       "  0.2538 -0.1757 -0.2167 -0.2427 -0.2760 -0.1233  0.1446 -0.0875 -0.2410  0.1761\n",
       " \n",
       " Columns 80 to 89 \n",
       "  0.0757  0.4434  0.1764  0.0775 -0.1069 -0.0113  0.5310 -0.0445 -0.0422 -0.0355\n",
       " \n",
       " Columns 90 to 99 \n",
       " -0.2237 -0.2798 -0.0905  0.3928 -0.1140  0.2959  0.1295  0.0115  0.1656 -0.2632\n",
       " \n",
       " Columns 100 to 109 \n",
       "  0.1289 -0.6851  0.1524 -0.1433 -0.2347  0.1211  0.1009  0.1501 -0.1797  0.0981\n",
       " \n",
       " Columns 110 to 119 \n",
       "  0.1436 -0.2666  0.0726  0.7582  0.1777 -0.0743 -0.0056 -0.1391  0.3674 -0.0680\n",
       " \n",
       " Columns 120 to 129 \n",
       " -0.3268 -0.0558  0.3126  0.1812 -0.1569  0.2469  0.0442  0.0276 -0.0393 -0.0186\n",
       " \n",
       " Columns 130 to 139 \n",
       " -0.0193 -0.0611  0.1977 -0.3307 -0.2188 -0.8176 -0.2823  0.0677 -0.0226 -0.2270\n",
       " \n",
       " Columns 140 to 149 \n",
       " -0.1383 -0.1921 -0.0648 -0.1815  0.1037  0.0242 -0.1847  0.1503  0.1482 -0.1745\n",
       " \n",
       " Columns 150 to 159 \n",
       " -0.0027 -0.1066 -0.1561  0.2026 -0.0890 -0.2724  0.0142 -0.0114 -0.2669 -0.0015\n",
       " \n",
       " Columns 160 to 169 \n",
       " -0.0088  0.0340 -0.0782  0.0745 -0.0715 -0.0881 -0.0085  0.3077  0.0030 -0.3857\n",
       " \n",
       " Columns 170 to 179 \n",
       " -0.1577  0.1806 -0.1160 -0.0023  0.2648  0.0515  0.0337 -0.0310  0.0452 -0.0838\n",
       " \n",
       " Columns 180 to 189 \n",
       " -0.1171 -0.2469  0.0921 -0.1196 -0.0896 -0.1197  0.0207  0.0514  0.0675 -0.0295\n",
       " \n",
       " Columns 190 to 199 \n",
       "  0.0268 -0.0472  0.0646  0.1447  0.0327 -0.1590  0.0565  0.2928 -0.0719 -0.0381\n",
       " \n",
       " Columns 200 to 209 \n",
       "  0.2454 -0.0213  0.0659  0.0957  0.0304  0.1239  0.0400  0.1246 -0.0277  0.0593\n",
       " \n",
       " Columns 210 to 219 \n",
       "  0.0888 -0.1093 -0.0410 -0.0061 -0.0334 -0.1023 -0.0524 -0.2287 -0.0389 -0.0176\n",
       " \n",
       " Columns 220 to 229 \n",
       " -0.2095  0.1765  0.0584 -0.0814  0.0619 -0.0791  0.0425  0.0272  0.1375 -0.1347\n",
       " \n",
       " Columns 230 to 239 \n",
       " -0.1170 -0.1460 -0.0002  0.1116 -0.1301 -0.0964  0.0286 -0.1961 -0.0784  0.1113\n",
       " \n",
       " Columns 240 to 249 \n",
       "  0.1064  0.0309 -0.0056 -0.1525  0.0731  0.0307  0.2099  0.1754  0.1253 -0.1794\n",
       " \n",
       " Columns 250 to 259 \n",
       "  0.0875  0.1595 -0.0865 -0.0835  0.0503  0.0687 -0.0615  0.2226  0.0290 -0.0722\n",
       " \n",
       " Columns 260 to 269 \n",
       "  0.0856 -0.0531  0.1593 -0.1590  0.1007 -0.1111  0.0117 -0.1096 -0.0218  0.0194\n",
       " \n",
       " Columns 270 to 279 \n",
       " -0.1122  0.0388  0.0844  0.2759 -0.0699  0.3519  0.0247 -0.1667  0.1599  0.1180\n",
       " \n",
       " Columns 280 to 289 \n",
       "  0.0371  0.2712  0.1720  0.1426 -0.4757  0.0984  0.1178  0.1306  0.1588  0.0361\n",
       " \n",
       " Columns 290 to 299 \n",
       " -0.0877 -0.0711  0.1480 -0.0444 -0.2020  0.0347  0.0769  0.0685 -0.1731  0.1031\n",
       " [torch.cuda.FloatTensor of size 1x300 (GPU 0)], Parameter containing:\n",
       " 1.00000e-02 *\n",
       "  -7.5030\n",
       " [torch.cuda.FloatTensor of size 1 (GPU 0)]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printQuestion(batch_num, example_num):\n",
    "    for i in batch_input[batch_num][\"question_input_tokens\"][example_num]:\n",
    "        print(look_up_token_reduced(i), end = ' ', sep = ' ')\n",
    "    print(\"\")\n",
    "        \n",
    "def printAnswer(batch_num, example_num):\n",
    "    for i in np.where(batch_input[batch_num][\"answer_labels\"][example_num]==1)[0]:\n",
    "        print(look_up_token_reduced(batch_input[batch_num][\"document_tokens\"][example_num][i]))\n",
    "    print(\"\")\n",
    "        \n",
    "def printContext(batch_num, example_num):\n",
    "    for i in batch_input[batch_num][\"document_tokens\"][example_num]:\n",
    "        print(look_up_token_reduced(i), end = ' ', sep = ' ')\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = 3\n",
    "example_num=3\n",
    "print(\"Context : \")\n",
    "printContext(batch_num, example_num)\n",
    "print(\"Question : \")\n",
    "printQuestion(batch_num, example_num)\n",
    "print(\"Answer : \")\n",
    "printAnswer(batch_num, example_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_token_embedding.shape, question_decoder_hidden_new.shape, final_output.shape, decoder_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "what\n",
      "territories\n",
      "were\n",
      "<UNK>\n",
      "by\n",
      "<UNK>\n",
      "in\n",
      "<UNK>\n",
      "?\n",
      "<END>\n",
      "<END>\n",
      "<END>\n",
      "<END>\n",
      "<END>\n",
      "<END>\n",
      "<END>\n",
      "<END>\n",
      "<END>\n",
      "<END>\n",
      "<END>\n",
      "<END>\n"
     ]
    }
   ],
   "source": [
    "question_token = START_TOKEN\n",
    "question_token_embedding = embedder(Variable(torch.from_numpy(np.array([question_token])).long())).cuda()\n",
    "question_decoder_hidden_new =  question_encoder_2_hidden.clone()\n",
    "# question_decoder_hidden_new =   questionEncoder2.initHidden()\n",
    "qLen = 0\n",
    "b = []\n",
    "while qLen <= 20:\n",
    "    decoder_output, question_decoder_hidden_new = questionDecoder(\n",
    "        question_token_embedding.expand(32,1,300),\n",
    "        question_decoder_hidden_new)\n",
    "    final_output = questionGenerator(decoder_output)\n",
    "    question_token = np.argmax(final_output.data, axis=2)[1][0]\n",
    "#     w = [np.argmax(final_output.data, axis=2)[i][0] for i in range(batch_size)]\n",
    "    #b.append([look_up_token_reduced(x) for x in w])\n",
    "    print(look_up_token_reduced(question_token))\n",
    "    question_token_embedding = embedder(Variable(torch.from_numpy(np.array([question_token])).long())).cuda()\n",
    "    qLen=qLen + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 2000])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "    2\n",
       "[torch.LongTensor of size 32x1]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(final_output.data, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2000])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(final_output.data, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 44\n",
       "[torch.LongTensor of size 1]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(final_output.data, axis = 2)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareSetForInferenceFromBatch(batch_number):\n",
    "    data_dict = batch_input[batch_number]\n",
    "    data_dict['answer_mask_1'] = data_dict['answer_labels']\n",
    "    data_dict['answer_mask_2'] = 1 - data_dict['answer_mask_1']\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepareSetForInference(context, answer, maxDocLength):\n",
    "    \n",
    "    passage = word_tokenize(context.lower())\n",
    "    doc_len = len(passage)\n",
    "    \n",
    "    ans_condition = word_tokenize(answer.lower())\n",
    "    \n",
    "    ans_len = len(ans_condition)\n",
    "        \n",
    "    start,end = find_sub_list(ans_condition,passage)\n",
    "\n",
    "    if start == -1:\n",
    "            print(\"Couldn' Find answer in text, Please try again !!\")\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    document_token = np.full((1, maxDocLength), END_TOKEN, dtype=np.int32)\n",
    "    document_length = np.zeros(1, dtype=np.int32)\n",
    "    answer_mask_1 = np.zeros((1,maxDocLength), dtype=np.int32)\n",
    "    answer_mask_2 = np.zeros((1,maxDocLength), dtype=np.int32)\n",
    "    answer_length = np.zeros(1, dtype=np.int32)\n",
    "    \n",
    "    document_length[0] = doc_len\n",
    "    answer_length[0] = ans_len\n",
    "    answer_mask_1[0,start:end] = 1\n",
    "    answer_mask_2 = 1 - answer_mask_1\n",
    "    \n",
    "    \n",
    "    for i in range(doc_len):\n",
    "        document_token[:,i] = look_up_word_reduced(passage[i])\n",
    "    \n",
    "    document_token = document_token.repeat(batch_size, axis = 0)\n",
    "    document_length = document_length.repeat(batch_size, axis = 0)\n",
    "    answer_mask_1 = answer_mask_1.repeat(batch_size, axis = 0)\n",
    "    answer_mask_2 = answer_mask_2.repeat(batch_size, axis = 0)\n",
    "    answer_length = answer_length.repeat(batch_size, axis = 0)\n",
    "    \n",
    "    \n",
    "    return {\n",
    "        \"document_tokens\" : document_token,\n",
    "        \"document_length\" : document_length,\n",
    "        \"answer_mask_1\" : answer_mask_1,\n",
    "        \"answer_mask_2\" : answer_mask_2,\n",
    "        \"answer_length\" : answer_length,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference2(context, answer, maxDocLen):\n",
    "\n",
    "    #data_dict = prepareSetForInference(context, answer, maxDocLen)\n",
    "    data_dict = prepareSetForInferenceFromBatch(2)\n",
    "    use_attention = False\n",
    "    use_cuda = True\n",
    "\n",
    "    answer_encoder_hidden_inf = answerEncoder2.hiddenState\n",
    "    question_encoder_hidden_1_inf = questionEncoder2_1.hiddenState\n",
    "    question_encoder_hidden_2_inf = questionEncoder2_2.hiddenState\n",
    "    question_decoder_hidden_inf = questionDecoder2.hiddenState\n",
    "\n",
    "    if use_attention:\n",
    "        attention_hidden_inf = attention.hidden_state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    inp = Variable(torch.from_numpy(data_dict[\"document_tokens\"]).long())\n",
    "    labels = Variable(torch.from_numpy(data_dict[\"answer_mask_1\"])).long() #Let's see if we want to use or not\n",
    "    if use_cuda:\n",
    "        labels = labels.cuda()\n",
    "\n",
    "    embedded_inp = embedder(inp)\n",
    "\n",
    "    if use_cuda:\n",
    "        embedded_inp = embedded_inp.cuda()\n",
    "\n",
    "\n",
    "    answer_encoder_hidden_inf = repackage_hidden(answer_encoder_hidden_inf)\n",
    "    answer_tags, answer_outputs, answer_encoder_hidden_inf = answerEncoder(embedded_inp, answer_encoder_hidden_inf)\n",
    "\n",
    "\n",
    "\n",
    "    if use_cuda:\n",
    "        answer_outputs = answer_outputs.cuda()\n",
    "        answer_tags = answer_tags.cuda() \n",
    "\n",
    "    ####USE ANSWER_TAGS T CONDITION QUESTIONS RHATHRE THAN PASSING ANSWERS\n",
    "\n",
    "    answer_mask_1 = Variable(torch.from_numpy(data_dict[\"answer_mask_1\"])).float().unsqueeze(-1)\n",
    "    answer_mask_2 = Variable(torch.from_numpy(data_dict[\"answer_mask_2\"])).float().unsqueeze(-1)\n",
    "\n",
    "    if use_cuda:\n",
    "        answer_mask_1 = answer_mask_1.cuda()\n",
    "        answer_mask_2 = answer_mask_2.cuda()\n",
    "\n",
    "    question_encoder_input1 = torch.mul(answer_mask_1, answer_outputs.float())\n",
    "    question_encoder_input2 = torch.mul(answer_mask_2, answer_outputs.float())\n",
    "\n",
    "    if use_cuda:\n",
    "        question_encoder_input1 = question_encoder_input1.cuda()\n",
    "        question_encoder_input2 = question_encoder_input2.cuda()\n",
    "\n",
    "    question_encoder_1_outputs , question_encoder_1_hidden = questionEncoder1(question_encoder_input1, question_encoder_hidden_1_inf)\n",
    "    question_encoder_2_outputs , question_encoder_2_hidden = questionEncoder2(question_encoder_input2, question_encoder_hidden_2_inf)\n",
    "\n",
    "\n",
    "\n",
    "    maxGenQuesLen = 20\n",
    "    currGenQuestionLen = 0\n",
    "\n",
    "    current_question_token = START_TOKEN\n",
    "    questionGenerated = []\n",
    "    while currGenQuestionLen < maxGenQuesLen:\n",
    "        question_token_embedding = embedder(Variable(torch.from_numpy(np.array([current_question_token])).long()))\n",
    "        if use_cuda:\n",
    "            question_token_embedding = question_token_embedding.cuda()\n",
    "\n",
    "        if use_attention:\n",
    "            attn_output, Attention_Weights = attention(question_decoder_hidden.squeeze(0).squeeze(0), attention_hidden.squeeze(0), answer_outputs[i])\n",
    "            decoder_output, attention_hidden = questionDecoder(\n",
    "                embedded_inputs[quesL:quesL+1].unsqueeze(1), attn_output)\n",
    "        else:\n",
    "            decoder_output, question_decoder_hidden_inf = questionDecoder2(\n",
    "                question_token_embedding.expand(32,1,300),\n",
    "                question_decoder_hidden_inf)\n",
    "\n",
    "        final_output = questionGenerator2(decoder_output.squeeze(1))\n",
    "        current_question_token = np.argmax(final_output.data, axis = 1)[0]\n",
    "        questionGenerated.append(look_up_token_reduced(current_question_token))\n",
    "        currGenQuestionLen += 1\n",
    "\n",
    "    return questionGenerated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "qGenerated = inference2(\"Some topic is being written here\", \"some topic\", max_document_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "\n",
       "Columns 0 to 12 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 13 to 25 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 26 to 38 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 39 to 51 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 52 to 64 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 65 to 77 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 78 to 90 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 91 to 103 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 104 to 116 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 117 to 129 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 130 to 142 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 143 to 155 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 156 to 168 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 169 to 181 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 182 to 194 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 195 to 207 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 208 to 220 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 221 to 233 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 234 to 246 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 247 to 259 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 260 to 272 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 273 to 285 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 286 to 298 \n",
       "    1     1     1     1     1     1     1     1     1     1     1     1     1\n",
       "\n",
       "Columns 299 to 299 \n",
       "    1\n",
       "[torch.cuda.ByteTensor of size 1x300 (GPU 0)]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_num = 3\n",
    "example_num = 0\n",
    "printQuestion(batch_num, example_num)\n",
    "printAnswer(batch_num, example_num)\n",
    "printContext(batch_num, example_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(X_train_comp_all_shuffled[3*32 + 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(X_train_ans_all_shuffled[3*32 + 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "' '.join(Y_train_ques_all_shuffled[3*32 + 20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference2(' '.join(X_train_comp_all_shuffled[3*32 + 20]), ' '.join(X_train_ans_all_shuffled[3*32 + 20]), max_document_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input[0]['question_input_tokens'][0], batch_input[0]['question_output_tokens'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
